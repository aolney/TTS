{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "# South Park TTS Training\n",
    "\n",
    "Adapted [this gist](https://gist.github.com/erogol/97516ad65b44dbddb8cd694953187c5b) for [SoS notebook](https://vatlab.github.io/sos-docs/).\n",
    "\n",
    "Assumes you have already [cloned TTS](https://github.com/mozilla/TTS) and done `apt install espeak`\n",
    "\n",
    "## Prepare Training/Testing Data\n",
    "\n",
    "Assumes you have already [prepared suitable data](https://github.com/aolney/SouthParkTTSData).\n",
    "The next step simply does a train/test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "kernel": "Bash"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1523 metadata.csv\n"
     ]
    }
   ],
   "source": [
    "#exit #comment to allow execution\n",
    "cd /y/south-park-1-to-20\n",
    "echo `wc -l metadata.csv`\n",
    "shuf metadata.csv > metadata_shuf.csv\n",
    "head -n 1400 metadata_shuf.csv > metadata_train.csv\n",
    "tail -n 123 metadata_shuf.csv > metadata_val.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "Bash"
   },
   "source": [
    "## Create config file for this training data\n",
    "\n",
    "Some of my local paths are hardcoded below. Parameters determined using [wiki documentation](https://github.com/mozilla/TTS/wiki/Dataset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "kernel": "Bash"
   },
   "outputs": [],
   "source": [
    "#exit #comment to allow execution\n",
    "cd /vm/TTS\n",
    "cp config.json \"config.json.$(date +%Y%m%d)\"\n",
    "cat > config.json << EOM\n",
    "{\n",
    "    \"run_name\": \"mozilla-no-loc-fattn-stopnet-sigmoid-loss_masking\",\n",
    "    \"run_description\": \"using forward attention, with original prenet, loss masking,separate stopnet, sigmoid. Compare this with 4817. Pytorch DPP\",\n",
    "\n",
    "    \"audio\":{\n",
    "        // Audio processing parameters\n",
    "        \"num_mels\": 80,         // size of the mel spec frame. \n",
    "        \"num_freq\": 1025,       // number of stft frequency levels. Size of the linear spectogram frame.\n",
    "        \"sample_rate\": 16000,   // wav sample-rate. If different than the original data, it is resampled.\n",
    "        \"frame_length_ms\": 50,  // stft window length in ms.\n",
    "        \"frame_shift_ms\": 12.5, // stft window hop-lengh in ms.\n",
    "        \"preemphasis\": 0.99,    // pre-emphasis to reduce spec noise and make it more structured. If 0.0, no -pre-emphasis.\n",
    "        \"min_level_db\": -100,   // normalization range\n",
    "        \"ref_level_db\": 40,     // reference level db, theoretically 20db is the sound of air.\n",
    "        \"power\": 1.1,           // value to sharpen wav signals after GL algorithm.\n",
    "        \"griffin_lim_iters\": 30,// #griffin-lim iterations. 30-60 is a good range. Larger the value, slower the generation.\n",
    "        // Normalization parameters\n",
    "        \"signal_norm\": true,    // normalize the spec values in range [0, 1]\n",
    "        \"symmetric_norm\": false, // move normalization to range [-1, 1]\n",
    "        \"max_norm\": 1,          // scale normalization to range [-max_norm, max_norm] or [0, max_norm]\n",
    "        \"clip_norm\": true,      // clip normalized values into the range.\n",
    "        \"mel_fmin\": 0.0,         // minimum freq level for mel-spec. ~50 for male and ~95 for female voices. Tune for dataset!!\n",
    "        \"mel_fmax\": 8000.0,        // maximum freq level for mel-spec. Tune for dataset!!\n",
    "        \"do_trim_silence\": false  // enable trimming of slience of audio as you load it. LJspeech (false), TWEB (false), Nancy (true)\n",
    "    },\n",
    "\n",
    "    \"distributed\":{\n",
    "        \"backend\": \"nccl\",\n",
    "        \"url\": \"tcp:\\/\\/localhost:54321\"\n",
    "    },\n",
    "\n",
    "    \"reinit_layers\": [],\n",
    "\n",
    "    \"model\": \"Tacotron2\",          // one of the model in models/    \n",
    "    \"grad_clip\": 1,                // upper limit for gradients for clipping.\n",
    "    \"epochs\": 1000,                // total number of epochs to train.\n",
    "    \"lr\": 0.0001,                  // Initial learning rate. If Noam decay is active, maximum learning rate.\n",
    "    \"lr_decay\": false,             // if true, Noam learning rate decaying is applied through training.\n",
    "    \"warmup_steps\": 4000,          // Noam decay steps to increase the learning rate from 0 to \"lr\"\n",
    "    \"windowing\": false,            // Enables attention windowing. Used only in eval mode.\n",
    "    \"memory_size\": 5,              // ONLY TACOTRON - memory queue size used to queue network predictions to feed autoregressive connection. Useful if r < 5. \n",
    "    \"attention_norm\": \"sigmoid\",   // softmax or sigmoid. Suggested to use softmax for Tacotron2 and sigmoid for Tacotron.\n",
    "    \"prenet_type\": \"original\",     // ONLY TACOTRON2 - \"original\" or \"bn\".\n",
    "    \"prenet_dropout\": true,        // ONLY TACOTRON2 - enable/disable dropout at prenet. \n",
    "    \"use_forward_attn\": true,      // ONLY TACOTRON2 - if it uses forward attention. In general, it aligns faster.\n",
    "    \"transition_agent\": false,     // ONLY TACOTRON2 - enable/disable transition agent of forward attention.\n",
    "    \"location_attn\": false,        // ONLY TACOTRON2 - enable_disable location sensitive attention. It is enabled for TACOTRON by default.\n",
    "    \"loss_masking\": true,         // enable / disable loss masking against the sequence padding.\n",
    "    \"enable_eos_bos_chars\": false, // enable/disable beginning of sentence and end of sentence chars.\n",
    "    \"stopnet\": true,               // Train stopnet predicting the end of synthesis. \n",
    "    \"separate_stopnet\": true,     // Train stopnet seperately if 'stopnet==true'. It prevents stopnet loss to influence the rest of the model. It causes a better model, but it trains SLOWER.\n",
    "    \"tb_model_param_stats\": false,     // true, plots param stats per layer on tensorboard. Might be memory consuming, but good for debugging. \n",
    "    \n",
    "    \"batch_size\": 32,       // Batch size for training. Lower values than 32 might cause hard to learn attention.\n",
    "    \"eval_batch_size\":16,   \n",
    "    \"r\": 1,                 // Number of frames to predict for step.\n",
    "    \"wd\": 0.000001,         // Weight decay weight.\n",
    "    \"checkpoint\": true,     // If true, it saves checkpoints per \"save_step\"\n",
    "    \"save_step\": 1000,      // Number of training steps expected to save traning stats and checkpoints.\n",
    "    \"print_step\": 10,       // Number of steps to log traning on console.\n",
    "    \"batch_group_size\": 0,  //Number of batches to shuffle after bucketing.\n",
    "\n",
    "    \"run_eval\": true,\n",
    "    \"test_delay_epochs\": 5,  //Until attention is aligned, testing only wastes computation time.\n",
    "    \"test_sentences_file\": null,  // set a file to load sentences to be used for testing. If it is null then we use default english sentences.\n",
    "    \"data_path\": \"/y/south-park-1-to-20/\",  // DATASET-RELATED: can overwritten from command argument\n",
    "    \"meta_file_train\": \"metadata_train.csv\",       // DATASET-RELATED: metafile for training dataloader.\n",
    "    \"meta_file_val\": \"metadata_val.csv\",    // DATASET-RELATED: metafile for evaluation dataloader.\n",
    "    \"dataset\": \"ljspeech2\",       // DATASET-RELATED: one of TTS.dataset.preprocessors depending on your target dataset. Use \"tts_cache\" for pre-computed dataset by extract_features.py\n",
    "    \"min_seq_len\": 0,       // DATASET-RELATED: minimum text length to use in training\n",
    "    \"max_seq_len\": 150,     // DATASET-RELATED: maximum text length\n",
    "    \"output_path\": \"models/cartman_models/\",      // DATASET-RELATED: output path for all training outputs.\n",
    "    \"num_loader_workers\": 4,        // number of training data loader processes. Don't set it too big. 4-8 are good values.\n",
    "    \"num_val_loader_workers\": 4,    // number of evaluation data loader processes.\n",
    "    \"phoneme_cache_path\": \"cartman_phonemes\",  // phoneme computation is slow, therefore, it caches results in the given folder.\n",
    "    \"use_phonemes\": true,           // use phonemes instead of raw characters. It is suggested for better pronounciation.\n",
    "    \"phoneme_language\": \"en-us\",     // depending on your target language, pick one from  https://github.com/bootphon/phonemizer#languages\n",
    "    \"text_cleaner\": \"phoneme_cleaners\"\n",
    "}\n",
    "EOM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "Bash"
   },
   "source": [
    "## Start Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "Bash"
   },
   "outputs": [],
   "source": [
    "cd /vm/TTS\n",
    "#remove phonemes (useful if we have changed the dataset)\n",
    "rm -rf cartman_phonemes\n",
    "python train.py --config_path config.json \n",
    "#2> error\n",
    "#start tensorboard in another terminal, e.g. tensorboard --logdir=/vm/TTS/models/cartman_models\n",
    "#tensorboard --logdir=my_run:<output_path in config.json> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "Bash"
   },
   "source": [
    "## Continue Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "Bash"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > Using CUDA:  True\n",
      " > Number of GPUs:  2\n",
      " > Forcing use of single GPU, id:  GeForce GTX 1080 Ti\n",
      " > Git Hash: 3b3c248\n",
      " > Experiment folder: /vm/TTS/models/cartman_models/mozilla-no-loc-fattn-stopnet-sigmoid-loss_masking-July-27-2019_02+25PM-3b3c248\n",
      " > Setting up Audio Processor...\n",
      " | > bits:None\n",
      " | > sample_rate:16000\n",
      " | > num_mels:80\n",
      " | > min_level_db:-100\n",
      " | > frame_shift_ms:12.5\n",
      " | > frame_length_ms:50\n",
      " | > ref_level_db:40\n",
      " | > num_freq:1025\n",
      " | > power:1.1\n",
      " | > preemphasis:0.99\n",
      " | > griffin_lim_iters:30\n",
      " | > signal_norm:True\n",
      " | > symmetric_norm:False\n",
      " | > mel_fmin:0.0\n",
      " | > mel_fmax:8000.0\n",
      " | > max_norm:1.0\n",
      " | > clip_norm:True\n",
      " | > do_trim_silence:False\n",
      " | > n_fft:2048\n",
      " | > hop_length:200\n",
      " | > win_length:800\n",
      " > Using model: Tacotron2\n",
      " | > Num output units : 1025\n",
      " > Model restored from step 45000\n",
      "\n",
      " > Model has 28179554 parameters\n",
      "\n",
      " > DataLoader initialization\n",
      " | > Data path: /y/south-park-1-to-20/\n",
      " | > Use phonemes: True\n",
      "   | > phoneme language: en-us\n",
      " | > Number of instances : 1400\n",
      " | > Max length sequence: 155\n",
      " | > Min length sequence: 2\n",
      " | > Avg length sequence: 31.577857142857145\n",
      " | > Num. instances discarded by max-min seq limits: 1\n",
      " | > Batch group size: 0.\n",
      "\n",
      " > Epoch 0/1000\n",
      "   | > Step:9/43  GlobalStep:45010  TotalLoss:0.00439  PostnetLoss:0.00209  DecoderLoss:0.00230  StopLoss:0.10378  GradNorm:0.00858  GradNormST:0.04018  AvgTextLen:13.3  AvgSpecLen:63.4  StepTime:0.49  LR:0.000100\n",
      "   | > Step:19/43  GlobalStep:45020  TotalLoss:0.00474  PostnetLoss:0.00225  DecoderLoss:0.00249  StopLoss:0.12278  GradNorm:0.00840  GradNormST:0.06242  AvgTextLen:23.7  AvgSpecLen:91.3  StepTime:0.57  LR:0.000100\n",
      "   | > Step:29/43  GlobalStep:45030  TotalLoss:0.00494  PostnetLoss:0.00232  DecoderLoss:0.00262  StopLoss:0.11294  GradNorm:0.00572  GradNormST:0.03715  AvgTextLen:34.4  AvgSpecLen:121.1  StepTime:0.97  LR:0.000100\n",
      "   | > Step:39/43  GlobalStep:45040  TotalLoss:0.00561  PostnetLoss:0.00261  DecoderLoss:0.00300  StopLoss:0.10184  GradNorm:0.00447  GradNormST:0.03795  AvgTextLen:55.5  AvgSpecLen:182.1  StepTime:1.27  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:45044  AvgTotalLoss:0.11980  AvgPostnetLoss:0.00217  AvgDecoderLoss:0.00242  AvgStopLoss:0.11521  EpochTime:35.21  AvgStepTime:0.80\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.11107   PostnetLoss: 0.00214   DecoderLoss:0.00253  StopLoss: 0.10640  \n",
      " | > Training Loss: 0.00217   Validation Loss: 0.00236\n",
      "\n",
      " > BEST MODEL (0.00236) : /vm/TTS/models/cartman_models/mozilla-no-loc-fattn-stopnet-sigmoid-loss_masking-July-27-2019_02+25PM-3b3c248/best_model.pth.tar\n",
      "\n",
      " > Epoch 1/1000\n",
      "   | > Step:5/43  GlobalStep:45050  TotalLoss:0.00344  PostnetLoss:0.00163  DecoderLoss:0.00181  StopLoss:0.12522  GradNorm:0.00569  GradNormST:0.04024  AvgTextLen:8.4  AvgSpecLen:50.9  StepTime:0.35  LR:0.000100\n",
      "   | > Step:15/43  GlobalStep:45060  TotalLoss:0.00383  PostnetLoss:0.00180  DecoderLoss:0.00202  StopLoss:0.13135  GradNorm:0.00532  GradNormST:0.05967  AvgTextLen:20.0  AvgSpecLen:82.5  StepTime:0.57  LR:0.000100\n",
      "   | > Step:25/43  GlobalStep:45070  TotalLoss:0.00500  PostnetLoss:0.00236  DecoderLoss:0.00264  StopLoss:0.10581  GradNorm:0.00523  GradNormST:0.03171  AvgTextLen:31.1  AvgSpecLen:122.8  StepTime:1.03  LR:0.000100\n",
      "   | > Step:35/43  GlobalStep:45080  TotalLoss:0.00483  PostnetLoss:0.00227  DecoderLoss:0.00257  StopLoss:0.09396  GradNorm:0.00562  GradNormST:0.03715  AvgTextLen:44.8  AvgSpecLen:153.4  StepTime:0.96  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:45088  AvgTotalLoss:0.11420  AvgPostnetLoss:0.00203  AvgDecoderLoss:0.00228  AvgStopLoss:0.10989  EpochTime:30.34  AvgStepTime:0.69\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.10524   PostnetLoss: 0.00206   DecoderLoss:0.00246  StopLoss: 0.10072  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Training Loss: 0.00203   Validation Loss: 0.00227\n",
      "\n",
      " > BEST MODEL (0.00227) : /vm/TTS/models/cartman_models/mozilla-no-loc-fattn-stopnet-sigmoid-loss_masking-July-27-2019_02+25PM-3b3c248/best_model.pth.tar\n",
      "\n",
      " > Epoch 2/1000\n",
      "   | > Step:1/43  GlobalStep:45090  TotalLoss:0.00242  PostnetLoss:0.00115  DecoderLoss:0.00126  StopLoss:0.12373  GradNorm:0.00457  GradNormST:0.04575  AvgTextLen:4.2  AvgSpecLen:32.8  StepTime:0.40  LR:0.000100\n",
      "   | > Step:11/43  GlobalStep:45100  TotalLoss:0.00363  PostnetLoss:0.00172  DecoderLoss:0.00191  StopLoss:0.12348  GradNorm:0.00518  GradNormST:0.06965  AvgTextLen:15.2  AvgSpecLen:74.1  StepTime:0.49  LR:0.000100\n",
      "   | > Step:21/43  GlobalStep:45110  TotalLoss:0.00373  PostnetLoss:0.00177  DecoderLoss:0.00196  StopLoss:0.10664  GradNorm:0.00403  GradNormST:0.03645  AvgTextLen:25.3  AvgSpecLen:103.2  StepTime:0.78  LR:0.000100\n",
      "   | > Step:31/43  GlobalStep:45120  TotalLoss:0.00419  PostnetLoss:0.00197  DecoderLoss:0.00222  StopLoss:0.09681  GradNorm:0.00442  GradNormST:0.04268  AvgTextLen:38.1  AvgSpecLen:144.3  StepTime:0.69  LR:0.000100\n",
      "   | > Step:41/43  GlobalStep:45130  TotalLoss:0.00491  PostnetLoss:0.00229  DecoderLoss:0.00262  StopLoss:0.09462  GradNorm:0.00346  GradNormST:0.02676  AvgTextLen:64.8  AvgSpecLen:232.2  StepTime:1.31  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:45132  AvgTotalLoss:0.10955  AvgPostnetLoss:0.00196  AvgDecoderLoss:0.00221  AvgStopLoss:0.10538  EpochTime:30.37  AvgStepTime:0.69\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.10077   PostnetLoss: 0.00201   DecoderLoss:0.00239  StopLoss: 0.09637  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Training Loss: 0.00196   Validation Loss: 0.00224\n",
      "\n",
      " > BEST MODEL (0.00224) : /vm/TTS/models/cartman_models/mozilla-no-loc-fattn-stopnet-sigmoid-loss_masking-July-27-2019_02+25PM-3b3c248/best_model.pth.tar\n",
      "\n",
      " > Epoch 3/1000\n",
      "   | > Step:7/43  GlobalStep:45140  TotalLoss:0.00327  PostnetLoss:0.00155  DecoderLoss:0.00171  StopLoss:0.11516  GradNorm:0.00503  GradNormST:0.06246  AvgTextLen:11.0  AvgSpecLen:57.9  StepTime:0.44  LR:0.000100\n",
      "   | > Step:17/43  GlobalStep:45150  TotalLoss:0.00432  PostnetLoss:0.00204  DecoderLoss:0.00228  StopLoss:0.09379  GradNorm:0.00459  GradNormST:0.03635  AvgTextLen:21.3  AvgSpecLen:90.3  StepTime:0.59  LR:0.000100\n",
      "   | > Step:27/43  GlobalStep:45160  TotalLoss:0.00440  PostnetLoss:0.00206  DecoderLoss:0.00234  StopLoss:0.10009  GradNorm:0.00451  GradNormST:0.04471  AvgTextLen:32.3  AvgSpecLen:115.4  StepTime:0.64  LR:0.000100\n",
      "   | > Step:37/43  GlobalStep:45170  TotalLoss:0.00481  PostnetLoss:0.00224  DecoderLoss:0.00258  StopLoss:0.10527  GradNorm:0.00373  GradNormST:0.02976  AvgTextLen:50.4  AvgSpecLen:170.5  StepTime:0.84  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:45176  AvgTotalLoss:0.10741  AvgPostnetLoss:0.00193  AvgDecoderLoss:0.00217  AvgStopLoss:0.10331  EpochTime:30.13  AvgStepTime:0.68\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.09542   PostnetLoss: 0.00199   DecoderLoss:0.00238  StopLoss: 0.09105  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Training Loss: 0.00193   Validation Loss: 0.00224\n",
      "\n",
      " > Epoch 4/1000\n",
      "   | > Step:3/43  GlobalStep:45180  TotalLoss:0.00305  PostnetLoss:0.00145  DecoderLoss:0.00160  StopLoss:0.12098  GradNorm:0.00481  GradNormST:0.04047  AvgTextLen:5.8  AvgSpecLen:37.5  StepTime:0.40  LR:0.000100\n",
      "   | > Step:13/43  GlobalStep:45190  TotalLoss:0.00359  PostnetLoss:0.00170  DecoderLoss:0.00188  StopLoss:0.09165  GradNorm:0.00418  GradNormST:0.03869  AvgTextLen:17.0  AvgSpecLen:73.2  StepTime:0.51  LR:0.000100\n",
      "   | > Step:23/43  GlobalStep:45200  TotalLoss:0.00463  PostnetLoss:0.00217  DecoderLoss:0.00246  StopLoss:0.10223  GradNorm:0.00432  GradNormST:0.03339  AvgTextLen:27.9  AvgSpecLen:104.6  StepTime:0.83  LR:0.000100\n",
      "   | > Step:33/43  GlobalStep:45210  TotalLoss:0.00423  PostnetLoss:0.00199  DecoderLoss:0.00224  StopLoss:0.09738  GradNorm:0.00329  GradNormST:0.03014  AvgTextLen:41.6  AvgSpecLen:143.8  StepTime:0.96  LR:0.000100\n",
      "   | > Step:43/43  GlobalStep:45220  TotalLoss:0.00479  PostnetLoss:0.00225  DecoderLoss:0.00254  StopLoss:0.10434  GradNorm:0.00434  GradNormST:0.05584  AvgTextLen:99.1  AvgSpecLen:311.1  StepTime:1.29  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:45220  AvgTotalLoss:0.10600  AvgPostnetLoss:0.00191  AvgDecoderLoss:0.00215  AvgStopLoss:0.10194  EpochTime:30.09  AvgStepTime:0.68\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.09500   PostnetLoss: 0.00201   DecoderLoss:0.00239  StopLoss: 0.09060  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Training Loss: 0.00191   Validation Loss: 0.00224\n",
      "\n",
      " > BEST MODEL (0.00224) : /vm/TTS/models/cartman_models/mozilla-no-loc-fattn-stopnet-sigmoid-loss_masking-July-27-2019_02+25PM-3b3c248/best_model.pth.tar\n",
      "\n",
      " > Epoch 5/1000\n",
      "   | > Step:9/43  GlobalStep:45230  TotalLoss:0.00351  PostnetLoss:0.00167  DecoderLoss:0.00184  StopLoss:0.09993  GradNorm:0.00419  GradNormST:0.02874  AvgTextLen:13.3  AvgSpecLen:63.4  StepTime:0.53  LR:0.000100\n",
      "   | > Step:19/43  GlobalStep:45240  TotalLoss:0.00401  PostnetLoss:0.00189  DecoderLoss:0.00212  StopLoss:0.11523  GradNorm:0.00397  GradNormST:0.04584  AvgTextLen:23.7  AvgSpecLen:91.3  StepTime:0.54  LR:0.000100\n",
      "   | > Step:29/43  GlobalStep:45250  TotalLoss:0.00440  PostnetLoss:0.00206  DecoderLoss:0.00234  StopLoss:0.10159  GradNorm:0.00360  GradNormST:0.02740  AvgTextLen:34.4  AvgSpecLen:121.1  StepTime:0.70  LR:0.000100\n",
      "   | > Step:39/43  GlobalStep:45260  TotalLoss:0.00517  PostnetLoss:0.00240  DecoderLoss:0.00277  StopLoss:0.09181  GradNorm:0.00403  GradNormST:0.02749  AvgTextLen:55.5  AvgSpecLen:182.1  StepTime:0.82  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:45264  AvgTotalLoss:0.10585  AvgPostnetLoss:0.00189  AvgDecoderLoss:0.00213  AvgStopLoss:0.10183  EpochTime:30.33  AvgStepTime:0.69\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.09253   PostnetLoss: 0.00201   DecoderLoss:0.00239  StopLoss: 0.08813  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Training Loss: 0.00189   Validation Loss: 0.00222\n",
      "\n",
      " > BEST MODEL (0.00222) : /vm/TTS/models/cartman_models/mozilla-no-loc-fattn-stopnet-sigmoid-loss_masking-July-27-2019_02+25PM-3b3c248/best_model.pth.tar\n",
      "\n",
      " > Epoch 6/1000\n",
      "   | > Step:5/43  GlobalStep:45270  TotalLoss:0.00305  PostnetLoss:0.00145  DecoderLoss:0.00160  StopLoss:0.12035  GradNorm:0.00409  GradNormST:0.04326  AvgTextLen:8.4  AvgSpecLen:50.9  StepTime:0.43  LR:0.000100\n",
      "   | > Step:15/43  GlobalStep:45280  TotalLoss:0.00357  PostnetLoss:0.00169  DecoderLoss:0.00188  StopLoss:0.11816  GradNorm:0.00447  GradNormST:0.05455  AvgTextLen:20.0  AvgSpecLen:82.5  StepTime:0.54  LR:0.000100\n",
      "   | > Step:25/43  GlobalStep:45290  TotalLoss:0.00454  PostnetLoss:0.00214  DecoderLoss:0.00240  StopLoss:0.09124  GradNorm:0.00494  GradNormST:0.02442  AvgTextLen:31.1  AvgSpecLen:122.8  StepTime:1.01  LR:0.000100\n",
      "   | > Step:35/43  GlobalStep:45300  TotalLoss:0.00460  PostnetLoss:0.00215  DecoderLoss:0.00245  StopLoss:0.09135  GradNorm:0.00490  GradNormST:0.02655  AvgTextLen:44.8  AvgSpecLen:153.4  StepTime:0.89  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:45308  AvgTotalLoss:0.10504  AvgPostnetLoss:0.00188  AvgDecoderLoss:0.00212  AvgStopLoss:0.10103  EpochTime:29.87  AvgStepTime:0.68\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.09628   PostnetLoss: 0.00211   DecoderLoss:0.00251  StopLoss: 0.09166  \n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00188   Validation Loss: 0.00235\n",
      "\n",
      " > Epoch 7/1000\n",
      "   | > Step:1/43  GlobalStep:45310  TotalLoss:0.00229  PostnetLoss:0.00110  DecoderLoss:0.00118  StopLoss:0.11448  GradNorm:0.00440  GradNormST:0.04173  AvgTextLen:4.2  AvgSpecLen:32.8  StepTime:0.32  LR:0.000100\n",
      "   | > Step:11/43  GlobalStep:45320  TotalLoss:0.00335  PostnetLoss:0.00159  DecoderLoss:0.00176  StopLoss:0.11202  GradNorm:0.00383  GradNormST:0.05836  AvgTextLen:15.2  AvgSpecLen:74.1  StepTime:0.49  LR:0.000100\n",
      "   | > Step:21/43  GlobalStep:45330  TotalLoss:0.00355  PostnetLoss:0.00168  DecoderLoss:0.00187  StopLoss:0.08541  GradNorm:0.00319  GradNormST:0.02847  AvgTextLen:25.3  AvgSpecLen:103.2  StepTime:0.76  LR:0.000100\n",
      "   | > Step:31/43  GlobalStep:45340  TotalLoss:0.00415  PostnetLoss:0.00195  DecoderLoss:0.00220  StopLoss:0.08819  GradNorm:0.00409  GradNormST:0.03360  AvgTextLen:38.1  AvgSpecLen:144.3  StepTime:0.89  LR:0.000100\n",
      "   | > Step:41/43  GlobalStep:45350  TotalLoss:0.00477  PostnetLoss:0.00223  DecoderLoss:0.00255  StopLoss:0.07875  GradNorm:0.00505  GradNormST:0.01880  AvgTextLen:64.8  AvgSpecLen:232.2  StepTime:1.29  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:45352  AvgTotalLoss:0.09891  AvgPostnetLoss:0.00186  AvgDecoderLoss:0.00209  AvgStopLoss:0.09496  EpochTime:30.25  AvgStepTime:0.69\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.09203   PostnetLoss: 0.00206   DecoderLoss:0.00246  StopLoss: 0.08750  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00186   Validation Loss: 0.00228\n",
      "\n",
      " > Epoch 8/1000\n",
      "   | > Step:7/43  GlobalStep:45360  TotalLoss:0.00311  PostnetLoss:0.00148  DecoderLoss:0.00163  StopLoss:0.10407  GradNorm:0.00427  GradNormST:0.04848  AvgTextLen:11.0  AvgSpecLen:57.9  StepTime:0.43  LR:0.000100\n",
      "   | > Step:17/43  GlobalStep:45370  TotalLoss:0.00410  PostnetLoss:0.00194  DecoderLoss:0.00216  StopLoss:0.08917  GradNorm:0.00424  GradNormST:0.02728  AvgTextLen:21.3  AvgSpecLen:90.3  StepTime:0.65  LR:0.000100\n",
      "   | > Step:27/43  GlobalStep:45380  TotalLoss:0.00422  PostnetLoss:0.00197  DecoderLoss:0.00225  StopLoss:0.08836  GradNorm:0.00356  GradNormST:0.04017  AvgTextLen:32.3  AvgSpecLen:115.4  StepTime:0.64  LR:0.000100\n",
      "   | > Step:37/43  GlobalStep:45390  TotalLoss:0.00462  PostnetLoss:0.00215  DecoderLoss:0.00247  StopLoss:0.09071  GradNorm:0.00320  GradNormST:0.02419  AvgTextLen:50.4  AvgSpecLen:170.5  StepTime:0.86  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:45396  AvgTotalLoss:0.09722  AvgPostnetLoss:0.00184  AvgDecoderLoss:0.00207  AvgStopLoss:0.09331  EpochTime:29.61  AvgStepTime:0.67\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.09175   PostnetLoss: 0.00204   DecoderLoss:0.00243  StopLoss: 0.08728  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00184   Validation Loss: 0.00228\n",
      "\n",
      " > Epoch 9/1000\n",
      "   | > Step:3/43  GlobalStep:45400  TotalLoss:0.00287  PostnetLoss:0.00136  DecoderLoss:0.00151  StopLoss:0.11999  GradNorm:0.00455  GradNormST:0.03940  AvgTextLen:5.8  AvgSpecLen:37.5  StepTime:0.49  LR:0.000100\n",
      "   | > Step:13/43  GlobalStep:45410  TotalLoss:0.00348  PostnetLoss:0.00166  DecoderLoss:0.00182  StopLoss:0.07536  GradNorm:0.00395  GradNormST:0.03237  AvgTextLen:17.0  AvgSpecLen:73.2  StepTime:0.54  LR:0.000100\n",
      "   | > Step:23/43  GlobalStep:45420  TotalLoss:0.00446  PostnetLoss:0.00208  DecoderLoss:0.00237  StopLoss:0.08625  GradNorm:0.00378  GradNormST:0.03015  AvgTextLen:27.9  AvgSpecLen:104.6  StepTime:0.71  LR:0.000100\n",
      "   | > Step:33/43  GlobalStep:45430  TotalLoss:0.00415  PostnetLoss:0.00195  DecoderLoss:0.00220  StopLoss:0.08972  GradNorm:0.00327  GradNormST:0.02264  AvgTextLen:41.6  AvgSpecLen:143.8  StepTime:0.94  LR:0.000100\n",
      "   | > Step:43/43  GlobalStep:45440  TotalLoss:0.00472  PostnetLoss:0.00219  DecoderLoss:0.00252  StopLoss:0.08909  GradNorm:0.00347  GradNormST:0.04762  AvgTextLen:99.1  AvgSpecLen:311.1  StepTime:1.33  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:45440  AvgTotalLoss:0.09753  AvgPostnetLoss:0.00183  AvgDecoderLoss:0.00206  AvgStopLoss:0.09364  EpochTime:30.14  AvgStepTime:0.68\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.09070   PostnetLoss: 0.00203   DecoderLoss:0.00244  StopLoss: 0.08623  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00183   Validation Loss: 0.00230\n",
      "\n",
      " > Epoch 10/1000\n",
      "   | > Step:9/43  GlobalStep:45450  TotalLoss:0.00332  PostnetLoss:0.00158  DecoderLoss:0.00174  StopLoss:0.08660  GradNorm:0.00476  GradNormST:0.02406  AvgTextLen:13.3  AvgSpecLen:63.4  StepTime:0.42  LR:0.000100\n",
      "   | > Step:19/43  GlobalStep:45460  TotalLoss:0.00386  PostnetLoss:0.00183  DecoderLoss:0.00204  StopLoss:0.10361  GradNorm:0.00463  GradNormST:0.03966  AvgTextLen:23.7  AvgSpecLen:91.3  StepTime:0.54  LR:0.000100\n"
     ]
    }
   ],
   "source": [
    "#/path/to/your/model.pth.tar\n",
    "cd /vm/TTS\n",
    "#remove phonemes (useful if we have changed the dataset)\n",
    "rm -rf cartman_phonemes\n",
    "python train.py --config_path config.json --restore_path /vm/TTS/models/cartman_models/mozilla-no-loc-fattn-stopnet-sigmoid-loss_masking-July-20-2019_05+33PM-3b3c248/checkpoint_45000.pth.tar 2> error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SoS",
   "language": "sos",
   "name": "sos"
  },
  "language_info": {
   "codemirror_mode": "sos",
   "file_extension": ".sos",
   "mimetype": "text/x-sos",
   "name": "sos",
   "nbconvert_exporter": "sos_notebook.converter.SoS_Exporter",
   "pygments_lexer": "sos"
  },
  "sos": {
   "kernels": [
    [
     "Bash",
     "bash",
     "Bash",
     "#E6EEFF"
    ],
    [
     "SoS",
     "sos",
     "",
     ""
    ]
   ],
   "version": "0.9.15.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

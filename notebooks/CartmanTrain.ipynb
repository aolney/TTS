{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "# South Park TTS Training\n",
    "\n",
    "Adapted [this gist](https://gist.github.com/erogol/97516ad65b44dbddb8cd694953187c5b) for [SoS notebook](https://vatlab.github.io/sos-docs/).\n",
    "\n",
    "Assumes you have already [cloned TTS](https://github.com/mozilla/TTS) and done `apt install espeak`\n",
    "\n",
    "## Prepare Training/Testing Data\n",
    "\n",
    "Assumes you have already [prepared suitable data](https://github.com/aolney/SouthParkTTSData).\n",
    "The next step simply does a train/test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "Bash"
   },
   "outputs": [],
   "source": [
    "#exit #comment to allow execution\n",
    "cd /y/south-park-1-to-20\n",
    "echo `wc -l metadata.csv`\n",
    "shuf metadata.csv > metadata_shuf.csv\n",
    "head -n 2200 metadata_shuf.csv > metadata_train.csv\n",
    "tail -n 293 metadata_shuf.csv > metadata_val.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "Bash"
   },
   "source": [
    "## Create config file for this training data\n",
    "\n",
    "Some of my local paths are hardcoded below. Parameters determined using [wiki documentation](https://github.com/mozilla/TTS/wiki/Dataset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "Bash"
   },
   "outputs": [],
   "source": [
    "#exit #comment to allow execution\n",
    "cd /vm/TTS\n",
    "cp config.json \"config.json.$(date +%Y%m%d)\"\n",
    "cat > config.json << EOM\n",
    "{\n",
    "    \"run_name\": \"mozilla-no-loc-fattn-stopnet-sigmoid-loss_masking\",\n",
    "    \"run_description\": \"using forward attention, with original prenet, loss masking,separate stopnet, sigmoid. Compare this with 4817. Pytorch DPP\",\n",
    "\n",
    "    \"audio\":{\n",
    "        // Audio processing parameters\n",
    "        \"num_mels\": 80,         // size of the mel spec frame. \n",
    "        \"num_freq\": 1025,       // number of stft frequency levels. Size of the linear spectogram frame.\n",
    "        \"sample_rate\": 16000,   // wav sample-rate. If different than the original data, it is resampled.\n",
    "        \"frame_length_ms\": 50,  // stft window length in ms.\n",
    "        \"frame_shift_ms\": 12.5, // stft window hop-lengh in ms.\n",
    "        \"preemphasis\": 0.99,    // pre-emphasis to reduce spec noise and make it more structured. If 0.0, no -pre-emphasis.\n",
    "        \"min_level_db\": -100,   // normalization range\n",
    "        \"ref_level_db\": 40,     // reference level db, theoretically 20db is the sound of air.\n",
    "        \"power\": 1.1,           // value to sharpen wav signals after GL algorithm.\n",
    "        \"griffin_lim_iters\": 30,// #griffin-lim iterations. 30-60 is a good range. Larger the value, slower the generation.\n",
    "        // Normalization parameters\n",
    "        \"signal_norm\": true,    // normalize the spec values in range [0, 1]\n",
    "        \"symmetric_norm\": false, // move normalization to range [-1, 1]\n",
    "        \"max_norm\": 1,          // scale normalization to range [-max_norm, max_norm] or [0, max_norm]\n",
    "        \"clip_norm\": true,      // clip normalized values into the range.\n",
    "        \"mel_fmin\": 0.0,         // minimum freq level for mel-spec. ~50 for male and ~95 for female voices. Tune for dataset!!\n",
    "        \"mel_fmax\": 8000.0,        // maximum freq level for mel-spec. Tune for dataset!!\n",
    "        \"do_trim_silence\": false  // enable trimming of slience of audio as you load it. LJspeech (false), TWEB (false), Nancy (true)\n",
    "    },\n",
    "\n",
    "    \"distributed\":{\n",
    "        \"backend\": \"nccl\",\n",
    "        \"url\": \"tcp:\\/\\/localhost:54321\"\n",
    "    },\n",
    "\n",
    "    \"reinit_layers\": [],\n",
    "\n",
    "    \"model\": \"Tacotron2\",          // one of the model in models/    \n",
    "    \"grad_clip\": 1,                // upper limit for gradients for clipping.\n",
    "    \"epochs\": 1000,                // total number of epochs to train.\n",
    "    \"lr\": 0.0001,                  // Initial learning rate. If Noam decay is active, maximum learning rate.\n",
    "    \"lr_decay\": false,             // if true, Noam learning rate decaying is applied through training.\n",
    "    \"warmup_steps\": 4000,          // Noam decay steps to increase the learning rate from 0 to \"lr\"\n",
    "    \"windowing\": false,            // Enables attention windowing. Used only in eval mode.\n",
    "    \"memory_size\": 5,              // ONLY TACOTRON - memory queue size used to queue network predictions to feed autoregressive connection. Useful if r < 5. \n",
    "    \"attention_norm\": \"sigmoid\",   // softmax or sigmoid. Suggested to use softmax for Tacotron2 and sigmoid for Tacotron.\n",
    "    \"prenet_type\": \"original\",     // ONLY TACOTRON2 - \"original\" or \"bn\".\n",
    "    \"prenet_dropout\": true,        // ONLY TACOTRON2 - enable/disable dropout at prenet. \n",
    "    \"use_forward_attn\": true,      // ONLY TACOTRON2 - if it uses forward attention. In general, it aligns faster.\n",
    "    \"transition_agent\": false,     // ONLY TACOTRON2 - enable/disable transition agent of forward attention.\n",
    "    \"location_attn\": false,        // ONLY TACOTRON2 - enable_disable location sensitive attention. It is enabled for TACOTRON by default.\n",
    "    \"loss_masking\": true,         // enable / disable loss masking against the sequence padding.\n",
    "    \"enable_eos_bos_chars\": false, // enable/disable beginning of sentence and end of sentence chars.\n",
    "    \"stopnet\": true,               // Train stopnet predicting the end of synthesis. \n",
    "    \"separate_stopnet\": true,     // Train stopnet seperately if 'stopnet==true'. It prevents stopnet loss to influence the rest of the model. It causes a better model, but it trains SLOWER.\n",
    "    \"tb_model_param_stats\": false,     // true, plots param stats per layer on tensorboard. Might be memory consuming, but good for debugging. \n",
    "    \n",
    "    \"batch_size\": 32,       // Batch size for training. Lower values than 32 might cause hard to learn attention.\n",
    "    \"eval_batch_size\":16,   \n",
    "    \"r\": 1,                 // Number of frames to predict for step.\n",
    "    \"wd\": 0.000001,         // Weight decay weight.\n",
    "    \"checkpoint\": true,     // If true, it saves checkpoints per \"save_step\"\n",
    "    \"save_step\": 1000,      // Number of training steps expected to save traning stats and checkpoints.\n",
    "    \"print_step\": 10,       // Number of steps to log traning on console.\n",
    "    \"batch_group_size\": 0,  //Number of batches to shuffle after bucketing.\n",
    "\n",
    "    \"run_eval\": true,\n",
    "    \"test_delay_epochs\": 5,  //Until attention is aligned, testing only wastes computation time.\n",
    "    \"test_sentences_file\": null,  // set a file to load sentences to be used for testing. If it is null then we use default english sentences.\n",
    "    \"data_path\": \"/y/south-park-1-to-20/\",  // DATASET-RELATED: can overwritten from command argument\n",
    "    \"meta_file_train\": \"metadata_train.csv\",       // DATASET-RELATED: metafile for training dataloader.\n",
    "    \"meta_file_val\": \"metadata_val.csv\",    // DATASET-RELATED: metafile for evaluation dataloader.\n",
    "    \"dataset\": \"ljspeech2\",       // DATASET-RELATED: one of TTS.dataset.preprocessors depending on your target dataset. Use \"tts_cache\" for pre-computed dataset by extract_features.py\n",
    "    \"min_seq_len\": 0,       // DATASET-RELATED: minimum text length to use in training\n",
    "    \"max_seq_len\": 160,     // DATASET-RELATED: maximum text length\n",
    "    \"output_path\": \"models/cartman_models/\",      // DATASET-RELATED: output path for all training outputs.\n",
    "    \"num_loader_workers\": 4,        // number of training data loader processes. Don't set it too big. 4-8 are good values.\n",
    "    \"num_val_loader_workers\": 4,    // number of evaluation data loader processes.\n",
    "    \"phoneme_cache_path\": \"cartman_phonemes\",  // phoneme computation is slow, therefore, it caches results in the given folder.\n",
    "    \"use_phonemes\": true,           // use phonemes instead of raw characters. It is suggested for better pronounciation.\n",
    "    \"phoneme_language\": \"en-us\",     // depending on your target language, pick one from  https://github.com/bootphon/phonemizer#languages\n",
    "    \"text_cleaner\": \"phoneme_cleaners\"\n",
    "}\n",
    "EOM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "Bash"
   },
   "source": [
    "## Start Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "Bash"
   },
   "outputs": [],
   "source": [
    "cd /vm/TTS\n",
    "#remove phonemes (useful if we have changed the dataset)\n",
    "rm -rf cartman_phonemes\n",
    "python train.py --config_path config.json 2> error\n",
    "#start tensorboard in another terminal, e.g. tensorboard --logdir=/vm/TTS/models/cartman_models\n",
    "#tensorboard --logdir=my_run:<output_path in config.json> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "Bash"
   },
   "source": [
    "## Continue Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "kernel": "Bash"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > Using CUDA:  True\n",
      " > Number of GPUs:  2\n",
      " > Forcing use of single GPU, id:  GeForce GTX 1080 Ti\n",
      " > Git Hash: de6aeb9\n",
      " > Experiment folder: /vm/TTS/models/cartman_models/mozilla-no-loc-fattn-stopnet-sigmoid-loss_masking-August-15-2019_11+26AM-de6aeb9\n",
      " > Setting up Audio Processor...\n",
      " | > bits:None\n",
      " | > sample_rate:16000\n",
      " | > num_mels:80\n",
      " | > min_level_db:-100\n",
      " | > frame_shift_ms:12.5\n",
      " | > frame_length_ms:50\n",
      " | > ref_level_db:40\n",
      " | > num_freq:1025\n",
      " | > power:1.1\n",
      " | > preemphasis:0.99\n",
      " | > griffin_lim_iters:30\n",
      " | > signal_norm:True\n",
      " | > symmetric_norm:False\n",
      " | > mel_fmin:0.0\n",
      " | > mel_fmax:8000.0\n",
      " | > max_norm:1.0\n",
      " | > clip_norm:True\n",
      " | > do_trim_silence:False\n",
      " | > n_fft:2048\n",
      " | > hop_length:200\n",
      " | > win_length:800\n",
      " > Using model: Tacotron2\n",
      " | > Num output units : 1025\n",
      " > Model restored from step 69000\n",
      "\n",
      " > Model has 28179554 parameters\n",
      "\n",
      " > DataLoader initialization\n",
      " | > Data path: /y/south-park-1-to-20/\n",
      " | > Use phonemes: True\n",
      "   | > phoneme language: en-us\n",
      " | > Number of instances : 2200\n",
      " | > Max length sequence: 155\n",
      " | > Min length sequence: 2\n",
      " | > Avg length sequence: 31.12272727272727\n",
      " | > Num. instances discarded by max-min seq limits: 0\n",
      " | > Batch group size: 0.\n",
      "\n",
      " > Epoch 0/1000\n",
      "   | > Step:9/68  GlobalStep:69010  TotalLoss:0.00200  PostnetLoss:0.00097  DecoderLoss:0.00103  StopLoss:0.08793  GradNorm:0.00387  GradNormST:0.04713  AvgTextLen:9.0  AvgSpecLen:42.1  StepTime:0.43  LR:0.000100\n",
      "   | > Step:19/68  GlobalStep:69020  TotalLoss:0.00232  PostnetLoss:0.00112  DecoderLoss:0.00120  StopLoss:0.11210  GradNorm:0.00363  GradNormST:0.01989  AvgTextLen:15.5  AvgSpecLen:63.2  StepTime:0.38  LR:0.000100\n",
      "   | > Step:29/68  GlobalStep:69030  TotalLoss:0.00255  PostnetLoss:0.00123  DecoderLoss:0.00132  StopLoss:0.12181  GradNorm:0.00333  GradNormST:0.03263  AvgTextLen:21.8  AvgSpecLen:83.4  StepTime:0.52  LR:0.000100\n",
      "   | > Step:39/68  GlobalStep:69040  TotalLoss:0.00289  PostnetLoss:0.00139  DecoderLoss:0.00150  StopLoss:0.07454  GradNorm:0.00330  GradNormST:0.01420  AvgTextLen:30.2  AvgSpecLen:110.4  StepTime:0.62  LR:0.000100\n",
      "   | > Step:49/68  GlobalStep:69050  TotalLoss:0.00306  PostnetLoss:0.00146  DecoderLoss:0.00159  StopLoss:0.07565  GradNorm:0.00314  GradNormST:0.01815  AvgTextLen:38.2  AvgSpecLen:127.7  StepTime:0.77  LR:0.000100\n",
      "   | > Step:59/68  GlobalStep:69060  TotalLoss:0.00327  PostnetLoss:0.00156  DecoderLoss:0.00171  StopLoss:0.07949  GradNorm:0.00296  GradNormST:0.01490  AvgTextLen:50.5  AvgSpecLen:155.9  StepTime:0.80  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:69069  AvgTotalLoss:0.09105  AvgPostnetLoss:0.00130  AvgDecoderLoss:0.00142  AvgStopLoss:0.08833  EpochTime:46.64  AvgStepTime:0.68\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.12858   PostnetLoss: 0.00397   DecoderLoss:0.00451  StopLoss: 0.12010  \n",
      "   | > TotalLoss: 0.09489   PostnetLoss: 0.00556   DecoderLoss:0.00634  StopLoss: 0.08299  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Training Loss: 0.00130   Validation Loss: 0.00509\n",
      "\n",
      " > BEST MODEL (0.00509) : /vm/TTS/models/cartman_models/mozilla-no-loc-fattn-stopnet-sigmoid-loss_masking-August-15-2019_11+26AM-de6aeb9/best_model.pth.tar\n",
      "\n",
      " > Epoch 1/1000\n",
      "   | > Step:0/68  GlobalStep:69070  TotalLoss:0.00178  PostnetLoss:0.00085  DecoderLoss:0.00093  StopLoss:0.10139  GradNorm:0.00447  GradNormST:0.03816  AvgTextLen:3.0  AvgSpecLen:25.3  StepTime:0.46  LR:0.000100\n",
      "   | > Step:10/68  GlobalStep:69080  TotalLoss:0.00212  PostnetLoss:0.00102  DecoderLoss:0.00110  StopLoss:0.07764  GradNorm:0.00476  GradNormST:0.03633  AvgTextLen:9.8  AvgSpecLen:48.6  StepTime:0.43  LR:0.000100\n",
      "   | > Step:20/68  GlobalStep:69090  TotalLoss:0.00246  PostnetLoss:0.00118  DecoderLoss:0.00127  StopLoss:0.07757  GradNorm:0.00367  GradNormST:0.01875  AvgTextLen:16.4  AvgSpecLen:64.8  StepTime:0.51  LR:0.000100\n",
      "   | > Step:30/68  GlobalStep:69100  TotalLoss:0.00264  PostnetLoss:0.00127  DecoderLoss:0.00137  StopLoss:0.11348  GradNorm:0.00329  GradNormST:0.04177  AvgTextLen:22.9  AvgSpecLen:85.8  StepTime:0.49  LR:0.000100\n",
      "   | > Step:40/68  GlobalStep:69110  TotalLoss:0.00286  PostnetLoss:0.00137  DecoderLoss:0.00149  StopLoss:0.07368  GradNorm:0.00357  GradNormST:0.01071  AvgTextLen:30.9  AvgSpecLen:105.3  StepTime:0.56  LR:0.000100\n",
      "   | > Step:50/68  GlobalStep:69120  TotalLoss:0.00306  PostnetLoss:0.00146  DecoderLoss:0.00160  StopLoss:0.08158  GradNorm:0.00348  GradNormST:0.01943  AvgTextLen:38.8  AvgSpecLen:132.0  StepTime:0.87  LR:0.000100\n",
      "   | > Step:60/68  GlobalStep:69130  TotalLoss:0.00338  PostnetLoss:0.00161  DecoderLoss:0.00177  StopLoss:0.06344  GradNorm:0.00320  GradNormST:0.01694  AvgTextLen:52.0  AvgSpecLen:172.7  StepTime:0.93  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:69138  AvgTotalLoss:0.09005  AvgPostnetLoss:0.00130  AvgDecoderLoss:0.00141  AvgStopLoss:0.08735  EpochTime:42.35  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.13530   PostnetLoss: 0.00408   DecoderLoss:0.00465  StopLoss: 0.12657  \n",
      "   | > TotalLoss: 0.09559   PostnetLoss: 0.00568   DecoderLoss:0.00646  StopLoss: 0.08346  \n",
      " | > Training Loss: 0.00130   Validation Loss: 0.00512\n",
      "\n",
      " > Epoch 2/1000\n",
      "   | > Step:1/68  GlobalStep:69140  TotalLoss:0.00188  PostnetLoss:0.00090  DecoderLoss:0.00098  StopLoss:0.11272  GradNorm:0.00434  GradNormST:0.02952  AvgTextLen:3.7  AvgSpecLen:28.6  StepTime:0.24  LR:0.000100\n",
      "   | > Step:11/68  GlobalStep:69150  TotalLoss:0.00210  PostnetLoss:0.00102  DecoderLoss:0.00108  StopLoss:0.09092  GradNorm:0.00434  GradNormST:0.04250  AvgTextLen:10.3  AvgSpecLen:49.4  StepTime:0.41  LR:0.000100\n",
      "   | > Step:21/68  GlobalStep:69160  TotalLoss:0.00237  PostnetLoss:0.00115  DecoderLoss:0.00123  StopLoss:0.10668  GradNorm:0.00368  GradNormST:0.03003  AvgTextLen:16.6  AvgSpecLen:66.3  StepTime:0.37  LR:0.000100\n",
      "   | > Step:31/68  GlobalStep:69170  TotalLoss:0.00264  PostnetLoss:0.00127  DecoderLoss:0.00137  StopLoss:0.11385  GradNorm:0.00331  GradNormST:0.04316  AvgTextLen:24.1  AvgSpecLen:82.7  StepTime:0.56  LR:0.000100\n",
      "   | > Step:41/68  GlobalStep:69180  TotalLoss:0.00299  PostnetLoss:0.00143  DecoderLoss:0.00156  StopLoss:0.06012  GradNorm:0.00365  GradNormST:0.01841  AvgTextLen:32.3  AvgSpecLen:109.4  StepTime:0.66  LR:0.000100\n",
      "   | > Step:51/68  GlobalStep:69190  TotalLoss:0.00306  PostnetLoss:0.00146  DecoderLoss:0.00160  StopLoss:0.08590  GradNorm:0.00311  GradNormST:0.01533  AvgTextLen:39.2  AvgSpecLen:126.9  StepTime:0.79  LR:0.000100\n",
      "   | > Step:61/68  GlobalStep:69200  TotalLoss:0.00331  PostnetLoss:0.00158  DecoderLoss:0.00174  StopLoss:0.07229  GradNorm:0.00326  GradNormST:0.02720  AvgTextLen:54.6  AvgSpecLen:165.5  StepTime:0.92  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:69207  AvgTotalLoss:0.08916  AvgPostnetLoss:0.00130  AvgDecoderLoss:0.00141  AvgStopLoss:0.08646  EpochTime:42.85  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.13297   PostnetLoss: 0.00405   DecoderLoss:0.00460  StopLoss: 0.12432  \n",
      "   | > TotalLoss: 0.09255   PostnetLoss: 0.00560   DecoderLoss:0.00638  StopLoss: 0.08057  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Training Loss: 0.00130   Validation Loss: 0.00510\n",
      "\n",
      " > Epoch 3/1000\n",
      "   | > Step:2/68  GlobalStep:69210  TotalLoss:0.00173  PostnetLoss:0.00084  DecoderLoss:0.00089  StopLoss:0.12382  GradNorm:0.00379  GradNormST:0.03244  AvgTextLen:4.0  AvgSpecLen:31.3  StepTime:0.58  LR:0.000100\n",
      "   | > Step:12/68  GlobalStep:69220  TotalLoss:0.00213  PostnetLoss:0.00102  DecoderLoss:0.00110  StopLoss:0.08660  GradNorm:0.00509  GradNormST:0.02522  AvgTextLen:11.0  AvgSpecLen:49.3  StepTime:0.44  LR:0.000100\n",
      "   | > Step:22/68  GlobalStep:69230  TotalLoss:0.00245  PostnetLoss:0.00118  DecoderLoss:0.00127  StopLoss:0.09518  GradNorm:0.00415  GradNormST:0.01905  AvgTextLen:18.0  AvgSpecLen:72.0  StepTime:0.50  LR:0.000100\n",
      "   | > Step:32/68  GlobalStep:69240  TotalLoss:0.00271  PostnetLoss:0.00130  DecoderLoss:0.00140  StopLoss:0.08870  GradNorm:0.00384  GradNormST:0.01532  AvgTextLen:24.4  AvgSpecLen:91.5  StepTime:0.61  LR:0.000100\n",
      "   | > Step:42/68  GlobalStep:69250  TotalLoss:0.00291  PostnetLoss:0.00139  DecoderLoss:0.00152  StopLoss:0.07636  GradNorm:0.00373  GradNormST:0.02113  AvgTextLen:33.1  AvgSpecLen:109.7  StepTime:0.59  LR:0.000100\n",
      "   | > Step:52/68  GlobalStep:69260  TotalLoss:0.00316  PostnetLoss:0.00151  DecoderLoss:0.00165  StopLoss:0.08165  GradNorm:0.00341  GradNormST:0.01778  AvgTextLen:42.1  AvgSpecLen:137.5  StepTime:0.78  LR:0.000100\n",
      "   | > Step:62/68  GlobalStep:69270  TotalLoss:0.00340  PostnetLoss:0.00162  DecoderLoss:0.00178  StopLoss:0.08104  GradNorm:0.00285  GradNormST:0.01915  AvgTextLen:54.0  AvgSpecLen:174.7  StepTime:0.76  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:69276  AvgTotalLoss:0.08459  AvgPostnetLoss:0.00130  AvgDecoderLoss:0.00141  AvgStopLoss:0.08188  EpochTime:43.04  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.12516   PostnetLoss: 0.00409   DecoderLoss:0.00463  StopLoss: 0.11644  \n",
      "   | > TotalLoss: 0.09019   PostnetLoss: 0.00555   DecoderLoss:0.00633  StopLoss: 0.07832  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Training Loss: 0.00130   Validation Loss: 0.00509\n",
      "\n",
      " > Epoch 4/1000\n",
      "   | > Step:3/68  GlobalStep:69280  TotalLoss:0.00180  PostnetLoss:0.00087  DecoderLoss:0.00093  StopLoss:0.09928  GradNorm:0.00414  GradNormST:0.02731  AvgTextLen:4.7  AvgSpecLen:29.7  StepTime:0.30  LR:0.000100\n",
      "   | > Step:13/68  GlobalStep:69290  TotalLoss:0.00207  PostnetLoss:0.00100  DecoderLoss:0.00107  StopLoss:0.11476  GradNorm:0.00371  GradNormST:0.02545  AvgTextLen:11.5  AvgSpecLen:55.2  StepTime:0.33  LR:0.000100\n",
      "   | > Step:23/68  GlobalStep:69300  TotalLoss:0.00255  PostnetLoss:0.00123  DecoderLoss:0.00132  StopLoss:0.07719  GradNorm:0.00430  GradNormST:0.02098  AvgTextLen:18.3  AvgSpecLen:72.2  StepTime:0.34  LR:0.000100\n",
      "   | > Step:33/68  GlobalStep:69310  TotalLoss:0.00274  PostnetLoss:0.00132  DecoderLoss:0.00142  StopLoss:0.08085  GradNorm:0.00387  GradNormST:0.01875  AvgTextLen:25.9  AvgSpecLen:88.9  StepTime:0.57  LR:0.000100\n",
      "   | > Step:43/68  GlobalStep:69320  TotalLoss:0.00301  PostnetLoss:0.00144  DecoderLoss:0.00157  StopLoss:0.07496  GradNorm:0.00390  GradNormST:0.02384  AvgTextLen:33.4  AvgSpecLen:115.7  StepTime:0.63  LR:0.000100\n",
      "   | > Step:53/68  GlobalStep:69330  TotalLoss:0.00313  PostnetLoss:0.00149  DecoderLoss:0.00163  StopLoss:0.07365  GradNorm:0.00342  GradNormST:0.01903  AvgTextLen:42.4  AvgSpecLen:137.6  StepTime:0.64  LR:0.000100\n",
      "   | > Step:63/68  GlobalStep:69340  TotalLoss:0.00346  PostnetLoss:0.00165  DecoderLoss:0.00181  StopLoss:0.05415  GradNorm:0.00307  GradNormST:0.01163  AvgTextLen:58.6  AvgSpecLen:185.1  StepTime:0.95  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:69345  AvgTotalLoss:0.08164  AvgPostnetLoss:0.00130  AvgDecoderLoss:0.00141  AvgStopLoss:0.07892  EpochTime:42.75  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.13378   PostnetLoss: 0.00406   DecoderLoss:0.00459  StopLoss: 0.12512  \n",
      "   | > TotalLoss: 0.09249   PostnetLoss: 0.00565   DecoderLoss:0.00641  StopLoss: 0.08044  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Training Loss: 0.00130   Validation Loss: 0.00517\n",
      "\n",
      " > Epoch 5/1000\n",
      "   | > Step:4/68  GlobalStep:69350  TotalLoss:0.00178  PostnetLoss:0.00086  DecoderLoss:0.00092  StopLoss:0.12973  GradNorm:0.00450  GradNormST:0.05488  AvgTextLen:4.8  AvgSpecLen:31.1  StepTime:0.32  LR:0.000100\n",
      "   | > Step:14/68  GlobalStep:69360  TotalLoss:0.00221  PostnetLoss:0.00107  DecoderLoss:0.00114  StopLoss:0.09621  GradNorm:0.00511  GradNormST:0.02215  AvgTextLen:12.2  AvgSpecLen:52.0  StepTime:0.35  LR:0.000100\n",
      "   | > Step:24/68  GlobalStep:69370  TotalLoss:0.00264  PostnetLoss:0.00127  DecoderLoss:0.00137  StopLoss:0.08145  GradNorm:0.00434  GradNormST:0.01860  AvgTextLen:19.2  AvgSpecLen:79.0  StepTime:0.52  LR:0.000100\n",
      "   | > Step:34/68  GlobalStep:69380  TotalLoss:0.00280  PostnetLoss:0.00134  DecoderLoss:0.00145  StopLoss:0.06328  GradNorm:0.00340  GradNormST:0.01125  AvgTextLen:26.2  AvgSpecLen:95.9  StepTime:0.58  LR:0.000100\n",
      "   | > Step:44/68  GlobalStep:69390  TotalLoss:0.00303  PostnetLoss:0.00145  DecoderLoss:0.00158  StopLoss:0.06653  GradNorm:0.00431  GradNormST:0.01981  AvgTextLen:33.8  AvgSpecLen:116.3  StepTime:0.64  LR:0.000100\n",
      "   | > Step:54/68  GlobalStep:69400  TotalLoss:0.00325  PostnetLoss:0.00155  DecoderLoss:0.00170  StopLoss:0.08976  GradNorm:0.00478  GradNormST:0.01593  AvgTextLen:42.2  AvgSpecLen:144.9  StepTime:0.86  LR:0.000100\n",
      "   | > Step:64/68  GlobalStep:69410  TotalLoss:0.00327  PostnetLoss:0.00156  DecoderLoss:0.00171  StopLoss:0.07390  GradNorm:0.00313  GradNormST:0.01296  AvgTextLen:60.6  AvgSpecLen:177.6  StepTime:0.82  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:69414  AvgTotalLoss:0.08852  AvgPostnetLoss:0.00130  AvgDecoderLoss:0.00141  AvgStopLoss:0.08581  EpochTime:43.18  AvgStepTime:0.63\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.13072   PostnetLoss: 0.00406   DecoderLoss:0.00459  StopLoss: 0.12208  \n",
      "   | > TotalLoss: 0.09676   PostnetLoss: 0.00558   DecoderLoss:0.00636  StopLoss: 0.08482  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Training Loss: 0.00130   Validation Loss: 0.00514\n",
      "\n",
      " > Epoch 6/1000\n",
      "   | > Step:5/68  GlobalStep:69420  TotalLoss:0.00189  PostnetLoss:0.00091  DecoderLoss:0.00098  StopLoss:0.11467  GradNorm:0.00407  GradNormST:0.03712  AvgTextLen:5.9  AvgSpecLen:33.2  StepTime:0.33  LR:0.000100\n",
      "   | > Step:15/68  GlobalStep:69430  TotalLoss:0.00229  PostnetLoss:0.00110  DecoderLoss:0.00118  StopLoss:0.09958  GradNorm:0.00384  GradNormST:0.05102  AvgTextLen:12.8  AvgSpecLen:59.1  StepTime:0.51  LR:0.000100\n",
      "   | > Step:25/68  GlobalStep:69440  TotalLoss:0.00246  PostnetLoss:0.00119  DecoderLoss:0.00128  StopLoss:0.08332  GradNorm:0.00375  GradNormST:0.01808  AvgTextLen:19.8  AvgSpecLen:73.3  StepTime:0.49  LR:0.000100\n",
      "   | > Step:35/68  GlobalStep:69450  TotalLoss:0.00279  PostnetLoss:0.00134  DecoderLoss:0.00145  StopLoss:0.12671  GradNorm:0.00411  GradNormST:0.06335  AvgTextLen:27.1  AvgSpecLen:104.7  StepTime:0.81  LR:0.000100\n",
      "   | > Step:45/68  GlobalStep:69460  TotalLoss:0.00297  PostnetLoss:0.00142  DecoderLoss:0.00154  StopLoss:0.09239  GradNorm:0.00474  GradNormST:0.02617  AvgTextLen:34.4  AvgSpecLen:118.6  StepTime:0.77  LR:0.000100\n",
      "   | > Step:55/68  GlobalStep:69470  TotalLoss:0.00333  PostnetLoss:0.00159  DecoderLoss:0.00174  StopLoss:0.07617  GradNorm:0.00495  GradNormST:0.01454  AvgTextLen:43.9  AvgSpecLen:145.2  StepTime:0.67  LR:0.000100\n",
      "   | > Step:65/68  GlobalStep:69480  TotalLoss:0.00355  PostnetLoss:0.00169  DecoderLoss:0.00186  StopLoss:0.06827  GradNorm:0.00338  GradNormST:0.01666  AvgTextLen:64.2  AvgSpecLen:207.4  StepTime:1.27  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:69483  AvgTotalLoss:0.09133  AvgPostnetLoss:0.00131  AvgDecoderLoss:0.00142  AvgStopLoss:0.08861  EpochTime:43.74  AvgStepTime:0.63\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.13086   PostnetLoss: 0.00406   DecoderLoss:0.00460  StopLoss: 0.12220  \n",
      "   | > TotalLoss: 0.09837   PostnetLoss: 0.00569   DecoderLoss:0.00648  StopLoss: 0.08620  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00131   Validation Loss: 0.00517\n",
      "\n",
      " > Epoch 7/1000\n",
      "   | > Step:6/68  GlobalStep:69490  TotalLoss:0.00186  PostnetLoss:0.00090  DecoderLoss:0.00096  StopLoss:0.08229  GradNorm:0.00405  GradNormST:0.03386  AvgTextLen:6.0  AvgSpecLen:34.8  StepTime:0.31  LR:0.000100\n",
      "   | > Step:16/68  GlobalStep:69500  TotalLoss:0.00231  PostnetLoss:0.00111  DecoderLoss:0.00120  StopLoss:0.08674  GradNorm:0.00340  GradNormST:0.02045  AvgTextLen:13.8  AvgSpecLen:60.6  StepTime:0.38  LR:0.000100\n",
      "   | > Step:26/68  GlobalStep:69510  TotalLoss:0.00252  PostnetLoss:0.00121  DecoderLoss:0.00131  StopLoss:0.12017  GradNorm:0.00363  GradNormST:0.04071  AvgTextLen:20.3  AvgSpecLen:77.5  StepTime:0.47  LR:0.000100\n",
      "   | > Step:36/68  GlobalStep:69520  TotalLoss:0.00276  PostnetLoss:0.00133  DecoderLoss:0.00144  StopLoss:0.09189  GradNorm:0.00354  GradNormST:0.01944  AvgTextLen:28.5  AvgSpecLen:100.2  StepTime:0.61  LR:0.000100\n",
      "   | > Step:46/68  GlobalStep:69530  TotalLoss:0.00301  PostnetLoss:0.00144  DecoderLoss:0.00157  StopLoss:0.06208  GradNorm:0.00464  GradNormST:0.00975  AvgTextLen:35.4  AvgSpecLen:122.6  StepTime:0.85  LR:0.000100\n",
      "   | > Step:56/68  GlobalStep:69540  TotalLoss:0.00330  PostnetLoss:0.00157  DecoderLoss:0.00172  StopLoss:0.06402  GradNorm:0.00543  GradNormST:0.01964  AvgTextLen:45.8  AvgSpecLen:145.4  StepTime:0.98  LR:0.000100\n",
      "   | > Step:66/68  GlobalStep:69550  TotalLoss:0.00378  PostnetLoss:0.00180  DecoderLoss:0.00198  StopLoss:0.06073  GradNorm:0.00328  GradNormST:0.01971  AvgTextLen:71.2  AvgSpecLen:220.8  StepTime:1.09  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:69552  AvgTotalLoss:0.09167  AvgPostnetLoss:0.00130  AvgDecoderLoss:0.00141  AvgStopLoss:0.08896  EpochTime:43.66  AvgStepTime:0.63\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.12320   PostnetLoss: 0.00419   DecoderLoss:0.00472  StopLoss: 0.11428  \n",
      "   | > TotalLoss: 0.09053   PostnetLoss: 0.00554   DecoderLoss:0.00629  StopLoss: 0.07869  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00130   Validation Loss: 0.00511\n",
      "\n",
      " > Epoch 8/1000\n",
      "   | > Step:7/68  GlobalStep:69560  TotalLoss:0.00200  PostnetLoss:0.00097  DecoderLoss:0.00103  StopLoss:0.10794  GradNorm:0.00391  GradNormST:0.03889  AvgTextLen:7.4  AvgSpecLen:44.6  StepTime:0.38  LR:0.000100\n",
      "   | > Step:17/68  GlobalStep:69570  TotalLoss:0.00234  PostnetLoss:0.00113  DecoderLoss:0.00121  StopLoss:0.09894  GradNorm:0.00910  GradNormST:0.01990  AvgTextLen:14.7  AvgSpecLen:66.0  StepTime:0.46  LR:0.000100\n",
      "   | > Step:27/68  GlobalStep:69580  TotalLoss:0.00265  PostnetLoss:0.00128  DecoderLoss:0.00138  StopLoss:0.07689  GradNorm:0.00361  GradNormST:0.02035  AvgTextLen:21.0  AvgSpecLen:80.2  StepTime:0.62  LR:0.000100\n",
      "   | > Step:37/68  GlobalStep:69590  TotalLoss:0.00288  PostnetLoss:0.00138  DecoderLoss:0.00150  StopLoss:0.06871  GradNorm:0.00440  GradNormST:0.01981  AvgTextLen:29.3  AvgSpecLen:102.4  StepTime:0.60  LR:0.000100\n",
      "   | > Step:47/68  GlobalStep:69600  TotalLoss:0.00313  PostnetLoss:0.00150  DecoderLoss:0.00163  StopLoss:0.07302  GradNorm:0.00569  GradNormST:0.01759  AvgTextLen:36.7  AvgSpecLen:126.9  StepTime:0.78  LR:0.000100\n",
      "   | > Step:57/68  GlobalStep:69610  TotalLoss:0.00344  PostnetLoss:0.00165  DecoderLoss:0.00179  StopLoss:0.05503  GradNorm:0.00693  GradNormST:0.02273  AvgTextLen:47.1  AvgSpecLen:161.0  StepTime:0.92  LR:0.000100\n",
      "   | > Step:67/68  GlobalStep:69620  TotalLoss:0.00370  PostnetLoss:0.00175  DecoderLoss:0.00194  StopLoss:0.06125  GradNorm:0.00298  GradNormST:0.02662  AvgTextLen:80.6  AvgSpecLen:246.5  StepTime:1.22  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:69621  AvgTotalLoss:0.07937  AvgPostnetLoss:0.00131  AvgDecoderLoss:0.00142  AvgStopLoss:0.07663  EpochTime:43.41  AvgStepTime:0.63\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.11825   PostnetLoss: 0.00417   DecoderLoss:0.00471  StopLoss: 0.10937  \n",
      "   | > TotalLoss: 0.08777   PostnetLoss: 0.00554   DecoderLoss:0.00627  StopLoss: 0.07596  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00131   Validation Loss: 0.00514\n",
      "\n",
      " > Epoch 9/1000\n",
      "   | > Step:8/68  GlobalStep:69630  TotalLoss:0.00205  PostnetLoss:0.00099  DecoderLoss:0.00106  StopLoss:0.09280  GradNorm:0.00431  GradNormST:0.02904  AvgTextLen:8.3  AvgSpecLen:44.0  StepTime:0.30  LR:0.000100\n",
      "   | > Step:18/68  GlobalStep:69640  TotalLoss:0.00224  PostnetLoss:0.00108  DecoderLoss:0.00116  StopLoss:0.06643  GradNorm:0.00321  GradNormST:0.02306  AvgTextLen:15.2  AvgSpecLen:64.5  StepTime:0.42  LR:0.000100\n",
      "   | > Step:28/68  GlobalStep:69650  TotalLoss:0.00258  PostnetLoss:0.00124  DecoderLoss:0.00134  StopLoss:0.07526  GradNorm:0.00373  GradNormST:0.02156  AvgTextLen:22.1  AvgSpecLen:85.4  StepTime:0.60  LR:0.000100\n",
      "   | > Step:38/68  GlobalStep:69660  TotalLoss:0.00281  PostnetLoss:0.00135  DecoderLoss:0.00146  StopLoss:0.06049  GradNorm:0.00386  GradNormST:0.01661  AvgTextLen:29.4  AvgSpecLen:104.1  StepTime:0.65  LR:0.000100\n",
      "   | > Step:48/68  GlobalStep:69670  TotalLoss:0.00306  PostnetLoss:0.00147  DecoderLoss:0.00159  StopLoss:0.06987  GradNorm:0.00479  GradNormST:0.01475  AvgTextLen:37.5  AvgSpecLen:123.8  StepTime:0.66  LR:0.000100\n",
      "   | > Step:58/68  GlobalStep:69680  TotalLoss:0.00339  PostnetLoss:0.00162  DecoderLoss:0.00177  StopLoss:0.07012  GradNorm:0.00611  GradNormST:0.02945  AvgTextLen:47.8  AvgSpecLen:158.0  StepTime:0.85  LR:0.000100\n",
      "   | > Step:68/68  GlobalStep:69690  TotalLoss:0.00364  PostnetLoss:0.00173  DecoderLoss:0.00191  StopLoss:0.05700  GradNorm:0.00538  GradNormST:0.02384  AvgTextLen:103.0  AvgSpecLen:308.9  StepTime:1.88  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:69690  AvgTotalLoss:0.07372  AvgPostnetLoss:0.00131  AvgDecoderLoss:0.00142  AvgStopLoss:0.07099  EpochTime:43.27  AvgStepTime:0.63\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.11277   PostnetLoss: 0.00412   DecoderLoss:0.00468  StopLoss: 0.10396  \n",
      "   | > TotalLoss: 0.08493   PostnetLoss: 0.00554   DecoderLoss:0.00628  StopLoss: 0.07311  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00131   Validation Loss: 0.00508\n",
      "\n",
      " > BEST MODEL (0.00508) : /vm/TTS/models/cartman_models/mozilla-no-loc-fattn-stopnet-sigmoid-loss_masking-August-15-2019_11+26AM-de6aeb9/best_model.pth.tar\n",
      "\n",
      " > Epoch 10/1000\n",
      "   | > Step:9/68  GlobalStep:69700  TotalLoss:0.00197  PostnetLoss:0.00096  DecoderLoss:0.00101  StopLoss:0.07333  GradNorm:0.00381  GradNormST:0.02920  AvgTextLen:9.0  AvgSpecLen:42.1  StepTime:0.36  LR:0.000100\n",
      "   | > Step:19/68  GlobalStep:69710  TotalLoss:0.00226  PostnetLoss:0.00110  DecoderLoss:0.00117  StopLoss:0.09528  GradNorm:0.00338  GradNormST:0.03599  AvgTextLen:15.5  AvgSpecLen:63.2  StepTime:0.38  LR:0.000100\n",
      "   | > Step:29/68  GlobalStep:69720  TotalLoss:0.00250  PostnetLoss:0.00120  DecoderLoss:0.00130  StopLoss:0.08354  GradNorm:0.00340  GradNormST:0.01781  AvgTextLen:21.8  AvgSpecLen:83.4  StepTime:0.55  LR:0.000100\n",
      "   | > Step:39/68  GlobalStep:69730  TotalLoss:0.00287  PostnetLoss:0.00138  DecoderLoss:0.00149  StopLoss:0.05989  GradNorm:0.00384  GradNormST:0.01290  AvgTextLen:30.2  AvgSpecLen:110.4  StepTime:0.76  LR:0.000100\n",
      "   | > Step:49/68  GlobalStep:69740  TotalLoss:0.00310  PostnetLoss:0.00148  DecoderLoss:0.00161  StopLoss:0.05313  GradNorm:0.00564  GradNormST:0.00997  AvgTextLen:38.2  AvgSpecLen:127.7  StepTime:0.74  LR:0.000100\n",
      "   | > Step:59/68  GlobalStep:69750  TotalLoss:0.00337  PostnetLoss:0.00161  DecoderLoss:0.00176  StopLoss:0.06314  GradNorm:0.00522  GradNormST:0.02274  AvgTextLen:50.5  AvgSpecLen:155.9  StepTime:0.81  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:69759  AvgTotalLoss:0.07423  AvgPostnetLoss:0.00130  AvgDecoderLoss:0.00141  AvgStopLoss:0.07151  EpochTime:43.16  AvgStepTime:0.63\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.11773   PostnetLoss: 0.00421   DecoderLoss:0.00477  StopLoss: 0.10875  \n",
      "   | > TotalLoss: 0.08491   PostnetLoss: 0.00552   DecoderLoss:0.00629  StopLoss: 0.07310  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00130   Validation Loss: 0.00514\n",
      "\n",
      " > Epoch 11/1000\n",
      "   | > Step:0/68  GlobalStep:69760  TotalLoss:0.00179  PostnetLoss:0.00086  DecoderLoss:0.00093  StopLoss:0.12648  GradNorm:0.00543  GradNormST:0.04348  AvgTextLen:3.0  AvgSpecLen:25.3  StepTime:0.57  LR:0.000100\n",
      "   | > Step:10/68  GlobalStep:69770  TotalLoss:0.00209  PostnetLoss:0.00101  DecoderLoss:0.00108  StopLoss:0.07002  GradNorm:0.00419  GradNormST:0.02280  AvgTextLen:9.8  AvgSpecLen:48.6  StepTime:0.51  LR:0.000100\n",
      "   | > Step:20/68  GlobalStep:69780  TotalLoss:0.00241  PostnetLoss:0.00116  DecoderLoss:0.00125  StopLoss:0.05996  GradNorm:0.00337  GradNormST:0.01333  AvgTextLen:16.4  AvgSpecLen:64.8  StepTime:0.40  LR:0.000100\n",
      "   | > Step:30/68  GlobalStep:69790  TotalLoss:0.00266  PostnetLoss:0.00127  DecoderLoss:0.00138  StopLoss:0.08048  GradNorm:0.00380  GradNormST:0.01747  AvgTextLen:22.9  AvgSpecLen:85.8  StepTime:0.49  LR:0.000100\n",
      "   | > Step:40/68  GlobalStep:69800  TotalLoss:0.00288  PostnetLoss:0.00138  DecoderLoss:0.00150  StopLoss:0.05256  GradNorm:0.00449  GradNormST:0.01244  AvgTextLen:30.9  AvgSpecLen:105.3  StepTime:0.56  LR:0.000100\n",
      "   | > Step:50/68  GlobalStep:69810  TotalLoss:0.00312  PostnetLoss:0.00149  DecoderLoss:0.00163  StopLoss:0.06535  GradNorm:0.00531  GradNormST:0.01667  AvgTextLen:38.8  AvgSpecLen:132.0  StepTime:0.79  LR:0.000100\n",
      "   | > Step:60/68  GlobalStep:69820  TotalLoss:0.00349  PostnetLoss:0.00167  DecoderLoss:0.00182  StopLoss:0.06101  GradNorm:0.00738  GradNormST:0.02091  AvgTextLen:52.0  AvgSpecLen:172.7  StepTime:0.97  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:69828  AvgTotalLoss:0.07375  AvgPostnetLoss:0.00131  AvgDecoderLoss:0.00142  AvgStopLoss:0.07102  EpochTime:43.31  AvgStepTime:0.63\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.12138   PostnetLoss: 0.00418   DecoderLoss:0.00473  StopLoss: 0.11248  \n",
      "   | > TotalLoss: 0.08163   PostnetLoss: 0.00556   DecoderLoss:0.00631  StopLoss: 0.06976  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00131   Validation Loss: 0.00509\n",
      "\n",
      " > Epoch 12/1000\n",
      "   | > Step:1/68  GlobalStep:69830  TotalLoss:0.00175  PostnetLoss:0.00085  DecoderLoss:0.00091  StopLoss:0.06842  GradNorm:0.00405  GradNormST:0.02686  AvgTextLen:3.7  AvgSpecLen:28.6  StepTime:0.38  LR:0.000100\n",
      "   | > Step:11/68  GlobalStep:69840  TotalLoss:0.00209  PostnetLoss:0.00101  DecoderLoss:0.00108  StopLoss:0.07256  GradNorm:0.00403  GradNormST:0.01871  AvgTextLen:10.3  AvgSpecLen:49.4  StepTime:0.39  LR:0.000100\n",
      "   | > Step:21/68  GlobalStep:69850  TotalLoss:0.00241  PostnetLoss:0.00116  DecoderLoss:0.00125  StopLoss:0.06400  GradNorm:0.00344  GradNormST:0.02034  AvgTextLen:16.6  AvgSpecLen:66.3  StepTime:0.44  LR:0.000100\n",
      "   | > Step:31/68  GlobalStep:69860  TotalLoss:0.00261  PostnetLoss:0.00126  DecoderLoss:0.00136  StopLoss:0.06745  GradNorm:0.00357  GradNormST:0.01355  AvgTextLen:24.1  AvgSpecLen:82.7  StepTime:0.57  LR:0.000100\n",
      "   | > Step:41/68  GlobalStep:69870  TotalLoss:0.00301  PostnetLoss:0.00145  DecoderLoss:0.00156  StopLoss:0.04513  GradNorm:0.00512  GradNormST:0.01145  AvgTextLen:32.3  AvgSpecLen:109.4  StepTime:0.64  LR:0.000100\n",
      "   | > Step:51/68  GlobalStep:69880  TotalLoss:0.00312  PostnetLoss:0.00150  DecoderLoss:0.00163  StopLoss:0.05616  GradNorm:0.00542  GradNormST:0.01068  AvgTextLen:39.2  AvgSpecLen:126.9  StepTime:0.75  LR:0.000100\n",
      "   | > Step:61/68  GlobalStep:69890  TotalLoss:0.00338  PostnetLoss:0.00162  DecoderLoss:0.00176  StopLoss:0.05225  GradNorm:0.00527  GradNormST:0.01517  AvgTextLen:54.6  AvgSpecLen:165.5  StepTime:0.92  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:69897  AvgTotalLoss:0.07301  AvgPostnetLoss:0.00131  AvgDecoderLoss:0.00142  AvgStopLoss:0.07029  EpochTime:42.65  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.11381   PostnetLoss: 0.00406   DecoderLoss:0.00461  StopLoss: 0.10514  \n",
      "   | > TotalLoss: 0.07869   PostnetLoss: 0.00544   DecoderLoss:0.00617  StopLoss: 0.06709  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00131   Validation Loss: 0.00510\n",
      "\n",
      " > Epoch 13/1000\n",
      "   | > Step:2/68  GlobalStep:69900  TotalLoss:0.00170  PostnetLoss:0.00083  DecoderLoss:0.00088  StopLoss:0.10830  GradNorm:0.00424  GradNormST:0.03418  AvgTextLen:4.0  AvgSpecLen:31.3  StepTime:0.42  LR:0.000100\n",
      "   | > Step:12/68  GlobalStep:69910  TotalLoss:0.00202  PostnetLoss:0.00098  DecoderLoss:0.00104  StopLoss:0.07826  GradNorm:0.00341  GradNormST:0.02714  AvgTextLen:11.0  AvgSpecLen:49.3  StepTime:0.44  LR:0.000100\n",
      "   | > Step:22/68  GlobalStep:69920  TotalLoss:0.00237  PostnetLoss:0.00114  DecoderLoss:0.00123  StopLoss:0.08145  GradNorm:0.00317  GradNormST:0.01929  AvgTextLen:18.0  AvgSpecLen:72.0  StepTime:0.41  LR:0.000100\n",
      "   | > Step:32/68  GlobalStep:69930  TotalLoss:0.00264  PostnetLoss:0.00127  DecoderLoss:0.00137  StopLoss:0.07784  GradNorm:0.00358  GradNormST:0.02472  AvgTextLen:24.4  AvgSpecLen:91.5  StepTime:0.57  LR:0.000100\n",
      "   | > Step:42/68  GlobalStep:69940  TotalLoss:0.00293  PostnetLoss:0.00140  DecoderLoss:0.00153  StopLoss:0.06879  GradNorm:0.00457  GradNormST:0.02783  AvgTextLen:33.1  AvgSpecLen:109.7  StepTime:0.64  LR:0.000100\n",
      "   | > Step:52/68  GlobalStep:69950  TotalLoss:0.00326  PostnetLoss:0.00156  DecoderLoss:0.00170  StopLoss:0.07097  GradNorm:0.00649  GradNormST:0.01730  AvgTextLen:42.1  AvgSpecLen:137.5  StepTime:0.86  LR:0.000100\n",
      "   | > Step:62/68  GlobalStep:69960  TotalLoss:0.00349  PostnetLoss:0.00167  DecoderLoss:0.00182  StopLoss:0.08662  GradNorm:0.00505  GradNormST:0.04221  AvgTextLen:54.0  AvgSpecLen:174.7  StepTime:0.75  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:69966  AvgTotalLoss:0.07376  AvgPostnetLoss:0.00131  AvgDecoderLoss:0.00141  AvgStopLoss:0.07104  EpochTime:42.88  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.11783   PostnetLoss: 0.00408   DecoderLoss:0.00460  StopLoss: 0.10915  \n",
      "   | > TotalLoss: 0.08130   PostnetLoss: 0.00573   DecoderLoss:0.00643  StopLoss: 0.06914  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00131   Validation Loss: 0.00521\n",
      "\n",
      " > Epoch 14/1000\n",
      "   | > Step:3/68  GlobalStep:69970  TotalLoss:0.00182  PostnetLoss:0.00088  DecoderLoss:0.00094  StopLoss:0.08743  GradNorm:0.00407  GradNormST:0.02041  AvgTextLen:4.7  AvgSpecLen:29.7  StepTime:0.29  LR:0.000100\n",
      "   | > Step:13/68  GlobalStep:69980  TotalLoss:0.00207  PostnetLoss:0.00100  DecoderLoss:0.00107  StopLoss:0.08902  GradNorm:0.00435  GradNormST:0.03059  AvgTextLen:11.5  AvgSpecLen:55.2  StepTime:0.40  LR:0.000100\n",
      "   | > Step:23/68  GlobalStep:69990  TotalLoss:0.00254  PostnetLoss:0.00122  DecoderLoss:0.00132  StopLoss:0.07761  GradNorm:0.00333  GradNormST:0.02031  AvgTextLen:18.3  AvgSpecLen:72.2  StepTime:0.46  LR:0.000100\n",
      "   | > Step:33/68  GlobalStep:70000  TotalLoss:0.00269  PostnetLoss:0.00129  DecoderLoss:0.00140  StopLoss:0.06328  GradNorm:0.00336  GradNormST:0.01184  AvgTextLen:25.9  AvgSpecLen:88.9  StepTime:0.43  LR:0.000100\n",
      " | | > Checkpoint saving : /vm/TTS/models/cartman_models/mozilla-no-loc-fattn-stopnet-sigmoid-loss_masking-August-15-2019_11+26AM-de6aeb9/checkpoint_70000.pth.tar\n",
      "   | > Step:43/68  GlobalStep:70010  TotalLoss:0.00301  PostnetLoss:0.00144  DecoderLoss:0.00157  StopLoss:0.06984  GradNorm:0.00484  GradNormST:0.02547  AvgTextLen:33.4  AvgSpecLen:115.7  StepTime:0.53  LR:0.000100\n",
      "   | > Step:53/68  GlobalStep:70020  TotalLoss:0.00321  PostnetLoss:0.00154  DecoderLoss:0.00167  StopLoss:0.05763  GradNorm:0.00591  GradNormST:0.01156  AvgTextLen:42.4  AvgSpecLen:137.6  StepTime:0.63  LR:0.000100\n",
      "   | > Step:63/68  GlobalStep:70030  TotalLoss:0.00354  PostnetLoss:0.00169  DecoderLoss:0.00185  StopLoss:0.05252  GradNorm:0.00507  GradNormST:0.01046  AvgTextLen:58.6  AvgSpecLen:185.1  StepTime:0.91  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:70035  AvgTotalLoss:0.07194  AvgPostnetLoss:0.00131  AvgDecoderLoss:0.00142  AvgStopLoss:0.06922  EpochTime:43.08  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.11155   PostnetLoss: 0.00407   DecoderLoss:0.00459  StopLoss: 0.10289  \n",
      "   | > TotalLoss: 0.07943   PostnetLoss: 0.00549   DecoderLoss:0.00625  StopLoss: 0.06768  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00131   Validation Loss: 0.00521\n",
      "\n",
      " > Epoch 15/1000\n",
      "   | > Step:4/68  GlobalStep:70040  TotalLoss:0.00185  PostnetLoss:0.00089  DecoderLoss:0.00096  StopLoss:0.11303  GradNorm:0.00538  GradNormST:0.04452  AvgTextLen:4.8  AvgSpecLen:31.1  StepTime:0.28  LR:0.000100\n",
      "   | > Step:14/68  GlobalStep:70050  TotalLoss:0.00218  PostnetLoss:0.00105  DecoderLoss:0.00113  StopLoss:0.08998  GradNorm:0.00397  GradNormST:0.02542  AvgTextLen:12.2  AvgSpecLen:52.0  StepTime:0.36  LR:0.000100\n",
      "   | > Step:24/68  GlobalStep:70060  TotalLoss:0.00259  PostnetLoss:0.00125  DecoderLoss:0.00135  StopLoss:0.06838  GradNorm:0.00343  GradNormST:0.01552  AvgTextLen:19.2  AvgSpecLen:79.0  StepTime:0.43  LR:0.000100\n",
      "   | > Step:34/68  GlobalStep:70070  TotalLoss:0.00282  PostnetLoss:0.00135  DecoderLoss:0.00147  StopLoss:0.05682  GradNorm:0.00365  GradNormST:0.00860  AvgTextLen:26.2  AvgSpecLen:95.9  StepTime:0.56  LR:0.000100\n",
      "   | > Step:44/68  GlobalStep:70080  TotalLoss:0.00302  PostnetLoss:0.00144  DecoderLoss:0.00157  StopLoss:0.05913  GradNorm:0.00423  GradNormST:0.01698  AvgTextLen:33.8  AvgSpecLen:116.3  StepTime:0.65  LR:0.000100\n",
      "   | > Step:54/68  GlobalStep:70090  TotalLoss:0.00325  PostnetLoss:0.00156  DecoderLoss:0.00170  StopLoss:0.05984  GradNorm:0.00531  GradNormST:0.01265  AvgTextLen:42.2  AvgSpecLen:144.9  StepTime:0.87  LR:0.000100\n",
      "   | > Step:64/68  GlobalStep:70100  TotalLoss:0.00331  PostnetLoss:0.00158  DecoderLoss:0.00173  StopLoss:0.06080  GradNorm:0.00401  GradNormST:0.01803  AvgTextLen:60.6  AvgSpecLen:177.6  StepTime:0.87  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:70104  AvgTotalLoss:0.07125  AvgPostnetLoss:0.00131  AvgDecoderLoss:0.00142  AvgStopLoss:0.06852  EpochTime:43.31  AvgStepTime:0.63\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.11546   PostnetLoss: 0.00402   DecoderLoss:0.00453  StopLoss: 0.10691  \n",
      "   | > TotalLoss: 0.08224   PostnetLoss: 0.00550   DecoderLoss:0.00626  StopLoss: 0.07048  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00131   Validation Loss: 0.00514\n",
      "\n",
      " > Epoch 16/1000\n",
      "   | > Step:5/68  GlobalStep:70110  TotalLoss:0.00188  PostnetLoss:0.00090  DecoderLoss:0.00098  StopLoss:0.10930  GradNorm:0.00503  GradNormST:0.03521  AvgTextLen:5.9  AvgSpecLen:33.2  StepTime:0.41  LR:0.000100\n",
      "   | > Step:15/68  GlobalStep:70120  TotalLoss:0.00232  PostnetLoss:0.00112  DecoderLoss:0.00120  StopLoss:0.06218  GradNorm:0.00436  GradNormST:0.02057  AvgTextLen:12.8  AvgSpecLen:59.1  StepTime:0.51  LR:0.000100\n",
      "   | > Step:25/68  GlobalStep:70130  TotalLoss:0.00248  PostnetLoss:0.00120  DecoderLoss:0.00128  StopLoss:0.06346  GradNorm:0.00404  GradNormST:0.01245  AvgTextLen:19.8  AvgSpecLen:73.3  StepTime:0.41  LR:0.000100\n",
      "   | > Step:35/68  GlobalStep:70140  TotalLoss:0.00281  PostnetLoss:0.00135  DecoderLoss:0.00146  StopLoss:0.07086  GradNorm:0.00440  GradNormST:0.02528  AvgTextLen:27.1  AvgSpecLen:104.7  StepTime:0.64  LR:0.000100\n",
      "   | > Step:45/68  GlobalStep:70150  TotalLoss:0.00293  PostnetLoss:0.00140  DecoderLoss:0.00153  StopLoss:0.07442  GradNorm:0.00441  GradNormST:0.03137  AvgTextLen:34.4  AvgSpecLen:118.6  StepTime:0.59  LR:0.000100\n",
      "   | > Step:55/68  GlobalStep:70160  TotalLoss:0.00328  PostnetLoss:0.00156  DecoderLoss:0.00171  StopLoss:0.05111  GradNorm:0.00492  GradNormST:0.00871  AvgTextLen:43.9  AvgSpecLen:145.2  StepTime:0.84  LR:0.000100\n",
      "   | > Step:65/68  GlobalStep:70170  TotalLoss:0.00356  PostnetLoss:0.00169  DecoderLoss:0.00187  StopLoss:0.05507  GradNorm:0.00426  GradNormST:0.01722  AvgTextLen:64.2  AvgSpecLen:207.4  StepTime:1.23  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:70173  AvgTotalLoss:0.07118  AvgPostnetLoss:0.00130  AvgDecoderLoss:0.00141  AvgStopLoss:0.06846  EpochTime:44.17  AvgStepTime:0.64\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.10863   PostnetLoss: 0.00406   DecoderLoss:0.00458  StopLoss: 0.09999  \n",
      "   | > TotalLoss: 0.08031   PostnetLoss: 0.00537   DecoderLoss:0.00609  StopLoss: 0.06886  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00130   Validation Loss: 0.00516\n",
      "\n",
      " > Epoch 17/1000\n",
      "   | > Step:6/68  GlobalStep:70180  TotalLoss:0.00188  PostnetLoss:0.00091  DecoderLoss:0.00097  StopLoss:0.07478  GradNorm:0.00420  GradNormST:0.03618  AvgTextLen:6.0  AvgSpecLen:34.8  StepTime:0.39  LR:0.000100\n",
      "   | > Step:16/68  GlobalStep:70190  TotalLoss:0.00238  PostnetLoss:0.00115  DecoderLoss:0.00123  StopLoss:0.06104  GradNorm:0.00505  GradNormST:0.02045  AvgTextLen:13.8  AvgSpecLen:60.6  StepTime:0.41  LR:0.000100\n",
      "   | > Step:26/68  GlobalStep:70200  TotalLoss:0.00249  PostnetLoss:0.00120  DecoderLoss:0.00129  StopLoss:0.06781  GradNorm:0.00340  GradNormST:0.01355  AvgTextLen:20.3  AvgSpecLen:77.5  StepTime:0.48  LR:0.000100\n",
      "   | > Step:36/68  GlobalStep:70210  TotalLoss:0.00279  PostnetLoss:0.00134  DecoderLoss:0.00145  StopLoss:0.06958  GradNorm:0.00488  GradNormST:0.01600  AvgTextLen:28.5  AvgSpecLen:100.2  StepTime:0.59  LR:0.000100\n",
      "   | > Step:46/68  GlobalStep:70220  TotalLoss:0.00302  PostnetLoss:0.00145  DecoderLoss:0.00157  StopLoss:0.06317  GradNorm:0.00534  GradNormST:0.01180  AvgTextLen:35.4  AvgSpecLen:122.6  StepTime:0.82  LR:0.000100\n",
      "   | > Step:56/68  GlobalStep:70230  TotalLoss:0.00325  PostnetLoss:0.00155  DecoderLoss:0.00170  StopLoss:0.05278  GradNorm:0.00456  GradNormST:0.02171  AvgTextLen:45.8  AvgSpecLen:145.4  StepTime:0.83  LR:0.000100\n",
      "   | > Step:66/68  GlobalStep:70240  TotalLoss:0.00373  PostnetLoss:0.00177  DecoderLoss:0.00196  StopLoss:0.04786  GradNorm:0.00343  GradNormST:0.01631  AvgTextLen:71.2  AvgSpecLen:220.8  StepTime:1.03  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:70242  AvgTotalLoss:0.07197  AvgPostnetLoss:0.00131  AvgDecoderLoss:0.00142  AvgStopLoss:0.06924  EpochTime:43.79  AvgStepTime:0.63\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.11881   PostnetLoss: 0.00410   DecoderLoss:0.00461  StopLoss: 0.11010  \n",
      "   | > TotalLoss: 0.08220   PostnetLoss: 0.00545   DecoderLoss:0.00618  StopLoss: 0.07057  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00131   Validation Loss: 0.00509\n",
      "\n",
      " > Epoch 18/1000\n",
      "   | > Step:7/68  GlobalStep:70250  TotalLoss:0.00201  PostnetLoss:0.00097  DecoderLoss:0.00104  StopLoss:0.07004  GradNorm:0.00493  GradNormST:0.03610  AvgTextLen:7.4  AvgSpecLen:44.6  StepTime:0.33  LR:0.000100\n",
      "   | > Step:17/68  GlobalStep:70260  TotalLoss:0.00230  PostnetLoss:0.00111  DecoderLoss:0.00119  StopLoss:0.07399  GradNorm:0.00391  GradNormST:0.02388  AvgTextLen:14.7  AvgSpecLen:66.0  StepTime:0.41  LR:0.000100\n",
      "   | > Step:27/68  GlobalStep:70270  TotalLoss:0.00260  PostnetLoss:0.00125  DecoderLoss:0.00135  StopLoss:0.06394  GradNorm:0.00348  GradNormST:0.01918  AvgTextLen:21.0  AvgSpecLen:80.2  StepTime:0.58  LR:0.000100\n",
      "   | > Step:37/68  GlobalStep:70280  TotalLoss:0.00283  PostnetLoss:0.00136  DecoderLoss:0.00147  StopLoss:0.06578  GradNorm:0.00449  GradNormST:0.01549  AvgTextLen:29.3  AvgSpecLen:102.4  StepTime:0.59  LR:0.000100\n",
      "   | > Step:47/68  GlobalStep:70290  TotalLoss:0.00309  PostnetLoss:0.00148  DecoderLoss:0.00161  StopLoss:0.06418  GradNorm:0.00540  GradNormST:0.01683  AvgTextLen:36.7  AvgSpecLen:126.9  StepTime:0.63  LR:0.000100\n",
      "   | > Step:57/68  GlobalStep:70300  TotalLoss:0.00330  PostnetLoss:0.00157  DecoderLoss:0.00173  StopLoss:0.05181  GradNorm:0.00398  GradNormST:0.01423  AvgTextLen:47.1  AvgSpecLen:161.0  StepTime:0.76  LR:0.000100\n",
      "   | > Step:67/68  GlobalStep:70310  TotalLoss:0.00368  PostnetLoss:0.00175  DecoderLoss:0.00194  StopLoss:0.04814  GradNorm:0.00300  GradNormST:0.01818  AvgTextLen:80.6  AvgSpecLen:246.5  StepTime:1.22  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:70311  AvgTotalLoss:0.06977  AvgPostnetLoss:0.00130  AvgDecoderLoss:0.00141  AvgStopLoss:0.06705  EpochTime:43.70  AvgStepTime:0.63\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.11333   PostnetLoss: 0.00418   DecoderLoss:0.00471  StopLoss: 0.10444  \n",
      "   | > TotalLoss: 0.08229   PostnetLoss: 0.00555   DecoderLoss:0.00632  StopLoss: 0.07042  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00130   Validation Loss: 0.00515\n",
      "\n",
      " > Epoch 19/1000\n",
      "   | > Step:8/68  GlobalStep:70320  TotalLoss:0.00211  PostnetLoss:0.00102  DecoderLoss:0.00109  StopLoss:0.07904  GradNorm:0.00630  GradNormST:0.02228  AvgTextLen:8.3  AvgSpecLen:44.0  StepTime:0.25  LR:0.000100\n",
      "   | > Step:18/68  GlobalStep:70330  TotalLoss:0.00227  PostnetLoss:0.00109  DecoderLoss:0.00117  StopLoss:0.06522  GradNorm:0.00482  GradNormST:0.02067  AvgTextLen:15.2  AvgSpecLen:64.5  StepTime:0.33  LR:0.000100\n",
      "   | > Step:28/68  GlobalStep:70340  TotalLoss:0.00258  PostnetLoss:0.00125  DecoderLoss:0.00134  StopLoss:0.07457  GradNorm:0.00387  GradNormST:0.01603  AvgTextLen:22.1  AvgSpecLen:85.4  StepTime:0.53  LR:0.000100\n",
      "   | > Step:38/68  GlobalStep:70350  TotalLoss:0.00279  PostnetLoss:0.00134  DecoderLoss:0.00145  StopLoss:0.05251  GradNorm:0.00382  GradNormST:0.00970  AvgTextLen:29.4  AvgSpecLen:104.1  StepTime:0.61  LR:0.000100\n",
      "   | > Step:48/68  GlobalStep:70360  TotalLoss:0.00305  PostnetLoss:0.00146  DecoderLoss:0.00158  StopLoss:0.04968  GradNorm:0.00458  GradNormST:0.01433  AvgTextLen:37.5  AvgSpecLen:123.8  StepTime:0.73  LR:0.000100\n",
      "   | > Step:58/68  GlobalStep:70370  TotalLoss:0.00336  PostnetLoss:0.00161  DecoderLoss:0.00175  StopLoss:0.05796  GradNorm:0.00454  GradNormST:0.01940  AvgTextLen:47.8  AvgSpecLen:158.0  StepTime:0.84  LR:0.000100\n",
      "   | > Step:68/68  GlobalStep:70380  TotalLoss:0.00361  PostnetLoss:0.00172  DecoderLoss:0.00190  StopLoss:0.05345  GradNorm:0.00310  GradNormST:0.02151  AvgTextLen:103.0  AvgSpecLen:308.9  StepTime:1.94  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:70380  AvgTotalLoss:0.07100  AvgPostnetLoss:0.00130  AvgDecoderLoss:0.00141  AvgStopLoss:0.06828  EpochTime:42.87  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.11412   PostnetLoss: 0.00420   DecoderLoss:0.00473  StopLoss: 0.10518  \n",
      "   | > TotalLoss: 0.08444   PostnetLoss: 0.00585   DecoderLoss:0.00664  StopLoss: 0.07195  \n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00130   Validation Loss: 0.00529\n",
      "\n",
      " > Epoch 20/1000\n",
      "   | > Step:9/68  GlobalStep:70390  TotalLoss:0.00200  PostnetLoss:0.00096  DecoderLoss:0.00103  StopLoss:0.06464  GradNorm:0.00457  GradNormST:0.03063  AvgTextLen:9.0  AvgSpecLen:42.1  StepTime:0.34  LR:0.000100\n",
      "   | > Step:19/68  GlobalStep:70400  TotalLoss:0.00232  PostnetLoss:0.00112  DecoderLoss:0.00120  StopLoss:0.08951  GradNorm:0.00538  GradNormST:0.03495  AvgTextLen:15.5  AvgSpecLen:63.2  StepTime:0.42  LR:0.000100\n",
      "   | > Step:29/68  GlobalStep:70410  TotalLoss:0.00251  PostnetLoss:0.00121  DecoderLoss:0.00130  StopLoss:0.07890  GradNorm:0.00335  GradNormST:0.01653  AvgTextLen:21.8  AvgSpecLen:83.4  StepTime:0.46  LR:0.000100\n",
      "   | > Step:39/68  GlobalStep:70420  TotalLoss:0.00287  PostnetLoss:0.00138  DecoderLoss:0.00149  StopLoss:0.05346  GradNorm:0.00354  GradNormST:0.00861  AvgTextLen:30.2  AvgSpecLen:110.4  StepTime:0.57  LR:0.000100\n",
      "   | > Step:49/68  GlobalStep:70430  TotalLoss:0.00312  PostnetLoss:0.00150  DecoderLoss:0.00163  StopLoss:0.04978  GradNorm:0.00542  GradNormST:0.01335  AvgTextLen:38.2  AvgSpecLen:127.7  StepTime:0.72  LR:0.000100\n",
      "   | > Step:59/68  GlobalStep:70440  TotalLoss:0.00324  PostnetLoss:0.00155  DecoderLoss:0.00170  StopLoss:0.05561  GradNorm:0.00373  GradNormST:0.01160  AvgTextLen:50.5  AvgSpecLen:155.9  StepTime:0.72  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:70449  AvgTotalLoss:0.06981  AvgPostnetLoss:0.00130  AvgDecoderLoss:0.00141  AvgStopLoss:0.06709  EpochTime:42.38  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.10695   PostnetLoss: 0.00420   DecoderLoss:0.00472  StopLoss: 0.09803  \n",
      "   | > TotalLoss: 0.07863   PostnetLoss: 0.00569   DecoderLoss:0.00643  StopLoss: 0.06650  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00130   Validation Loss: 0.00517\n",
      "\n",
      " > Epoch 21/1000\n",
      "   | > Step:0/68  GlobalStep:70450  TotalLoss:0.00183  PostnetLoss:0.00087  DecoderLoss:0.00096  StopLoss:0.09530  GradNorm:0.00635  GradNormST:0.03139  AvgTextLen:3.0  AvgSpecLen:25.3  StepTime:0.43  LR:0.000100\n",
      "   | > Step:10/68  GlobalStep:70460  TotalLoss:0.00208  PostnetLoss:0.00101  DecoderLoss:0.00108  StopLoss:0.09073  GradNorm:0.00439  GradNormST:0.04470  AvgTextLen:9.8  AvgSpecLen:48.6  StepTime:0.43  LR:0.000100\n",
      "   | > Step:20/68  GlobalStep:70470  TotalLoss:0.00247  PostnetLoss:0.00119  DecoderLoss:0.00127  StopLoss:0.05654  GradNorm:0.00540  GradNormST:0.00951  AvgTextLen:16.4  AvgSpecLen:64.8  StepTime:0.40  LR:0.000100\n",
      "   | > Step:30/68  GlobalStep:70480  TotalLoss:0.00263  PostnetLoss:0.00126  DecoderLoss:0.00136  StopLoss:0.06901  GradNorm:0.00431  GradNormST:0.01190  AvgTextLen:22.9  AvgSpecLen:85.8  StepTime:0.58  LR:0.000100\n",
      "   | > Step:40/68  GlobalStep:70490  TotalLoss:0.00285  PostnetLoss:0.00137  DecoderLoss:0.00148  StopLoss:0.05245  GradNorm:0.00359  GradNormST:0.01017  AvgTextLen:30.9  AvgSpecLen:105.3  StepTime:0.63  LR:0.000100\n",
      "   | > Step:50/68  GlobalStep:70500  TotalLoss:0.00311  PostnetLoss:0.00149  DecoderLoss:0.00162  StopLoss:0.06193  GradNorm:0.00508  GradNormST:0.01186  AvgTextLen:38.8  AvgSpecLen:132.0  StepTime:0.86  LR:0.000100\n",
      "   | > Step:60/68  GlobalStep:70510  TotalLoss:0.00339  PostnetLoss:0.00162  DecoderLoss:0.00177  StopLoss:0.04651  GradNorm:0.00428  GradNormST:0.00838  AvgTextLen:52.0  AvgSpecLen:172.7  StepTime:0.94  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:70518  AvgTotalLoss:0.06978  AvgPostnetLoss:0.00130  AvgDecoderLoss:0.00141  AvgStopLoss:0.06707  EpochTime:42.94  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.11411   PostnetLoss: 0.00425   DecoderLoss:0.00476  StopLoss: 0.10510  \n",
      "   | > TotalLoss: 0.08200   PostnetLoss: 0.00598   DecoderLoss:0.00673  StopLoss: 0.06929  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00130   Validation Loss: 0.00542\n",
      "\n",
      " > Epoch 22/1000\n",
      "   | > Step:1/68  GlobalStep:70520  TotalLoss:0.00180  PostnetLoss:0.00087  DecoderLoss:0.00093  StopLoss:0.10293  GradNorm:0.00470  GradNormST:0.03710  AvgTextLen:3.7  AvgSpecLen:28.6  StepTime:0.36  LR:0.000100\n",
      "   | > Step:11/68  GlobalStep:70530  TotalLoss:0.00209  PostnetLoss:0.00101  DecoderLoss:0.00108  StopLoss:0.07213  GradNorm:0.00404  GradNormST:0.02751  AvgTextLen:10.3  AvgSpecLen:49.4  StepTime:0.34  LR:0.000100\n",
      "   | > Step:21/68  GlobalStep:70540  TotalLoss:0.00242  PostnetLoss:0.00117  DecoderLoss:0.00125  StopLoss:0.06396  GradNorm:0.00446  GradNormST:0.01292  AvgTextLen:16.6  AvgSpecLen:66.3  StepTime:0.43  LR:0.000100\n",
      "   | > Step:31/68  GlobalStep:70550  TotalLoss:0.00267  PostnetLoss:0.00128  DecoderLoss:0.00138  StopLoss:0.06852  GradNorm:0.00414  GradNormST:0.01741  AvgTextLen:24.1  AvgSpecLen:82.7  StepTime:0.52  LR:0.000100\n",
      "   | > Step:41/68  GlobalStep:70560  TotalLoss:0.00290  PostnetLoss:0.00139  DecoderLoss:0.00151  StopLoss:0.04215  GradNorm:0.00327  GradNormST:0.01052  AvgTextLen:32.3  AvgSpecLen:109.4  StepTime:0.53  LR:0.000100\n",
      "   | > Step:51/68  GlobalStep:70570  TotalLoss:0.00303  PostnetLoss:0.00145  DecoderLoss:0.00158  StopLoss:0.05516  GradNorm:0.00376  GradNormST:0.01339  AvgTextLen:39.2  AvgSpecLen:126.9  StepTime:0.73  LR:0.000100\n",
      "   | > Step:61/68  GlobalStep:70580  TotalLoss:0.00328  PostnetLoss:0.00156  DecoderLoss:0.00171  StopLoss:0.06185  GradNorm:0.00307  GradNormST:0.03092  AvgTextLen:54.6  AvgSpecLen:165.5  StepTime:0.90  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:70587  AvgTotalLoss:0.07087  AvgPostnetLoss:0.00129  AvgDecoderLoss:0.00140  AvgStopLoss:0.06818  EpochTime:41.85  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.11190   PostnetLoss: 0.00422   DecoderLoss:0.00475  StopLoss: 0.10294  \n",
      "   | > TotalLoss: 0.08132   PostnetLoss: 0.00607   DecoderLoss:0.00684  StopLoss: 0.06841  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00129   Validation Loss: 0.00544\n",
      "\n",
      " > Epoch 23/1000\n",
      "   | > Step:2/68  GlobalStep:70590  TotalLoss:0.00174  PostnetLoss:0.00084  DecoderLoss:0.00090  StopLoss:0.10541  GradNorm:0.00403  GradNormST:0.02219  AvgTextLen:4.0  AvgSpecLen:31.3  StepTime:0.58  LR:0.000100\n",
      "   | > Step:12/68  GlobalStep:70600  TotalLoss:0.00209  PostnetLoss:0.00101  DecoderLoss:0.00109  StopLoss:0.08561  GradNorm:0.00410  GradNormST:0.04150  AvgTextLen:11.0  AvgSpecLen:49.3  StepTime:0.43  LR:0.000100\n",
      "   | > Step:22/68  GlobalStep:70610  TotalLoss:0.00245  PostnetLoss:0.00118  DecoderLoss:0.00127  StopLoss:0.08210  GradNorm:0.00428  GradNormST:0.01823  AvgTextLen:18.0  AvgSpecLen:72.0  StepTime:0.49  LR:0.000100\n",
      "   | > Step:32/68  GlobalStep:70620  TotalLoss:0.00266  PostnetLoss:0.00128  DecoderLoss:0.00138  StopLoss:0.07897  GradNorm:0.00401  GradNormST:0.01554  AvgTextLen:24.4  AvgSpecLen:91.5  StepTime:0.60  LR:0.000100\n",
      "   | > Step:42/68  GlobalStep:70630  TotalLoss:0.00287  PostnetLoss:0.00137  DecoderLoss:0.00150  StopLoss:0.06300  GradNorm:0.00311  GradNormST:0.01556  AvgTextLen:33.1  AvgSpecLen:109.7  StepTime:0.60  LR:0.000100\n",
      "   | > Step:52/68  GlobalStep:70640  TotalLoss:0.00313  PostnetLoss:0.00150  DecoderLoss:0.00163  StopLoss:0.06143  GradNorm:0.00314  GradNormST:0.01630  AvgTextLen:42.1  AvgSpecLen:137.5  StepTime:0.81  LR:0.000100\n",
      "   | > Step:62/68  GlobalStep:70650  TotalLoss:0.00338  PostnetLoss:0.00161  DecoderLoss:0.00177  StopLoss:0.06135  GradNorm:0.00276  GradNormST:0.01605  AvgTextLen:54.0  AvgSpecLen:174.7  StepTime:0.77  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:70656  AvgTotalLoss:0.07337  AvgPostnetLoss:0.00129  AvgDecoderLoss:0.00140  AvgStopLoss:0.07068  EpochTime:43.14  AvgStepTime:0.63\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.11799   PostnetLoss: 0.00427   DecoderLoss:0.00481  StopLoss: 0.10892  \n",
      "   | > TotalLoss: 0.08490   PostnetLoss: 0.00617   DecoderLoss:0.00696  StopLoss: 0.07177  \n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00129   Validation Loss: 0.00545\n",
      "\n",
      " > Epoch 24/1000\n",
      "   | > Step:3/68  GlobalStep:70660  TotalLoss:0.00175  PostnetLoss:0.00085  DecoderLoss:0.00090  StopLoss:0.08992  GradNorm:0.00397  GradNormST:0.01865  AvgTextLen:4.7  AvgSpecLen:29.7  StepTime:0.29  LR:0.000100\n",
      "   | > Step:13/68  GlobalStep:70670  TotalLoss:0.00208  PostnetLoss:0.00101  DecoderLoss:0.00108  StopLoss:0.10851  GradNorm:0.00395  GradNormST:0.02367  AvgTextLen:11.5  AvgSpecLen:55.2  StepTime:0.33  LR:0.000100\n",
      "   | > Step:23/68  GlobalStep:70680  TotalLoss:0.00248  PostnetLoss:0.00120  DecoderLoss:0.00128  StopLoss:0.07041  GradNorm:0.00428  GradNormST:0.01805  AvgTextLen:18.3  AvgSpecLen:72.2  StepTime:0.34  LR:0.000100\n",
      "   | > Step:33/68  GlobalStep:70690  TotalLoss:0.00277  PostnetLoss:0.00133  DecoderLoss:0.00144  StopLoss:0.06915  GradNorm:0.00493  GradNormST:0.01375  AvgTextLen:25.9  AvgSpecLen:88.9  StepTime:0.57  LR:0.000100\n",
      "   | > Step:43/68  GlobalStep:70700  TotalLoss:0.00287  PostnetLoss:0.00137  DecoderLoss:0.00149  StopLoss:0.07274  GradNorm:0.00369  GradNormST:0.01960  AvgTextLen:33.4  AvgSpecLen:115.7  StepTime:0.61  LR:0.000100\n",
      "   | > Step:53/68  GlobalStep:70710  TotalLoss:0.00304  PostnetLoss:0.00145  DecoderLoss:0.00158  StopLoss:0.05514  GradNorm:0.00285  GradNormST:0.01122  AvgTextLen:42.4  AvgSpecLen:137.6  StepTime:0.82  LR:0.000100\n",
      "   | > Step:63/68  GlobalStep:70720  TotalLoss:0.00343  PostnetLoss:0.00164  DecoderLoss:0.00179  StopLoss:0.04990  GradNorm:0.00297  GradNormST:0.01597  AvgTextLen:58.6  AvgSpecLen:185.1  StepTime:0.93  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:70725  AvgTotalLoss:0.07297  AvgPostnetLoss:0.00128  AvgDecoderLoss:0.00139  AvgStopLoss:0.07030  EpochTime:42.59  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.11361   PostnetLoss: 0.00424   DecoderLoss:0.00478  StopLoss: 0.10458  \n",
      "   | > TotalLoss: 0.08049   PostnetLoss: 0.00616   DecoderLoss:0.00691  StopLoss: 0.06742  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00128   Validation Loss: 0.00545\n",
      "\n",
      " > Epoch 25/1000\n",
      "   | > Step:4/68  GlobalStep:70730  TotalLoss:0.00182  PostnetLoss:0.00088  DecoderLoss:0.00094  StopLoss:0.12857  GradNorm:0.00729  GradNormST:0.05585  AvgTextLen:4.8  AvgSpecLen:31.1  StepTime:0.28  LR:0.000100\n",
      "   | > Step:14/68  GlobalStep:70740  TotalLoss:0.00214  PostnetLoss:0.00103  DecoderLoss:0.00110  StopLoss:0.07622  GradNorm:0.00408  GradNormST:0.01994  AvgTextLen:12.2  AvgSpecLen:52.0  StepTime:0.37  LR:0.000100\n",
      "   | > Step:24/68  GlobalStep:70750  TotalLoss:0.00260  PostnetLoss:0.00125  DecoderLoss:0.00134  StopLoss:0.05741  GradNorm:0.00490  GradNormST:0.01390  AvgTextLen:19.2  AvgSpecLen:79.0  StepTime:0.54  LR:0.000100\n",
      "   | > Step:34/68  GlobalStep:70760  TotalLoss:0.00277  PostnetLoss:0.00133  DecoderLoss:0.00144  StopLoss:0.06501  GradNorm:0.00365  GradNormST:0.01342  AvgTextLen:26.2  AvgSpecLen:95.9  StepTime:0.63  LR:0.000100\n",
      "   | > Step:44/68  GlobalStep:70770  TotalLoss:0.00298  PostnetLoss:0.00143  DecoderLoss:0.00155  StopLoss:0.06226  GradNorm:0.00368  GradNormST:0.01474  AvgTextLen:33.8  AvgSpecLen:116.3  StepTime:0.67  LR:0.000100\n",
      "   | > Step:54/68  GlobalStep:70780  TotalLoss:0.00318  PostnetLoss:0.00152  DecoderLoss:0.00166  StopLoss:0.05582  GradNorm:0.00305  GradNormST:0.00924  AvgTextLen:42.2  AvgSpecLen:144.9  StepTime:0.81  LR:0.000100\n",
      "   | > Step:64/68  GlobalStep:70790  TotalLoss:0.00324  PostnetLoss:0.00154  DecoderLoss:0.00169  StopLoss:0.05697  GradNorm:0.00310  GradNormST:0.01514  AvgTextLen:60.6  AvgSpecLen:177.6  StepTime:0.81  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:70794  AvgTotalLoss:0.07091  AvgPostnetLoss:0.00128  AvgDecoderLoss:0.00139  AvgStopLoss:0.06824  EpochTime:43.00  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.11162   PostnetLoss: 0.00432   DecoderLoss:0.00488  StopLoss: 0.10243  \n",
      "   | > TotalLoss: 0.08162   PostnetLoss: 0.00622   DecoderLoss:0.00700  StopLoss: 0.06840  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00128   Validation Loss: 0.00539\n",
      "\n",
      " > Epoch 26/1000\n",
      "   | > Step:5/68  GlobalStep:70800  TotalLoss:0.00185  PostnetLoss:0.00089  DecoderLoss:0.00096  StopLoss:0.08483  GradNorm:0.00422  GradNormST:0.02217  AvgTextLen:5.9  AvgSpecLen:33.2  StepTime:0.35  LR:0.000100\n",
      "   | > Step:15/68  GlobalStep:70810  TotalLoss:0.00233  PostnetLoss:0.00113  DecoderLoss:0.00120  StopLoss:0.06915  GradNorm:0.00620  GradNormST:0.04041  AvgTextLen:12.8  AvgSpecLen:59.1  StepTime:0.55  LR:0.000100\n",
      "   | > Step:25/68  GlobalStep:70820  TotalLoss:0.00248  PostnetLoss:0.00120  DecoderLoss:0.00128  StopLoss:0.06803  GradNorm:0.00465  GradNormST:0.02283  AvgTextLen:19.8  AvgSpecLen:73.3  StepTime:0.36  LR:0.000100\n",
      "   | > Step:35/68  GlobalStep:70830  TotalLoss:0.00276  PostnetLoss:0.00133  DecoderLoss:0.00144  StopLoss:0.07129  GradNorm:0.00383  GradNormST:0.02862  AvgTextLen:27.1  AvgSpecLen:104.7  StepTime:0.75  LR:0.000100\n",
      "   | > Step:45/68  GlobalStep:70840  TotalLoss:0.00288  PostnetLoss:0.00138  DecoderLoss:0.00150  StopLoss:0.06952  GradNorm:0.00399  GradNormST:0.02174  AvgTextLen:34.4  AvgSpecLen:118.6  StepTime:0.54  LR:0.000100\n",
      "   | > Step:55/68  GlobalStep:70850  TotalLoss:0.00322  PostnetLoss:0.00154  DecoderLoss:0.00168  StopLoss:0.05842  GradNorm:0.00339  GradNormST:0.00926  AvgTextLen:43.9  AvgSpecLen:145.2  StepTime:0.67  LR:0.000100\n",
      "   | > Step:65/68  GlobalStep:70860  TotalLoss:0.00348  PostnetLoss:0.00166  DecoderLoss:0.00182  StopLoss:0.05247  GradNorm:0.00310  GradNormST:0.01662  AvgTextLen:64.2  AvgSpecLen:207.4  StepTime:1.20  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:70863  AvgTotalLoss:0.07003  AvgPostnetLoss:0.00129  AvgDecoderLoss:0.00140  AvgStopLoss:0.06735  EpochTime:42.74  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.11399   PostnetLoss: 0.00424   DecoderLoss:0.00476  StopLoss: 0.10499  \n",
      "   | > TotalLoss: 0.08333   PostnetLoss: 0.00584   DecoderLoss:0.00657  StopLoss: 0.07091  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00129   Validation Loss: 0.00522\n",
      "\n",
      " > Epoch 27/1000\n",
      "   | > Step:6/68  GlobalStep:70870  TotalLoss:0.00186  PostnetLoss:0.00090  DecoderLoss:0.00096  StopLoss:0.07074  GradNorm:0.00433  GradNormST:0.03651  AvgTextLen:6.0  AvgSpecLen:34.8  StepTime:0.36  LR:0.000100\n",
      "   | > Step:16/68  GlobalStep:70880  TotalLoss:0.00228  PostnetLoss:0.00110  DecoderLoss:0.00118  StopLoss:0.05327  GradNorm:0.00460  GradNormST:0.01169  AvgTextLen:13.8  AvgSpecLen:60.6  StepTime:0.39  LR:0.000100\n",
      "   | > Step:26/68  GlobalStep:70890  TotalLoss:0.00258  PostnetLoss:0.00125  DecoderLoss:0.00133  StopLoss:0.06625  GradNorm:0.00545  GradNormST:0.01488  AvgTextLen:20.3  AvgSpecLen:77.5  StepTime:0.50  LR:0.000100\n",
      "   | > Step:36/68  GlobalStep:70900  TotalLoss:0.00277  PostnetLoss:0.00133  DecoderLoss:0.00144  StopLoss:0.06232  GradNorm:0.00492  GradNormST:0.01377  AvgTextLen:28.5  AvgSpecLen:100.2  StepTime:0.51  LR:0.000100\n",
      "   | > Step:46/68  GlobalStep:70910  TotalLoss:0.00293  PostnetLoss:0.00141  DecoderLoss:0.00153  StopLoss:0.06476  GradNorm:0.00409  GradNormST:0.01212  AvgTextLen:35.4  AvgSpecLen:122.6  StepTime:0.87  LR:0.000100\n",
      "   | > Step:56/68  GlobalStep:70920  TotalLoss:0.00318  PostnetLoss:0.00152  DecoderLoss:0.00166  StopLoss:0.04914  GradNorm:0.00336  GradNormST:0.01925  AvgTextLen:45.8  AvgSpecLen:145.4  StepTime:1.07  LR:0.000100\n",
      "   | > Step:66/68  GlobalStep:70930  TotalLoss:0.00366  PostnetLoss:0.00174  DecoderLoss:0.00192  StopLoss:0.04853  GradNorm:0.00298  GradNormST:0.01347  AvgTextLen:71.2  AvgSpecLen:220.8  StepTime:1.05  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:70932  AvgTotalLoss:0.07062  AvgPostnetLoss:0.00129  AvgDecoderLoss:0.00140  AvgStopLoss:0.06793  EpochTime:43.32  AvgStepTime:0.63\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.12278   PostnetLoss: 0.00428   DecoderLoss:0.00481  StopLoss: 0.11369  \n",
      "   | > TotalLoss: 0.08751   PostnetLoss: 0.00621   DecoderLoss:0.00697  StopLoss: 0.07433  \n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00129   Validation Loss: 0.00548\n",
      "\n",
      " > Epoch 28/1000\n",
      "   | > Step:7/68  GlobalStep:70940  TotalLoss:0.00199  PostnetLoss:0.00096  DecoderLoss:0.00102  StopLoss:0.08192  GradNorm:0.00478  GradNormST:0.03764  AvgTextLen:7.4  AvgSpecLen:44.6  StepTime:0.40  LR:0.000100\n",
      "   | > Step:17/68  GlobalStep:70950  TotalLoss:0.00228  PostnetLoss:0.00110  DecoderLoss:0.00118  StopLoss:0.09041  GradNorm:0.00412  GradNormST:0.01638  AvgTextLen:14.7  AvgSpecLen:66.0  StepTime:0.41  LR:0.000100\n",
      "   | > Step:27/68  GlobalStep:70960  TotalLoss:0.00267  PostnetLoss:0.00129  DecoderLoss:0.00138  StopLoss:0.08286  GradNorm:0.00500  GradNormST:0.02000  AvgTextLen:21.0  AvgSpecLen:80.2  StepTime:0.63  LR:0.000100\n",
      "   | > Step:37/68  GlobalStep:70970  TotalLoss:0.00286  PostnetLoss:0.00138  DecoderLoss:0.00148  StopLoss:0.05971  GradNorm:0.00567  GradNormST:0.01091  AvgTextLen:29.3  AvgSpecLen:102.4  StepTime:0.51  LR:0.000100\n",
      "   | > Step:47/68  GlobalStep:70980  TotalLoss:0.00304  PostnetLoss:0.00146  DecoderLoss:0.00158  StopLoss:0.06828  GradNorm:0.00445  GradNormST:0.02088  AvgTextLen:36.7  AvgSpecLen:126.9  StepTime:0.70  LR:0.000100\n",
      "   | > Step:57/68  GlobalStep:70990  TotalLoss:0.00328  PostnetLoss:0.00157  DecoderLoss:0.00171  StopLoss:0.04822  GradNorm:0.00377  GradNormST:0.00893  AvgTextLen:47.1  AvgSpecLen:161.0  StepTime:0.75  LR:0.000100\n",
      "   | > Step:67/68  GlobalStep:71000  TotalLoss:0.00362  PostnetLoss:0.00172  DecoderLoss:0.00190  StopLoss:0.05329  GradNorm:0.00281  GradNormST:0.01890  AvgTextLen:80.6  AvgSpecLen:246.5  StepTime:1.25  LR:0.000100\n",
      " | | > Checkpoint saving : /vm/TTS/models/cartman_models/mozilla-no-loc-fattn-stopnet-sigmoid-loss_masking-August-15-2019_11+26AM-de6aeb9/checkpoint_71000.pth.tar\n",
      "warning: audio amplitude out of range, auto clipped.\n",
      "   | > EPOCH END -- GlobalStep:71001  AvgTotalLoss:0.07091  AvgPostnetLoss:0.00129  AvgDecoderLoss:0.00140  AvgStopLoss:0.06822  EpochTime:44.50  AvgStepTime:0.64\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.11793   PostnetLoss: 0.00423   DecoderLoss:0.00475  StopLoss: 0.10894  \n",
      "   | > TotalLoss: 0.08563   PostnetLoss: 0.00604   DecoderLoss:0.00678  StopLoss: 0.07281  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00129   Validation Loss: 0.00545\n",
      "\n",
      " > Epoch 29/1000\n",
      "   | > Step:8/68  GlobalStep:71010  TotalLoss:0.00201  PostnetLoss:0.00097  DecoderLoss:0.00104  StopLoss:0.08372  GradNorm:0.00434  GradNormST:0.01838  AvgTextLen:8.3  AvgSpecLen:44.0  StepTime:0.31  LR:0.000100\n",
      "   | > Step:18/68  GlobalStep:71020  TotalLoss:0.00220  PostnetLoss:0.00106  DecoderLoss:0.00114  StopLoss:0.07153  GradNorm:0.00332  GradNormST:0.01860  AvgTextLen:15.2  AvgSpecLen:64.5  StepTime:0.43  LR:0.000100\n",
      "   | > Step:28/68  GlobalStep:71030  TotalLoss:0.00258  PostnetLoss:0.00124  DecoderLoss:0.00133  StopLoss:0.07083  GradNorm:0.00441  GradNormST:0.01702  AvgTextLen:22.1  AvgSpecLen:85.4  StepTime:0.59  LR:0.000100\n",
      "   | > Step:38/68  GlobalStep:71040  TotalLoss:0.00282  PostnetLoss:0.00136  DecoderLoss:0.00146  StopLoss:0.05237  GradNorm:0.00513  GradNormST:0.00919  AvgTextLen:29.4  AvgSpecLen:104.1  StepTime:0.65  LR:0.000100\n",
      "   | > Step:48/68  GlobalStep:71050  TotalLoss:0.00305  PostnetLoss:0.00146  DecoderLoss:0.00158  StopLoss:0.06030  GradNorm:0.00514  GradNormST:0.01313  AvgTextLen:37.5  AvgSpecLen:123.8  StepTime:0.85  LR:0.000100\n",
      "   | > Step:58/68  GlobalStep:71060  TotalLoss:0.00333  PostnetLoss:0.00160  DecoderLoss:0.00173  StopLoss:0.05974  GradNorm:0.00497  GradNormST:0.01353  AvgTextLen:47.8  AvgSpecLen:158.0  StepTime:0.73  LR:0.000100\n",
      "   | > Step:68/68  GlobalStep:71070  TotalLoss:0.00357  PostnetLoss:0.00169  DecoderLoss:0.00187  StopLoss:0.05002  GradNorm:0.00357  GradNormST:0.01870  AvgTextLen:103.0  AvgSpecLen:308.9  StepTime:2.00  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:71070  AvgTotalLoss:0.07118  AvgPostnetLoss:0.00129  AvgDecoderLoss:0.00139  AvgStopLoss:0.06850  EpochTime:43.74  AvgStepTime:0.63\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.11523   PostnetLoss: 0.00428   DecoderLoss:0.00482  StopLoss: 0.10613  \n",
      "   | > TotalLoss: 0.08427   PostnetLoss: 0.00599   DecoderLoss:0.00674  StopLoss: 0.07154  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00129   Validation Loss: 0.00539\n",
      "\n",
      " > Epoch 30/1000\n",
      "   | > Step:9/68  GlobalStep:71080  TotalLoss:0.00196  PostnetLoss:0.00095  DecoderLoss:0.00101  StopLoss:0.06916  GradNorm:0.00399  GradNormST:0.03605  AvgTextLen:9.0  AvgSpecLen:42.1  StepTime:0.41  LR:0.000100\n",
      "   | > Step:19/68  GlobalStep:71090  TotalLoss:0.00225  PostnetLoss:0.00108  DecoderLoss:0.00116  StopLoss:0.09530  GradNorm:0.00337  GradNormST:0.01731  AvgTextLen:15.5  AvgSpecLen:63.2  StepTime:0.42  LR:0.000100\n",
      "   | > Step:29/68  GlobalStep:71100  TotalLoss:0.00252  PostnetLoss:0.00121  DecoderLoss:0.00130  StopLoss:0.08114  GradNorm:0.00360  GradNormST:0.01314  AvgTextLen:21.8  AvgSpecLen:83.4  StepTime:0.63  LR:0.000100\n",
      "   | > Step:39/68  GlobalStep:71110  TotalLoss:0.00288  PostnetLoss:0.00138  DecoderLoss:0.00150  StopLoss:0.05848  GradNorm:0.00513  GradNormST:0.01413  AvgTextLen:30.2  AvgSpecLen:110.4  StepTime:0.60  LR:0.000100\n",
      "   | > Step:49/68  GlobalStep:71120  TotalLoss:0.00309  PostnetLoss:0.00149  DecoderLoss:0.00161  StopLoss:0.05557  GradNorm:0.00548  GradNormST:0.00895  AvgTextLen:38.2  AvgSpecLen:127.7  StepTime:0.71  LR:0.000100\n",
      "   | > Step:59/68  GlobalStep:71130  TotalLoss:0.00327  PostnetLoss:0.00156  DecoderLoss:0.00171  StopLoss:0.05883  GradNorm:0.00502  GradNormST:0.01007  AvgTextLen:50.5  AvgSpecLen:155.9  StepTime:0.95  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:71139  AvgTotalLoss:0.07224  AvgPostnetLoss:0.00128  AvgDecoderLoss:0.00139  AvgStopLoss:0.06957  EpochTime:43.54  AvgStepTime:0.63\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.11984   PostnetLoss: 0.00423   DecoderLoss:0.00476  StopLoss: 0.11086  \n",
      "   | > TotalLoss: 0.08915   PostnetLoss: 0.00625   DecoderLoss:0.00703  StopLoss: 0.07586  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00128   Validation Loss: 0.00555\n",
      "\n",
      " > Epoch 31/1000\n",
      "   | > Step:0/68  GlobalStep:71140  TotalLoss:0.00182  PostnetLoss:0.00087  DecoderLoss:0.00095  StopLoss:0.12533  GradNorm:0.00564  GradNormST:0.03772  AvgTextLen:3.0  AvgSpecLen:25.3  StepTime:0.54  LR:0.000100\n",
      "   | > Step:10/68  GlobalStep:71150  TotalLoss:0.00201  PostnetLoss:0.00097  DecoderLoss:0.00104  StopLoss:0.06687  GradNorm:0.00357  GradNormST:0.03314  AvgTextLen:9.8  AvgSpecLen:48.6  StepTime:0.58  LR:0.000100\n",
      "   | > Step:20/68  GlobalStep:71160  TotalLoss:0.00240  PostnetLoss:0.00115  DecoderLoss:0.00124  StopLoss:0.05724  GradNorm:0.00334  GradNormST:0.01833  AvgTextLen:16.4  AvgSpecLen:64.8  StepTime:0.49  LR:0.000100\n",
      "   | > Step:30/68  GlobalStep:71170  TotalLoss:0.00259  PostnetLoss:0.00125  DecoderLoss:0.00134  StopLoss:0.05718  GradNorm:0.00409  GradNormST:0.00858  AvgTextLen:22.9  AvgSpecLen:85.8  StepTime:0.63  LR:0.000100\n",
      "   | > Step:40/68  GlobalStep:71180  TotalLoss:0.00281  PostnetLoss:0.00135  DecoderLoss:0.00146  StopLoss:0.05761  GradNorm:0.00467  GradNormST:0.00970  AvgTextLen:30.9  AvgSpecLen:105.3  StepTime:0.50  LR:0.000100\n",
      "   | > Step:50/68  GlobalStep:71190  TotalLoss:0.00305  PostnetLoss:0.00146  DecoderLoss:0.00159  StopLoss:0.07198  GradNorm:0.00499  GradNormST:0.01905  AvgTextLen:38.8  AvgSpecLen:132.0  StepTime:0.78  LR:0.000100\n",
      "   | > Step:60/68  GlobalStep:71200  TotalLoss:0.00335  PostnetLoss:0.00160  DecoderLoss:0.00175  StopLoss:0.04642  GradNorm:0.00531  GradNormST:0.01050  AvgTextLen:52.0  AvgSpecLen:172.7  StepTime:0.99  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:71208  AvgTotalLoss:0.07278  AvgPostnetLoss:0.00128  AvgDecoderLoss:0.00138  AvgStopLoss:0.07012  EpochTime:43.44  AvgStepTime:0.63\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.12249   PostnetLoss: 0.00422   DecoderLoss:0.00477  StopLoss: 0.11350  \n",
      "   | > TotalLoss: 0.08832   PostnetLoss: 0.00631   DecoderLoss:0.00713  StopLoss: 0.07488  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00128   Validation Loss: 0.00558\n",
      "\n",
      " > Epoch 32/1000\n",
      "   | > Step:1/68  GlobalStep:71210  TotalLoss:0.00179  PostnetLoss:0.00086  DecoderLoss:0.00093  StopLoss:0.09850  GradNorm:0.00431  GradNormST:0.03819  AvgTextLen:3.7  AvgSpecLen:28.6  StepTime:0.47  LR:0.000100\n",
      "   | > Step:11/68  GlobalStep:71220  TotalLoss:0.00204  PostnetLoss:0.00099  DecoderLoss:0.00105  StopLoss:0.07025  GradNorm:0.00358  GradNormST:0.02024  AvgTextLen:10.3  AvgSpecLen:49.4  StepTime:0.41  LR:0.000100\n",
      "   | > Step:21/68  GlobalStep:71230  TotalLoss:0.00236  PostnetLoss:0.00114  DecoderLoss:0.00122  StopLoss:0.06951  GradNorm:0.00391  GradNormST:0.01517  AvgTextLen:16.6  AvgSpecLen:66.3  StepTime:0.34  LR:0.000100\n",
      "   | > Step:31/68  GlobalStep:71240  TotalLoss:0.00261  PostnetLoss:0.00126  DecoderLoss:0.00135  StopLoss:0.07121  GradNorm:0.00413  GradNormST:0.01675  AvgTextLen:24.1  AvgSpecLen:82.7  StepTime:0.54  LR:0.000100\n",
      "   | > Step:41/68  GlobalStep:71250  TotalLoss:0.00290  PostnetLoss:0.00139  DecoderLoss:0.00150  StopLoss:0.04499  GradNorm:0.00409  GradNormST:0.00960  AvgTextLen:32.3  AvgSpecLen:109.4  StepTime:0.71  LR:0.000100\n",
      "   | > Step:51/68  GlobalStep:71260  TotalLoss:0.00296  PostnetLoss:0.00142  DecoderLoss:0.00155  StopLoss:0.06058  GradNorm:0.00430  GradNormST:0.01170  AvgTextLen:39.2  AvgSpecLen:126.9  StepTime:0.64  LR:0.000100\n",
      "   | > Step:61/68  GlobalStep:71270  TotalLoss:0.00329  PostnetLoss:0.00157  DecoderLoss:0.00172  StopLoss:0.05796  GradNorm:0.00445  GradNormST:0.02346  AvgTextLen:54.6  AvgSpecLen:165.5  StepTime:0.92  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:71277  AvgTotalLoss:0.07150  AvgPostnetLoss:0.00127  AvgDecoderLoss:0.00138  AvgStopLoss:0.06885  EpochTime:43.71  AvgStepTime:0.63\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.13159   PostnetLoss: 0.00436   DecoderLoss:0.00488  StopLoss: 0.12235  \n",
      "   | > TotalLoss: 0.09246   PostnetLoss: 0.00672   DecoderLoss:0.00757  StopLoss: 0.07817  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00127   Validation Loss: 0.00593\n",
      "\n",
      " > Epoch 33/1000\n",
      "   | > Step:2/68  GlobalStep:71280  TotalLoss:0.00170  PostnetLoss:0.00082  DecoderLoss:0.00088  StopLoss:0.09263  GradNorm:0.00396  GradNormST:0.03090  AvgTextLen:4.0  AvgSpecLen:31.3  StepTime:0.43  LR:0.000100\n",
      "   | > Step:12/68  GlobalStep:71290  TotalLoss:0.00198  PostnetLoss:0.00095  DecoderLoss:0.00103  StopLoss:0.08886  GradNorm:0.00341  GradNormST:0.02599  AvgTextLen:11.0  AvgSpecLen:49.3  StepTime:0.46  LR:0.000100\n",
      "   | > Step:22/68  GlobalStep:71300  TotalLoss:0.00238  PostnetLoss:0.00115  DecoderLoss:0.00124  StopLoss:0.07691  GradNorm:0.00377  GradNormST:0.01310  AvgTextLen:18.0  AvgSpecLen:72.0  StepTime:0.49  LR:0.000100\n",
      "   | > Step:32/68  GlobalStep:71310  TotalLoss:0.00264  PostnetLoss:0.00127  DecoderLoss:0.00137  StopLoss:0.07091  GradNorm:0.00373  GradNormST:0.01691  AvgTextLen:24.4  AvgSpecLen:91.5  StepTime:0.58  LR:0.000100\n",
      "   | > Step:42/68  GlobalStep:71320  TotalLoss:0.00285  PostnetLoss:0.00137  DecoderLoss:0.00149  StopLoss:0.06193  GradNorm:0.00407  GradNormST:0.01597  AvgTextLen:33.1  AvgSpecLen:109.7  StepTime:0.61  LR:0.000100\n",
      "   | > Step:52/68  GlobalStep:71330  TotalLoss:0.00315  PostnetLoss:0.00151  DecoderLoss:0.00164  StopLoss:0.06979  GradNorm:0.00462  GradNormST:0.01868  AvgTextLen:42.1  AvgSpecLen:137.5  StepTime:0.78  LR:0.000100\n",
      "   | > Step:62/68  GlobalStep:71340  TotalLoss:0.00336  PostnetLoss:0.00161  DecoderLoss:0.00176  StopLoss:0.07036  GradNorm:0.00426  GradNormST:0.01982  AvgTextLen:54.0  AvgSpecLen:174.7  StepTime:0.77  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:71346  AvgTotalLoss:0.07155  AvgPostnetLoss:0.00127  AvgDecoderLoss:0.00138  AvgStopLoss:0.06890  EpochTime:44.07  AvgStepTime:0.64\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.12001   PostnetLoss: 0.00421   DecoderLoss:0.00475  StopLoss: 0.11104  \n",
      "   | > TotalLoss: 0.08475   PostnetLoss: 0.00629   DecoderLoss:0.00708  StopLoss: 0.07138  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00127   Validation Loss: 0.00567\n",
      "\n",
      " > Epoch 34/1000\n",
      "   | > Step:3/68  GlobalStep:71350  TotalLoss:0.00175  PostnetLoss:0.00085  DecoderLoss:0.00090  StopLoss:0.08452  GradNorm:0.00463  GradNormST:0.02869  AvgTextLen:4.7  AvgSpecLen:29.7  StepTime:0.31  LR:0.000100\n",
      "   | > Step:13/68  GlobalStep:71360  TotalLoss:0.00207  PostnetLoss:0.00100  DecoderLoss:0.00107  StopLoss:0.09005  GradNorm:0.00396  GradNormST:0.02455  AvgTextLen:11.5  AvgSpecLen:55.2  StepTime:0.43  LR:0.000100\n",
      "   | > Step:23/68  GlobalStep:71370  TotalLoss:0.00247  PostnetLoss:0.00119  DecoderLoss:0.00128  StopLoss:0.07855  GradNorm:0.00336  GradNormST:0.01673  AvgTextLen:18.3  AvgSpecLen:72.2  StepTime:0.43  LR:0.000100\n",
      "   | > Step:33/68  GlobalStep:71380  TotalLoss:0.00271  PostnetLoss:0.00130  DecoderLoss:0.00140  StopLoss:0.07367  GradNorm:0.00475  GradNormST:0.01492  AvgTextLen:25.9  AvgSpecLen:88.9  StepTime:0.59  LR:0.000100\n",
      "   | > Step:43/68  GlobalStep:71390  TotalLoss:0.00288  PostnetLoss:0.00138  DecoderLoss:0.00150  StopLoss:0.06133  GradNorm:0.00376  GradNormST:0.01167  AvgTextLen:33.4  AvgSpecLen:115.7  StepTime:0.64  LR:0.000100\n",
      "   | > Step:53/68  GlobalStep:71400  TotalLoss:0.00307  PostnetLoss:0.00147  DecoderLoss:0.00160  StopLoss:0.05787  GradNorm:0.00397  GradNormST:0.01188  AvgTextLen:42.4  AvgSpecLen:137.6  StepTime:0.67  LR:0.000100\n",
      "   | > Step:63/68  GlobalStep:71410  TotalLoss:0.00340  PostnetLoss:0.00162  DecoderLoss:0.00178  StopLoss:0.04552  GradNorm:0.00357  GradNormST:0.01263  AvgTextLen:58.6  AvgSpecLen:185.1  StepTime:1.02  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:71415  AvgTotalLoss:0.07128  AvgPostnetLoss:0.00127  AvgDecoderLoss:0.00138  AvgStopLoss:0.06863  EpochTime:43.69  AvgStepTime:0.63\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.11392   PostnetLoss: 0.00432   DecoderLoss:0.00485  StopLoss: 0.10475  \n",
      "   | > TotalLoss: 0.08420   PostnetLoss: 0.00632   DecoderLoss:0.00708  StopLoss: 0.07079  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00127   Validation Loss: 0.00560\n",
      "\n",
      " > Epoch 35/1000\n",
      "   | > Step:4/68  GlobalStep:71420  TotalLoss:0.00179  PostnetLoss:0.00087  DecoderLoss:0.00093  StopLoss:0.10181  GradNorm:0.00432  GradNormST:0.04269  AvgTextLen:4.8  AvgSpecLen:31.1  StepTime:0.26  LR:0.000100\n",
      "   | > Step:14/68  GlobalStep:71430  TotalLoss:0.00213  PostnetLoss:0.00103  DecoderLoss:0.00110  StopLoss:0.08211  GradNorm:0.00391  GradNormST:0.02130  AvgTextLen:12.2  AvgSpecLen:52.0  StepTime:0.33  LR:0.000100\n",
      "   | > Step:24/68  GlobalStep:71440  TotalLoss:0.00248  PostnetLoss:0.00120  DecoderLoss:0.00129  StopLoss:0.07100  GradNorm:0.00332  GradNormST:0.01152  AvgTextLen:19.2  AvgSpecLen:79.0  StepTime:0.52  LR:0.000100\n",
      "   | > Step:34/68  GlobalStep:71450  TotalLoss:0.00275  PostnetLoss:0.00132  DecoderLoss:0.00143  StopLoss:0.05960  GradNorm:0.00400  GradNormST:0.01011  AvgTextLen:26.2  AvgSpecLen:95.9  StepTime:0.49  LR:0.000100\n",
      "   | > Step:44/68  GlobalStep:71460  TotalLoss:0.00293  PostnetLoss:0.00141  DecoderLoss:0.00152  StopLoss:0.05917  GradNorm:0.00475  GradNormST:0.02177  AvgTextLen:33.8  AvgSpecLen:116.3  StepTime:0.68  LR:0.000100\n",
      "   | > Step:54/68  GlobalStep:71470  TotalLoss:0.00315  PostnetLoss:0.00150  DecoderLoss:0.00164  StopLoss:0.05985  GradNorm:0.00497  GradNormST:0.01262  AvgTextLen:42.2  AvgSpecLen:144.9  StepTime:0.72  LR:0.000100\n",
      "   | > Step:64/68  GlobalStep:71480  TotalLoss:0.00321  PostnetLoss:0.00153  DecoderLoss:0.00168  StopLoss:0.05271  GradNorm:0.00299  GradNormST:0.01133  AvgTextLen:60.6  AvgSpecLen:177.6  StepTime:0.81  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:71484  AvgTotalLoss:0.06937  AvgPostnetLoss:0.00127  AvgDecoderLoss:0.00137  AvgStopLoss:0.06673  EpochTime:42.68  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.11804   PostnetLoss: 0.00418   DecoderLoss:0.00470  StopLoss: 0.10916  \n",
      "   | > TotalLoss: 0.08221   PostnetLoss: 0.00619   DecoderLoss:0.00696  StopLoss: 0.06906  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00127   Validation Loss: 0.00549\n",
      "\n",
      " > Epoch 36/1000\n",
      "   | > Step:5/68  GlobalStep:71490  TotalLoss:0.00178  PostnetLoss:0.00086  DecoderLoss:0.00092  StopLoss:0.08616  GradNorm:0.00369  GradNormST:0.01821  AvgTextLen:5.9  AvgSpecLen:33.2  StepTime:0.32  LR:0.000100\n",
      "   | > Step:15/68  GlobalStep:71500  TotalLoss:0.00218  PostnetLoss:0.00106  DecoderLoss:0.00113  StopLoss:0.06451  GradNorm:0.00371  GradNormST:0.03225  AvgTextLen:12.8  AvgSpecLen:59.1  StepTime:0.54  LR:0.000100\n",
      "   | > Step:25/68  GlobalStep:71510  TotalLoss:0.00243  PostnetLoss:0.00117  DecoderLoss:0.00126  StopLoss:0.06580  GradNorm:0.00388  GradNormST:0.01208  AvgTextLen:19.8  AvgSpecLen:73.3  StepTime:0.52  LR:0.000100\n",
      "   | > Step:35/68  GlobalStep:71520  TotalLoss:0.00277  PostnetLoss:0.00133  DecoderLoss:0.00144  StopLoss:0.07229  GradNorm:0.00404  GradNormST:0.02591  AvgTextLen:27.1  AvgSpecLen:104.7  StepTime:0.74  LR:0.000100\n",
      "   | > Step:45/68  GlobalStep:71530  TotalLoss:0.00288  PostnetLoss:0.00138  DecoderLoss:0.00150  StopLoss:0.07454  GradNorm:0.00376  GradNormST:0.02258  AvgTextLen:34.4  AvgSpecLen:118.6  StepTime:0.63  LR:0.000100\n",
      "   | > Step:55/68  GlobalStep:71540  TotalLoss:0.00316  PostnetLoss:0.00152  DecoderLoss:0.00165  StopLoss:0.05758  GradNorm:0.00402  GradNormST:0.00870  AvgTextLen:43.9  AvgSpecLen:145.2  StepTime:0.85  LR:0.000100\n",
      "   | > Step:65/68  GlobalStep:71550  TotalLoss:0.00345  PostnetLoss:0.00164  DecoderLoss:0.00181  StopLoss:0.05439  GradNorm:0.00307  GradNormST:0.01743  AvgTextLen:64.2  AvgSpecLen:207.4  StepTime:1.20  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:71553  AvgTotalLoss:0.07314  AvgPostnetLoss:0.00127  AvgDecoderLoss:0.00137  AvgStopLoss:0.07050  EpochTime:43.18  AvgStepTime:0.63\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.12645   PostnetLoss: 0.00418   DecoderLoss:0.00470  StopLoss: 0.11757  \n",
      "   | > TotalLoss: 0.09067   PostnetLoss: 0.00645   DecoderLoss:0.00725  StopLoss: 0.07696  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00127   Validation Loss: 0.00571\n",
      "\n",
      " > Epoch 37/1000\n",
      "   | > Step:6/68  GlobalStep:71560  TotalLoss:0.00184  PostnetLoss:0.00089  DecoderLoss:0.00095  StopLoss:0.08676  GradNorm:0.00458  GradNormST:0.05405  AvgTextLen:6.0  AvgSpecLen:34.8  StepTime:0.36  LR:0.000100\n",
      "   | > Step:16/68  GlobalStep:71570  TotalLoss:0.00227  PostnetLoss:0.00109  DecoderLoss:0.00117  StopLoss:0.06081  GradNorm:0.00414  GradNormST:0.01463  AvgTextLen:13.8  AvgSpecLen:60.6  StepTime:0.29  LR:0.000100\n",
      "   | > Step:26/68  GlobalStep:71580  TotalLoss:0.00247  PostnetLoss:0.00119  DecoderLoss:0.00128  StopLoss:0.06090  GradNorm:0.00355  GradNormST:0.01110  AvgTextLen:20.3  AvgSpecLen:77.5  StepTime:0.48  LR:0.000100\n",
      "   | > Step:36/68  GlobalStep:71590  TotalLoss:0.00267  PostnetLoss:0.00128  DecoderLoss:0.00139  StopLoss:0.07495  GradNorm:0.00342  GradNormST:0.01466  AvgTextLen:28.5  AvgSpecLen:100.2  StepTime:0.58  LR:0.000100\n",
      "   | > Step:46/68  GlobalStep:71600  TotalLoss:0.00287  PostnetLoss:0.00138  DecoderLoss:0.00149  StopLoss:0.06971  GradNorm:0.00404  GradNormST:0.01596  AvgTextLen:35.4  AvgSpecLen:122.6  StepTime:0.67  LR:0.000100\n",
      "   | > Step:56/68  GlobalStep:71610  TotalLoss:0.00314  PostnetLoss:0.00150  DecoderLoss:0.00164  StopLoss:0.05245  GradNorm:0.00390  GradNormST:0.02054  AvgTextLen:45.8  AvgSpecLen:145.4  StepTime:1.04  LR:0.000100\n",
      "   | > Step:66/68  GlobalStep:71620  TotalLoss:0.00364  PostnetLoss:0.00173  DecoderLoss:0.00191  StopLoss:0.05092  GradNorm:0.00313  GradNormST:0.01289  AvgTextLen:71.2  AvgSpecLen:220.8  StepTime:1.14  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:71622  AvgTotalLoss:0.07256  AvgPostnetLoss:0.00127  AvgDecoderLoss:0.00137  AvgStopLoss:0.06992  EpochTime:43.03  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.11685   PostnetLoss: 0.00419   DecoderLoss:0.00470  StopLoss: 0.10795  \n",
      "   | > TotalLoss: 0.08551   PostnetLoss: 0.00610   DecoderLoss:0.00684  StopLoss: 0.07256  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00127   Validation Loss: 0.00551\n",
      "\n",
      " > Epoch 38/1000\n",
      "   | > Step:7/68  GlobalStep:71630  TotalLoss:0.00192  PostnetLoss:0.00093  DecoderLoss:0.00099  StopLoss:0.06926  GradNorm:0.00419  GradNormST:0.02274  AvgTextLen:7.4  AvgSpecLen:44.6  StepTime:0.38  LR:0.000100\n",
      "   | > Step:17/68  GlobalStep:71640  TotalLoss:0.00222  PostnetLoss:0.00107  DecoderLoss:0.00114  StopLoss:0.08542  GradNorm:0.00449  GradNormST:0.01974  AvgTextLen:14.7  AvgSpecLen:66.0  StepTime:0.45  LR:0.000100\n",
      "   | > Step:27/68  GlobalStep:71650  TotalLoss:0.00254  PostnetLoss:0.00122  DecoderLoss:0.00132  StopLoss:0.07375  GradNorm:0.00348  GradNormST:0.01780  AvgTextLen:21.0  AvgSpecLen:80.2  StepTime:0.70  LR:0.000100\n",
      "   | > Step:37/68  GlobalStep:71660  TotalLoss:0.00275  PostnetLoss:0.00132  DecoderLoss:0.00143  StopLoss:0.06241  GradNorm:0.00362  GradNormST:0.01574  AvgTextLen:29.3  AvgSpecLen:102.4  StepTime:0.60  LR:0.000100\n",
      "   | > Step:47/68  GlobalStep:71670  TotalLoss:0.00298  PostnetLoss:0.00143  DecoderLoss:0.00155  StopLoss:0.06001  GradNorm:0.00314  GradNormST:0.01440  AvgTextLen:36.7  AvgSpecLen:126.9  StepTime:0.75  LR:0.000100\n",
      "   | > Step:57/68  GlobalStep:71680  TotalLoss:0.00323  PostnetLoss:0.00154  DecoderLoss:0.00169  StopLoss:0.05447  GradNorm:0.00375  GradNormST:0.00819  AvgTextLen:47.1  AvgSpecLen:161.0  StepTime:0.81  LR:0.000100\n",
      "   | > Step:67/68  GlobalStep:71690  TotalLoss:0.00365  PostnetLoss:0.00173  DecoderLoss:0.00192  StopLoss:0.05764  GradNorm:0.00321  GradNormST:0.02146  AvgTextLen:80.6  AvgSpecLen:246.5  StepTime:1.28  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:71691  AvgTotalLoss:0.07222  AvgPostnetLoss:0.00127  AvgDecoderLoss:0.00137  AvgStopLoss:0.06958  EpochTime:42.21  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.11449   PostnetLoss: 0.00422   DecoderLoss:0.00474  StopLoss: 0.10552  \n",
      "   | > TotalLoss: 0.08497   PostnetLoss: 0.00613   DecoderLoss:0.00688  StopLoss: 0.07195  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00127   Validation Loss: 0.00551\n",
      "\n",
      " > Epoch 39/1000\n",
      "   | > Step:8/68  GlobalStep:71700  TotalLoss:0.00196  PostnetLoss:0.00095  DecoderLoss:0.00101  StopLoss:0.08800  GradNorm:0.00459  GradNormST:0.01951  AvgTextLen:8.3  AvgSpecLen:44.0  StepTime:0.24  LR:0.000100\n",
      "   | > Step:18/68  GlobalStep:71710  TotalLoss:0.00222  PostnetLoss:0.00107  DecoderLoss:0.00115  StopLoss:0.08858  GradNorm:0.00388  GradNormST:0.01874  AvgTextLen:15.2  AvgSpecLen:64.5  StepTime:0.43  LR:0.000100\n",
      "   | > Step:28/68  GlobalStep:71720  TotalLoss:0.00255  PostnetLoss:0.00123  DecoderLoss:0.00132  StopLoss:0.08206  GradNorm:0.00346  GradNormST:0.01967  AvgTextLen:22.1  AvgSpecLen:85.4  StepTime:0.58  LR:0.000100\n",
      "   | > Step:38/68  GlobalStep:71730  TotalLoss:0.00271  PostnetLoss:0.00130  DecoderLoss:0.00140  StopLoss:0.06868  GradNorm:0.00394  GradNormST:0.01584  AvgTextLen:29.4  AvgSpecLen:104.1  StepTime:0.56  LR:0.000100\n",
      "   | > Step:48/68  GlobalStep:71740  TotalLoss:0.00293  PostnetLoss:0.00141  DecoderLoss:0.00153  StopLoss:0.05579  GradNorm:0.00325  GradNormST:0.00972  AvgTextLen:37.5  AvgSpecLen:123.8  StepTime:0.57  LR:0.000100\n",
      "   | > Step:58/68  GlobalStep:71750  TotalLoss:0.00323  PostnetLoss:0.00155  DecoderLoss:0.00169  StopLoss:0.05982  GradNorm:0.00282  GradNormST:0.01515  AvgTextLen:47.8  AvgSpecLen:158.0  StepTime:0.91  LR:0.000100\n",
      "   | > Step:68/68  GlobalStep:71760  TotalLoss:0.00353  PostnetLoss:0.00168  DecoderLoss:0.00185  StopLoss:0.05580  GradNorm:0.00285  GradNormST:0.02127  AvgTextLen:103.0  AvgSpecLen:308.9  StepTime:1.89  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:71760  AvgTotalLoss:0.07347  AvgPostnetLoss:0.00126  AvgDecoderLoss:0.00137  AvgStopLoss:0.07083  EpochTime:42.69  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.12111   PostnetLoss: 0.00416   DecoderLoss:0.00467  StopLoss: 0.11229  \n",
      "   | > TotalLoss: 0.08879   PostnetLoss: 0.00620   DecoderLoss:0.00698  StopLoss: 0.07562  \n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00126   Validation Loss: 0.00547\n",
      "\n",
      " > Epoch 40/1000\n",
      "   | > Step:9/68  GlobalStep:71770  TotalLoss:0.00192  PostnetLoss:0.00093  DecoderLoss:0.00099  StopLoss:0.05920  GradNorm:0.00364  GradNormST:0.02287  AvgTextLen:9.0  AvgSpecLen:42.1  StepTime:0.41  LR:0.000100\n",
      "   | > Step:19/68  GlobalStep:71780  TotalLoss:0.00221  PostnetLoss:0.00107  DecoderLoss:0.00114  StopLoss:0.08713  GradNorm:0.00356  GradNormST:0.01836  AvgTextLen:15.5  AvgSpecLen:63.2  StepTime:0.43  LR:0.000100\n",
      "   | > Step:29/68  GlobalStep:71790  TotalLoss:0.00249  PostnetLoss:0.00120  DecoderLoss:0.00129  StopLoss:0.09027  GradNorm:0.00325  GradNormST:0.01672  AvgTextLen:21.8  AvgSpecLen:83.4  StepTime:0.49  LR:0.000100\n",
      "   | > Step:39/68  GlobalStep:71800  TotalLoss:0.00280  PostnetLoss:0.00135  DecoderLoss:0.00146  StopLoss:0.06037  GradNorm:0.00345  GradNormST:0.01091  AvgTextLen:30.2  AvgSpecLen:110.4  StepTime:0.64  LR:0.000100\n",
      "   | > Step:49/68  GlobalStep:71810  TotalLoss:0.00296  PostnetLoss:0.00142  DecoderLoss:0.00154  StopLoss:0.05132  GradNorm:0.00328  GradNormST:0.00848  AvgTextLen:38.2  AvgSpecLen:127.7  StepTime:0.75  LR:0.000100\n",
      "   | > Step:59/68  GlobalStep:71820  TotalLoss:0.00317  PostnetLoss:0.00151  DecoderLoss:0.00165  StopLoss:0.05477  GradNorm:0.00277  GradNormST:0.00962  AvgTextLen:50.5  AvgSpecLen:155.9  StepTime:0.74  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:71829  AvgTotalLoss:0.07272  AvgPostnetLoss:0.00126  AvgDecoderLoss:0.00137  AvgStopLoss:0.07009  EpochTime:43.80  AvgStepTime:0.63\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.11885   PostnetLoss: 0.00417   DecoderLoss:0.00468  StopLoss: 0.11001  \n",
      "   | > TotalLoss: 0.08672   PostnetLoss: 0.00602   DecoderLoss:0.00677  StopLoss: 0.07393  \n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00126   Validation Loss: 0.00541\n",
      "\n",
      " > Epoch 41/1000\n",
      "   | > Step:0/68  GlobalStep:71830  TotalLoss:0.00182  PostnetLoss:0.00087  DecoderLoss:0.00095  StopLoss:0.12125  GradNorm:0.00544  GradNormST:0.03886  AvgTextLen:3.0  AvgSpecLen:25.3  StepTime:0.52  LR:0.000100\n",
      "   | > Step:10/68  GlobalStep:71840  TotalLoss:0.00205  PostnetLoss:0.00099  DecoderLoss:0.00106  StopLoss:0.07928  GradNorm:0.00377  GradNormST:0.02724  AvgTextLen:9.8  AvgSpecLen:48.6  StepTime:0.51  LR:0.000100\n",
      "   | > Step:20/68  GlobalStep:71850  TotalLoss:0.00233  PostnetLoss:0.00113  DecoderLoss:0.00121  StopLoss:0.06387  GradNorm:0.00356  GradNormST:0.02154  AvgTextLen:16.4  AvgSpecLen:64.8  StepTime:0.52  LR:0.000100\n",
      "   | > Step:30/68  GlobalStep:71860  TotalLoss:0.00256  PostnetLoss:0.00123  DecoderLoss:0.00133  StopLoss:0.07140  GradNorm:0.00339  GradNormST:0.01476  AvgTextLen:22.9  AvgSpecLen:85.8  StepTime:0.51  LR:0.000100\n",
      "   | > Step:40/68  GlobalStep:71870  TotalLoss:0.00278  PostnetLoss:0.00134  DecoderLoss:0.00144  StopLoss:0.04971  GradNorm:0.00328  GradNormST:0.00800  AvgTextLen:30.9  AvgSpecLen:105.3  StepTime:0.60  LR:0.000100\n",
      "   | > Step:50/68  GlobalStep:71880  TotalLoss:0.00298  PostnetLoss:0.00143  DecoderLoss:0.00156  StopLoss:0.06945  GradNorm:0.00313  GradNormST:0.01640  AvgTextLen:38.8  AvgSpecLen:132.0  StepTime:0.83  LR:0.000100\n",
      "   | > Step:60/68  GlobalStep:71890  TotalLoss:0.00324  PostnetLoss:0.00155  DecoderLoss:0.00169  StopLoss:0.04590  GradNorm:0.00291  GradNormST:0.01143  AvgTextLen:52.0  AvgSpecLen:172.7  StepTime:1.01  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:71898  AvgTotalLoss:0.07226  AvgPostnetLoss:0.00126  AvgDecoderLoss:0.00137  AvgStopLoss:0.06963  EpochTime:43.98  AvgStepTime:0.64\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.12253   PostnetLoss: 0.00423   DecoderLoss:0.00476  StopLoss: 0.11355  \n",
      "   | > TotalLoss: 0.08684   PostnetLoss: 0.00621   DecoderLoss:0.00700  StopLoss: 0.07364  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00126   Validation Loss: 0.00555\n",
      "\n",
      " > Epoch 42/1000\n",
      "   | > Step:1/68  GlobalStep:71900  TotalLoss:0.00176  PostnetLoss:0.00085  DecoderLoss:0.00091  StopLoss:0.09351  GradNorm:0.00453  GradNormST:0.03590  AvgTextLen:3.7  AvgSpecLen:28.6  StepTime:0.38  LR:0.000100\n",
      "   | > Step:11/68  GlobalStep:71910  TotalLoss:0.00199  PostnetLoss:0.00096  DecoderLoss:0.00102  StopLoss:0.07212  GradNorm:0.00356  GradNormST:0.01465  AvgTextLen:10.3  AvgSpecLen:49.4  StepTime:0.38  LR:0.000100\n",
      "   | > Step:21/68  GlobalStep:71920  TotalLoss:0.00233  PostnetLoss:0.00113  DecoderLoss:0.00120  StopLoss:0.06349  GradNorm:0.00378  GradNormST:0.01641  AvgTextLen:16.6  AvgSpecLen:66.3  StepTime:0.45  LR:0.000100\n",
      "   | > Step:31/68  GlobalStep:71930  TotalLoss:0.00257  PostnetLoss:0.00123  DecoderLoss:0.00133  StopLoss:0.07253  GradNorm:0.00348  GradNormST:0.02042  AvgTextLen:24.1  AvgSpecLen:82.7  StepTime:0.43  LR:0.000100\n",
      "   | > Step:41/68  GlobalStep:71940  TotalLoss:0.00283  PostnetLoss:0.00136  DecoderLoss:0.00147  StopLoss:0.04545  GradNorm:0.00350  GradNormST:0.01103  AvgTextLen:32.3  AvgSpecLen:109.4  StepTime:0.64  LR:0.000100\n",
      "   | > Step:51/68  GlobalStep:71950  TotalLoss:0.00296  PostnetLoss:0.00142  DecoderLoss:0.00154  StopLoss:0.05941  GradNorm:0.00303  GradNormST:0.01158  AvgTextLen:39.2  AvgSpecLen:126.9  StepTime:0.65  LR:0.000100\n",
      "   | > Step:61/68  GlobalStep:71960  TotalLoss:0.00323  PostnetLoss:0.00154  DecoderLoss:0.00169  StopLoss:0.06758  GradNorm:0.00285  GradNormST:0.03725  AvgTextLen:54.6  AvgSpecLen:165.5  StepTime:0.92  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:71967  AvgTotalLoss:0.07450  AvgPostnetLoss:0.00126  AvgDecoderLoss:0.00137  AvgStopLoss:0.07187  EpochTime:42.90  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.11730   PostnetLoss: 0.00422   DecoderLoss:0.00474  StopLoss: 0.10835  \n",
      "   | > TotalLoss: 0.08621   PostnetLoss: 0.00603   DecoderLoss:0.00678  StopLoss: 0.07339  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00126   Validation Loss: 0.00540\n",
      "\n",
      " > Epoch 43/1000\n",
      "   | > Step:2/68  GlobalStep:71970  TotalLoss:0.00166  PostnetLoss:0.00080  DecoderLoss:0.00086  StopLoss:0.08972  GradNorm:0.00355  GradNormST:0.02698  AvgTextLen:4.0  AvgSpecLen:31.3  StepTime:0.33  LR:0.000100\n",
      "   | > Step:12/68  GlobalStep:71980  TotalLoss:0.00202  PostnetLoss:0.00098  DecoderLoss:0.00105  StopLoss:0.09324  GradNorm:0.00370  GradNormST:0.02180  AvgTextLen:11.0  AvgSpecLen:49.3  StepTime:0.44  LR:0.000100\n",
      "   | > Step:22/68  GlobalStep:71990  TotalLoss:0.00237  PostnetLoss:0.00114  DecoderLoss:0.00123  StopLoss:0.07828  GradNorm:0.00368  GradNormST:0.01677  AvgTextLen:18.0  AvgSpecLen:72.0  StepTime:0.49  LR:0.000100\n",
      "   | > Step:32/68  GlobalStep:72000  TotalLoss:0.00255  PostnetLoss:0.00123  DecoderLoss:0.00132  StopLoss:0.08295  GradNorm:0.00321  GradNormST:0.01988  AvgTextLen:24.4  AvgSpecLen:91.5  StepTime:0.55  LR:0.000100\n",
      " | | > Checkpoint saving : /vm/TTS/models/cartman_models/mozilla-no-loc-fattn-stopnet-sigmoid-loss_masking-August-15-2019_11+26AM-de6aeb9/checkpoint_72000.pth.tar\n",
      "   | > Step:42/68  GlobalStep:72010  TotalLoss:0.00281  PostnetLoss:0.00135  DecoderLoss:0.00147  StopLoss:0.06635  GradNorm:0.00304  GradNormST:0.02017  AvgTextLen:33.1  AvgSpecLen:109.7  StepTime:0.64  LR:0.000100\n",
      "   | > Step:52/68  GlobalStep:72020  TotalLoss:0.00307  PostnetLoss:0.00147  DecoderLoss:0.00160  StopLoss:0.06961  GradNorm:0.00350  GradNormST:0.01998  AvgTextLen:42.1  AvgSpecLen:137.5  StepTime:0.78  LR:0.000100\n",
      "   | > Step:62/68  GlobalStep:72030  TotalLoss:0.00332  PostnetLoss:0.00159  DecoderLoss:0.00174  StopLoss:0.07081  GradNorm:0.00373  GradNormST:0.02365  AvgTextLen:54.0  AvgSpecLen:174.7  StepTime:0.77  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:72036  AvgTotalLoss:0.07592  AvgPostnetLoss:0.00126  AvgDecoderLoss:0.00137  AvgStopLoss:0.07329  EpochTime:44.05  AvgStepTime:0.64\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.12355   PostnetLoss: 0.00414   DecoderLoss:0.00466  StopLoss: 0.11475  \n",
      "   | > TotalLoss: 0.08652   PostnetLoss: 0.00587   DecoderLoss:0.00662  StopLoss: 0.07403  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00126   Validation Loss: 0.00537\n",
      "\n",
      " > Epoch 44/1000\n",
      "   | > Step:3/68  GlobalStep:72040  TotalLoss:0.00177  PostnetLoss:0.00086  DecoderLoss:0.00091  StopLoss:0.08583  GradNorm:0.00409  GradNormST:0.02523  AvgTextLen:4.7  AvgSpecLen:29.7  StepTime:0.33  LR:0.000100\n",
      "   | > Step:13/68  GlobalStep:72050  TotalLoss:0.00202  PostnetLoss:0.00097  DecoderLoss:0.00105  StopLoss:0.09647  GradNorm:0.00352  GradNormST:0.03138  AvgTextLen:11.5  AvgSpecLen:55.2  StepTime:0.41  LR:0.000100\n",
      "   | > Step:23/68  GlobalStep:72060  TotalLoss:0.00245  PostnetLoss:0.00118  DecoderLoss:0.00127  StopLoss:0.06552  GradNorm:0.00344  GradNormST:0.01336  AvgTextLen:18.3  AvgSpecLen:72.2  StepTime:0.35  LR:0.000100\n",
      "   | > Step:33/68  GlobalStep:72070  TotalLoss:0.00263  PostnetLoss:0.00127  DecoderLoss:0.00137  StopLoss:0.06952  GradNorm:0.00379  GradNormST:0.01241  AvgTextLen:25.9  AvgSpecLen:88.9  StepTime:0.59  LR:0.000100\n",
      "   | > Step:43/68  GlobalStep:72080  TotalLoss:0.00285  PostnetLoss:0.00137  DecoderLoss:0.00148  StopLoss:0.06517  GradNorm:0.00330  GradNormST:0.01363  AvgTextLen:33.4  AvgSpecLen:115.7  StepTime:0.51  LR:0.000100\n",
      "   | > Step:53/68  GlobalStep:72090  TotalLoss:0.00304  PostnetLoss:0.00146  DecoderLoss:0.00159  StopLoss:0.05711  GradNorm:0.00351  GradNormST:0.01728  AvgTextLen:42.4  AvgSpecLen:137.6  StepTime:0.78  LR:0.000100\n",
      "   | > Step:63/68  GlobalStep:72100  TotalLoss:0.00338  PostnetLoss:0.00161  DecoderLoss:0.00177  StopLoss:0.06021  GradNorm:0.00409  GradNormST:0.02378  AvgTextLen:58.6  AvgSpecLen:185.1  StepTime:0.91  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:72105  AvgTotalLoss:0.07709  AvgPostnetLoss:0.00127  AvgDecoderLoss:0.00137  AvgStopLoss:0.07445  EpochTime:42.98  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.12221   PostnetLoss: 0.00418   DecoderLoss:0.00471  StopLoss: 0.11332  \n",
      "   | > TotalLoss: 0.08610   PostnetLoss: 0.00615   DecoderLoss:0.00694  StopLoss: 0.07301  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00127   Validation Loss: 0.00552\n",
      "\n",
      " > Epoch 45/1000\n",
      "   | > Step:4/68  GlobalStep:72110  TotalLoss:0.00173  PostnetLoss:0.00083  DecoderLoss:0.00089  StopLoss:0.12487  GradNorm:0.00380  GradNormST:0.05484  AvgTextLen:4.8  AvgSpecLen:31.1  StepTime:0.25  LR:0.000100\n",
      "   | > Step:14/68  GlobalStep:72120  TotalLoss:0.00212  PostnetLoss:0.00103  DecoderLoss:0.00109  StopLoss:0.08162  GradNorm:0.00385  GradNormST:0.02370  AvgTextLen:12.2  AvgSpecLen:52.0  StepTime:0.35  LR:0.000100\n",
      "   | > Step:24/68  GlobalStep:72130  TotalLoss:0.00253  PostnetLoss:0.00122  DecoderLoss:0.00131  StopLoss:0.07370  GradNorm:0.00375  GradNormST:0.01165  AvgTextLen:19.2  AvgSpecLen:79.0  StepTime:0.52  LR:0.000100\n",
      "   | > Step:34/68  GlobalStep:72140  TotalLoss:0.00273  PostnetLoss:0.00131  DecoderLoss:0.00141  StopLoss:0.05737  GradNorm:0.00425  GradNormST:0.00969  AvgTextLen:26.2  AvgSpecLen:95.9  StepTime:0.58  LR:0.000100\n",
      "   | > Step:44/68  GlobalStep:72150  TotalLoss:0.00292  PostnetLoss:0.00140  DecoderLoss:0.00152  StopLoss:0.06290  GradNorm:0.00378  GradNormST:0.01564  AvgTextLen:33.8  AvgSpecLen:116.3  StepTime:0.70  LR:0.000100\n",
      "   | > Step:54/68  GlobalStep:72160  TotalLoss:0.00310  PostnetLoss:0.00148  DecoderLoss:0.00162  StopLoss:0.06782  GradNorm:0.00392  GradNormST:0.01766  AvgTextLen:42.2  AvgSpecLen:144.9  StepTime:0.88  LR:0.000100\n",
      "   | > Step:64/68  GlobalStep:72170  TotalLoss:0.00327  PostnetLoss:0.00156  DecoderLoss:0.00171  StopLoss:0.05582  GradNorm:0.00411  GradNormST:0.01443  AvgTextLen:60.6  AvgSpecLen:177.6  StepTime:0.81  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:72174  AvgTotalLoss:0.07452  AvgPostnetLoss:0.00127  AvgDecoderLoss:0.00137  AvgStopLoss:0.07188  EpochTime:43.48  AvgStepTime:0.63\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.11634   PostnetLoss: 0.00418   DecoderLoss:0.00468  StopLoss: 0.10748  \n",
      "   | > TotalLoss: 0.08587   PostnetLoss: 0.00585   DecoderLoss:0.00656  StopLoss: 0.07346  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00127   Validation Loss: 0.00540\n",
      "\n",
      " > Epoch 46/1000\n",
      "   | > Step:5/68  GlobalStep:72180  TotalLoss:0.00181  PostnetLoss:0.00087  DecoderLoss:0.00093  StopLoss:0.08081  GradNorm:0.00440  GradNormST:0.01964  AvgTextLen:5.9  AvgSpecLen:33.2  StepTime:0.31  LR:0.000100\n",
      "   | > Step:15/68  GlobalStep:72190  TotalLoss:0.00220  PostnetLoss:0.00107  DecoderLoss:0.00113  StopLoss:0.06368  GradNorm:0.00345  GradNormST:0.01955  AvgTextLen:12.8  AvgSpecLen:59.1  StepTime:0.45  LR:0.000100\n",
      "   | > Step:25/68  GlobalStep:72200  TotalLoss:0.00243  PostnetLoss:0.00117  DecoderLoss:0.00126  StopLoss:0.08369  GradNorm:0.00390  GradNormST:0.01449  AvgTextLen:19.8  AvgSpecLen:73.3  StepTime:0.46  LR:0.000100\n",
      "   | > Step:35/68  GlobalStep:72210  TotalLoss:0.00269  PostnetLoss:0.00129  DecoderLoss:0.00140  StopLoss:0.06974  GradNorm:0.00364  GradNormST:0.02005  AvgTextLen:27.1  AvgSpecLen:104.7  StepTime:0.78  LR:0.000100\n",
      "   | > Step:45/68  GlobalStep:72220  TotalLoss:0.00285  PostnetLoss:0.00137  DecoderLoss:0.00148  StopLoss:0.07692  GradNorm:0.00456  GradNormST:0.01572  AvgTextLen:34.4  AvgSpecLen:118.6  StepTime:0.58  LR:0.000100\n",
      "   | > Step:55/68  GlobalStep:72230  TotalLoss:0.00317  PostnetLoss:0.00152  DecoderLoss:0.00165  StopLoss:0.07321  GradNorm:0.00422  GradNormST:0.01762  AvgTextLen:43.9  AvgSpecLen:145.2  StepTime:0.84  LR:0.000100\n",
      "   | > Step:65/68  GlobalStep:72240  TotalLoss:0.00346  PostnetLoss:0.00164  DecoderLoss:0.00182  StopLoss:0.06677  GradNorm:0.00463  GradNormST:0.03184  AvgTextLen:64.2  AvgSpecLen:207.4  StepTime:1.19  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:72243  AvgTotalLoss:0.07729  AvgPostnetLoss:0.00127  AvgDecoderLoss:0.00137  AvgStopLoss:0.07464  EpochTime:42.16  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.11699   PostnetLoss: 0.00426   DecoderLoss:0.00480  StopLoss: 0.10793  \n",
      "   | > TotalLoss: 0.08589   PostnetLoss: 0.00585   DecoderLoss:0.00656  StopLoss: 0.07349  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00127   Validation Loss: 0.00536\n",
      "\n",
      " > Epoch 47/1000\n",
      "   | > Step:6/68  GlobalStep:72250  TotalLoss:0.00188  PostnetLoss:0.00091  DecoderLoss:0.00096  StopLoss:0.06982  GradNorm:0.00651  GradNormST:0.02678  AvgTextLen:6.0  AvgSpecLen:34.8  StepTime:0.45  LR:0.000100\n",
      "   | > Step:16/68  GlobalStep:72260  TotalLoss:0.00229  PostnetLoss:0.00110  DecoderLoss:0.00118  StopLoss:0.06013  GradNorm:0.00417  GradNormST:0.01702  AvgTextLen:13.8  AvgSpecLen:60.6  StepTime:0.41  LR:0.000100\n",
      "   | > Step:26/68  GlobalStep:72270  TotalLoss:0.00247  PostnetLoss:0.00119  DecoderLoss:0.00128  StopLoss:0.06416  GradNorm:0.00384  GradNormST:0.01281  AvgTextLen:20.3  AvgSpecLen:77.5  StepTime:0.50  LR:0.000100\n",
      "   | > Step:36/68  GlobalStep:72280  TotalLoss:0.00271  PostnetLoss:0.00131  DecoderLoss:0.00140  StopLoss:0.10324  GradNorm:0.00441  GradNormST:0.01941  AvgTextLen:28.5  AvgSpecLen:100.2  StepTime:0.52  LR:0.000100\n",
      "   | > Step:46/68  GlobalStep:72290  TotalLoss:0.00296  PostnetLoss:0.00142  DecoderLoss:0.00154  StopLoss:0.06952  GradNorm:0.00563  GradNormST:0.01904  AvgTextLen:35.4  AvgSpecLen:122.6  StepTime:0.89  LR:0.000100\n",
      "   | > Step:56/68  GlobalStep:72300  TotalLoss:0.00318  PostnetLoss:0.00152  DecoderLoss:0.00166  StopLoss:0.04809  GradNorm:0.00515  GradNormST:0.01150  AvgTextLen:45.8  AvgSpecLen:145.4  StepTime:1.03  LR:0.000100\n",
      "   | > Step:66/68  GlobalStep:72310  TotalLoss:0.00366  PostnetLoss:0.00174  DecoderLoss:0.00192  StopLoss:0.06371  GradNorm:0.00409  GradNormST:0.03147  AvgTextLen:71.2  AvgSpecLen:220.8  StepTime:1.10  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:72312  AvgTotalLoss:0.07768  AvgPostnetLoss:0.00129  AvgDecoderLoss:0.00139  AvgStopLoss:0.07501  EpochTime:44.55  AvgStepTime:0.65\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.11785   PostnetLoss: 0.00426   DecoderLoss:0.00477  StopLoss: 0.10881  \n",
      "   | > TotalLoss: 0.08060   PostnetLoss: 0.00564   DecoderLoss:0.00635  StopLoss: 0.06861  \n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00129   Validation Loss: 0.00528\n",
      "\n",
      " > Epoch 48/1000\n",
      "   | > Step:7/68  GlobalStep:72320  TotalLoss:0.00195  PostnetLoss:0.00095  DecoderLoss:0.00101  StopLoss:0.08181  GradNorm:0.00491  GradNormST:0.03164  AvgTextLen:7.4  AvgSpecLen:44.6  StepTime:0.40  LR:0.000100\n",
      "   | > Step:17/68  GlobalStep:72330  TotalLoss:0.00220  PostnetLoss:0.00107  DecoderLoss:0.00113  StopLoss:0.08899  GradNorm:0.00474  GradNormST:0.01808  AvgTextLen:14.7  AvgSpecLen:66.0  StepTime:0.42  LR:0.000100\n",
      "   | > Step:27/68  GlobalStep:72340  TotalLoss:0.00255  PostnetLoss:0.00123  DecoderLoss:0.00132  StopLoss:0.09779  GradNorm:0.00391  GradNormST:0.03683  AvgTextLen:21.0  AvgSpecLen:80.2  StepTime:0.67  LR:0.000100\n",
      "   | > Step:37/68  GlobalStep:72350  TotalLoss:0.00284  PostnetLoss:0.00137  DecoderLoss:0.00147  StopLoss:0.05895  GradNorm:0.00489  GradNormST:0.01889  AvgTextLen:29.3  AvgSpecLen:102.4  StepTime:0.64  LR:0.000100\n",
      "   | > Step:47/68  GlobalStep:72360  TotalLoss:0.00309  PostnetLoss:0.00148  DecoderLoss:0.00160  StopLoss:0.06361  GradNorm:0.00612  GradNormST:0.01596  AvgTextLen:36.7  AvgSpecLen:126.9  StepTime:0.76  LR:0.000100\n",
      "   | > Step:57/68  GlobalStep:72370  TotalLoss:0.00338  PostnetLoss:0.00162  DecoderLoss:0.00176  StopLoss:0.04775  GradNorm:0.00799  GradNormST:0.01939  AvgTextLen:47.1  AvgSpecLen:161.0  StepTime:0.91  LR:0.000100\n",
      "   | > Step:67/68  GlobalStep:72380  TotalLoss:0.00368  PostnetLoss:0.00175  DecoderLoss:0.00193  StopLoss:0.07045  GradNorm:0.00614  GradNormST:0.04097  AvgTextLen:80.6  AvgSpecLen:246.5  StepTime:1.26  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:72381  AvgTotalLoss:0.07355  AvgPostnetLoss:0.00129  AvgDecoderLoss:0.00139  AvgStopLoss:0.07086  EpochTime:43.95  AvgStepTime:0.64\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.13047   PostnetLoss: 0.00428   DecoderLoss:0.00476  StopLoss: 0.12144  \n",
      "   | > TotalLoss: 0.08695   PostnetLoss: 0.00589   DecoderLoss:0.00658  StopLoss: 0.07448  \n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00129   Validation Loss: 0.00541\n",
      "\n",
      " > Epoch 49/1000\n",
      "   | > Step:8/68  GlobalStep:72390  TotalLoss:0.00197  PostnetLoss:0.00095  DecoderLoss:0.00102  StopLoss:0.09378  GradNorm:0.00412  GradNormST:0.02423  AvgTextLen:8.3  AvgSpecLen:44.0  StepTime:0.30  LR:0.000100\n",
      "   | > Step:18/68  GlobalStep:72400  TotalLoss:0.00221  PostnetLoss:0.00107  DecoderLoss:0.00115  StopLoss:0.07982  GradNorm:0.00453  GradNormST:0.01581  AvgTextLen:15.2  AvgSpecLen:64.5  StepTime:0.33  LR:0.000100\n",
      "   | > Step:28/68  GlobalStep:72410  TotalLoss:0.00251  PostnetLoss:0.00121  DecoderLoss:0.00130  StopLoss:0.08798  GradNorm:0.00349  GradNormST:0.01818  AvgTextLen:22.1  AvgSpecLen:85.4  StepTime:0.52  LR:0.000100\n",
      "   | > Step:38/68  GlobalStep:72420  TotalLoss:0.00274  PostnetLoss:0.00132  DecoderLoss:0.00142  StopLoss:0.06015  GradNorm:0.00442  GradNormST:0.01512  AvgTextLen:29.4  AvgSpecLen:104.1  StepTime:0.59  LR:0.000100\n",
      "   | > Step:48/68  GlobalStep:72430  TotalLoss:0.00303  PostnetLoss:0.00145  DecoderLoss:0.00157  StopLoss:0.05038  GradNorm:0.00575  GradNormST:0.00813  AvgTextLen:37.5  AvgSpecLen:123.8  StepTime:0.66  LR:0.000100\n",
      "   | > Step:58/68  GlobalStep:72440  TotalLoss:0.00343  PostnetLoss:0.00164  DecoderLoss:0.00179  StopLoss:0.07099  GradNorm:0.00862  GradNormST:0.03054  AvgTextLen:47.8  AvgSpecLen:158.0  StepTime:0.71  LR:0.000100\n",
      "   | > Step:68/68  GlobalStep:72450  TotalLoss:0.00357  PostnetLoss:0.00170  DecoderLoss:0.00187  StopLoss:0.06038  GradNorm:0.00482  GradNormST:0.03214  AvgTextLen:103.0  AvgSpecLen:308.9  StepTime:1.94  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:72450  AvgTotalLoss:0.06953  AvgPostnetLoss:0.00129  AvgDecoderLoss:0.00139  AvgStopLoss:0.06685  EpochTime:41.52  AvgStepTime:0.60\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.11815   PostnetLoss: 0.00418   DecoderLoss:0.00466  StopLoss: 0.10931  \n",
      "   | > TotalLoss: 0.07896   PostnetLoss: 0.00560   DecoderLoss:0.00626  StopLoss: 0.06711  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00129   Validation Loss: 0.00531\n",
      "\n",
      " > Epoch 50/1000\n",
      "   | > Step:9/68  GlobalStep:72460  TotalLoss:0.00193  PostnetLoss:0.00093  DecoderLoss:0.00099  StopLoss:0.05762  GradNorm:0.00374  GradNormST:0.02193  AvgTextLen:9.0  AvgSpecLen:42.1  StepTime:0.43  LR:0.000100\n",
      "   | > Step:19/68  GlobalStep:72470  TotalLoss:0.00222  PostnetLoss:0.00107  DecoderLoss:0.00115  StopLoss:0.08377  GradNorm:0.00337  GradNormST:0.02255  AvgTextLen:15.5  AvgSpecLen:63.2  StepTime:0.41  LR:0.000100\n",
      "   | > Step:29/68  GlobalStep:72480  TotalLoss:0.00245  PostnetLoss:0.00118  DecoderLoss:0.00127  StopLoss:0.08147  GradNorm:0.00330  GradNormST:0.01382  AvgTextLen:21.8  AvgSpecLen:83.4  StepTime:0.56  LR:0.000100\n",
      "   | > Step:39/68  GlobalStep:72490  TotalLoss:0.00281  PostnetLoss:0.00135  DecoderLoss:0.00146  StopLoss:0.05450  GradNorm:0.00393  GradNormST:0.01268  AvgTextLen:30.2  AvgSpecLen:110.4  StepTime:0.68  LR:0.000100\n",
      "   | > Step:49/68  GlobalStep:72500  TotalLoss:0.00304  PostnetLoss:0.00146  DecoderLoss:0.00158  StopLoss:0.04739  GradNorm:0.00477  GradNormST:0.00915  AvgTextLen:38.2  AvgSpecLen:127.7  StepTime:0.80  LR:0.000100\n",
      "   | > Step:59/68  GlobalStep:72510  TotalLoss:0.00338  PostnetLoss:0.00162  DecoderLoss:0.00176  StopLoss:0.05301  GradNorm:0.00879  GradNormST:0.01541  AvgTextLen:50.5  AvgSpecLen:155.9  StepTime:0.94  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:72519  AvgTotalLoss:0.07047  AvgPostnetLoss:0.00129  AvgDecoderLoss:0.00139  AvgStopLoss:0.06779  EpochTime:42.74  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.10960   PostnetLoss: 0.00416   DecoderLoss:0.00465  StopLoss: 0.10078  \n",
      "   | > TotalLoss: 0.08119   PostnetLoss: 0.00572   DecoderLoss:0.00640  StopLoss: 0.06907  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00129   Validation Loss: 0.00537\n",
      "\n",
      " > Epoch 51/1000\n",
      "   | > Step:0/68  GlobalStep:72520  TotalLoss:0.00184  PostnetLoss:0.00088  DecoderLoss:0.00096  StopLoss:0.09513  GradNorm:0.00740  GradNormST:0.02995  AvgTextLen:3.0  AvgSpecLen:25.3  StepTime:0.51  LR:0.000100\n",
      "   | > Step:10/68  GlobalStep:72530  TotalLoss:0.00204  PostnetLoss:0.00098  DecoderLoss:0.00106  StopLoss:0.05937  GradNorm:0.00368  GradNormST:0.01581  AvgTextLen:9.8  AvgSpecLen:48.6  StepTime:0.44  LR:0.000100\n",
      "   | > Step:20/68  GlobalStep:72540  TotalLoss:0.00250  PostnetLoss:0.00121  DecoderLoss:0.00129  StopLoss:0.05605  GradNorm:0.00417  GradNormST:0.01729  AvgTextLen:16.4  AvgSpecLen:64.8  StepTime:0.51  LR:0.000100\n",
      "   | > Step:30/68  GlobalStep:72550  TotalLoss:0.00252  PostnetLoss:0.00121  DecoderLoss:0.00131  StopLoss:0.07261  GradNorm:0.00346  GradNormST:0.02339  AvgTextLen:22.9  AvgSpecLen:85.8  StepTime:0.54  LR:0.000100\n",
      "   | > Step:40/68  GlobalStep:72560  TotalLoss:0.00279  PostnetLoss:0.00134  DecoderLoss:0.00144  StopLoss:0.05092  GradNorm:0.00396  GradNormST:0.01074  AvgTextLen:30.9  AvgSpecLen:105.3  StepTime:0.63  LR:0.000100\n",
      "   | > Step:50/68  GlobalStep:72570  TotalLoss:0.00305  PostnetLoss:0.00146  DecoderLoss:0.00159  StopLoss:0.06158  GradNorm:0.00526  GradNormST:0.02038  AvgTextLen:38.8  AvgSpecLen:132.0  StepTime:0.88  LR:0.000100\n",
      "   | > Step:60/68  GlobalStep:72580  TotalLoss:0.00338  PostnetLoss:0.00163  DecoderLoss:0.00176  StopLoss:0.04664  GradNorm:0.00768  GradNormST:0.01182  AvgTextLen:52.0  AvgSpecLen:172.7  StepTime:0.86  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:72588  AvgTotalLoss:0.07007  AvgPostnetLoss:0.00128  AvgDecoderLoss:0.00138  AvgStopLoss:0.06740  EpochTime:42.90  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.10630   PostnetLoss: 0.00425   DecoderLoss:0.00475  StopLoss: 0.09729  \n",
      "   | > TotalLoss: 0.08183   PostnetLoss: 0.00583   DecoderLoss:0.00655  StopLoss: 0.06945  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00128   Validation Loss: 0.00548\n",
      "\n",
      " > Epoch 52/1000\n",
      "   | > Step:1/68  GlobalStep:72590  TotalLoss:0.00181  PostnetLoss:0.00087  DecoderLoss:0.00093  StopLoss:0.07485  GradNorm:0.00509  GradNormST:0.02701  AvgTextLen:3.7  AvgSpecLen:28.6  StepTime:0.31  LR:0.000100\n",
      "   | > Step:11/68  GlobalStep:72600  TotalLoss:0.00201  PostnetLoss:0.00097  DecoderLoss:0.00104  StopLoss:0.05500  GradNorm:0.00392  GradNormST:0.01203  AvgTextLen:10.3  AvgSpecLen:49.4  StepTime:0.39  LR:0.000100\n",
      "   | > Step:21/68  GlobalStep:72610  TotalLoss:0.00233  PostnetLoss:0.00113  DecoderLoss:0.00121  StopLoss:0.09262  GradNorm:0.00354  GradNormST:0.03639  AvgTextLen:16.6  AvgSpecLen:66.3  StepTime:0.40  LR:0.000100\n",
      "   | > Step:31/68  GlobalStep:72620  TotalLoss:0.00255  PostnetLoss:0.00123  DecoderLoss:0.00132  StopLoss:0.10094  GradNorm:0.00319  GradNormST:0.04529  AvgTextLen:24.1  AvgSpecLen:82.7  StepTime:0.61  LR:0.000100\n",
      "   | > Step:41/68  GlobalStep:72630  TotalLoss:0.00288  PostnetLoss:0.00138  DecoderLoss:0.00150  StopLoss:0.05181  GradNorm:0.00405  GradNormST:0.01109  AvgTextLen:32.3  AvgSpecLen:109.4  StepTime:0.71  LR:0.000100\n",
      "   | > Step:51/68  GlobalStep:72640  TotalLoss:0.00298  PostnetLoss:0.00143  DecoderLoss:0.00155  StopLoss:0.06523  GradNorm:0.00457  GradNormST:0.00950  AvgTextLen:39.2  AvgSpecLen:126.9  StepTime:0.74  LR:0.000100\n",
      "   | > Step:61/68  GlobalStep:72650  TotalLoss:0.00338  PostnetLoss:0.00162  DecoderLoss:0.00176  StopLoss:0.05984  GradNorm:0.00751  GradNormST:0.01626  AvgTextLen:54.6  AvgSpecLen:165.5  StepTime:0.97  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:72657  AvgTotalLoss:0.07632  AvgPostnetLoss:0.00127  AvgDecoderLoss:0.00138  AvgStopLoss:0.07367  EpochTime:44.54  AvgStepTime:0.65\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.10638   PostnetLoss: 0.00425   DecoderLoss:0.00470  StopLoss: 0.09742  \n",
      "   | > TotalLoss: 0.07763   PostnetLoss: 0.00583   DecoderLoss:0.00653  StopLoss: 0.06527  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00127   Validation Loss: 0.00556\n",
      "\n",
      " > Epoch 53/1000\n",
      "   | > Step:2/68  GlobalStep:72660  TotalLoss:0.00177  PostnetLoss:0.00086  DecoderLoss:0.00091  StopLoss:0.09534  GradNorm:0.00498  GradNormST:0.02477  AvgTextLen:4.0  AvgSpecLen:31.3  StepTime:0.44  LR:0.000100\n",
      "   | > Step:12/68  GlobalStep:72670  TotalLoss:0.00206  PostnetLoss:0.00099  DecoderLoss:0.00106  StopLoss:0.07476  GradNorm:0.00411  GradNormST:0.02052  AvgTextLen:11.0  AvgSpecLen:49.3  StepTime:0.44  LR:0.000100\n",
      "   | > Step:22/68  GlobalStep:72680  TotalLoss:0.00241  PostnetLoss:0.00117  DecoderLoss:0.00125  StopLoss:0.07932  GradNorm:0.00373  GradNormST:0.01649  AvgTextLen:18.0  AvgSpecLen:72.0  StepTime:0.50  LR:0.000100\n",
      "   | > Step:32/68  GlobalStep:72690  TotalLoss:0.00258  PostnetLoss:0.00124  DecoderLoss:0.00134  StopLoss:0.08297  GradNorm:0.00334  GradNormST:0.01347  AvgTextLen:24.4  AvgSpecLen:91.5  StepTime:0.63  LR:0.000100\n",
      "   | > Step:42/68  GlobalStep:72700  TotalLoss:0.00277  PostnetLoss:0.00133  DecoderLoss:0.00144  StopLoss:0.06171  GradNorm:0.00369  GradNormST:0.02064  AvgTextLen:33.1  AvgSpecLen:109.7  StepTime:0.63  LR:0.000100\n",
      "   | > Step:52/68  GlobalStep:72710  TotalLoss:0.00315  PostnetLoss:0.00151  DecoderLoss:0.00164  StopLoss:0.06079  GradNorm:0.00611  GradNormST:0.01835  AvgTextLen:42.1  AvgSpecLen:137.5  StepTime:0.77  LR:0.000100\n",
      "   | > Step:62/68  GlobalStep:72720  TotalLoss:0.00338  PostnetLoss:0.00162  DecoderLoss:0.00176  StopLoss:0.08433  GradNorm:0.00575  GradNormST:0.01584  AvgTextLen:54.0  AvgSpecLen:174.7  StepTime:0.81  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:72726  AvgTotalLoss:0.07717  AvgPostnetLoss:0.00127  AvgDecoderLoss:0.00137  AvgStopLoss:0.07452  EpochTime:44.81  AvgStepTime:0.65\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.11927   PostnetLoss: 0.00431   DecoderLoss:0.00478  StopLoss: 0.11018  \n",
      "   | > TotalLoss: 0.08239   PostnetLoss: 0.00596   DecoderLoss:0.00666  StopLoss: 0.06977  \n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00127   Validation Loss: 0.00560\n",
      "\n",
      " > Epoch 54/1000\n",
      "   | > Step:3/68  GlobalStep:72730  TotalLoss:0.00180  PostnetLoss:0.00088  DecoderLoss:0.00093  StopLoss:0.06479  GradNorm:0.00545  GradNormST:0.02097  AvgTextLen:4.7  AvgSpecLen:29.7  StepTime:0.37  LR:0.000100\n",
      "   | > Step:13/68  GlobalStep:72740  TotalLoss:0.00205  PostnetLoss:0.00100  DecoderLoss:0.00106  StopLoss:0.10594  GradNorm:0.00478  GradNormST:0.02638  AvgTextLen:11.5  AvgSpecLen:55.2  StepTime:0.43  LR:0.000100\n",
      "   | > Step:23/68  GlobalStep:72750  TotalLoss:0.00246  PostnetLoss:0.00119  DecoderLoss:0.00127  StopLoss:0.07322  GradNorm:0.00397  GradNormST:0.01773  AvgTextLen:18.3  AvgSpecLen:72.2  StepTime:0.46  LR:0.000100\n",
      "   | > Step:33/68  GlobalStep:72760  TotalLoss:0.00268  PostnetLoss:0.00129  DecoderLoss:0.00139  StopLoss:0.07343  GradNorm:0.00375  GradNormST:0.01626  AvgTextLen:25.9  AvgSpecLen:88.9  StepTime:0.58  LR:0.000100\n",
      "   | > Step:43/68  GlobalStep:72770  TotalLoss:0.00283  PostnetLoss:0.00136  DecoderLoss:0.00147  StopLoss:0.06372  GradNorm:0.00377  GradNormST:0.01617  AvgTextLen:33.4  AvgSpecLen:115.7  StepTime:0.62  LR:0.000100\n",
      "   | > Step:53/68  GlobalStep:72780  TotalLoss:0.00308  PostnetLoss:0.00148  DecoderLoss:0.00160  StopLoss:0.05558  GradNorm:0.00590  GradNormST:0.01128  AvgTextLen:42.4  AvgSpecLen:137.6  StepTime:0.64  LR:0.000100\n",
      "   | > Step:63/68  GlobalStep:72790  TotalLoss:0.00346  PostnetLoss:0.00165  DecoderLoss:0.00181  StopLoss:0.04612  GradNorm:0.00591  GradNormST:0.00899  AvgTextLen:58.6  AvgSpecLen:185.1  StepTime:0.91  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:72795  AvgTotalLoss:0.07229  AvgPostnetLoss:0.00127  AvgDecoderLoss:0.00138  AvgStopLoss:0.06964  EpochTime:42.99  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.11654   PostnetLoss: 0.00420   DecoderLoss:0.00468  StopLoss: 0.10766  \n",
      "   | > TotalLoss: 0.07906   PostnetLoss: 0.00581   DecoderLoss:0.00652  StopLoss: 0.06672  \n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00127   Validation Loss: 0.00545\n",
      "\n",
      " > Epoch 55/1000\n",
      "   | > Step:4/68  GlobalStep:72800  TotalLoss:0.00179  PostnetLoss:0.00086  DecoderLoss:0.00092  StopLoss:0.10672  GradNorm:0.00551  GradNormST:0.04334  AvgTextLen:4.8  AvgSpecLen:31.1  StepTime:0.30  LR:0.000100\n",
      "   | > Step:14/68  GlobalStep:72810  TotalLoss:0.00223  PostnetLoss:0.00108  DecoderLoss:0.00115  StopLoss:0.07881  GradNorm:0.00521  GradNormST:0.01940  AvgTextLen:12.2  AvgSpecLen:52.0  StepTime:0.29  LR:0.000100\n",
      "   | > Step:24/68  GlobalStep:72820  TotalLoss:0.00253  PostnetLoss:0.00122  DecoderLoss:0.00131  StopLoss:0.06795  GradNorm:0.00379  GradNormST:0.01330  AvgTextLen:19.2  AvgSpecLen:79.0  StepTime:0.49  LR:0.000100\n",
      "   | > Step:34/68  GlobalStep:72830  TotalLoss:0.00271  PostnetLoss:0.00130  DecoderLoss:0.00141  StopLoss:0.05670  GradNorm:0.00329  GradNormST:0.00931  AvgTextLen:26.2  AvgSpecLen:95.9  StepTime:0.59  LR:0.000100\n",
      "   | > Step:44/68  GlobalStep:72840  TotalLoss:0.00289  PostnetLoss:0.00139  DecoderLoss:0.00150  StopLoss:0.05914  GradNorm:0.00404  GradNormST:0.01992  AvgTextLen:33.8  AvgSpecLen:116.3  StepTime:0.65  LR:0.000100\n",
      "   | > Step:54/68  GlobalStep:72850  TotalLoss:0.00312  PostnetLoss:0.00149  DecoderLoss:0.00162  StopLoss:0.06101  GradNorm:0.00449  GradNormST:0.01193  AvgTextLen:42.2  AvgSpecLen:144.9  StepTime:0.82  LR:0.000100\n",
      "   | > Step:64/68  GlobalStep:72860  TotalLoss:0.00321  PostnetLoss:0.00153  DecoderLoss:0.00168  StopLoss:0.05551  GradNorm:0.00442  GradNormST:0.01691  AvgTextLen:60.6  AvgSpecLen:177.6  StepTime:0.81  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:72864  AvgTotalLoss:0.06995  AvgPostnetLoss:0.00127  AvgDecoderLoss:0.00137  AvgStopLoss:0.06730  EpochTime:42.48  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.11059   PostnetLoss: 0.00427   DecoderLoss:0.00477  StopLoss: 0.10155  \n",
      "   | > TotalLoss: 0.07717   PostnetLoss: 0.00592   DecoderLoss:0.00661  StopLoss: 0.06464  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00127   Validation Loss: 0.00549\n",
      "\n",
      " > Epoch 56/1000\n",
      "   | > Step:5/68  GlobalStep:72870  TotalLoss:0.00185  PostnetLoss:0.00090  DecoderLoss:0.00095  StopLoss:0.07004  GradNorm:0.00662  GradNormST:0.02207  AvgTextLen:5.9  AvgSpecLen:33.2  StepTime:0.31  LR:0.000100\n",
      "   | > Step:15/68  GlobalStep:72880  TotalLoss:0.00226  PostnetLoss:0.00109  DecoderLoss:0.00116  StopLoss:0.07343  GradNorm:0.00413  GradNormST:0.02676  AvgTextLen:12.8  AvgSpecLen:59.1  StepTime:0.56  LR:0.000100\n",
      "   | > Step:25/68  GlobalStep:72890  TotalLoss:0.00237  PostnetLoss:0.00114  DecoderLoss:0.00122  StopLoss:0.06930  GradNorm:0.00363  GradNormST:0.01343  AvgTextLen:19.8  AvgSpecLen:73.3  StepTime:0.48  LR:0.000100\n",
      "   | > Step:35/68  GlobalStep:72900  TotalLoss:0.00264  PostnetLoss:0.00127  DecoderLoss:0.00137  StopLoss:0.08008  GradNorm:0.00313  GradNormST:0.03478  AvgTextLen:27.1  AvgSpecLen:104.7  StepTime:0.77  LR:0.000100\n",
      "   | > Step:45/68  GlobalStep:72910  TotalLoss:0.00280  PostnetLoss:0.00134  DecoderLoss:0.00146  StopLoss:0.08146  GradNorm:0.00312  GradNormST:0.01885  AvgTextLen:34.4  AvgSpecLen:118.6  StepTime:0.72  LR:0.000100\n",
      "   | > Step:55/68  GlobalStep:72920  TotalLoss:0.00318  PostnetLoss:0.00153  DecoderLoss:0.00165  StopLoss:0.05964  GradNorm:0.00500  GradNormST:0.01224  AvgTextLen:43.9  AvgSpecLen:145.2  StepTime:0.83  LR:0.000100\n",
      "   | > Step:65/68  GlobalStep:72930  TotalLoss:0.00345  PostnetLoss:0.00164  DecoderLoss:0.00180  StopLoss:0.04889  GradNorm:0.00431  GradNormST:0.01707  AvgTextLen:64.2  AvgSpecLen:207.4  StepTime:1.25  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:72933  AvgTotalLoss:0.07386  AvgPostnetLoss:0.00126  AvgDecoderLoss:0.00136  AvgStopLoss:0.07123  EpochTime:43.41  AvgStepTime:0.63\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.11118   PostnetLoss: 0.00432   DecoderLoss:0.00483  StopLoss: 0.10203  \n",
      "   | > TotalLoss: 0.07594   PostnetLoss: 0.00576   DecoderLoss:0.00647  StopLoss: 0.06371  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00126   Validation Loss: 0.00540\n",
      "\n",
      " > Epoch 57/1000\n",
      "   | > Step:6/68  GlobalStep:72940  TotalLoss:0.00182  PostnetLoss:0.00089  DecoderLoss:0.00093  StopLoss:0.06584  GradNorm:0.00561  GradNormST:0.02329  AvgTextLen:6.0  AvgSpecLen:34.8  StepTime:0.38  LR:0.000100\n",
      "   | > Step:16/68  GlobalStep:72950  TotalLoss:0.00225  PostnetLoss:0.00108  DecoderLoss:0.00116  StopLoss:0.06272  GradNorm:0.00512  GradNormST:0.01828  AvgTextLen:13.8  AvgSpecLen:60.6  StepTime:0.38  LR:0.000100\n",
      "   | > Step:26/68  GlobalStep:72960  TotalLoss:0.00242  PostnetLoss:0.00117  DecoderLoss:0.00125  StopLoss:0.05550  GradNorm:0.00391  GradNormST:0.01364  AvgTextLen:20.3  AvgSpecLen:77.5  StepTime:0.38  LR:0.000100\n",
      "   | > Step:36/68  GlobalStep:72970  TotalLoss:0.00265  PostnetLoss:0.00127  DecoderLoss:0.00137  StopLoss:0.07232  GradNorm:0.00350  GradNormST:0.01537  AvgTextLen:28.5  AvgSpecLen:100.2  StepTime:0.57  LR:0.000100\n",
      "   | > Step:46/68  GlobalStep:72980  TotalLoss:0.00284  PostnetLoss:0.00136  DecoderLoss:0.00147  StopLoss:0.05584  GradNorm:0.00352  GradNormST:0.01054  AvgTextLen:35.4  AvgSpecLen:122.6  StepTime:0.85  LR:0.000100\n",
      "   | > Step:56/68  GlobalStep:72990  TotalLoss:0.00314  PostnetLoss:0.00150  DecoderLoss:0.00164  StopLoss:0.05031  GradNorm:0.00464  GradNormST:0.02114  AvgTextLen:45.8  AvgSpecLen:145.4  StepTime:1.06  LR:0.000100\n",
      "   | > Step:66/68  GlobalStep:73000  TotalLoss:0.00361  PostnetLoss:0.00172  DecoderLoss:0.00189  StopLoss:0.05330  GradNorm:0.07713  GradNormST:0.01913  AvgTextLen:71.2  AvgSpecLen:220.8  StepTime:1.07  LR:0.000100\n",
      " | | > Checkpoint saving : /vm/TTS/models/cartman_models/mozilla-no-loc-fattn-stopnet-sigmoid-loss_masking-August-15-2019_11+26AM-de6aeb9/checkpoint_73000.pth.tar\n",
      "warning: audio amplitude out of range, auto clipped.\n",
      "   | > EPOCH END -- GlobalStep:73002  AvgTotalLoss:0.07122  AvgPostnetLoss:0.00126  AvgDecoderLoss:0.00136  AvgStopLoss:0.06859  EpochTime:43.33  AvgStepTime:0.63\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.09714   PostnetLoss: 0.00425   DecoderLoss:0.00477  StopLoss: 0.08812  \n",
      "   | > TotalLoss: 0.07191   PostnetLoss: 0.00582   DecoderLoss:0.00655  StopLoss: 0.05954  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00126   Validation Loss: 0.00556\n",
      "\n",
      " > Epoch 58/1000\n",
      "   | > Step:7/68  GlobalStep:73010  TotalLoss:0.00209  PostnetLoss:0.00101  DecoderLoss:0.00107  StopLoss:0.06441  GradNorm:0.00711  GradNormST:0.01349  AvgTextLen:7.4  AvgSpecLen:44.6  StepTime:0.36  LR:0.000100\n",
      "   | > Step:17/68  GlobalStep:73020  TotalLoss:0.00234  PostnetLoss:0.00113  DecoderLoss:0.00121  StopLoss:0.08263  GradNorm:0.00501  GradNormST:0.01481  AvgTextLen:14.7  AvgSpecLen:66.0  StepTime:0.44  LR:0.000100\n",
      "   | > Step:27/68  GlobalStep:73030  TotalLoss:0.00269  PostnetLoss:0.00130  DecoderLoss:0.00140  StopLoss:0.08843  GradNorm:0.00464  GradNormST:0.03851  AvgTextLen:21.0  AvgSpecLen:80.2  StepTime:0.64  LR:0.000100\n",
      "   | > Step:37/68  GlobalStep:73040  TotalLoss:0.00287  PostnetLoss:0.00138  DecoderLoss:0.00149  StopLoss:0.06610  GradNorm:0.00379  GradNormST:0.01641  AvgTextLen:29.3  AvgSpecLen:102.4  StepTime:0.63  LR:0.000100\n",
      "   | > Step:47/68  GlobalStep:73050  TotalLoss:0.00306  PostnetLoss:0.00147  DecoderLoss:0.00159  StopLoss:0.05427  GradNorm:0.00491  GradNormST:0.01251  AvgTextLen:36.7  AvgSpecLen:126.9  StepTime:0.66  LR:0.000100\n",
      "   | > Step:57/68  GlobalStep:73060  TotalLoss:0.00328  PostnetLoss:0.00157  DecoderLoss:0.00171  StopLoss:0.04693  GradNorm:0.00420  GradNormST:0.01009  AvgTextLen:47.1  AvgSpecLen:161.0  StepTime:0.70  LR:0.000100\n",
      "   | > Step:67/68  GlobalStep:73070  TotalLoss:0.00374  PostnetLoss:0.00178  DecoderLoss:0.00197  StopLoss:0.05101  GradNorm:0.00496  GradNormST:0.01864  AvgTextLen:80.6  AvgSpecLen:246.5  StepTime:1.22  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:73071  AvgTotalLoss:0.07039  AvgPostnetLoss:0.00132  AvgDecoderLoss:0.00143  AvgStopLoss:0.06765  EpochTime:43.11  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.11148   PostnetLoss: 0.00428   DecoderLoss:0.00482  StopLoss: 0.10238  \n",
      "   | > TotalLoss: 0.07812   PostnetLoss: 0.00610   DecoderLoss:0.00686  StopLoss: 0.06517  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00132   Validation Loss: 0.00542\n",
      "\n",
      " > Epoch 59/1000\n",
      "   | > Step:8/68  GlobalStep:73080  TotalLoss:0.00214  PostnetLoss:0.00103  DecoderLoss:0.00110  StopLoss:0.08649  GradNorm:0.00578  GradNormST:0.02687  AvgTextLen:8.3  AvgSpecLen:44.0  StepTime:0.30  LR:0.000100\n",
      "   | > Step:18/68  GlobalStep:73090  TotalLoss:0.00229  PostnetLoss:0.00110  DecoderLoss:0.00119  StopLoss:0.06193  GradNorm:0.00498  GradNormST:0.01278  AvgTextLen:15.2  AvgSpecLen:64.5  StepTime:0.42  LR:0.000100\n",
      "   | > Step:28/68  GlobalStep:73100  TotalLoss:0.00261  PostnetLoss:0.00126  DecoderLoss:0.00135  StopLoss:0.08324  GradNorm:0.00446  GradNormST:0.01568  AvgTextLen:22.1  AvgSpecLen:85.4  StepTime:0.54  LR:0.000100\n",
      "   | > Step:38/68  GlobalStep:73110  TotalLoss:0.00273  PostnetLoss:0.00131  DecoderLoss:0.00141  StopLoss:0.06255  GradNorm:0.00403  GradNormST:0.01778  AvgTextLen:29.4  AvgSpecLen:104.1  StepTime:0.64  LR:0.000100\n",
      "   | > Step:48/68  GlobalStep:73120  TotalLoss:0.00297  PostnetLoss:0.00143  DecoderLoss:0.00155  StopLoss:0.06470  GradNorm:0.00364  GradNormST:0.01589  AvgTextLen:37.5  AvgSpecLen:123.8  StepTime:0.55  LR:0.000100\n",
      "   | > Step:58/68  GlobalStep:73130  TotalLoss:0.00323  PostnetLoss:0.00155  DecoderLoss:0.00168  StopLoss:0.05518  GradNorm:0.00388  GradNormST:0.01414  AvgTextLen:47.8  AvgSpecLen:158.0  StepTime:0.83  LR:0.000100\n",
      "   | > Step:68/68  GlobalStep:73140  TotalLoss:0.00356  PostnetLoss:0.00170  DecoderLoss:0.00187  StopLoss:0.05035  GradNorm:0.00299  GradNormST:0.02186  AvgTextLen:103.0  AvgSpecLen:308.9  StepTime:1.90  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:73140  AvgTotalLoss:0.06798  AvgPostnetLoss:0.00128  AvgDecoderLoss:0.00139  AvgStopLoss:0.06530  EpochTime:42.71  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.10798   PostnetLoss: 0.00436   DecoderLoss:0.00490  StopLoss: 0.09872  \n",
      "   | > TotalLoss: 0.08060   PostnetLoss: 0.00632   DecoderLoss:0.00710  StopLoss: 0.06719  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00128   Validation Loss: 0.00569\n",
      "\n",
      " > Epoch 60/1000\n",
      "   | > Step:9/68  GlobalStep:73150  TotalLoss:0.00195  PostnetLoss:0.00094  DecoderLoss:0.00101  StopLoss:0.06196  GradNorm:0.00439  GradNormST:0.02632  AvgTextLen:9.0  AvgSpecLen:42.1  StepTime:0.41  LR:0.000100\n",
      "   | > Step:19/68  GlobalStep:73160  TotalLoss:0.00225  PostnetLoss:0.00109  DecoderLoss:0.00116  StopLoss:0.07447  GradNorm:0.00605  GradNormST:0.02098  AvgTextLen:15.5  AvgSpecLen:63.2  StepTime:0.41  LR:0.000100\n",
      "   | > Step:29/68  GlobalStep:73170  TotalLoss:0.00244  PostnetLoss:0.00118  DecoderLoss:0.00126  StopLoss:0.08371  GradNorm:0.00417  GradNormST:0.01298  AvgTextLen:21.8  AvgSpecLen:83.4  StepTime:0.57  LR:0.000100\n",
      "   | > Step:39/68  GlobalStep:73180  TotalLoss:0.00283  PostnetLoss:0.00136  DecoderLoss:0.00147  StopLoss:0.05183  GradNorm:0.00422  GradNormST:0.00828  AvgTextLen:30.2  AvgSpecLen:110.4  StepTime:0.56  LR:0.000100\n",
      "   | > Step:49/68  GlobalStep:73190  TotalLoss:0.00297  PostnetLoss:0.00142  DecoderLoss:0.00155  StopLoss:0.04906  GradNorm:0.00420  GradNormST:0.00896  AvgTextLen:38.2  AvgSpecLen:127.7  StepTime:0.73  LR:0.000100\n",
      "   | > Step:59/68  GlobalStep:73200  TotalLoss:0.00323  PostnetLoss:0.00154  DecoderLoss:0.00169  StopLoss:0.05621  GradNorm:0.00436  GradNormST:0.01086  AvgTextLen:50.5  AvgSpecLen:155.9  StepTime:0.72  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:73209  AvgTotalLoss:0.06766  AvgPostnetLoss:0.00127  AvgDecoderLoss:0.00137  AvgStopLoss:0.06502  EpochTime:43.18  AvgStepTime:0.63\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.10525   PostnetLoss: 0.00427   DecoderLoss:0.00480  StopLoss: 0.09617  \n",
      "   | > TotalLoss: 0.08025   PostnetLoss: 0.00623   DecoderLoss:0.00703  StopLoss: 0.06700  \n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00127   Validation Loss: 0.00569\n",
      "\n",
      " > Epoch 61/1000\n",
      "   | > Step:0/68  GlobalStep:73210  TotalLoss:0.00183  PostnetLoss:0.00087  DecoderLoss:0.00096  StopLoss:0.11291  GradNorm:0.00537  GradNormST:0.04042  AvgTextLen:3.0  AvgSpecLen:25.3  StepTime:0.41  LR:0.000100\n",
      "   | > Step:10/68  GlobalStep:73220  TotalLoss:0.00201  PostnetLoss:0.00097  DecoderLoss:0.00104  StopLoss:0.08561  GradNorm:0.00422  GradNormST:0.03216  AvgTextLen:9.8  AvgSpecLen:48.6  StepTime:0.55  LR:0.000100\n",
      "   | > Step:20/68  GlobalStep:73230  TotalLoss:0.00241  PostnetLoss:0.00116  DecoderLoss:0.00125  StopLoss:0.05766  GradNorm:0.00438  GradNormST:0.01961  AvgTextLen:16.4  AvgSpecLen:64.8  StepTime:0.49  LR:0.000100\n",
      "   | > Step:30/68  GlobalStep:73240  TotalLoss:0.00254  PostnetLoss:0.00123  DecoderLoss:0.00131  StopLoss:0.06590  GradNorm:0.00454  GradNormST:0.01019  AvgTextLen:22.9  AvgSpecLen:85.8  StepTime:0.56  LR:0.000100\n",
      "   | > Step:40/68  GlobalStep:73250  TotalLoss:0.00278  PostnetLoss:0.00133  DecoderLoss:0.00144  StopLoss:0.05313  GradNorm:0.00406  GradNormST:0.00950  AvgTextLen:30.9  AvgSpecLen:105.3  StepTime:0.53  LR:0.000100\n",
      "   | > Step:50/68  GlobalStep:73260  TotalLoss:0.00297  PostnetLoss:0.00142  DecoderLoss:0.00155  StopLoss:0.05501  GradNorm:0.00391  GradNormST:0.01659  AvgTextLen:38.8  AvgSpecLen:132.0  StepTime:0.73  LR:0.000100\n",
      "   | > Step:60/68  GlobalStep:73270  TotalLoss:0.00324  PostnetLoss:0.00155  DecoderLoss:0.00169  StopLoss:0.04222  GradNorm:0.00391  GradNormST:0.01051  AvgTextLen:52.0  AvgSpecLen:172.7  StepTime:0.99  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:73278  AvgTotalLoss:0.06845  AvgPostnetLoss:0.00126  AvgDecoderLoss:0.00136  AvgStopLoss:0.06583  EpochTime:43.46  AvgStepTime:0.63\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.11216   PostnetLoss: 0.00433   DecoderLoss:0.00487  StopLoss: 0.10296  \n",
      "   | > TotalLoss: 0.08185   PostnetLoss: 0.00652   DecoderLoss:0.00734  StopLoss: 0.06798  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00126   Validation Loss: 0.00590\n",
      "\n",
      " > Epoch 62/1000\n",
      "   | > Step:1/68  GlobalStep:73280  TotalLoss:0.00172  PostnetLoss:0.00083  DecoderLoss:0.00089  StopLoss:0.07538  GradNorm:0.00444  GradNormST:0.02742  AvgTextLen:3.7  AvgSpecLen:28.6  StepTime:0.25  LR:0.000100\n",
      "   | > Step:11/68  GlobalStep:73290  TotalLoss:0.00200  PostnetLoss:0.00097  DecoderLoss:0.00103  StopLoss:0.06046  GradNorm:0.00441  GradNormST:0.01657  AvgTextLen:10.3  AvgSpecLen:49.4  StepTime:0.38  LR:0.000100\n",
      "   | > Step:21/68  GlobalStep:73300  TotalLoss:0.00236  PostnetLoss:0.00114  DecoderLoss:0.00122  StopLoss:0.07789  GradNorm:0.00465  GradNormST:0.02457  AvgTextLen:16.6  AvgSpecLen:66.3  StepTime:0.46  LR:0.000100\n",
      "   | > Step:31/68  GlobalStep:73310  TotalLoss:0.00258  PostnetLoss:0.00125  DecoderLoss:0.00134  StopLoss:0.06099  GradNorm:0.00460  GradNormST:0.02069  AvgTextLen:24.1  AvgSpecLen:82.7  StepTime:0.47  LR:0.000100\n",
      "   | > Step:41/68  GlobalStep:73320  TotalLoss:0.00282  PostnetLoss:0.00136  DecoderLoss:0.00146  StopLoss:0.04456  GradNorm:0.00354  GradNormST:0.01033  AvgTextLen:32.3  AvgSpecLen:109.4  StepTime:0.65  LR:0.000100\n",
      "   | > Step:51/68  GlobalStep:73330  TotalLoss:0.00296  PostnetLoss:0.00142  DecoderLoss:0.00154  StopLoss:0.05206  GradNorm:0.00351  GradNormST:0.01085  AvgTextLen:39.2  AvgSpecLen:126.9  StepTime:0.79  LR:0.000100\n",
      "   | > Step:61/68  GlobalStep:73340  TotalLoss:0.00318  PostnetLoss:0.00152  DecoderLoss:0.00166  StopLoss:0.04860  GradNorm:0.00392  GradNormST:0.01682  AvgTextLen:54.6  AvgSpecLen:165.5  StepTime:0.94  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:73347  AvgTotalLoss:0.06693  AvgPostnetLoss:0.00126  AvgDecoderLoss:0.00136  AvgStopLoss:0.06432  EpochTime:42.76  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.10210   PostnetLoss: 0.00438   DecoderLoss:0.00491  StopLoss: 0.09281  \n",
      "   | > TotalLoss: 0.07985   PostnetLoss: 0.00617   DecoderLoss:0.00691  StopLoss: 0.06678  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00126   Validation Loss: 0.00572\n",
      "\n",
      " > Epoch 63/1000\n",
      "   | > Step:2/68  GlobalStep:73350  TotalLoss:0.00167  PostnetLoss:0.00081  DecoderLoss:0.00086  StopLoss:0.07558  GradNorm:0.00468  GradNormST:0.02524  AvgTextLen:4.0  AvgSpecLen:31.3  StepTime:0.42  LR:0.000100\n",
      "   | > Step:12/68  GlobalStep:73360  TotalLoss:0.00196  PostnetLoss:0.00095  DecoderLoss:0.00102  StopLoss:0.08291  GradNorm:0.00370  GradNormST:0.02219  AvgTextLen:11.0  AvgSpecLen:49.3  StepTime:0.37  LR:0.000100\n",
      "   | > Step:22/68  GlobalStep:73370  TotalLoss:0.00238  PostnetLoss:0.00115  DecoderLoss:0.00123  StopLoss:0.07373  GradNorm:0.00400  GradNormST:0.01542  AvgTextLen:18.0  AvgSpecLen:72.0  StepTime:0.49  LR:0.000100\n",
      "   | > Step:32/68  GlobalStep:73380  TotalLoss:0.00256  PostnetLoss:0.00123  DecoderLoss:0.00133  StopLoss:0.07565  GradNorm:0.00391  GradNormST:0.02325  AvgTextLen:24.4  AvgSpecLen:91.5  StepTime:0.46  LR:0.000100\n",
      "   | > Step:42/68  GlobalStep:73390  TotalLoss:0.00277  PostnetLoss:0.00133  DecoderLoss:0.00144  StopLoss:0.05889  GradNorm:0.00345  GradNormST:0.01734  AvgTextLen:33.1  AvgSpecLen:109.7  StepTime:0.49  LR:0.000100\n",
      "   | > Step:52/68  GlobalStep:73400  TotalLoss:0.00304  PostnetLoss:0.00146  DecoderLoss:0.00158  StopLoss:0.06327  GradNorm:0.00350  GradNormST:0.01563  AvgTextLen:42.1  AvgSpecLen:137.5  StepTime:0.80  LR:0.000100\n",
      "   | > Step:62/68  GlobalStep:73410  TotalLoss:0.00327  PostnetLoss:0.00157  DecoderLoss:0.00171  StopLoss:0.05536  GradNorm:0.00373  GradNormST:0.01631  AvgTextLen:54.0  AvgSpecLen:174.7  StepTime:0.75  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:73416  AvgTotalLoss:0.06846  AvgPostnetLoss:0.00125  AvgDecoderLoss:0.00135  AvgStopLoss:0.06585  EpochTime:41.17  AvgStepTime:0.60\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.11571   PostnetLoss: 0.00437   DecoderLoss:0.00488  StopLoss: 0.10647  \n",
      "   | > TotalLoss: 0.08360   PostnetLoss: 0.00641   DecoderLoss:0.00719  StopLoss: 0.07000  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00125   Validation Loss: 0.00580\n",
      "\n",
      " > Epoch 64/1000\n",
      "   | > Step:3/68  GlobalStep:73420  TotalLoss:0.00178  PostnetLoss:0.00086  DecoderLoss:0.00092  StopLoss:0.07020  GradNorm:0.00581  GradNormST:0.02019  AvgTextLen:4.7  AvgSpecLen:29.7  StepTime:0.28  LR:0.000100\n",
      "   | > Step:13/68  GlobalStep:73430  TotalLoss:0.00204  PostnetLoss:0.00099  DecoderLoss:0.00105  StopLoss:0.11137  GradNorm:0.00509  GradNormST:0.02600  AvgTextLen:11.5  AvgSpecLen:55.2  StepTime:0.44  LR:0.000100\n",
      "   | > Step:23/68  GlobalStep:73440  TotalLoss:0.00238  PostnetLoss:0.00115  DecoderLoss:0.00123  StopLoss:0.06607  GradNorm:0.00366  GradNormST:0.01837  AvgTextLen:18.3  AvgSpecLen:72.2  StepTime:0.45  LR:0.000100\n",
      "   | > Step:33/68  GlobalStep:73450  TotalLoss:0.00263  PostnetLoss:0.00127  DecoderLoss:0.00136  StopLoss:0.06463  GradNorm:0.00380  GradNormST:0.01357  AvgTextLen:25.9  AvgSpecLen:88.9  StepTime:0.52  LR:0.000100\n",
      "   | > Step:43/68  GlobalStep:73460  TotalLoss:0.00277  PostnetLoss:0.00133  DecoderLoss:0.00143  StopLoss:0.06756  GradNorm:0.00305  GradNormST:0.01926  AvgTextLen:33.4  AvgSpecLen:115.7  StepTime:0.62  LR:0.000100\n",
      "   | > Step:53/68  GlobalStep:73470  TotalLoss:0.00296  PostnetLoss:0.00142  DecoderLoss:0.00154  StopLoss:0.04794  GradNorm:0.00336  GradNormST:0.00853  AvgTextLen:42.4  AvgSpecLen:137.6  StepTime:0.73  LR:0.000100\n",
      "   | > Step:63/68  GlobalStep:73480  TotalLoss:0.00334  PostnetLoss:0.00160  DecoderLoss:0.00175  StopLoss:0.04704  GradNorm:0.00318  GradNormST:0.01536  AvgTextLen:58.6  AvgSpecLen:185.1  StepTime:1.00  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:73485  AvgTotalLoss:0.06854  AvgPostnetLoss:0.00125  AvgDecoderLoss:0.00135  AvgStopLoss:0.06594  EpochTime:42.37  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.11047   PostnetLoss: 0.00431   DecoderLoss:0.00483  StopLoss: 0.10133  \n",
      "   | > TotalLoss: 0.08395   PostnetLoss: 0.00659   DecoderLoss:0.00736  StopLoss: 0.07000  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00125   Validation Loss: 0.00587\n",
      "\n",
      " > Epoch 65/1000\n",
      "   | > Step:4/68  GlobalStep:73490  TotalLoss:0.00173  PostnetLoss:0.00083  DecoderLoss:0.00090  StopLoss:0.11650  GradNorm:0.00396  GradNormST:0.04200  AvgTextLen:4.8  AvgSpecLen:31.1  StepTime:0.28  LR:0.000100\n",
      "   | > Step:14/68  GlobalStep:73500  TotalLoss:0.00212  PostnetLoss:0.00103  DecoderLoss:0.00109  StopLoss:0.07323  GradNorm:0.00473  GradNormST:0.02084  AvgTextLen:12.2  AvgSpecLen:52.0  StepTime:0.37  LR:0.000100\n",
      "   | > Step:24/68  GlobalStep:73510  TotalLoss:0.00248  PostnetLoss:0.00119  DecoderLoss:0.00128  StopLoss:0.07303  GradNorm:0.00412  GradNormST:0.01428  AvgTextLen:19.2  AvgSpecLen:79.0  StepTime:0.51  LR:0.000100\n",
      "   | > Step:34/68  GlobalStep:73520  TotalLoss:0.00270  PostnetLoss:0.00130  DecoderLoss:0.00140  StopLoss:0.04792  GradNorm:0.00378  GradNormST:0.00809  AvgTextLen:26.2  AvgSpecLen:95.9  StepTime:0.48  LR:0.000100\n",
      "   | > Step:44/68  GlobalStep:73530  TotalLoss:0.00283  PostnetLoss:0.00136  DecoderLoss:0.00147  StopLoss:0.06743  GradNorm:0.00336  GradNormST:0.02354  AvgTextLen:33.8  AvgSpecLen:116.3  StepTime:0.65  LR:0.000100\n",
      "   | > Step:54/68  GlobalStep:73540  TotalLoss:0.00311  PostnetLoss:0.00149  DecoderLoss:0.00162  StopLoss:0.06327  GradNorm:0.00340  GradNormST:0.01115  AvgTextLen:42.2  AvgSpecLen:144.9  StepTime:0.84  LR:0.000100\n",
      "   | > Step:64/68  GlobalStep:73550  TotalLoss:0.00316  PostnetLoss:0.00151  DecoderLoss:0.00166  StopLoss:0.05083  GradNorm:0.00358  GradNormST:0.01348  AvgTextLen:60.6  AvgSpecLen:177.6  StepTime:0.80  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:73554  AvgTotalLoss:0.06717  AvgPostnetLoss:0.00125  AvgDecoderLoss:0.00134  AvgStopLoss:0.06458  EpochTime:42.41  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.10853   PostnetLoss: 0.00432   DecoderLoss:0.00483  StopLoss: 0.09939  \n",
      "   | > TotalLoss: 0.08436   PostnetLoss: 0.00656   DecoderLoss:0.00735  StopLoss: 0.07044  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00125   Validation Loss: 0.00588\n",
      "\n",
      " > Epoch 66/1000\n",
      "   | > Step:5/68  GlobalStep:73560  TotalLoss:0.00178  PostnetLoss:0.00086  DecoderLoss:0.00092  StopLoss:0.07500  GradNorm:0.00376  GradNormST:0.01537  AvgTextLen:5.9  AvgSpecLen:33.2  StepTime:0.36  LR:0.000100\n",
      "   | > Step:15/68  GlobalStep:73570  TotalLoss:0.00218  PostnetLoss:0.00105  DecoderLoss:0.00112  StopLoss:0.05907  GradNorm:0.00455  GradNormST:0.02498  AvgTextLen:12.8  AvgSpecLen:59.1  StepTime:0.54  LR:0.000100\n",
      "   | > Step:25/68  GlobalStep:73580  TotalLoss:0.00241  PostnetLoss:0.00116  DecoderLoss:0.00125  StopLoss:0.06242  GradNorm:0.00405  GradNormST:0.00980  AvgTextLen:19.8  AvgSpecLen:73.3  StepTime:0.48  LR:0.000100\n",
      "   | > Step:35/68  GlobalStep:73590  TotalLoss:0.00263  PostnetLoss:0.00127  DecoderLoss:0.00136  StopLoss:0.08063  GradNorm:0.00386  GradNormST:0.03205  AvgTextLen:27.1  AvgSpecLen:104.7  StepTime:0.74  LR:0.000100\n",
      "   | > Step:45/68  GlobalStep:73600  TotalLoss:0.00278  PostnetLoss:0.00133  DecoderLoss:0.00145  StopLoss:0.06586  GradNorm:0.00325  GradNormST:0.02097  AvgTextLen:34.4  AvgSpecLen:118.6  StepTime:0.58  LR:0.000100\n",
      "   | > Step:55/68  GlobalStep:73610  TotalLoss:0.00307  PostnetLoss:0.00147  DecoderLoss:0.00160  StopLoss:0.05100  GradNorm:0.00343  GradNormST:0.00913  AvgTextLen:43.9  AvgSpecLen:145.2  StepTime:0.79  LR:0.000100\n",
      "   | > Step:65/68  GlobalStep:73620  TotalLoss:0.00346  PostnetLoss:0.00165  DecoderLoss:0.00181  StopLoss:0.05033  GradNorm:0.00484  GradNormST:0.01582  AvgTextLen:64.2  AvgSpecLen:207.4  StepTime:1.18  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:73623  AvgTotalLoss:0.06801  AvgPostnetLoss:0.00125  AvgDecoderLoss:0.00135  AvgStopLoss:0.06542  EpochTime:42.56  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.10987   PostnetLoss: 0.00436   DecoderLoss:0.00488  StopLoss: 0.10063  \n",
      "   | > TotalLoss: 0.08420   PostnetLoss: 0.00655   DecoderLoss:0.00733  StopLoss: 0.07032  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00125   Validation Loss: 0.00587\n",
      "\n",
      " > Epoch 67/1000\n",
      "   | > Step:6/68  GlobalStep:73630  TotalLoss:0.00177  PostnetLoss:0.00086  DecoderLoss:0.00091  StopLoss:0.06717  GradNorm:0.00424  GradNormST:0.03166  AvgTextLen:6.0  AvgSpecLen:34.8  StepTime:0.37  LR:0.000100\n",
      "   | > Step:16/68  GlobalStep:73640  TotalLoss:0.00224  PostnetLoss:0.00108  DecoderLoss:0.00115  StopLoss:0.05614  GradNorm:0.00438  GradNormST:0.01745  AvgTextLen:13.8  AvgSpecLen:60.6  StepTime:0.36  LR:0.000100\n",
      "   | > Step:26/68  GlobalStep:73650  TotalLoss:0.00245  PostnetLoss:0.00118  DecoderLoss:0.00127  StopLoss:0.05273  GradNorm:0.00367  GradNormST:0.01266  AvgTextLen:20.3  AvgSpecLen:77.5  StepTime:0.46  LR:0.000100\n",
      "   | > Step:36/68  GlobalStep:73660  TotalLoss:0.00262  PostnetLoss:0.00126  DecoderLoss:0.00136  StopLoss:0.08433  GradNorm:0.00347  GradNormST:0.01614  AvgTextLen:28.5  AvgSpecLen:100.2  StepTime:0.55  LR:0.000100\n",
      "   | > Step:46/68  GlobalStep:73670  TotalLoss:0.00283  PostnetLoss:0.00136  DecoderLoss:0.00147  StopLoss:0.06462  GradNorm:0.00344  GradNormST:0.01454  AvgTextLen:35.4  AvgSpecLen:122.6  StepTime:0.67  LR:0.000100\n",
      "   | > Step:56/68  GlobalStep:73680  TotalLoss:0.00311  PostnetLoss:0.00149  DecoderLoss:0.00162  StopLoss:0.06070  GradNorm:0.01683  GradNormST:0.03233  AvgTextLen:45.8  AvgSpecLen:145.4  StepTime:0.96  LR:0.000100\n",
      "   | > Step:66/68  GlobalStep:73690  TotalLoss:0.00368  PostnetLoss:0.00175  DecoderLoss:0.00193  StopLoss:0.11863  GradNorm:0.00379  GradNormST:0.05330  AvgTextLen:71.2  AvgSpecLen:220.8  StepTime:1.04  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:73692  AvgTotalLoss:0.07335  AvgPostnetLoss:0.00125  AvgDecoderLoss:0.00135  AvgStopLoss:0.07074  EpochTime:41.81  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.12043   PostnetLoss: 0.00426   DecoderLoss:0.00478  StopLoss: 0.11139  \n",
      "   | > TotalLoss: 0.08430   PostnetLoss: 0.00675   DecoderLoss:0.00756  StopLoss: 0.06999  \n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00125   Validation Loss: 0.00596\n",
      "\n",
      " > Epoch 68/1000\n",
      "   | > Step:7/68  GlobalStep:73700  TotalLoss:0.00190  PostnetLoss:0.00092  DecoderLoss:0.00098  StopLoss:0.10242  GradNorm:0.00409  GradNormST:0.06234  AvgTextLen:7.4  AvgSpecLen:44.6  StepTime:0.42  LR:0.000100\n",
      "   | > Step:17/68  GlobalStep:73710  TotalLoss:0.00219  PostnetLoss:0.00106  DecoderLoss:0.00113  StopLoss:0.11474  GradNorm:0.00380  GradNormST:0.04025  AvgTextLen:14.7  AvgSpecLen:66.0  StepTime:0.42  LR:0.000100\n",
      "   | > Step:27/68  GlobalStep:73720  TotalLoss:0.00249  PostnetLoss:0.00120  DecoderLoss:0.00129  StopLoss:0.08955  GradNorm:0.00354  GradNormST:0.04737  AvgTextLen:21.0  AvgSpecLen:80.2  StepTime:0.69  LR:0.000100\n",
      "   | > Step:37/68  GlobalStep:73730  TotalLoss:0.00274  PostnetLoss:0.00132  DecoderLoss:0.00142  StopLoss:0.08007  GradNorm:0.00364  GradNormST:0.02315  AvgTextLen:29.3  AvgSpecLen:102.4  StepTime:0.62  LR:0.000100\n",
      "   | > Step:47/68  GlobalStep:73740  TotalLoss:0.00294  PostnetLoss:0.00141  DecoderLoss:0.00153  StopLoss:0.06660  GradNorm:0.00333  GradNormST:0.01258  AvgTextLen:36.7  AvgSpecLen:126.9  StepTime:0.70  LR:0.000100\n",
      "   | > Step:57/68  GlobalStep:73750  TotalLoss:0.00321  PostnetLoss:0.00153  DecoderLoss:0.00167  StopLoss:0.05603  GradNorm:0.00352  GradNormST:0.01321  AvgTextLen:47.1  AvgSpecLen:161.0  StepTime:0.82  LR:0.000100\n",
      "   | > Step:67/68  GlobalStep:73760  TotalLoss:0.00355  PostnetLoss:0.00169  DecoderLoss:0.00186  StopLoss:0.08165  GradNorm:0.00299  GradNormST:0.02491  AvgTextLen:80.6  AvgSpecLen:246.5  StepTime:1.22  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:73761  AvgTotalLoss:0.08761  AvgPostnetLoss:0.00126  AvgDecoderLoss:0.00135  AvgStopLoss:0.08500  EpochTime:42.20  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.11008   PostnetLoss: 0.00433   DecoderLoss:0.00483  StopLoss: 0.10092  \n",
      "   | > TotalLoss: 0.08347   PostnetLoss: 0.00636   DecoderLoss:0.00710  StopLoss: 0.07001  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00126   Validation Loss: 0.00570\n",
      "\n",
      " > Epoch 69/1000\n",
      "   | > Step:8/68  GlobalStep:73770  TotalLoss:0.00192  PostnetLoss:0.00093  DecoderLoss:0.00099  StopLoss:0.09714  GradNorm:0.00447  GradNormST:0.02241  AvgTextLen:8.3  AvgSpecLen:44.0  StepTime:0.29  LR:0.000100\n",
      "   | > Step:18/68  GlobalStep:73780  TotalLoss:0.00212  PostnetLoss:0.00102  DecoderLoss:0.00110  StopLoss:0.07006  GradNorm:0.00340  GradNormST:0.01348  AvgTextLen:15.2  AvgSpecLen:64.5  StepTime:0.43  LR:0.000100\n",
      "   | > Step:28/68  GlobalStep:73790  TotalLoss:0.00247  PostnetLoss:0.00119  DecoderLoss:0.00128  StopLoss:0.08898  GradNorm:0.00331  GradNormST:0.01960  AvgTextLen:22.1  AvgSpecLen:85.4  StepTime:0.58  LR:0.000100\n",
      "   | > Step:38/68  GlobalStep:73800  TotalLoss:0.00265  PostnetLoss:0.00128  DecoderLoss:0.00138  StopLoss:0.08612  GradNorm:0.00340  GradNormST:0.03074  AvgTextLen:29.4  AvgSpecLen:104.1  StepTime:0.50  LR:0.000100\n",
      "   | > Step:48/68  GlobalStep:73810  TotalLoss:0.00289  PostnetLoss:0.00139  DecoderLoss:0.00150  StopLoss:0.05957  GradNorm:0.00321  GradNormST:0.01154  AvgTextLen:37.5  AvgSpecLen:123.8  StepTime:0.71  LR:0.000100\n",
      "   | > Step:58/68  GlobalStep:73820  TotalLoss:0.00316  PostnetLoss:0.00152  DecoderLoss:0.00164  StopLoss:0.07616  GradNorm:0.00375  GradNormST:0.01973  AvgTextLen:47.8  AvgSpecLen:158.0  StepTime:0.88  LR:0.000100\n",
      "   | > Step:68/68  GlobalStep:73830  TotalLoss:0.00346  PostnetLoss:0.00165  DecoderLoss:0.00181  StopLoss:0.05284  GradNorm:0.00292  GradNormST:0.02000  AvgTextLen:103.0  AvgSpecLen:308.9  StepTime:1.88  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:73830  AvgTotalLoss:0.07780  AvgPostnetLoss:0.00124  AvgDecoderLoss:0.00134  AvgStopLoss:0.07521  EpochTime:42.67  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.12029   PostnetLoss: 0.00436   DecoderLoss:0.00486  StopLoss: 0.11106  \n",
      "   | > TotalLoss: 0.08700   PostnetLoss: 0.00640   DecoderLoss:0.00715  StopLoss: 0.07345  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00124   Validation Loss: 0.00577\n",
      "\n",
      " > Epoch 70/1000\n",
      "   | > Step:9/68  GlobalStep:73840  TotalLoss:0.00191  PostnetLoss:0.00092  DecoderLoss:0.00098  StopLoss:0.05957  GradNorm:0.00381  GradNormST:0.01962  AvgTextLen:9.0  AvgSpecLen:42.1  StepTime:0.43  LR:0.000100\n",
      "   | > Step:19/68  GlobalStep:73850  TotalLoss:0.00217  PostnetLoss:0.00105  DecoderLoss:0.00112  StopLoss:0.09633  GradNorm:0.00404  GradNormST:0.02194  AvgTextLen:15.5  AvgSpecLen:63.2  StepTime:0.42  LR:0.000100\n",
      "   | > Step:29/68  GlobalStep:73860  TotalLoss:0.00244  PostnetLoss:0.00118  DecoderLoss:0.00127  StopLoss:0.09594  GradNorm:0.00341  GradNormST:0.01876  AvgTextLen:21.8  AvgSpecLen:83.4  StepTime:0.56  LR:0.000100\n",
      "   | > Step:39/68  GlobalStep:73870  TotalLoss:0.00271  PostnetLoss:0.00130  DecoderLoss:0.00141  StopLoss:0.06732  GradNorm:0.00314  GradNormST:0.01846  AvgTextLen:30.2  AvgSpecLen:110.4  StepTime:0.67  LR:0.000100\n",
      "   | > Step:49/68  GlobalStep:73880  TotalLoss:0.00291  PostnetLoss:0.00140  DecoderLoss:0.00151  StopLoss:0.06522  GradNorm:0.00307  GradNormST:0.01518  AvgTextLen:38.2  AvgSpecLen:127.7  StepTime:0.62  LR:0.000100\n",
      "   | > Step:59/68  GlobalStep:73890  TotalLoss:0.00313  PostnetLoss:0.00150  DecoderLoss:0.00164  StopLoss:0.05772  GradNorm:0.00400  GradNormST:0.00882  AvgTextLen:50.5  AvgSpecLen:155.9  StepTime:0.80  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:73899  AvgTotalLoss:0.07650  AvgPostnetLoss:0.00124  AvgDecoderLoss:0.00134  AvgStopLoss:0.07392  EpochTime:42.18  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.12281   PostnetLoss: 0.00424   DecoderLoss:0.00475  StopLoss: 0.11382  \n",
      "   | > TotalLoss: 0.09017   PostnetLoss: 0.00640   DecoderLoss:0.00716  StopLoss: 0.07661  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00124   Validation Loss: 0.00579\n",
      "\n",
      " > Epoch 71/1000\n",
      "   | > Step:0/68  GlobalStep:73900  TotalLoss:0.00171  PostnetLoss:0.00082  DecoderLoss:0.00089  StopLoss:0.10031  GradNorm:0.00506  GradNormST:0.03343  AvgTextLen:3.0  AvgSpecLen:25.3  StepTime:0.43  LR:0.000100\n",
      "   | > Step:10/68  GlobalStep:73910  TotalLoss:0.00195  PostnetLoss:0.00095  DecoderLoss:0.00101  StopLoss:0.08417  GradNorm:0.00378  GradNormST:0.03915  AvgTextLen:9.8  AvgSpecLen:48.6  StepTime:0.62  LR:0.000100\n",
      "   | > Step:20/68  GlobalStep:73920  TotalLoss:0.00228  PostnetLoss:0.00110  DecoderLoss:0.00118  StopLoss:0.06025  GradNorm:0.00358  GradNormST:0.01532  AvgTextLen:16.4  AvgSpecLen:64.8  StepTime:0.48  LR:0.000100\n",
      "   | > Step:30/68  GlobalStep:73930  TotalLoss:0.00248  PostnetLoss:0.00119  DecoderLoss:0.00128  StopLoss:0.07184  GradNorm:0.00326  GradNormST:0.01450  AvgTextLen:22.9  AvgSpecLen:85.8  StepTime:0.60  LR:0.000100\n",
      "   | > Step:40/68  GlobalStep:73940  TotalLoss:0.00271  PostnetLoss:0.00131  DecoderLoss:0.00140  StopLoss:0.05779  GradNorm:0.00340  GradNormST:0.01105  AvgTextLen:30.9  AvgSpecLen:105.3  StepTime:0.60  LR:0.000100\n",
      "   | > Step:50/68  GlobalStep:73950  TotalLoss:0.00292  PostnetLoss:0.00140  DecoderLoss:0.00152  StopLoss:0.06408  GradNorm:0.00362  GradNormST:0.01411  AvgTextLen:38.8  AvgSpecLen:132.0  StepTime:0.80  LR:0.000100\n",
      "   | > Step:60/68  GlobalStep:73960  TotalLoss:0.00323  PostnetLoss:0.00155  DecoderLoss:0.00168  StopLoss:0.05141  GradNorm:0.00399  GradNormST:0.01158  AvgTextLen:52.0  AvgSpecLen:172.7  StepTime:1.00  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:73968  AvgTotalLoss:0.07715  AvgPostnetLoss:0.00124  AvgDecoderLoss:0.00134  AvgStopLoss:0.07457  EpochTime:43.20  AvgStepTime:0.63\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.12131   PostnetLoss: 0.00435   DecoderLoss:0.00485  StopLoss: 0.11212  \n",
      "   | > TotalLoss: 0.08691   PostnetLoss: 0.00636   DecoderLoss:0.00710  StopLoss: 0.07345  \n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00124   Validation Loss: 0.00570\n",
      "\n",
      " > Epoch 72/1000\n",
      "   | > Step:1/68  GlobalStep:73970  TotalLoss:0.00172  PostnetLoss:0.00083  DecoderLoss:0.00089  StopLoss:0.07954  GradNorm:0.00390  GradNormST:0.03136  AvgTextLen:3.7  AvgSpecLen:28.6  StepTime:0.32  LR:0.000100\n",
      "   | > Step:11/68  GlobalStep:73980  TotalLoss:0.00198  PostnetLoss:0.00096  DecoderLoss:0.00102  StopLoss:0.06456  GradNorm:0.00401  GradNormST:0.01723  AvgTextLen:10.3  AvgSpecLen:49.4  StepTime:0.48  LR:0.000100\n",
      "   | > Step:21/68  GlobalStep:73990  TotalLoss:0.00227  PostnetLoss:0.00110  DecoderLoss:0.00117  StopLoss:0.08189  GradNorm:0.00342  GradNormST:0.02154  AvgTextLen:16.6  AvgSpecLen:66.3  StepTime:0.34  LR:0.000100\n",
      "   | > Step:31/68  GlobalStep:74000  TotalLoss:0.00250  PostnetLoss:0.00121  DecoderLoss:0.00130  StopLoss:0.07350  GradNorm:0.00343  GradNormST:0.01815  AvgTextLen:24.1  AvgSpecLen:82.7  StepTime:0.43  LR:0.000100\n",
      " | | > Checkpoint saving : /vm/TTS/models/cartman_models/mozilla-no-loc-fattn-stopnet-sigmoid-loss_masking-August-15-2019_11+26AM-de6aeb9/checkpoint_74000.pth.tar\n",
      "warning: audio amplitude out of range, auto clipped.\n",
      "   | > Step:41/68  GlobalStep:74010  TotalLoss:0.00279  PostnetLoss:0.00134  DecoderLoss:0.00145  StopLoss:0.04714  GradNorm:0.00359  GradNormST:0.01166  AvgTextLen:32.3  AvgSpecLen:109.4  StepTime:0.62  LR:0.000100\n",
      "   | > Step:51/68  GlobalStep:74020  TotalLoss:0.00289  PostnetLoss:0.00139  DecoderLoss:0.00150  StopLoss:0.06566  GradNorm:0.00338  GradNormST:0.01257  AvgTextLen:39.2  AvgSpecLen:126.9  StepTime:0.62  LR:0.000100\n",
      "   | > Step:61/68  GlobalStep:74030  TotalLoss:0.00319  PostnetLoss:0.00153  DecoderLoss:0.00166  StopLoss:0.05649  GradNorm:0.00455  GradNormST:0.02662  AvgTextLen:54.6  AvgSpecLen:165.5  StepTime:0.89  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:74037  AvgTotalLoss:0.07600  AvgPostnetLoss:0.00124  AvgDecoderLoss:0.00134  AvgStopLoss:0.07342  EpochTime:42.18  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.11603   PostnetLoss: 0.00429   DecoderLoss:0.00478  StopLoss: 0.10696  \n",
      "   | > TotalLoss: 0.08687   PostnetLoss: 0.00630   DecoderLoss:0.00704  StopLoss: 0.07353  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00124   Validation Loss: 0.00570\n",
      "\n",
      " > Epoch 73/1000\n",
      "   | > Step:2/68  GlobalStep:74040  TotalLoss:0.00167  PostnetLoss:0.00081  DecoderLoss:0.00086  StopLoss:0.10252  GradNorm:0.00417  GradNormST:0.03255  AvgTextLen:4.0  AvgSpecLen:31.3  StepTime:0.45  LR:0.000100\n",
      "   | > Step:12/68  GlobalStep:74050  TotalLoss:0.00195  PostnetLoss:0.00094  DecoderLoss:0.00101  StopLoss:0.07667  GradNorm:0.00352  GradNormST:0.01867  AvgTextLen:11.0  AvgSpecLen:49.3  StepTime:0.36  LR:0.000100\n",
      "   | > Step:22/68  GlobalStep:74060  TotalLoss:0.00231  PostnetLoss:0.00111  DecoderLoss:0.00120  StopLoss:0.09510  GradNorm:0.00384  GradNormST:0.02820  AvgTextLen:18.0  AvgSpecLen:72.0  StepTime:0.40  LR:0.000100\n",
      "   | > Step:32/68  GlobalStep:74070  TotalLoss:0.00253  PostnetLoss:0.00122  DecoderLoss:0.00131  StopLoss:0.08959  GradNorm:0.00353  GradNormST:0.01936  AvgTextLen:24.4  AvgSpecLen:91.5  StepTime:0.55  LR:0.000100\n",
      "   | > Step:42/68  GlobalStep:74080  TotalLoss:0.00274  PostnetLoss:0.00132  DecoderLoss:0.00142  StopLoss:0.06729  GradNorm:0.00336  GradNormST:0.01690  AvgTextLen:33.1  AvgSpecLen:109.7  StepTime:0.58  LR:0.000100\n",
      "   | > Step:52/68  GlobalStep:74090  TotalLoss:0.00302  PostnetLoss:0.00145  DecoderLoss:0.00157  StopLoss:0.08303  GradNorm:0.00424  GradNormST:0.01950  AvgTextLen:42.1  AvgSpecLen:137.5  StepTime:0.80  LR:0.000100\n",
      "   | > Step:62/68  GlobalStep:74100  TotalLoss:0.00323  PostnetLoss:0.00154  DecoderLoss:0.00168  StopLoss:0.07596  GradNorm:0.00404  GradNormST:0.02565  AvgTextLen:54.0  AvgSpecLen:174.7  StepTime:0.75  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:74106  AvgTotalLoss:0.07582  AvgPostnetLoss:0.00124  AvgDecoderLoss:0.00133  AvgStopLoss:0.07325  EpochTime:42.55  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.11364   PostnetLoss: 0.00422   DecoderLoss:0.00472  StopLoss: 0.10470  \n",
      "   | > TotalLoss: 0.08420   PostnetLoss: 0.00612   DecoderLoss:0.00685  StopLoss: 0.07123  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00124   Validation Loss: 0.00555\n",
      "\n",
      " > Epoch 74/1000\n",
      "   | > Step:3/68  GlobalStep:74110  TotalLoss:0.00174  PostnetLoss:0.00084  DecoderLoss:0.00090  StopLoss:0.08436  GradNorm:0.00437  GradNormST:0.02749  AvgTextLen:4.7  AvgSpecLen:29.7  StepTime:0.36  LR:0.000100\n",
      "   | > Step:13/68  GlobalStep:74120  TotalLoss:0.00198  PostnetLoss:0.00096  DecoderLoss:0.00102  StopLoss:0.09696  GradNorm:0.00401  GradNormST:0.03192  AvgTextLen:11.5  AvgSpecLen:55.2  StepTime:0.42  LR:0.000100\n",
      "   | > Step:23/68  GlobalStep:74130  TotalLoss:0.00239  PostnetLoss:0.00115  DecoderLoss:0.00123  StopLoss:0.06961  GradNorm:0.00357  GradNormST:0.01453  AvgTextLen:18.3  AvgSpecLen:72.2  StepTime:0.43  LR:0.000100\n",
      "   | > Step:33/68  GlobalStep:74140  TotalLoss:0.00261  PostnetLoss:0.00126  DecoderLoss:0.00136  StopLoss:0.08401  GradNorm:0.00344  GradNormST:0.01597  AvgTextLen:25.9  AvgSpecLen:88.9  StepTime:0.54  LR:0.000100\n",
      "   | > Step:43/68  GlobalStep:74150  TotalLoss:0.00279  PostnetLoss:0.00134  DecoderLoss:0.00145  StopLoss:0.06581  GradNorm:0.00354  GradNormST:0.01206  AvgTextLen:33.4  AvgSpecLen:115.7  StepTime:0.51  LR:0.000100\n",
      "   | > Step:53/68  GlobalStep:74160  TotalLoss:0.00294  PostnetLoss:0.00141  DecoderLoss:0.00154  StopLoss:0.05803  GradNorm:0.00354  GradNormST:0.01272  AvgTextLen:42.4  AvgSpecLen:137.6  StepTime:0.82  LR:0.000100\n",
      "   | > Step:63/68  GlobalStep:74170  TotalLoss:0.00334  PostnetLoss:0.00160  DecoderLoss:0.00174  StopLoss:0.05588  GradNorm:0.00409  GradNormST:0.01698  AvgTextLen:58.6  AvgSpecLen:185.1  StepTime:0.92  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:74175  AvgTotalLoss:0.07657  AvgPostnetLoss:0.00124  AvgDecoderLoss:0.00134  AvgStopLoss:0.07399  EpochTime:43.02  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.11436   PostnetLoss: 0.00420   DecoderLoss:0.00469  StopLoss: 0.10547  \n",
      "   | > TotalLoss: 0.08555   PostnetLoss: 0.00615   DecoderLoss:0.00688  StopLoss: 0.07252  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00124   Validation Loss: 0.00564\n",
      "\n",
      " > Epoch 75/1000\n",
      "   | > Step:4/68  GlobalStep:74180  TotalLoss:0.00172  PostnetLoss:0.00083  DecoderLoss:0.00089  StopLoss:0.12559  GradNorm:0.00402  GradNormST:0.06135  AvgTextLen:4.8  AvgSpecLen:31.1  StepTime:0.29  LR:0.000100\n",
      "   | > Step:14/68  GlobalStep:74190  TotalLoss:0.00207  PostnetLoss:0.00100  DecoderLoss:0.00107  StopLoss:0.08554  GradNorm:0.00424  GradNormST:0.02155  AvgTextLen:12.2  AvgSpecLen:52.0  StepTime:0.34  LR:0.000100\n",
      "   | > Step:24/68  GlobalStep:74200  TotalLoss:0.00247  PostnetLoss:0.00119  DecoderLoss:0.00128  StopLoss:0.07395  GradNorm:0.00359  GradNormST:0.01454  AvgTextLen:19.2  AvgSpecLen:79.0  StepTime:0.48  LR:0.000100\n",
      "   | > Step:34/68  GlobalStep:74210  TotalLoss:0.00268  PostnetLoss:0.00129  DecoderLoss:0.00139  StopLoss:0.05882  GradNorm:0.00410  GradNormST:0.00999  AvgTextLen:26.2  AvgSpecLen:95.9  StepTime:0.58  LR:0.000100\n",
      "   | > Step:44/68  GlobalStep:74220  TotalLoss:0.00285  PostnetLoss:0.00137  DecoderLoss:0.00148  StopLoss:0.07317  GradNorm:0.00369  GradNormST:0.02577  AvgTextLen:33.8  AvgSpecLen:116.3  StepTime:0.73  LR:0.000100\n",
      "   | > Step:54/68  GlobalStep:74230  TotalLoss:0.00303  PostnetLoss:0.00145  DecoderLoss:0.00158  StopLoss:0.07210  GradNorm:0.00421  GradNormST:0.01180  AvgTextLen:42.2  AvgSpecLen:144.9  StepTime:0.73  LR:0.000100\n",
      "   | > Step:64/68  GlobalStep:74240  TotalLoss:0.00317  PostnetLoss:0.00151  DecoderLoss:0.00166  StopLoss:0.06224  GradNorm:0.00398  GradNormST:0.01951  AvgTextLen:60.6  AvgSpecLen:177.6  StepTime:0.81  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:74244  AvgTotalLoss:0.07577  AvgPostnetLoss:0.00124  AvgDecoderLoss:0.00133  AvgStopLoss:0.07320  EpochTime:41.63  AvgStepTime:0.60\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.11448   PostnetLoss: 0.00422   DecoderLoss:0.00472  StopLoss: 0.10554  \n",
      "   | > TotalLoss: 0.08433   PostnetLoss: 0.00625   DecoderLoss:0.00701  StopLoss: 0.07107  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00124   Validation Loss: 0.00565\n",
      "\n",
      " > Epoch 76/1000\n",
      "   | > Step:5/68  GlobalStep:74250  TotalLoss:0.00177  PostnetLoss:0.00085  DecoderLoss:0.00092  StopLoss:0.08150  GradNorm:0.00395  GradNormST:0.01838  AvgTextLen:5.9  AvgSpecLen:33.2  StepTime:0.31  LR:0.000100\n",
      "   | > Step:15/68  GlobalStep:74260  TotalLoss:0.00213  PostnetLoss:0.00103  DecoderLoss:0.00110  StopLoss:0.07927  GradNorm:0.00355  GradNormST:0.02966  AvgTextLen:12.8  AvgSpecLen:59.1  StepTime:0.56  LR:0.000100\n",
      "   | > Step:25/68  GlobalStep:74270  TotalLoss:0.00235  PostnetLoss:0.00113  DecoderLoss:0.00122  StopLoss:0.06500  GradNorm:0.00350  GradNormST:0.01177  AvgTextLen:19.8  AvgSpecLen:73.3  StepTime:0.48  LR:0.000100\n",
      "   | > Step:35/68  GlobalStep:74280  TotalLoss:0.00264  PostnetLoss:0.00127  DecoderLoss:0.00137  StopLoss:0.07011  GradNorm:0.00442  GradNormST:0.01755  AvgTextLen:27.1  AvgSpecLen:104.7  StepTime:0.74  LR:0.000100\n",
      "   | > Step:45/68  GlobalStep:74290  TotalLoss:0.00279  PostnetLoss:0.00134  DecoderLoss:0.00145  StopLoss:0.06915  GradNorm:0.00412  GradNormST:0.02122  AvgTextLen:34.4  AvgSpecLen:118.6  StepTime:0.70  LR:0.000100\n",
      "   | > Step:55/68  GlobalStep:74300  TotalLoss:0.00309  PostnetLoss:0.00148  DecoderLoss:0.00161  StopLoss:0.06115  GradNorm:0.00430  GradNormST:0.00905  AvgTextLen:43.9  AvgSpecLen:145.2  StepTime:0.82  LR:0.000100\n",
      "   | > Step:65/68  GlobalStep:74310  TotalLoss:0.00341  PostnetLoss:0.00162  DecoderLoss:0.00179  StopLoss:0.05462  GradNorm:0.00416  GradNormST:0.01790  AvgTextLen:64.2  AvgSpecLen:207.4  StepTime:1.19  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:74313  AvgTotalLoss:0.07591  AvgPostnetLoss:0.00124  AvgDecoderLoss:0.00134  AvgStopLoss:0.07333  EpochTime:42.41  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.11137   PostnetLoss: 0.00434   DecoderLoss:0.00484  StopLoss: 0.10219  \n",
      "   | > TotalLoss: 0.08356   PostnetLoss: 0.00628   DecoderLoss:0.00701  StopLoss: 0.07027  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00124   Validation Loss: 0.00568\n",
      "\n",
      " > Epoch 77/1000\n",
      "   | > Step:6/68  GlobalStep:74320  TotalLoss:0.00177  PostnetLoss:0.00086  DecoderLoss:0.00091  StopLoss:0.06826  GradNorm:0.00374  GradNormST:0.03007  AvgTextLen:6.0  AvgSpecLen:34.8  StepTime:0.40  LR:0.000100\n",
      "   | > Step:16/68  GlobalStep:74330  TotalLoss:0.00221  PostnetLoss:0.00107  DecoderLoss:0.00114  StopLoss:0.07394  GradNorm:0.00380  GradNormST:0.01899  AvgTextLen:13.8  AvgSpecLen:60.6  StepTime:0.33  LR:0.000100\n",
      "   | > Step:26/68  GlobalStep:74340  TotalLoss:0.00243  PostnetLoss:0.00118  DecoderLoss:0.00125  StopLoss:0.06583  GradNorm:0.00418  GradNormST:0.01196  AvgTextLen:20.3  AvgSpecLen:77.5  StepTime:0.46  LR:0.000100\n",
      "   | > Step:36/68  GlobalStep:74350  TotalLoss:0.00263  PostnetLoss:0.00127  DecoderLoss:0.00136  StopLoss:0.08486  GradNorm:0.00389  GradNormST:0.01681  AvgTextLen:28.5  AvgSpecLen:100.2  StepTime:0.58  LR:0.000100\n",
      "   | > Step:46/68  GlobalStep:74360  TotalLoss:0.00284  PostnetLoss:0.00136  DecoderLoss:0.00147  StopLoss:0.06673  GradNorm:0.00432  GradNormST:0.01598  AvgTextLen:35.4  AvgSpecLen:122.6  StepTime:0.81  LR:0.000100\n",
      "   | > Step:56/68  GlobalStep:74370  TotalLoss:0.00309  PostnetLoss:0.00148  DecoderLoss:0.00161  StopLoss:0.05485  GradNorm:0.00459  GradNormST:0.01366  AvgTextLen:45.8  AvgSpecLen:145.4  StepTime:0.96  LR:0.000100\n",
      "   | > Step:66/68  GlobalStep:74380  TotalLoss:0.00356  PostnetLoss:0.00169  DecoderLoss:0.00186  StopLoss:0.05536  GradNorm:0.00335  GradNormST:0.01718  AvgTextLen:71.2  AvgSpecLen:220.8  StepTime:1.10  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:74382  AvgTotalLoss:0.07559  AvgPostnetLoss:0.00124  AvgDecoderLoss:0.00133  AvgStopLoss:0.07302  EpochTime:42.45  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.11565   PostnetLoss: 0.00424   DecoderLoss:0.00473  StopLoss: 0.10668  \n",
      "   | > TotalLoss: 0.08534   PostnetLoss: 0.00616   DecoderLoss:0.00689  StopLoss: 0.07230  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00124   Validation Loss: 0.00564\n",
      "\n",
      " > Epoch 78/1000\n",
      "   | > Step:7/68  GlobalStep:74390  TotalLoss:0.00187  PostnetLoss:0.00091  DecoderLoss:0.00096  StopLoss:0.08553  GradNorm:0.00365  GradNormST:0.02897  AvgTextLen:7.4  AvgSpecLen:44.6  StepTime:0.38  LR:0.000100\n",
      "   | > Step:17/68  GlobalStep:74400  TotalLoss:0.00210  PostnetLoss:0.00102  DecoderLoss:0.00109  StopLoss:0.10728  GradNorm:0.00349  GradNormST:0.02022  AvgTextLen:14.7  AvgSpecLen:66.0  StepTime:0.45  LR:0.000100\n",
      "   | > Step:27/68  GlobalStep:74410  TotalLoss:0.00248  PostnetLoss:0.00119  DecoderLoss:0.00128  StopLoss:0.08669  GradNorm:0.00349  GradNormST:0.03085  AvgTextLen:21.0  AvgSpecLen:80.2  StepTime:0.53  LR:0.000100\n",
      "   | > Step:37/68  GlobalStep:74420  TotalLoss:0.00274  PostnetLoss:0.00132  DecoderLoss:0.00142  StopLoss:0.07809  GradNorm:0.00470  GradNormST:0.02011  AvgTextLen:29.3  AvgSpecLen:102.4  StepTime:0.49  LR:0.000100\n",
      "   | > Step:47/68  GlobalStep:74430  TotalLoss:0.00293  PostnetLoss:0.00141  DecoderLoss:0.00152  StopLoss:0.06183  GradNorm:0.00467  GradNormST:0.01278  AvgTextLen:36.7  AvgSpecLen:126.9  StepTime:0.75  LR:0.000100\n",
      "   | > Step:57/68  GlobalStep:74440  TotalLoss:0.00319  PostnetLoss:0.00153  DecoderLoss:0.00166  StopLoss:0.05352  GradNorm:0.00483  GradNormST:0.01227  AvgTextLen:47.1  AvgSpecLen:161.0  StepTime:0.70  LR:0.000100\n",
      "   | > Step:67/68  GlobalStep:74450  TotalLoss:0.00354  PostnetLoss:0.00169  DecoderLoss:0.00185  StopLoss:0.04982  GradNorm:0.00337  GradNormST:0.01726  AvgTextLen:80.6  AvgSpecLen:246.5  StepTime:1.22  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:74451  AvgTotalLoss:0.07633  AvgPostnetLoss:0.00123  AvgDecoderLoss:0.00133  AvgStopLoss:0.07377  EpochTime:42.40  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.10704   PostnetLoss: 0.00421   DecoderLoss:0.00470  StopLoss: 0.09813  \n",
      "   | > TotalLoss: 0.07937   PostnetLoss: 0.00595   DecoderLoss:0.00664  StopLoss: 0.06677  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00123   Validation Loss: 0.00551\n",
      "\n",
      " > Epoch 79/1000\n",
      "   | > Step:8/68  GlobalStep:74460  TotalLoss:0.00190  PostnetLoss:0.00092  DecoderLoss:0.00098  StopLoss:0.09321  GradNorm:0.00385  GradNormST:0.02347  AvgTextLen:8.3  AvgSpecLen:44.0  StepTime:0.32  LR:0.000100\n",
      "   | > Step:18/68  GlobalStep:74470  TotalLoss:0.00215  PostnetLoss:0.00104  DecoderLoss:0.00111  StopLoss:0.07494  GradNorm:0.00356  GradNormST:0.01674  AvgTextLen:15.2  AvgSpecLen:64.5  StepTime:0.43  LR:0.000100\n",
      "   | > Step:28/68  GlobalStep:74480  TotalLoss:0.00244  PostnetLoss:0.00118  DecoderLoss:0.00126  StopLoss:0.07580  GradNorm:0.00392  GradNormST:0.01487  AvgTextLen:22.1  AvgSpecLen:85.4  StepTime:0.54  LR:0.000100\n",
      "   | > Step:38/68  GlobalStep:74490  TotalLoss:0.00264  PostnetLoss:0.00127  DecoderLoss:0.00137  StopLoss:0.07868  GradNorm:0.00438  GradNormST:0.01989  AvgTextLen:29.4  AvgSpecLen:104.1  StepTime:0.59  LR:0.000100\n",
      "   | > Step:48/68  GlobalStep:74500  TotalLoss:0.00290  PostnetLoss:0.00140  DecoderLoss:0.00150  StopLoss:0.05698  GradNorm:0.00446  GradNormST:0.01284  AvgTextLen:37.5  AvgSpecLen:123.8  StepTime:0.66  LR:0.000100\n",
      "   | > Step:58/68  GlobalStep:74510  TotalLoss:0.00317  PostnetLoss:0.00152  DecoderLoss:0.00165  StopLoss:0.06803  GradNorm:0.00522  GradNormST:0.02155  AvgTextLen:47.8  AvgSpecLen:158.0  StepTime:0.85  LR:0.000100\n",
      "   | > Step:68/68  GlobalStep:74520  TotalLoss:0.00347  PostnetLoss:0.00165  DecoderLoss:0.00181  StopLoss:0.06058  GradNorm:0.00329  GradNormST:0.02266  AvgTextLen:103.0  AvgSpecLen:308.9  StepTime:1.86  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:74520  AvgTotalLoss:0.07276  AvgPostnetLoss:0.00124  AvgDecoderLoss:0.00133  AvgStopLoss:0.07020  EpochTime:42.95  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.11185   PostnetLoss: 0.00426   DecoderLoss:0.00476  StopLoss: 0.10283  \n",
      "   | > TotalLoss: 0.07876   PostnetLoss: 0.00605   DecoderLoss:0.00677  StopLoss: 0.06594  \n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00124   Validation Loss: 0.00561\n",
      "\n",
      " > Epoch 80/1000\n",
      "   | > Step:9/68  GlobalStep:74530  TotalLoss:0.00188  PostnetLoss:0.00091  DecoderLoss:0.00097  StopLoss:0.06938  GradNorm:0.00375  GradNormST:0.03125  AvgTextLen:9.0  AvgSpecLen:42.1  StepTime:0.39  LR:0.000100\n",
      "   | > Step:19/68  GlobalStep:74540  TotalLoss:0.00213  PostnetLoss:0.00103  DecoderLoss:0.00110  StopLoss:0.08965  GradNorm:0.00331  GradNormST:0.02069  AvgTextLen:15.5  AvgSpecLen:63.2  StepTime:0.47  LR:0.000100\n",
      "   | > Step:29/68  GlobalStep:74550  TotalLoss:0.00241  PostnetLoss:0.00117  DecoderLoss:0.00125  StopLoss:0.08769  GradNorm:0.00407  GradNormST:0.02045  AvgTextLen:21.8  AvgSpecLen:83.4  StepTime:0.56  LR:0.000100\n",
      "   | > Step:39/68  GlobalStep:74560  TotalLoss:0.00275  PostnetLoss:0.00133  DecoderLoss:0.00142  StopLoss:0.05121  GradNorm:0.00461  GradNormST:0.01030  AvgTextLen:30.2  AvgSpecLen:110.4  StepTime:0.64  LR:0.000100\n",
      "   | > Step:49/68  GlobalStep:74570  TotalLoss:0.00292  PostnetLoss:0.00140  DecoderLoss:0.00151  StopLoss:0.04956  GradNorm:0.00462  GradNormST:0.00982  AvgTextLen:38.2  AvgSpecLen:127.7  StepTime:0.60  LR:0.000100\n",
      "   | > Step:59/68  GlobalStep:74580  TotalLoss:0.00313  PostnetLoss:0.00150  DecoderLoss:0.00163  StopLoss:0.05308  GradNorm:0.00506  GradNormST:0.01611  AvgTextLen:50.5  AvgSpecLen:155.9  StepTime:0.93  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:74589  AvgTotalLoss:0.07376  AvgPostnetLoss:0.00124  AvgDecoderLoss:0.00133  AvgStopLoss:0.07119  EpochTime:42.24  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.11202   PostnetLoss: 0.00424   DecoderLoss:0.00474  StopLoss: 0.10305  \n",
      "   | > TotalLoss: 0.07851   PostnetLoss: 0.00587   DecoderLoss:0.00657  StopLoss: 0.06607  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00124   Validation Loss: 0.00549\n",
      "\n",
      " > Epoch 81/1000\n",
      "   | > Step:0/68  GlobalStep:74590  TotalLoss:0.00176  PostnetLoss:0.00084  DecoderLoss:0.00092  StopLoss:0.09740  GradNorm:0.00550  GradNormST:0.04993  AvgTextLen:3.0  AvgSpecLen:25.3  StepTime:0.53  LR:0.000100\n",
      "   | > Step:10/68  GlobalStep:74600  TotalLoss:0.00200  PostnetLoss:0.00097  DecoderLoss:0.00103  StopLoss:0.06285  GradNorm:0.00398  GradNormST:0.01542  AvgTextLen:9.8  AvgSpecLen:48.6  StepTime:0.43  LR:0.000100\n",
      "   | > Step:20/68  GlobalStep:74610  TotalLoss:0.00226  PostnetLoss:0.00109  DecoderLoss:0.00116  StopLoss:0.05491  GradNorm:0.00344  GradNormST:0.01124  AvgTextLen:16.4  AvgSpecLen:64.8  StepTime:0.41  LR:0.000100\n",
      "   | > Step:30/68  GlobalStep:74620  TotalLoss:0.00250  PostnetLoss:0.00121  DecoderLoss:0.00129  StopLoss:0.05947  GradNorm:0.00344  GradNormST:0.01052  AvgTextLen:22.9  AvgSpecLen:85.8  StepTime:0.60  LR:0.000100\n",
      "   | > Step:40/68  GlobalStep:74630  TotalLoss:0.00270  PostnetLoss:0.00130  DecoderLoss:0.00140  StopLoss:0.06052  GradNorm:0.00455  GradNormST:0.01186  AvgTextLen:30.9  AvgSpecLen:105.3  StepTime:0.49  LR:0.000100\n",
      "   | > Step:50/68  GlobalStep:74640  TotalLoss:0.00294  PostnetLoss:0.00141  DecoderLoss:0.00153  StopLoss:0.07117  GradNorm:0.00704  GradNormST:0.01787  AvgTextLen:38.8  AvgSpecLen:132.0  StepTime:0.78  LR:0.000100\n",
      "   | > Step:60/68  GlobalStep:74650  TotalLoss:0.00324  PostnetLoss:0.00155  DecoderLoss:0.00169  StopLoss:0.04949  GradNorm:0.00533  GradNormST:0.01136  AvgTextLen:52.0  AvgSpecLen:172.7  StepTime:1.00  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:74658  AvgTotalLoss:0.07306  AvgPostnetLoss:0.00124  AvgDecoderLoss:0.00133  AvgStopLoss:0.07049  EpochTime:42.71  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.11349   PostnetLoss: 0.00426   DecoderLoss:0.00475  StopLoss: 0.10448  \n",
      "   | > TotalLoss: 0.07707   PostnetLoss: 0.00599   DecoderLoss:0.00670  StopLoss: 0.06438  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00124   Validation Loss: 0.00559\n",
      "\n",
      " > Epoch 82/1000\n",
      "   | > Step:1/68  GlobalStep:74660  TotalLoss:0.00174  PostnetLoss:0.00084  DecoderLoss:0.00090  StopLoss:0.09336  GradNorm:0.00484  GradNormST:0.02444  AvgTextLen:3.7  AvgSpecLen:28.6  StepTime:0.36  LR:0.000100\n",
      "   | > Step:11/68  GlobalStep:74670  TotalLoss:0.00198  PostnetLoss:0.00096  DecoderLoss:0.00101  StopLoss:0.05397  GradNorm:0.00405  GradNormST:0.01152  AvgTextLen:10.3  AvgSpecLen:49.4  StepTime:0.38  LR:0.000100\n",
      "   | > Step:21/68  GlobalStep:74680  TotalLoss:0.00226  PostnetLoss:0.00109  DecoderLoss:0.00117  StopLoss:0.07053  GradNorm:0.00370  GradNormST:0.01808  AvgTextLen:16.6  AvgSpecLen:66.3  StepTime:0.46  LR:0.000100\n",
      "   | > Step:31/68  GlobalStep:74690  TotalLoss:0.00243  PostnetLoss:0.00117  DecoderLoss:0.00126  StopLoss:0.07026  GradNorm:0.00368  GradNormST:0.02355  AvgTextLen:24.1  AvgSpecLen:82.7  StepTime:0.43  LR:0.000100\n",
      "   | > Step:41/68  GlobalStep:74700  TotalLoss:0.00279  PostnetLoss:0.00134  DecoderLoss:0.00145  StopLoss:0.04249  GradNorm:0.00432  GradNormST:0.01154  AvgTextLen:32.3  AvgSpecLen:109.4  StepTime:0.64  LR:0.000100\n",
      "   | > Step:51/68  GlobalStep:74710  TotalLoss:0.00292  PostnetLoss:0.00140  DecoderLoss:0.00152  StopLoss:0.06157  GradNorm:0.00443  GradNormST:0.01421  AvgTextLen:39.2  AvgSpecLen:126.9  StepTime:0.80  LR:0.000100\n",
      "   | > Step:61/68  GlobalStep:74720  TotalLoss:0.00323  PostnetLoss:0.00155  DecoderLoss:0.00168  StopLoss:0.04997  GradNorm:0.00529  GradNormST:0.01294  AvgTextLen:54.6  AvgSpecLen:165.5  StepTime:0.93  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:74727  AvgTotalLoss:0.07154  AvgPostnetLoss:0.00123  AvgDecoderLoss:0.00133  AvgStopLoss:0.06898  EpochTime:42.26  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.11117   PostnetLoss: 0.00422   DecoderLoss:0.00471  StopLoss: 0.10225  \n",
      "   | > TotalLoss: 0.07378   PostnetLoss: 0.00604   DecoderLoss:0.00676  StopLoss: 0.06098  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00123   Validation Loss: 0.00557\n",
      "\n",
      " > Epoch 83/1000\n",
      "   | > Step:2/68  GlobalStep:74730  TotalLoss:0.00162  PostnetLoss:0.00079  DecoderLoss:0.00083  StopLoss:0.10379  GradNorm:0.00421  GradNormST:0.03853  AvgTextLen:4.0  AvgSpecLen:31.3  StepTime:0.57  LR:0.000100\n",
      "   | > Step:12/68  GlobalStep:74740  TotalLoss:0.00197  PostnetLoss:0.00095  DecoderLoss:0.00101  StopLoss:0.06680  GradNorm:0.00371  GradNormST:0.01118  AvgTextLen:11.0  AvgSpecLen:49.3  StepTime:0.42  LR:0.000100\n",
      "   | > Step:22/68  GlobalStep:74750  TotalLoss:0.00225  PostnetLoss:0.00109  DecoderLoss:0.00116  StopLoss:0.07937  GradNorm:0.00346  GradNormST:0.01740  AvgTextLen:18.0  AvgSpecLen:72.0  StepTime:0.48  LR:0.000100\n",
      "   | > Step:32/68  GlobalStep:74760  TotalLoss:0.00251  PostnetLoss:0.00121  DecoderLoss:0.00130  StopLoss:0.06553  GradNorm:0.00369  GradNormST:0.01492  AvgTextLen:24.4  AvgSpecLen:91.5  StepTime:0.56  LR:0.000100\n",
      "   | > Step:42/68  GlobalStep:74770  TotalLoss:0.00276  PostnetLoss:0.00132  DecoderLoss:0.00143  StopLoss:0.06486  GradNorm:0.00371  GradNormST:0.02451  AvgTextLen:33.1  AvgSpecLen:109.7  StepTime:0.57  LR:0.000100\n",
      "   | > Step:52/68  GlobalStep:74780  TotalLoss:0.00302  PostnetLoss:0.00145  DecoderLoss:0.00157  StopLoss:0.06378  GradNorm:0.00456  GradNormST:0.01715  AvgTextLen:42.1  AvgSpecLen:137.5  StepTime:0.79  LR:0.000100\n",
      "   | > Step:62/68  GlobalStep:74790  TotalLoss:0.00325  PostnetLoss:0.00156  DecoderLoss:0.00169  StopLoss:0.06457  GradNorm:0.00440  GradNormST:0.02356  AvgTextLen:54.0  AvgSpecLen:174.7  StepTime:0.76  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:74796  AvgTotalLoss:0.06942  AvgPostnetLoss:0.00123  AvgDecoderLoss:0.00133  AvgStopLoss:0.06686  EpochTime:41.78  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.11747   PostnetLoss: 0.00416   DecoderLoss:0.00465  StopLoss: 0.10866  \n",
      "   | > TotalLoss: 0.07348   PostnetLoss: 0.00605   DecoderLoss:0.00678  StopLoss: 0.06066  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00123   Validation Loss: 0.00554\n",
      "\n",
      " > Epoch 84/1000\n",
      "   | > Step:3/68  GlobalStep:74800  TotalLoss:0.00171  PostnetLoss:0.00083  DecoderLoss:0.00088  StopLoss:0.07557  GradNorm:0.00418  GradNormST:0.02771  AvgTextLen:4.7  AvgSpecLen:29.7  StepTime:0.31  LR:0.000100\n",
      "   | > Step:13/68  GlobalStep:74810  TotalLoss:0.00198  PostnetLoss:0.00096  DecoderLoss:0.00102  StopLoss:0.09663  GradNorm:0.00392  GradNormST:0.03248  AvgTextLen:11.5  AvgSpecLen:55.2  StepTime:0.44  LR:0.000100\n",
      "   | > Step:23/68  GlobalStep:74820  TotalLoss:0.00237  PostnetLoss:0.00114  DecoderLoss:0.00123  StopLoss:0.07037  GradNorm:0.00346  GradNormST:0.01708  AvgTextLen:18.3  AvgSpecLen:72.2  StepTime:0.45  LR:0.000100\n",
      "   | > Step:33/68  GlobalStep:74830  TotalLoss:0.00259  PostnetLoss:0.00125  DecoderLoss:0.00134  StopLoss:0.05028  GradNorm:0.00480  GradNormST:0.01298  AvgTextLen:25.9  AvgSpecLen:88.9  StepTime:0.53  LR:0.000100\n",
      "   | > Step:43/68  GlobalStep:74840  TotalLoss:0.00277  PostnetLoss:0.00133  DecoderLoss:0.00144  StopLoss:0.06272  GradNorm:0.00382  GradNormST:0.02250  AvgTextLen:33.4  AvgSpecLen:115.7  StepTime:0.63  LR:0.000100\n",
      "   | > Step:53/68  GlobalStep:74850  TotalLoss:0.00296  PostnetLoss:0.00142  DecoderLoss:0.00154  StopLoss:0.04959  GradNorm:0.00448  GradNormST:0.01347  AvgTextLen:42.4  AvgSpecLen:137.6  StepTime:0.76  LR:0.000100\n",
      "   | > Step:63/68  GlobalStep:74860  TotalLoss:0.00334  PostnetLoss:0.00160  DecoderLoss:0.00174  StopLoss:0.04555  GradNorm:0.00461  GradNormST:0.01019  AvgTextLen:58.6  AvgSpecLen:185.1  StepTime:0.90  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:74865  AvgTotalLoss:0.06898  AvgPostnetLoss:0.00123  AvgDecoderLoss:0.00132  AvgStopLoss:0.06642  EpochTime:42.63  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.10991   PostnetLoss: 0.00421   DecoderLoss:0.00470  StopLoss: 0.10100  \n",
      "   | > TotalLoss: 0.07249   PostnetLoss: 0.00601   DecoderLoss:0.00672  StopLoss: 0.05977  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00123   Validation Loss: 0.00557\n",
      "\n",
      " > Epoch 85/1000\n",
      "   | > Step:4/68  GlobalStep:74870  TotalLoss:0.00167  PostnetLoss:0.00081  DecoderLoss:0.00086  StopLoss:0.11963  GradNorm:0.00391  GradNormST:0.04920  AvgTextLen:4.8  AvgSpecLen:31.1  StepTime:0.30  LR:0.000100\n",
      "   | > Step:14/68  GlobalStep:74880  TotalLoss:0.00206  PostnetLoss:0.00100  DecoderLoss:0.00107  StopLoss:0.09429  GradNorm:0.00367  GradNormST:0.02075  AvgTextLen:12.2  AvgSpecLen:52.0  StepTime:0.34  LR:0.000100\n",
      "   | > Step:24/68  GlobalStep:74890  TotalLoss:0.00243  PostnetLoss:0.00117  DecoderLoss:0.00126  StopLoss:0.06326  GradNorm:0.00345  GradNormST:0.02180  AvgTextLen:19.2  AvgSpecLen:79.0  StepTime:0.49  LR:0.000100\n",
      "   | > Step:34/68  GlobalStep:74900  TotalLoss:0.00262  PostnetLoss:0.00126  DecoderLoss:0.00136  StopLoss:0.05745  GradNorm:0.00340  GradNormST:0.00903  AvgTextLen:26.2  AvgSpecLen:95.9  StepTime:0.59  LR:0.000100\n",
      "   | > Step:44/68  GlobalStep:74910  TotalLoss:0.00282  PostnetLoss:0.00135  DecoderLoss:0.00146  StopLoss:0.06187  GradNorm:0.00438  GradNormST:0.02398  AvgTextLen:33.8  AvgSpecLen:116.3  StepTime:0.71  LR:0.000100\n",
      "   | > Step:54/68  GlobalStep:74920  TotalLoss:0.00306  PostnetLoss:0.00146  DecoderLoss:0.00159  StopLoss:0.05816  GradNorm:0.00456  GradNormST:0.01138  AvgTextLen:42.2  AvgSpecLen:144.9  StepTime:0.82  LR:0.000100\n",
      "   | > Step:64/68  GlobalStep:74930  TotalLoss:0.00311  PostnetLoss:0.00149  DecoderLoss:0.00163  StopLoss:0.05283  GradNorm:0.00321  GradNormST:0.01767  AvgTextLen:60.6  AvgSpecLen:177.6  StepTime:0.81  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:74934  AvgTotalLoss:0.06984  AvgPostnetLoss:0.00123  AvgDecoderLoss:0.00132  AvgStopLoss:0.06728  EpochTime:42.57  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.10623   PostnetLoss: 0.00425   DecoderLoss:0.00475  StopLoss: 0.09723  \n",
      "   | > TotalLoss: 0.07160   PostnetLoss: 0.00601   DecoderLoss:0.00673  StopLoss: 0.05887  \n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00123   Validation Loss: 0.00553\n",
      "\n",
      " > Epoch 86/1000\n",
      "   | > Step:5/68  GlobalStep:74940  TotalLoss:0.00178  PostnetLoss:0.00086  DecoderLoss:0.00092  StopLoss:0.07891  GradNorm:0.00385  GradNormST:0.01953  AvgTextLen:5.9  AvgSpecLen:33.2  StepTime:0.27  LR:0.000100\n",
      "   | > Step:15/68  GlobalStep:74950  TotalLoss:0.00211  PostnetLoss:0.00102  DecoderLoss:0.00109  StopLoss:0.07669  GradNorm:0.00377  GradNormST:0.03649  AvgTextLen:12.8  AvgSpecLen:59.1  StepTime:0.52  LR:0.000100\n",
      "   | > Step:25/68  GlobalStep:74960  TotalLoss:0.00230  PostnetLoss:0.00111  DecoderLoss:0.00119  StopLoss:0.07240  GradNorm:0.00340  GradNormST:0.01479  AvgTextLen:19.8  AvgSpecLen:73.3  StepTime:0.39  LR:0.000100\n",
      "   | > Step:35/68  GlobalStep:74970  TotalLoss:0.00263  PostnetLoss:0.00127  DecoderLoss:0.00136  StopLoss:0.06547  GradNorm:0.00365  GradNormST:0.02337  AvgTextLen:27.1  AvgSpecLen:104.7  StepTime:0.63  LR:0.000100\n",
      "   | > Step:45/68  GlobalStep:74980  TotalLoss:0.00274  PostnetLoss:0.00132  DecoderLoss:0.00142  StopLoss:0.06270  GradNorm:0.00300  GradNormST:0.02273  AvgTextLen:34.4  AvgSpecLen:118.6  StepTime:0.66  LR:0.000100\n",
      "   | > Step:55/68  GlobalStep:74990  TotalLoss:0.00307  PostnetLoss:0.00147  DecoderLoss:0.00160  StopLoss:0.04468  GradNorm:0.00369  GradNormST:0.00934  AvgTextLen:43.9  AvgSpecLen:145.2  StepTime:0.67  LR:0.000100\n",
      "   | > Step:65/68  GlobalStep:75000  TotalLoss:0.00332  PostnetLoss:0.00158  DecoderLoss:0.00173  StopLoss:0.04552  GradNorm:0.00313  GradNormST:0.01183  AvgTextLen:64.2  AvgSpecLen:207.4  StepTime:1.20  LR:0.000100\n",
      " | | > Checkpoint saving : /vm/TTS/models/cartman_models/mozilla-no-loc-fattn-stopnet-sigmoid-loss_masking-August-15-2019_11+26AM-de6aeb9/checkpoint_75000.pth.tar\n",
      "warning: audio amplitude out of range, auto clipped.\n",
      "   | > EPOCH END -- GlobalStep:75003  AvgTotalLoss:0.06713  AvgPostnetLoss:0.00122  AvgDecoderLoss:0.00132  AvgStopLoss:0.06459  EpochTime:42.73  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.11205   PostnetLoss: 0.00436   DecoderLoss:0.00487  StopLoss: 0.10282  \n",
      "   | > TotalLoss: 0.07963   PostnetLoss: 0.00640   DecoderLoss:0.00717  StopLoss: 0.06606  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00122   Validation Loss: 0.00574\n",
      "\n",
      " > Epoch 87/1000\n",
      "   | > Step:6/68  GlobalStep:75010  TotalLoss:0.00178  PostnetLoss:0.00086  DecoderLoss:0.00091  StopLoss:0.05606  GradNorm:0.00463  GradNormST:0.01836  AvgTextLen:6.0  AvgSpecLen:34.8  StepTime:0.39  LR:0.000100\n",
      "   | > Step:16/68  GlobalStep:75020  TotalLoss:0.00219  PostnetLoss:0.00106  DecoderLoss:0.00113  StopLoss:0.05786  GradNorm:0.00454  GradNormST:0.01662  AvgTextLen:13.8  AvgSpecLen:60.6  StepTime:0.37  LR:0.000100\n",
      "   | > Step:26/68  GlobalStep:75030  TotalLoss:0.00242  PostnetLoss:0.00117  DecoderLoss:0.00125  StopLoss:0.06148  GradNorm:0.00406  GradNormST:0.01238  AvgTextLen:20.3  AvgSpecLen:77.5  StepTime:0.50  LR:0.000100\n",
      "   | > Step:36/68  GlobalStep:75040  TotalLoss:0.00264  PostnetLoss:0.00127  DecoderLoss:0.00137  StopLoss:0.07789  GradNorm:0.00386  GradNormST:0.01540  AvgTextLen:28.5  AvgSpecLen:100.2  StepTime:0.56  LR:0.000100\n",
      "   | > Step:46/68  GlobalStep:75050  TotalLoss:0.00279  PostnetLoss:0.00134  DecoderLoss:0.00145  StopLoss:0.05385  GradNorm:0.00370  GradNormST:0.01000  AvgTextLen:35.4  AvgSpecLen:122.6  StepTime:0.90  LR:0.000100\n",
      "   | > Step:56/68  GlobalStep:75060  TotalLoss:0.00304  PostnetLoss:0.00146  DecoderLoss:0.00158  StopLoss:0.05889  GradNorm:0.00366  GradNormST:0.02068  AvgTextLen:45.8  AvgSpecLen:145.4  StepTime:1.05  LR:0.000100\n",
      "   | > Step:66/68  GlobalStep:75070  TotalLoss:0.00351  PostnetLoss:0.00167  DecoderLoss:0.00184  StopLoss:0.05408  GradNorm:0.00294  GradNormST:0.01867  AvgTextLen:71.2  AvgSpecLen:220.8  StepTime:1.06  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:75072  AvgTotalLoss:0.06408  AvgPostnetLoss:0.00123  AvgDecoderLoss:0.00132  AvgStopLoss:0.06153  EpochTime:42.82  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.11006   PostnetLoss: 0.00435   DecoderLoss:0.00487  StopLoss: 0.10084  \n",
      "   | > TotalLoss: 0.07230   PostnetLoss: 0.00627   DecoderLoss:0.00701  StopLoss: 0.05902  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00123   Validation Loss: 0.00569\n",
      "\n",
      " > Epoch 88/1000\n",
      "   | > Step:7/68  GlobalStep:75080  TotalLoss:0.00186  PostnetLoss:0.00090  DecoderLoss:0.00095  StopLoss:0.06243  GradNorm:0.00425  GradNormST:0.01845  AvgTextLen:7.4  AvgSpecLen:44.6  StepTime:0.38  LR:0.000100\n",
      "   | > Step:17/68  GlobalStep:75090  TotalLoss:0.00216  PostnetLoss:0.00104  DecoderLoss:0.00111  StopLoss:0.07545  GradNorm:0.00391  GradNormST:0.01500  AvgTextLen:14.7  AvgSpecLen:66.0  StepTime:0.42  LR:0.000100\n",
      "   | > Step:27/68  GlobalStep:75100  TotalLoss:0.00242  PostnetLoss:0.00117  DecoderLoss:0.00125  StopLoss:0.08708  GradNorm:0.00344  GradNormST:0.03780  AvgTextLen:21.0  AvgSpecLen:80.2  StepTime:0.54  LR:0.000100\n",
      "   | > Step:37/68  GlobalStep:75110  TotalLoss:0.00272  PostnetLoss:0.00131  DecoderLoss:0.00141  StopLoss:0.06252  GradNorm:0.00398  GradNormST:0.01395  AvgTextLen:29.3  AvgSpecLen:102.4  StepTime:0.63  LR:0.000100\n",
      "   | > Step:47/68  GlobalStep:75120  TotalLoss:0.00290  PostnetLoss:0.00139  DecoderLoss:0.00151  StopLoss:0.06376  GradNorm:0.00413  GradNormST:0.01596  AvgTextLen:36.7  AvgSpecLen:126.9  StepTime:0.68  LR:0.000100\n",
      "   | > Step:57/68  GlobalStep:75130  TotalLoss:0.00311  PostnetLoss:0.00149  DecoderLoss:0.00162  StopLoss:0.04610  GradNorm:0.00411  GradNormST:0.00954  AvgTextLen:47.1  AvgSpecLen:161.0  StepTime:0.90  LR:0.000100\n",
      "   | > Step:67/68  GlobalStep:75140  TotalLoss:0.00347  PostnetLoss:0.00166  DecoderLoss:0.00181  StopLoss:0.07577  GradNorm:0.00321  GradNormST:0.05074  AvgTextLen:80.6  AvgSpecLen:246.5  StepTime:1.21  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:75141  AvgTotalLoss:0.06902  AvgPostnetLoss:0.00123  AvgDecoderLoss:0.00132  AvgStopLoss:0.06647  EpochTime:42.81  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.10478   PostnetLoss: 0.00433   DecoderLoss:0.00485  StopLoss: 0.09561  \n",
      "   | > TotalLoss: 0.07095   PostnetLoss: 0.00620   DecoderLoss:0.00694  StopLoss: 0.05781  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00123   Validation Loss: 0.00568\n",
      "\n",
      " > Epoch 89/1000\n",
      "   | > Step:8/68  GlobalStep:75150  TotalLoss:0.00201  PostnetLoss:0.00097  DecoderLoss:0.00104  StopLoss:0.08695  GradNorm:0.00591  GradNormST:0.02089  AvgTextLen:8.3  AvgSpecLen:44.0  StepTime:0.29  LR:0.000100\n",
      "   | > Step:18/68  GlobalStep:75160  TotalLoss:0.00213  PostnetLoss:0.00103  DecoderLoss:0.00110  StopLoss:0.07192  GradNorm:0.00369  GradNormST:0.01498  AvgTextLen:15.2  AvgSpecLen:64.5  StepTime:0.40  LR:0.000100\n",
      "   | > Step:28/68  GlobalStep:75170  TotalLoss:0.00242  PostnetLoss:0.00117  DecoderLoss:0.00125  StopLoss:0.06786  GradNorm:0.00357  GradNormST:0.01446  AvgTextLen:22.1  AvgSpecLen:85.4  StepTime:0.55  LR:0.000100\n",
      "   | > Step:38/68  GlobalStep:75180  TotalLoss:0.00266  PostnetLoss:0.00128  DecoderLoss:0.00138  StopLoss:0.09112  GradNorm:0.00336  GradNormST:0.03182  AvgTextLen:29.4  AvgSpecLen:104.1  StepTime:0.61  LR:0.000100\n",
      "   | > Step:48/68  GlobalStep:75190  TotalLoss:0.00286  PostnetLoss:0.00137  DecoderLoss:0.00148  StopLoss:0.05510  GradNorm:0.00365  GradNormST:0.01180  AvgTextLen:37.5  AvgSpecLen:123.8  StepTime:0.73  LR:0.000100\n",
      "   | > Step:58/68  GlobalStep:75200  TotalLoss:0.00314  PostnetLoss:0.00151  DecoderLoss:0.00163  StopLoss:0.05328  GradNorm:0.00385  GradNormST:0.01709  AvgTextLen:47.8  AvgSpecLen:158.0  StepTime:0.82  LR:0.000100\n",
      "   | > Step:68/68  GlobalStep:75210  TotalLoss:0.00346  PostnetLoss:0.00165  DecoderLoss:0.00181  StopLoss:0.09647  GradNorm:0.00338  GradNormST:0.09861  AvgTextLen:103.0  AvgSpecLen:308.9  StepTime:1.89  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:75210  AvgTotalLoss:0.06660  AvgPostnetLoss:0.00123  AvgDecoderLoss:0.00132  AvgStopLoss:0.06405  EpochTime:42.93  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.10424   PostnetLoss: 0.00436   DecoderLoss:0.00485  StopLoss: 0.09503  \n",
      "   | > TotalLoss: 0.07705   PostnetLoss: 0.00629   DecoderLoss:0.00701  StopLoss: 0.06375  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00123   Validation Loss: 0.00575\n",
      "\n",
      " > Epoch 90/1000\n",
      "   | > Step:9/68  GlobalStep:75220  TotalLoss:0.00184  PostnetLoss:0.00089  DecoderLoss:0.00095  StopLoss:0.07274  GradNorm:0.00441  GradNormST:0.02256  AvgTextLen:9.0  AvgSpecLen:42.1  StepTime:0.41  LR:0.000100\n",
      "   | > Step:19/68  GlobalStep:75230  TotalLoss:0.00218  PostnetLoss:0.00106  DecoderLoss:0.00113  StopLoss:0.08327  GradNorm:0.00426  GradNormST:0.01999  AvgTextLen:15.5  AvgSpecLen:63.2  StepTime:0.42  LR:0.000100\n",
      "   | > Step:29/68  GlobalStep:75240  TotalLoss:0.00242  PostnetLoss:0.00116  DecoderLoss:0.00125  StopLoss:0.09250  GradNorm:0.00364  GradNormST:0.02040  AvgTextLen:21.8  AvgSpecLen:83.4  StepTime:0.56  LR:0.000100\n",
      "   | > Step:39/68  GlobalStep:75250  TotalLoss:0.00267  PostnetLoss:0.00129  DecoderLoss:0.00139  StopLoss:0.04238  GradNorm:0.00335  GradNormST:0.00840  AvgTextLen:30.2  AvgSpecLen:110.4  StepTime:0.56  LR:0.000100\n",
      "   | > Step:49/68  GlobalStep:75260  TotalLoss:0.00288  PostnetLoss:0.00138  DecoderLoss:0.00149  StopLoss:0.05254  GradNorm:0.00362  GradNormST:0.01086  AvgTextLen:38.2  AvgSpecLen:127.7  StepTime:0.64  LR:0.000100\n",
      "   | > Step:59/68  GlobalStep:75270  TotalLoss:0.00308  PostnetLoss:0.00147  DecoderLoss:0.00160  StopLoss:0.04907  GradNorm:0.00366  GradNormST:0.01254  AvgTextLen:50.5  AvgSpecLen:155.9  StepTime:0.86  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:75279  AvgTotalLoss:0.06713  AvgPostnetLoss:0.00123  AvgDecoderLoss:0.00132  AvgStopLoss:0.06458  EpochTime:43.32  AvgStepTime:0.63\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.10779   PostnetLoss: 0.00441   DecoderLoss:0.00491  StopLoss: 0.09847  \n",
      "   | > TotalLoss: 0.08300   PostnetLoss: 0.00651   DecoderLoss:0.00726  StopLoss: 0.06922  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00123   Validation Loss: 0.00600\n",
      "\n",
      " > Epoch 91/1000\n",
      "   | > Step:0/68  GlobalStep:75280  TotalLoss:0.00175  PostnetLoss:0.00084  DecoderLoss:0.00091  StopLoss:0.11353  GradNorm:0.00587  GradNormST:0.03928  AvgTextLen:3.0  AvgSpecLen:25.3  StepTime:0.38  LR:0.000100\n",
      "   | > Step:10/68  GlobalStep:75290  TotalLoss:0.00197  PostnetLoss:0.00095  DecoderLoss:0.00102  StopLoss:0.06322  GradNorm:0.00445  GradNormST:0.02065  AvgTextLen:9.8  AvgSpecLen:48.6  StepTime:0.51  LR:0.000100\n",
      "   | > Step:20/68  GlobalStep:75300  TotalLoss:0.00229  PostnetLoss:0.00111  DecoderLoss:0.00118  StopLoss:0.04942  GradNorm:0.00442  GradNormST:0.01251  AvgTextLen:16.4  AvgSpecLen:64.8  StepTime:0.52  LR:0.000100\n",
      "   | > Step:30/68  GlobalStep:75310  TotalLoss:0.00245  PostnetLoss:0.00118  DecoderLoss:0.00127  StopLoss:0.05851  GradNorm:0.00410  GradNormST:0.01139  AvgTextLen:22.9  AvgSpecLen:85.8  StepTime:0.50  LR:0.000100\n",
      "   | > Step:40/68  GlobalStep:75320  TotalLoss:0.00275  PostnetLoss:0.00133  DecoderLoss:0.00143  StopLoss:0.04791  GradNorm:0.00402  GradNormST:0.00947  AvgTextLen:30.9  AvgSpecLen:105.3  StepTime:0.60  LR:0.000100\n",
      "   | > Step:50/68  GlobalStep:75330  TotalLoss:0.00293  PostnetLoss:0.00140  DecoderLoss:0.00152  StopLoss:0.05295  GradNorm:0.00388  GradNormST:0.01399  AvgTextLen:38.8  AvgSpecLen:132.0  StepTime:0.86  LR:0.000100\n",
      "   | > Step:60/68  GlobalStep:75340  TotalLoss:0.00318  PostnetLoss:0.00153  DecoderLoss:0.00165  StopLoss:0.04550  GradNorm:0.00410  GradNormST:0.01250  AvgTextLen:52.0  AvgSpecLen:172.7  StepTime:0.79  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:75348  AvgTotalLoss:0.06573  AvgPostnetLoss:0.00123  AvgDecoderLoss:0.00132  AvgStopLoss:0.06318  EpochTime:42.15  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.10370   PostnetLoss: 0.00440   DecoderLoss:0.00492  StopLoss: 0.09438  \n",
      "   | > TotalLoss: 0.06973   PostnetLoss: 0.00623   DecoderLoss:0.00697  StopLoss: 0.05653  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00123   Validation Loss: 0.00573\n",
      "\n",
      " > Epoch 92/1000\n",
      "   | > Step:1/68  GlobalStep:75350  TotalLoss:0.00171  PostnetLoss:0.00083  DecoderLoss:0.00088  StopLoss:0.08264  GradNorm:0.00517  GradNormST:0.02883  AvgTextLen:3.7  AvgSpecLen:28.6  StepTime:0.29  LR:0.000100\n",
      "   | > Step:11/68  GlobalStep:75360  TotalLoss:0.00193  PostnetLoss:0.00094  DecoderLoss:0.00100  StopLoss:0.07318  GradNorm:0.00361  GradNormST:0.01836  AvgTextLen:10.3  AvgSpecLen:49.4  StepTime:0.39  LR:0.000100\n",
      "   | > Step:21/68  GlobalStep:75370  TotalLoss:0.00227  PostnetLoss:0.00110  DecoderLoss:0.00117  StopLoss:0.05998  GradNorm:0.00426  GradNormST:0.01381  AvgTextLen:16.6  AvgSpecLen:66.3  StepTime:0.44  LR:0.000100\n",
      "   | > Step:31/68  GlobalStep:75380  TotalLoss:0.00250  PostnetLoss:0.00121  DecoderLoss:0.00129  StopLoss:0.05841  GradNorm:0.00375  GradNormST:0.01658  AvgTextLen:24.1  AvgSpecLen:82.7  StepTime:0.43  LR:0.000100\n",
      "   | > Step:41/68  GlobalStep:75390  TotalLoss:0.00278  PostnetLoss:0.00134  DecoderLoss:0.00144  StopLoss:0.03985  GradNorm:0.00357  GradNormST:0.00860  AvgTextLen:32.3  AvgSpecLen:109.4  StepTime:0.68  LR:0.000100\n",
      "   | > Step:51/68  GlobalStep:75400  TotalLoss:0.00285  PostnetLoss:0.00137  DecoderLoss:0.00148  StopLoss:0.04855  GradNorm:0.00303  GradNormST:0.00906  AvgTextLen:39.2  AvgSpecLen:126.9  StepTime:0.75  LR:0.000100\n",
      "   | > Step:61/68  GlobalStep:75410  TotalLoss:0.00313  PostnetLoss:0.00149  DecoderLoss:0.00163  StopLoss:0.04610  GradNorm:0.00379  GradNormST:0.01235  AvgTextLen:54.6  AvgSpecLen:165.5  StepTime:0.90  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:75417  AvgTotalLoss:0.06364  AvgPostnetLoss:0.00122  AvgDecoderLoss:0.00132  AvgStopLoss:0.06110  EpochTime:41.47  AvgStepTime:0.60\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.10259   PostnetLoss: 0.00440   DecoderLoss:0.00487  StopLoss: 0.09331  \n",
      "   | > TotalLoss: 0.06997   PostnetLoss: 0.00645   DecoderLoss:0.00717  StopLoss: 0.05635  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00122   Validation Loss: 0.00590\n",
      "\n",
      " > Epoch 93/1000\n",
      "   | > Step:2/68  GlobalStep:75420  TotalLoss:0.00163  PostnetLoss:0.00079  DecoderLoss:0.00084  StopLoss:0.07968  GradNorm:0.00433  GradNormST:0.02558  AvgTextLen:4.0  AvgSpecLen:31.3  StepTime:0.55  LR:0.000100\n",
      "   | > Step:12/68  GlobalStep:75430  TotalLoss:0.00196  PostnetLoss:0.00094  DecoderLoss:0.00101  StopLoss:0.06760  GradNorm:0.00358  GradNormST:0.01343  AvgTextLen:11.0  AvgSpecLen:49.3  StepTime:0.41  LR:0.000100\n",
      "   | > Step:22/68  GlobalStep:75440  TotalLoss:0.00228  PostnetLoss:0.00110  DecoderLoss:0.00118  StopLoss:0.08209  GradNorm:0.00398  GradNormST:0.01917  AvgTextLen:18.0  AvgSpecLen:72.0  StepTime:0.45  LR:0.000100\n",
      "   | > Step:32/68  GlobalStep:75450  TotalLoss:0.00251  PostnetLoss:0.00121  DecoderLoss:0.00130  StopLoss:0.07558  GradNorm:0.00366  GradNormST:0.01718  AvgTextLen:24.4  AvgSpecLen:91.5  StepTime:0.57  LR:0.000100\n",
      "   | > Step:42/68  GlobalStep:75460  TotalLoss:0.00270  PostnetLoss:0.00130  DecoderLoss:0.00140  StopLoss:0.06177  GradNorm:0.00362  GradNormST:0.02123  AvgTextLen:33.1  AvgSpecLen:109.7  StepTime:0.64  LR:0.000100\n",
      "   | > Step:52/68  GlobalStep:75470  TotalLoss:0.00298  PostnetLoss:0.00143  DecoderLoss:0.00155  StopLoss:0.06099  GradNorm:0.00428  GradNormST:0.01341  AvgTextLen:42.1  AvgSpecLen:137.5  StepTime:0.72  LR:0.000100\n",
      "   | > Step:62/68  GlobalStep:75480  TotalLoss:0.00321  PostnetLoss:0.00153  DecoderLoss:0.00167  StopLoss:0.06713  GradNorm:0.00386  GradNormST:0.02094  AvgTextLen:54.0  AvgSpecLen:174.7  StepTime:0.75  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:75486  AvgTotalLoss:0.06808  AvgPostnetLoss:0.00122  AvgDecoderLoss:0.00132  AvgStopLoss:0.06554  EpochTime:42.45  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.10345   PostnetLoss: 0.00439   DecoderLoss:0.00487  StopLoss: 0.09418  \n",
      "   | > TotalLoss: 0.07167   PostnetLoss: 0.00678   DecoderLoss:0.00751  StopLoss: 0.05738  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00122   Validation Loss: 0.00606\n",
      "\n",
      " > Epoch 94/1000\n",
      "   | > Step:3/68  GlobalStep:75490  TotalLoss:0.00171  PostnetLoss:0.00083  DecoderLoss:0.00088  StopLoss:0.07464  GradNorm:0.00428  GradNormST:0.01718  AvgTextLen:4.7  AvgSpecLen:29.7  StepTime:0.27  LR:0.000100\n",
      "   | > Step:13/68  GlobalStep:75500  TotalLoss:0.00194  PostnetLoss:0.00094  DecoderLoss:0.00100  StopLoss:0.07895  GradNorm:0.00372  GradNormST:0.02388  AvgTextLen:11.5  AvgSpecLen:55.2  StepTime:0.39  LR:0.000100\n",
      "   | > Step:23/68  GlobalStep:75510  TotalLoss:0.00229  PostnetLoss:0.00111  DecoderLoss:0.00119  StopLoss:0.06267  GradNorm:0.00352  GradNormST:0.01329  AvgTextLen:18.3  AvgSpecLen:72.2  StepTime:0.45  LR:0.000100\n",
      "   | > Step:33/68  GlobalStep:75520  TotalLoss:0.00256  PostnetLoss:0.00123  DecoderLoss:0.00132  StopLoss:0.05463  GradNorm:0.00384  GradNormST:0.00955  AvgTextLen:25.9  AvgSpecLen:88.9  StepTime:0.55  LR:0.000100\n",
      "   | > Step:43/68  GlobalStep:75530  TotalLoss:0.00272  PostnetLoss:0.00131  DecoderLoss:0.00141  StopLoss:0.06098  GradNorm:0.00339  GradNormST:0.01480  AvgTextLen:33.4  AvgSpecLen:115.7  StepTime:0.61  LR:0.000100\n",
      "   | > Step:53/68  GlobalStep:75540  TotalLoss:0.00295  PostnetLoss:0.00142  DecoderLoss:0.00153  StopLoss:0.04699  GradNorm:0.00383  GradNormST:0.00705  AvgTextLen:42.4  AvgSpecLen:137.6  StepTime:0.79  LR:0.000100\n",
      "   | > Step:63/68  GlobalStep:75550  TotalLoss:0.00330  PostnetLoss:0.00158  DecoderLoss:0.00172  StopLoss:0.03924  GradNorm:0.00467  GradNormST:0.00747  AvgTextLen:58.6  AvgSpecLen:185.1  StepTime:0.92  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:75555  AvgTotalLoss:0.06708  AvgPostnetLoss:0.00123  AvgDecoderLoss:0.00132  AvgStopLoss:0.06453  EpochTime:42.18  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.10407   PostnetLoss: 0.00446   DecoderLoss:0.00494  StopLoss: 0.09467  \n",
      "   | > TotalLoss: 0.06899   PostnetLoss: 0.00644   DecoderLoss:0.00714  StopLoss: 0.05541  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00123   Validation Loss: 0.00588\n",
      "\n",
      " > Epoch 95/1000\n",
      "   | > Step:4/68  GlobalStep:75560  TotalLoss:0.00170  PostnetLoss:0.00082  DecoderLoss:0.00088  StopLoss:0.11722  GradNorm:0.00402  GradNormST:0.04114  AvgTextLen:4.8  AvgSpecLen:31.1  StepTime:0.24  LR:0.000100\n",
      "   | > Step:14/68  GlobalStep:75570  TotalLoss:0.00206  PostnetLoss:0.00100  DecoderLoss:0.00106  StopLoss:0.08068  GradNorm:0.00418  GradNormST:0.01960  AvgTextLen:12.2  AvgSpecLen:52.0  StepTime:0.34  LR:0.000100\n",
      "   | > Step:24/68  GlobalStep:75580  TotalLoss:0.00241  PostnetLoss:0.00116  DecoderLoss:0.00124  StopLoss:0.05944  GradNorm:0.00384  GradNormST:0.01075  AvgTextLen:19.2  AvgSpecLen:79.0  StepTime:0.53  LR:0.000100\n",
      "   | > Step:34/68  GlobalStep:75590  TotalLoss:0.00261  PostnetLoss:0.00126  DecoderLoss:0.00136  StopLoss:0.04974  GradNorm:0.00368  GradNormST:0.00917  AvgTextLen:26.2  AvgSpecLen:95.9  StepTime:0.48  LR:0.000100\n",
      "   | > Step:44/68  GlobalStep:75600  TotalLoss:0.00281  PostnetLoss:0.00135  DecoderLoss:0.00146  StopLoss:0.06642  GradNorm:0.00377  GradNormST:0.02342  AvgTextLen:33.8  AvgSpecLen:116.3  StepTime:0.55  LR:0.000100\n",
      "   | > Step:54/68  GlobalStep:75610  TotalLoss:0.00303  PostnetLoss:0.00145  DecoderLoss:0.00158  StopLoss:0.04669  GradNorm:0.00405  GradNormST:0.00808  AvgTextLen:42.2  AvgSpecLen:144.9  StepTime:0.81  LR:0.000100\n",
      "   | > Step:64/68  GlobalStep:75620  TotalLoss:0.00311  PostnetLoss:0.00149  DecoderLoss:0.00163  StopLoss:0.05300  GradNorm:0.00366  GradNormST:0.01546  AvgTextLen:60.6  AvgSpecLen:177.6  StepTime:0.80  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:75624  AvgTotalLoss:0.06549  AvgPostnetLoss:0.00123  AvgDecoderLoss:0.00132  AvgStopLoss:0.06294  EpochTime:41.92  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.10719   PostnetLoss: 0.00446   DecoderLoss:0.00494  StopLoss: 0.09778  \n",
      "   | > TotalLoss: 0.07286   PostnetLoss: 0.00670   DecoderLoss:0.00743  StopLoss: 0.05873  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00123   Validation Loss: 0.00606\n",
      "\n",
      " > Epoch 96/1000\n",
      "   | > Step:5/68  GlobalStep:75630  TotalLoss:0.00170  PostnetLoss:0.00082  DecoderLoss:0.00088  StopLoss:0.07715  GradNorm:0.00377  GradNormST:0.01547  AvgTextLen:5.9  AvgSpecLen:33.2  StepTime:0.32  LR:0.000100\n",
      "   | > Step:15/68  GlobalStep:75640  TotalLoss:0.00212  PostnetLoss:0.00103  DecoderLoss:0.00109  StopLoss:0.06962  GradNorm:0.00395  GradNormST:0.02394  AvgTextLen:12.8  AvgSpecLen:59.1  StepTime:0.54  LR:0.000100\n",
      "   | > Step:25/68  GlobalStep:75650  TotalLoss:0.00236  PostnetLoss:0.00114  DecoderLoss:0.00122  StopLoss:0.05629  GradNorm:0.00414  GradNormST:0.01008  AvgTextLen:19.8  AvgSpecLen:73.3  StepTime:0.45  LR:0.000100\n",
      "   | > Step:35/68  GlobalStep:75660  TotalLoss:0.00258  PostnetLoss:0.00124  DecoderLoss:0.00133  StopLoss:0.06953  GradNorm:0.00369  GradNormST:0.02895  AvgTextLen:27.1  AvgSpecLen:104.7  StepTime:0.63  LR:0.000100\n",
      "   | > Step:45/68  GlobalStep:75670  TotalLoss:0.00275  PostnetLoss:0.00132  DecoderLoss:0.00143  StopLoss:0.06359  GradNorm:0.00369  GradNormST:0.01778  AvgTextLen:34.4  AvgSpecLen:118.6  StepTime:0.54  LR:0.000100\n",
      "   | > Step:55/68  GlobalStep:75680  TotalLoss:0.00308  PostnetLoss:0.00148  DecoderLoss:0.00160  StopLoss:0.06016  GradNorm:0.00454  GradNormST:0.02003  AvgTextLen:43.9  AvgSpecLen:145.2  StepTime:0.67  LR:0.000100\n",
      "   | > Step:65/68  GlobalStep:75690  TotalLoss:0.00336  PostnetLoss:0.00160  DecoderLoss:0.00176  StopLoss:0.04637  GradNorm:0.00472  GradNormST:0.01539  AvgTextLen:64.2  AvgSpecLen:207.4  StepTime:1.19  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:75693  AvgTotalLoss:0.06337  AvgPostnetLoss:0.00123  AvgDecoderLoss:0.00132  AvgStopLoss:0.06082  EpochTime:42.58  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.10521   PostnetLoss: 0.00444   DecoderLoss:0.00490  StopLoss: 0.09588  \n",
      "   | > TotalLoss: 0.07332   PostnetLoss: 0.00689   DecoderLoss:0.00760  StopLoss: 0.05883  \n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00123   Validation Loss: 0.00605\n",
      "\n",
      " > Epoch 97/1000\n",
      "   | > Step:6/68  GlobalStep:75700  TotalLoss:0.00177  PostnetLoss:0.00086  DecoderLoss:0.00091  StopLoss:0.07304  GradNorm:0.00383  GradNormST:0.03602  AvgTextLen:6.0  AvgSpecLen:34.8  StepTime:0.39  LR:0.000100\n",
      "   | > Step:16/68  GlobalStep:75710  TotalLoss:0.00213  PostnetLoss:0.00103  DecoderLoss:0.00110  StopLoss:0.05661  GradNorm:0.00369  GradNormST:0.01602  AvgTextLen:13.8  AvgSpecLen:60.6  StepTime:0.29  LR:0.000100\n",
      "   | > Step:26/68  GlobalStep:75720  TotalLoss:0.00238  PostnetLoss:0.00115  DecoderLoss:0.00123  StopLoss:0.05431  GradNorm:0.00365  GradNormST:0.01186  AvgTextLen:20.3  AvgSpecLen:77.5  StepTime:0.47  LR:0.000100\n",
      "   | > Step:36/68  GlobalStep:75730  TotalLoss:0.00258  PostnetLoss:0.00124  DecoderLoss:0.00134  StopLoss:0.08002  GradNorm:0.00342  GradNormST:0.01646  AvgTextLen:28.5  AvgSpecLen:100.2  StepTime:0.47  LR:0.000100\n",
      "   | > Step:46/68  GlobalStep:75740  TotalLoss:0.00276  PostnetLoss:0.00133  DecoderLoss:0.00144  StopLoss:0.06424  GradNorm:0.00326  GradNormST:0.01418  AvgTextLen:35.4  AvgSpecLen:122.6  StepTime:0.84  LR:0.000100\n",
      "   | > Step:56/68  GlobalStep:75750  TotalLoss:0.00308  PostnetLoss:0.00148  DecoderLoss:0.00160  StopLoss:0.06249  GradNorm:0.00527  GradNormST:0.03695  AvgTextLen:45.8  AvgSpecLen:145.4  StepTime:0.95  LR:0.000100\n",
      "   | > Step:66/68  GlobalStep:75760  TotalLoss:0.00357  PostnetLoss:0.00170  DecoderLoss:0.00187  StopLoss:0.04405  GradNorm:0.00546  GradNormST:0.01559  AvgTextLen:71.2  AvgSpecLen:220.8  StepTime:1.05  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:75762  AvgTotalLoss:0.06769  AvgPostnetLoss:0.00123  AvgDecoderLoss:0.00132  AvgStopLoss:0.06515  EpochTime:42.08  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.11835   PostnetLoss: 0.00449   DecoderLoss:0.00493  StopLoss: 0.10894  \n",
      "   | > TotalLoss: 0.07869   PostnetLoss: 0.00715   DecoderLoss:0.00789  StopLoss: 0.06365  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00123   Validation Loss: 0.00629\n",
      "\n",
      " > Epoch 98/1000\n",
      "   | > Step:7/68  GlobalStep:75770  TotalLoss:0.00182  PostnetLoss:0.00088  DecoderLoss:0.00094  StopLoss:0.09868  GradNorm:0.00376  GradNormST:0.05055  AvgTextLen:7.4  AvgSpecLen:44.6  StepTime:0.31  LR:0.000100\n",
      "   | > Step:17/68  GlobalStep:75780  TotalLoss:0.00214  PostnetLoss:0.00104  DecoderLoss:0.00111  StopLoss:0.07375  GradNorm:0.00369  GradNormST:0.01848  AvgTextLen:14.7  AvgSpecLen:66.0  StepTime:0.42  LR:0.000100\n",
      "   | > Step:27/68  GlobalStep:75790  TotalLoss:0.00240  PostnetLoss:0.00116  DecoderLoss:0.00124  StopLoss:0.06533  GradNorm:0.00390  GradNormST:0.01436  AvgTextLen:21.0  AvgSpecLen:80.2  StepTime:0.62  LR:0.000100\n",
      "   | > Step:37/68  GlobalStep:75800  TotalLoss:0.00265  PostnetLoss:0.00127  DecoderLoss:0.00137  StopLoss:0.06040  GradNorm:0.00376  GradNormST:0.01641  AvgTextLen:29.3  AvgSpecLen:102.4  StepTime:0.63  LR:0.000100\n",
      "   | > Step:47/68  GlobalStep:75810  TotalLoss:0.00286  PostnetLoss:0.00138  DecoderLoss:0.00149  StopLoss:0.05544  GradNorm:0.00333  GradNormST:0.01906  AvgTextLen:36.7  AvgSpecLen:126.9  StepTime:0.59  LR:0.000100\n",
      "   | > Step:57/68  GlobalStep:75820  TotalLoss:0.00316  PostnetLoss:0.00151  DecoderLoss:0.00165  StopLoss:0.04612  GradNorm:0.00572  GradNormST:0.00710  AvgTextLen:47.1  AvgSpecLen:161.0  StepTime:0.81  LR:0.000100\n",
      "   | > Step:67/68  GlobalStep:75830  TotalLoss:0.00350  PostnetLoss:0.00167  DecoderLoss:0.00183  StopLoss:0.05774  GradNorm:0.00377  GradNormST:0.02085  AvgTextLen:80.6  AvgSpecLen:246.5  StepTime:1.22  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:75831  AvgTotalLoss:0.06515  AvgPostnetLoss:0.00123  AvgDecoderLoss:0.00132  AvgStopLoss:0.06260  EpochTime:41.55  AvgStepTime:0.60\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.10850   PostnetLoss: 0.00442   DecoderLoss:0.00489  StopLoss: 0.09919  \n",
      "   | > TotalLoss: 0.08164   PostnetLoss: 0.00710   DecoderLoss:0.00783  StopLoss: 0.06671  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00123   Validation Loss: 0.00629\n",
      "\n",
      " > Epoch 99/1000\n",
      "   | > Step:8/68  GlobalStep:75840  TotalLoss:0.00185  PostnetLoss:0.00090  DecoderLoss:0.00095  StopLoss:0.07914  GradNorm:0.00358  GradNormST:0.02135  AvgTextLen:8.3  AvgSpecLen:44.0  StepTime:0.33  LR:0.000100\n",
      "   | > Step:18/68  GlobalStep:75850  TotalLoss:0.00214  PostnetLoss:0.00103  DecoderLoss:0.00111  StopLoss:0.06166  GradNorm:0.00335  GradNormST:0.01161  AvgTextLen:15.2  AvgSpecLen:64.5  StepTime:0.36  LR:0.000100\n",
      "   | > Step:28/68  GlobalStep:75860  TotalLoss:0.00242  PostnetLoss:0.00117  DecoderLoss:0.00125  StopLoss:0.06562  GradNorm:0.00364  GradNormST:0.01389  AvgTextLen:22.1  AvgSpecLen:85.4  StepTime:0.54  LR:0.000100\n",
      "   | > Step:38/68  GlobalStep:75870  TotalLoss:0.00259  PostnetLoss:0.00125  DecoderLoss:0.00134  StopLoss:0.06706  GradNorm:0.00337  GradNormST:0.01986  AvgTextLen:29.4  AvgSpecLen:104.1  StepTime:0.54  LR:0.000100\n",
      "   | > Step:48/68  GlobalStep:75880  TotalLoss:0.00286  PostnetLoss:0.00138  DecoderLoss:0.00149  StopLoss:0.04667  GradNorm:0.00359  GradNormST:0.00917  AvgTextLen:37.5  AvgSpecLen:123.8  StepTime:0.56  LR:0.000100\n",
      "   | > Step:58/68  GlobalStep:75890  TotalLoss:0.00314  PostnetLoss:0.00151  DecoderLoss:0.00163  StopLoss:0.06067  GradNorm:0.00588  GradNormST:0.01654  AvgTextLen:47.8  AvgSpecLen:158.0  StepTime:0.70  LR:0.000100\n",
      "   | > Step:68/68  GlobalStep:75900  TotalLoss:0.00350  PostnetLoss:0.00167  DecoderLoss:0.00183  StopLoss:0.05448  GradNorm:0.00588  GradNormST:0.02708  AvgTextLen:103.0  AvgSpecLen:308.9  StepTime:1.86  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:75900  AvgTotalLoss:0.06300  AvgPostnetLoss:0.00122  AvgDecoderLoss:0.00132  AvgStopLoss:0.06046  EpochTime:42.74  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.10302   PostnetLoss: 0.00447   DecoderLoss:0.00494  StopLoss: 0.09361  \n",
      "   | > TotalLoss: 0.08096   PostnetLoss: 0.00707   DecoderLoss:0.00784  StopLoss: 0.06605  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00122   Validation Loss: 0.00638\n",
      "\n",
      " > Epoch 100/1000\n",
      "   | > Step:9/68  GlobalStep:75910  TotalLoss:0.00187  PostnetLoss:0.00091  DecoderLoss:0.00096  StopLoss:0.05536  GradNorm:0.00382  GradNormST:0.02273  AvgTextLen:9.0  AvgSpecLen:42.1  StepTime:0.42  LR:0.000100\n",
      "   | > Step:19/68  GlobalStep:75920  TotalLoss:0.00212  PostnetLoss:0.00103  DecoderLoss:0.00109  StopLoss:0.08777  GradNorm:0.00377  GradNormST:0.02931  AvgTextLen:15.5  AvgSpecLen:63.2  StepTime:0.44  LR:0.000100\n",
      "   | > Step:29/68  GlobalStep:75930  TotalLoss:0.00239  PostnetLoss:0.00115  DecoderLoss:0.00124  StopLoss:0.08360  GradNorm:0.00384  GradNormST:0.01926  AvgTextLen:21.8  AvgSpecLen:83.4  StepTime:0.60  LR:0.000100\n",
      "   | > Step:39/68  GlobalStep:75940  TotalLoss:0.00267  PostnetLoss:0.00129  DecoderLoss:0.00138  StopLoss:0.05028  GradNorm:0.00305  GradNormST:0.00984  AvgTextLen:30.2  AvgSpecLen:110.4  StepTime:0.62  LR:0.000100\n",
      "   | > Step:49/68  GlobalStep:75950  TotalLoss:0.00284  PostnetLoss:0.00137  DecoderLoss:0.00147  StopLoss:0.05085  GradNorm:0.00338  GradNormST:0.01432  AvgTextLen:38.2  AvgSpecLen:127.7  StepTime:0.76  LR:0.000100\n",
      "   | > Step:59/68  GlobalStep:75960  TotalLoss:0.00315  PostnetLoss:0.00151  DecoderLoss:0.00164  StopLoss:0.04889  GradNorm:0.00987  GradNormST:0.01174  AvgTextLen:50.5  AvgSpecLen:155.9  StepTime:0.92  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:75969  AvgTotalLoss:0.06424  AvgPostnetLoss:0.00123  AvgDecoderLoss:0.00132  AvgStopLoss:0.06170  EpochTime:43.03  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.10958   PostnetLoss: 0.00439   DecoderLoss:0.00485  StopLoss: 0.10033  \n",
      "   | > TotalLoss: 0.07669   PostnetLoss: 0.00735   DecoderLoss:0.00812  StopLoss: 0.06123  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00123   Validation Loss: 0.00638\n",
      "\n",
      " > Epoch 101/1000\n",
      "   | > Step:0/68  GlobalStep:75970  TotalLoss:0.00171  PostnetLoss:0.00082  DecoderLoss:0.00090  StopLoss:0.11073  GradNorm:0.00489  GradNormST:0.03281  AvgTextLen:3.0  AvgSpecLen:25.3  StepTime:0.56  LR:0.000100\n",
      "   | > Step:10/68  GlobalStep:75980  TotalLoss:0.00196  PostnetLoss:0.00094  DecoderLoss:0.00101  StopLoss:0.06369  GradNorm:0.00404  GradNormST:0.01923  AvgTextLen:9.8  AvgSpecLen:48.6  StepTime:0.52  LR:0.000100\n",
      "   | > Step:20/68  GlobalStep:75990  TotalLoss:0.00230  PostnetLoss:0.00111  DecoderLoss:0.00119  StopLoss:0.05407  GradNorm:0.00409  GradNormST:0.02565  AvgTextLen:16.4  AvgSpecLen:64.8  StepTime:0.48  LR:0.000100\n",
      "   | > Step:30/68  GlobalStep:76000  TotalLoss:0.00246  PostnetLoss:0.00119  DecoderLoss:0.00127  StopLoss:0.07378  GradNorm:0.00353  GradNormST:0.02106  AvgTextLen:22.9  AvgSpecLen:85.8  StepTime:0.57  LR:0.000100\n",
      " | | > Checkpoint saving : /vm/TTS/models/cartman_models/mozilla-no-loc-fattn-stopnet-sigmoid-loss_masking-August-15-2019_11+26AM-de6aeb9/checkpoint_76000.pth.tar\n",
      "warning: audio amplitude out of range, auto clipped.\n",
      "   | > Step:40/68  GlobalStep:76010  TotalLoss:0.00264  PostnetLoss:0.00127  DecoderLoss:0.00136  StopLoss:0.04845  GradNorm:0.00345  GradNormST:0.00829  AvgTextLen:30.9  AvgSpecLen:105.3  StepTime:0.63  LR:0.000100\n",
      "   | > Step:50/68  GlobalStep:76020  TotalLoss:0.00292  PostnetLoss:0.00140  DecoderLoss:0.00152  StopLoss:0.06237  GradNorm:0.00413  GradNormST:0.01822  AvgTextLen:38.8  AvgSpecLen:132.0  StepTime:0.80  LR:0.000100\n",
      "   | > Step:60/68  GlobalStep:76030  TotalLoss:0.00321  PostnetLoss:0.00154  DecoderLoss:0.00167  StopLoss:0.04380  GradNorm:0.00548  GradNormST:0.01089  AvgTextLen:52.0  AvgSpecLen:172.7  StepTime:0.92  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:76038  AvgTotalLoss:0.06926  AvgPostnetLoss:0.00123  AvgDecoderLoss:0.00132  AvgStopLoss:0.06672  EpochTime:42.24  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.10329   PostnetLoss: 0.00446   DecoderLoss:0.00491  StopLoss: 0.09392  \n",
      "   | > TotalLoss: 0.07985   PostnetLoss: 0.00694   DecoderLoss:0.00763  StopLoss: 0.06528  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00123   Validation Loss: 0.00623\n",
      "\n",
      " > Epoch 102/1000\n",
      "   | > Step:1/68  GlobalStep:76040  TotalLoss:0.00171  PostnetLoss:0.00083  DecoderLoss:0.00088  StopLoss:0.07754  GradNorm:0.00581  GradNormST:0.02649  AvgTextLen:3.7  AvgSpecLen:28.6  StepTime:0.38  LR:0.000100\n",
      "   | > Step:11/68  GlobalStep:76050  TotalLoss:0.00195  PostnetLoss:0.00095  DecoderLoss:0.00100  StopLoss:0.07241  GradNorm:0.00363  GradNormST:0.02323  AvgTextLen:10.3  AvgSpecLen:49.4  StepTime:0.39  LR:0.000100\n",
      "   | > Step:21/68  GlobalStep:76060  TotalLoss:0.00226  PostnetLoss:0.00109  DecoderLoss:0.00116  StopLoss:0.06032  GradNorm:0.00357  GradNormST:0.01734  AvgTextLen:16.6  AvgSpecLen:66.3  StepTime:0.43  LR:0.000100\n",
      "   | > Step:31/68  GlobalStep:76070  TotalLoss:0.00245  PostnetLoss:0.00118  DecoderLoss:0.00127  StopLoss:0.09423  GradNorm:0.00356  GradNormST:0.03589  AvgTextLen:24.1  AvgSpecLen:82.7  StepTime:0.54  LR:0.000100\n",
      "   | > Step:41/68  GlobalStep:76080  TotalLoss:0.00275  PostnetLoss:0.00132  DecoderLoss:0.00143  StopLoss:0.04982  GradNorm:0.00348  GradNormST:0.01134  AvgTextLen:32.3  AvgSpecLen:109.4  StepTime:0.67  LR:0.000100\n",
      "   | > Step:51/68  GlobalStep:76090  TotalLoss:0.00282  PostnetLoss:0.00136  DecoderLoss:0.00147  StopLoss:0.06514  GradNorm:0.00311  GradNormST:0.01398  AvgTextLen:39.2  AvgSpecLen:126.9  StepTime:0.76  LR:0.000100\n",
      "   | > Step:61/68  GlobalStep:76100  TotalLoss:0.00322  PostnetLoss:0.00154  DecoderLoss:0.00168  StopLoss:0.06182  GradNorm:0.00644  GradNormST:0.03549  AvgTextLen:54.6  AvgSpecLen:165.5  StepTime:0.90  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:76107  AvgTotalLoss:0.06831  AvgPostnetLoss:0.00122  AvgDecoderLoss:0.00131  AvgStopLoss:0.06577  EpochTime:42.46  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.10132   PostnetLoss: 0.00436   DecoderLoss:0.00481  StopLoss: 0.09216  \n",
      "   | > TotalLoss: 0.07719   PostnetLoss: 0.00696   DecoderLoss:0.00761  StopLoss: 0.06263  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00122   Validation Loss: 0.00608\n",
      "\n",
      " > Epoch 103/1000\n",
      "   | > Step:2/68  GlobalStep:76110  TotalLoss:0.00165  PostnetLoss:0.00080  DecoderLoss:0.00085  StopLoss:0.08918  GradNorm:0.00462  GradNormST:0.02957  AvgTextLen:4.0  AvgSpecLen:31.3  StepTime:0.57  LR:0.000100\n",
      "   | > Step:12/68  GlobalStep:76120  TotalLoss:0.00197  PostnetLoss:0.00095  DecoderLoss:0.00101  StopLoss:0.07490  GradNorm:0.00441  GradNormST:0.02148  AvgTextLen:11.0  AvgSpecLen:49.3  StepTime:0.44  LR:0.000100\n",
      "   | > Step:22/68  GlobalStep:76130  TotalLoss:0.00232  PostnetLoss:0.00112  DecoderLoss:0.00120  StopLoss:0.08293  GradNorm:0.00407  GradNormST:0.01772  AvgTextLen:18.0  AvgSpecLen:72.0  StepTime:0.45  LR:0.000100\n",
      "   | > Step:32/68  GlobalStep:76140  TotalLoss:0.00249  PostnetLoss:0.00120  DecoderLoss:0.00129  StopLoss:0.07239  GradNorm:0.00330  GradNormST:0.01498  AvgTextLen:24.4  AvgSpecLen:91.5  StepTime:0.61  LR:0.000100\n",
      "   | > Step:42/68  GlobalStep:76150  TotalLoss:0.00268  PostnetLoss:0.00129  DecoderLoss:0.00140  StopLoss:0.05964  GradNorm:0.00298  GradNormST:0.01371  AvgTextLen:33.1  AvgSpecLen:109.7  StepTime:0.49  LR:0.000100\n",
      "   | > Step:52/68  GlobalStep:76160  TotalLoss:0.00300  PostnetLoss:0.00144  DecoderLoss:0.00155  StopLoss:0.05706  GradNorm:0.00471  GradNormST:0.01374  AvgTextLen:42.1  AvgSpecLen:137.5  StepTime:0.68  LR:0.000100\n",
      "   | > Step:62/68  GlobalStep:76170  TotalLoss:0.00323  PostnetLoss:0.00155  DecoderLoss:0.00168  StopLoss:0.06387  GradNorm:0.00441  GradNormST:0.01711  AvgTextLen:54.0  AvgSpecLen:174.7  StepTime:0.77  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:76176  AvgTotalLoss:0.06714  AvgPostnetLoss:0.00122  AvgDecoderLoss:0.00131  AvgStopLoss:0.06461  EpochTime:42.35  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.09724   PostnetLoss: 0.00440   DecoderLoss:0.00486  StopLoss: 0.08798  \n",
      "   | > TotalLoss: 0.07130   PostnetLoss: 0.00622   DecoderLoss:0.00688  StopLoss: 0.05819  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00122   Validation Loss: 0.00576\n",
      "\n",
      " > Epoch 104/1000\n",
      "   | > Step:3/68  GlobalStep:76180  TotalLoss:0.00174  PostnetLoss:0.00084  DecoderLoss:0.00089  StopLoss:0.08809  GradNorm:0.00444  GradNormST:0.02819  AvgTextLen:4.7  AvgSpecLen:29.7  StepTime:0.32  LR:0.000100\n",
      "   | > Step:13/68  GlobalStep:76190  TotalLoss:0.00197  PostnetLoss:0.00096  DecoderLoss:0.00101  StopLoss:0.08946  GradNorm:0.00424  GradNormST:0.02957  AvgTextLen:11.5  AvgSpecLen:55.2  StepTime:0.41  LR:0.000100\n",
      "   | > Step:23/68  GlobalStep:76200  TotalLoss:0.00230  PostnetLoss:0.00111  DecoderLoss:0.00119  StopLoss:0.07566  GradNorm:0.00319  GradNormST:0.01451  AvgTextLen:18.3  AvgSpecLen:72.2  StepTime:0.41  LR:0.000100\n",
      "   | > Step:33/68  GlobalStep:76210  TotalLoss:0.00250  PostnetLoss:0.00121  DecoderLoss:0.00129  StopLoss:0.05687  GradNorm:0.00307  GradNormST:0.00931  AvgTextLen:25.9  AvgSpecLen:88.9  StepTime:0.43  LR:0.000100\n",
      "   | > Step:43/68  GlobalStep:76220  TotalLoss:0.00271  PostnetLoss:0.00130  DecoderLoss:0.00140  StopLoss:0.06325  GradNorm:0.00338  GradNormST:0.02130  AvgTextLen:33.4  AvgSpecLen:115.7  StepTime:0.66  LR:0.000100\n",
      "   | > Step:53/68  GlobalStep:76230  TotalLoss:0.00288  PostnetLoss:0.00139  DecoderLoss:0.00150  StopLoss:0.05175  GradNorm:0.00307  GradNormST:0.01203  AvgTextLen:42.4  AvgSpecLen:137.6  StepTime:0.74  LR:0.000100\n",
      "   | > Step:63/68  GlobalStep:76240  TotalLoss:0.00331  PostnetLoss:0.00159  DecoderLoss:0.00172  StopLoss:0.04604  GradNorm:0.00578  GradNormST:0.01517  AvgTextLen:58.6  AvgSpecLen:185.1  StepTime:0.91  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:76245  AvgTotalLoss:0.07079  AvgPostnetLoss:0.00122  AvgDecoderLoss:0.00131  AvgStopLoss:0.06826  EpochTime:42.12  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.10988   PostnetLoss: 0.00437   DecoderLoss:0.00484  StopLoss: 0.10067  \n",
      "   | > TotalLoss: 0.08000   PostnetLoss: 0.00669   DecoderLoss:0.00736  StopLoss: 0.06596  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00122   Validation Loss: 0.00608\n",
      "\n",
      " > Epoch 105/1000\n",
      "   | > Step:4/68  GlobalStep:76250  TotalLoss:0.00177  PostnetLoss:0.00086  DecoderLoss:0.00091  StopLoss:0.13255  GradNorm:0.00706  GradNormST:0.03800  AvgTextLen:4.8  AvgSpecLen:31.1  StepTime:0.27  LR:0.000100\n",
      "   | > Step:14/68  GlobalStep:76260  TotalLoss:0.00208  PostnetLoss:0.00101  DecoderLoss:0.00107  StopLoss:0.06766  GradNorm:0.00479  GradNormST:0.02092  AvgTextLen:12.2  AvgSpecLen:52.0  StepTime:0.33  LR:0.000100\n",
      "   | > Step:24/68  GlobalStep:76270  TotalLoss:0.00235  PostnetLoss:0.00113  DecoderLoss:0.00122  StopLoss:0.07080  GradNorm:0.00315  GradNormST:0.01185  AvgTextLen:19.2  AvgSpecLen:79.0  StepTime:0.54  LR:0.000100\n",
      "   | > Step:34/68  GlobalStep:76280  TotalLoss:0.00257  PostnetLoss:0.00124  DecoderLoss:0.00133  StopLoss:0.05445  GradNorm:0.00316  GradNormST:0.00883  AvgTextLen:26.2  AvgSpecLen:95.9  StepTime:0.48  LR:0.000100\n",
      "   | > Step:44/68  GlobalStep:76290  TotalLoss:0.00278  PostnetLoss:0.00134  DecoderLoss:0.00145  StopLoss:0.06412  GradNorm:0.00302  GradNormST:0.01822  AvgTextLen:33.8  AvgSpecLen:116.3  StepTime:0.65  LR:0.000100\n",
      "   | > Step:54/68  GlobalStep:76300  TotalLoss:0.00296  PostnetLoss:0.00142  DecoderLoss:0.00154  StopLoss:0.05513  GradNorm:0.00295  GradNormST:0.00826  AvgTextLen:42.2  AvgSpecLen:144.9  StepTime:0.71  LR:0.000100\n",
      "   | > Step:64/68  GlobalStep:76310  TotalLoss:0.00305  PostnetLoss:0.00146  DecoderLoss:0.00159  StopLoss:0.05059  GradNorm:0.00305  GradNormST:0.01089  AvgTextLen:60.6  AvgSpecLen:177.6  StepTime:0.80  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:76314  AvgTotalLoss:0.07155  AvgPostnetLoss:0.00122  AvgDecoderLoss:0.00131  AvgStopLoss:0.06902  EpochTime:42.03  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.10450   PostnetLoss: 0.00425   DecoderLoss:0.00471  StopLoss: 0.09554  \n",
      "   | > TotalLoss: 0.07247   PostnetLoss: 0.00623   DecoderLoss:0.00689  StopLoss: 0.05934  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00122   Validation Loss: 0.00576\n",
      "\n",
      " > Epoch 106/1000\n",
      "   | > Step:5/68  GlobalStep:76320  TotalLoss:0.00179  PostnetLoss:0.00086  DecoderLoss:0.00093  StopLoss:0.09521  GradNorm:0.00499  GradNormST:0.02406  AvgTextLen:5.9  AvgSpecLen:33.2  StepTime:0.31  LR:0.000100\n",
      "   | > Step:15/68  GlobalStep:76330  TotalLoss:0.00223  PostnetLoss:0.00108  DecoderLoss:0.00114  StopLoss:0.06102  GradNorm:0.00618  GradNormST:0.01844  AvgTextLen:12.8  AvgSpecLen:59.1  StepTime:0.58  LR:0.000100\n",
      "   | > Step:25/68  GlobalStep:76340  TotalLoss:0.00234  PostnetLoss:0.00113  DecoderLoss:0.00121  StopLoss:0.08002  GradNorm:0.00416  GradNormST:0.01793  AvgTextLen:19.8  AvgSpecLen:73.3  StepTime:0.49  LR:0.000100\n",
      "   | > Step:35/68  GlobalStep:76350  TotalLoss:0.00259  PostnetLoss:0.00125  DecoderLoss:0.00134  StopLoss:0.06859  GradNorm:0.00306  GradNormST:0.02484  AvgTextLen:27.1  AvgSpecLen:104.7  StepTime:0.74  LR:0.000100\n",
      "   | > Step:45/68  GlobalStep:76360  TotalLoss:0.00267  PostnetLoss:0.00128  DecoderLoss:0.00139  StopLoss:0.06753  GradNorm:0.00298  GradNormST:0.02045  AvgTextLen:34.4  AvgSpecLen:118.6  StepTime:0.54  LR:0.000100\n",
      "   | > Step:55/68  GlobalStep:76370  TotalLoss:0.00300  PostnetLoss:0.00144  DecoderLoss:0.00156  StopLoss:0.06300  GradNorm:0.00323  GradNormST:0.01319  AvgTextLen:43.9  AvgSpecLen:145.2  StepTime:0.66  LR:0.000100\n",
      "   | > Step:65/68  GlobalStep:76380  TotalLoss:0.00332  PostnetLoss:0.00159  DecoderLoss:0.00173  StopLoss:0.05088  GradNorm:0.00364  GradNormST:0.02025  AvgTextLen:64.2  AvgSpecLen:207.4  StepTime:1.23  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:76383  AvgTotalLoss:0.07261  AvgPostnetLoss:0.00121  AvgDecoderLoss:0.00131  AvgStopLoss:0.07009  EpochTime:42.33  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.10926   PostnetLoss: 0.00437   DecoderLoss:0.00485  StopLoss: 0.10004  \n",
      "   | > TotalLoss: 0.07610   PostnetLoss: 0.00631   DecoderLoss:0.00704  StopLoss: 0.06276  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00121   Validation Loss: 0.00582\n",
      "\n",
      " > Epoch 107/1000\n",
      "   | > Step:6/68  GlobalStep:76390  TotalLoss:0.00177  PostnetLoss:0.00086  DecoderLoss:0.00091  StopLoss:0.06766  GradNorm:0.00396  GradNormST:0.03090  AvgTextLen:6.0  AvgSpecLen:34.8  StepTime:0.41  LR:0.000100\n",
      "   | > Step:16/68  GlobalStep:76400  TotalLoss:0.00216  PostnetLoss:0.00105  DecoderLoss:0.00111  StopLoss:0.07537  GradNorm:0.00546  GradNormST:0.02410  AvgTextLen:13.8  AvgSpecLen:60.6  StepTime:0.38  LR:0.000100\n",
      "   | > Step:26/68  GlobalStep:76410  TotalLoss:0.00236  PostnetLoss:0.00114  DecoderLoss:0.00122  StopLoss:0.05725  GradNorm:0.00432  GradNormST:0.01290  AvgTextLen:20.3  AvgSpecLen:77.5  StepTime:0.42  LR:0.000100\n",
      "   | > Step:36/68  GlobalStep:76420  TotalLoss:0.00257  PostnetLoss:0.00124  DecoderLoss:0.00133  StopLoss:0.09447  GradNorm:0.00316  GradNormST:0.01864  AvgTextLen:28.5  AvgSpecLen:100.2  StepTime:0.47  LR:0.000100\n",
      "   | > Step:46/68  GlobalStep:76430  TotalLoss:0.00272  PostnetLoss:0.00131  DecoderLoss:0.00141  StopLoss:0.05775  GradNorm:0.00311  GradNormST:0.01255  AvgTextLen:35.4  AvgSpecLen:122.6  StepTime:0.79  LR:0.000100\n",
      "   | > Step:56/68  GlobalStep:76440  TotalLoss:0.00298  PostnetLoss:0.00143  DecoderLoss:0.00155  StopLoss:0.05735  GradNorm:0.00357  GradNormST:0.03049  AvgTextLen:45.8  AvgSpecLen:145.4  StepTime:0.95  LR:0.000100\n",
      "   | > Step:66/68  GlobalStep:76450  TotalLoss:0.00347  PostnetLoss:0.00165  DecoderLoss:0.00181  StopLoss:0.04742  GradNorm:0.00317  GradNormST:0.01315  AvgTextLen:71.2  AvgSpecLen:220.8  StepTime:1.03  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:76452  AvgTotalLoss:0.07234  AvgPostnetLoss:0.00121  AvgDecoderLoss:0.00130  AvgStopLoss:0.06984  EpochTime:42.38  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.11065   PostnetLoss: 0.00432   DecoderLoss:0.00481  StopLoss: 0.10152  \n",
      "   | > TotalLoss: 0.07410   PostnetLoss: 0.00629   DecoderLoss:0.00702  StopLoss: 0.06079  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00121   Validation Loss: 0.00579\n",
      "\n",
      " > Epoch 108/1000\n",
      "   | > Step:7/68  GlobalStep:76460  TotalLoss:0.00184  PostnetLoss:0.00089  DecoderLoss:0.00094  StopLoss:0.07444  GradNorm:0.00408  GradNormST:0.03058  AvgTextLen:7.4  AvgSpecLen:44.6  StepTime:0.42  LR:0.000100\n",
      "   | > Step:17/68  GlobalStep:76470  TotalLoss:0.00211  PostnetLoss:0.00102  DecoderLoss:0.00109  StopLoss:0.07439  GradNorm:0.00394  GradNormST:0.01847  AvgTextLen:14.7  AvgSpecLen:66.0  StepTime:0.39  LR:0.000100\n",
      "   | > Step:27/68  GlobalStep:76480  TotalLoss:0.00244  PostnetLoss:0.00118  DecoderLoss:0.00126  StopLoss:0.08182  GradNorm:0.00372  GradNormST:0.03244  AvgTextLen:21.0  AvgSpecLen:80.2  StepTime:0.54  LR:0.000100\n",
      "   | > Step:37/68  GlobalStep:76490  TotalLoss:0.00262  PostnetLoss:0.00126  DecoderLoss:0.00136  StopLoss:0.05979  GradNorm:0.00338  GradNormST:0.01306  AvgTextLen:29.3  AvgSpecLen:102.4  StepTime:0.60  LR:0.000100\n",
      "   | > Step:47/68  GlobalStep:76500  TotalLoss:0.00279  PostnetLoss:0.00134  DecoderLoss:0.00145  StopLoss:0.06606  GradNorm:0.00299  GradNormST:0.01414  AvgTextLen:36.7  AvgSpecLen:126.9  StepTime:0.58  LR:0.000100\n",
      "   | > Step:57/68  GlobalStep:76510  TotalLoss:0.00304  PostnetLoss:0.00146  DecoderLoss:0.00159  StopLoss:0.04788  GradNorm:0.00312  GradNormST:0.00707  AvgTextLen:47.1  AvgSpecLen:161.0  StepTime:0.81  LR:0.000100\n",
      "   | > Step:67/68  GlobalStep:76520  TotalLoss:0.00342  PostnetLoss:0.00163  DecoderLoss:0.00179  StopLoss:0.04919  GradNorm:0.00259  GradNormST:0.01463  AvgTextLen:80.6  AvgSpecLen:246.5  StepTime:1.23  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:76521  AvgTotalLoss:0.07083  AvgPostnetLoss:0.00120  AvgDecoderLoss:0.00130  AvgStopLoss:0.06833  EpochTime:41.58  AvgStepTime:0.60\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.11382   PostnetLoss: 0.00433   DecoderLoss:0.00481  StopLoss: 0.10469  \n",
      "   | > TotalLoss: 0.07542   PostnetLoss: 0.00644   DecoderLoss:0.00717  StopLoss: 0.06181  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00120   Validation Loss: 0.00582\n",
      "\n",
      " > Epoch 109/1000\n",
      "   | > Step:8/68  GlobalStep:76530  TotalLoss:0.00192  PostnetLoss:0.00093  DecoderLoss:0.00099  StopLoss:0.09395  GradNorm:0.00541  GradNormST:0.02003  AvgTextLen:8.3  AvgSpecLen:44.0  StepTime:0.25  LR:0.000100\n",
      "   | > Step:18/68  GlobalStep:76540  TotalLoss:0.00214  PostnetLoss:0.00104  DecoderLoss:0.00110  StopLoss:0.10140  GradNorm:0.00470  GradNormST:0.02558  AvgTextLen:15.2  AvgSpecLen:64.5  StepTime:0.57  LR:0.000100\n",
      "   | > Step:28/68  GlobalStep:76550  TotalLoss:0.00244  PostnetLoss:0.00118  DecoderLoss:0.00126  StopLoss:0.08517  GradNorm:0.00471  GradNormST:0.01715  AvgTextLen:22.1  AvgSpecLen:85.4  StepTime:0.45  LR:0.000100\n",
      "   | > Step:38/68  GlobalStep:76560  TotalLoss:0.00259  PostnetLoss:0.00125  DecoderLoss:0.00134  StopLoss:0.06013  GradNorm:0.00391  GradNormST:0.01211  AvgTextLen:29.4  AvgSpecLen:104.1  StepTime:0.61  LR:0.000100\n",
      "   | > Step:48/68  GlobalStep:76570  TotalLoss:0.00281  PostnetLoss:0.00135  DecoderLoss:0.00146  StopLoss:0.05857  GradNorm:0.00288  GradNormST:0.01364  AvgTextLen:37.5  AvgSpecLen:123.8  StepTime:0.70  LR:0.000100\n",
      "   | > Step:58/68  GlobalStep:76580  TotalLoss:0.00306  PostnetLoss:0.00147  DecoderLoss:0.00159  StopLoss:0.05864  GradNorm:0.00289  GradNormST:0.01214  AvgTextLen:47.8  AvgSpecLen:158.0  StepTime:0.70  LR:0.000100\n",
      "   | > Step:68/68  GlobalStep:76590  TotalLoss:0.00338  PostnetLoss:0.00161  DecoderLoss:0.00176  StopLoss:0.04889  GradNorm:0.00283  GradNormST:0.02199  AvgTextLen:103.0  AvgSpecLen:308.9  StepTime:1.86  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:76590  AvgTotalLoss:0.07146  AvgPostnetLoss:0.00121  AvgDecoderLoss:0.00130  AvgStopLoss:0.06896  EpochTime:42.46  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.10677   PostnetLoss: 0.00434   DecoderLoss:0.00482  StopLoss: 0.09761  \n",
      "   | > TotalLoss: 0.07171   PostnetLoss: 0.00630   DecoderLoss:0.00701  StopLoss: 0.05840  \n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00121   Validation Loss: 0.00580\n",
      "\n",
      " > Epoch 110/1000\n",
      "   | > Step:9/68  GlobalStep:76600  TotalLoss:0.00180  PostnetLoss:0.00088  DecoderLoss:0.00092  StopLoss:0.06200  GradNorm:0.00445  GradNormST:0.02428  AvgTextLen:9.0  AvgSpecLen:42.1  StepTime:0.42  LR:0.000100\n",
      "   | > Step:19/68  GlobalStep:76610  TotalLoss:0.00222  PostnetLoss:0.00108  DecoderLoss:0.00114  StopLoss:0.08918  GradNorm:0.00560  GradNormST:0.02490  AvgTextLen:15.5  AvgSpecLen:63.2  StepTime:0.42  LR:0.000100\n",
      "   | > Step:29/68  GlobalStep:76620  TotalLoss:0.00239  PostnetLoss:0.00115  DecoderLoss:0.00123  StopLoss:0.08964  GradNorm:0.00500  GradNormST:0.02453  AvgTextLen:21.8  AvgSpecLen:83.4  StepTime:0.59  LR:0.000100\n",
      "   | > Step:39/68  GlobalStep:76630  TotalLoss:0.00269  PostnetLoss:0.00130  DecoderLoss:0.00139  StopLoss:0.04937  GradNorm:0.00402  GradNormST:0.01062  AvgTextLen:30.2  AvgSpecLen:110.4  StepTime:0.63  LR:0.000100\n",
      "   | > Step:49/68  GlobalStep:76640  TotalLoss:0.00283  PostnetLoss:0.00136  DecoderLoss:0.00147  StopLoss:0.05138  GradNorm:0.00338  GradNormST:0.01004  AvgTextLen:38.2  AvgSpecLen:127.7  StepTime:0.69  LR:0.000100\n",
      "   | > Step:59/68  GlobalStep:76650  TotalLoss:0.00304  PostnetLoss:0.00146  DecoderLoss:0.00159  StopLoss:0.05390  GradNorm:0.00311  GradNormST:0.01690  AvgTextLen:50.5  AvgSpecLen:155.9  StepTime:0.75  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:76659  AvgTotalLoss:0.06903  AvgPostnetLoss:0.00121  AvgDecoderLoss:0.00130  AvgStopLoss:0.06651  EpochTime:42.55  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.10859   PostnetLoss: 0.00439   DecoderLoss:0.00488  StopLoss: 0.09932  \n",
      "   | > TotalLoss: 0.08260   PostnetLoss: 0.00644   DecoderLoss:0.00717  StopLoss: 0.06899  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00121   Validation Loss: 0.00590\n",
      "\n",
      " > Epoch 111/1000\n",
      "   | > Step:0/68  GlobalStep:76660  TotalLoss:0.00168  PostnetLoss:0.00081  DecoderLoss:0.00088  StopLoss:0.10933  GradNorm:0.00512  GradNormST:0.03691  AvgTextLen:3.0  AvgSpecLen:25.3  StepTime:0.47  LR:0.000100\n",
      "   | > Step:10/68  GlobalStep:76670  TotalLoss:0.00192  PostnetLoss:0.00093  DecoderLoss:0.00099  StopLoss:0.07155  GradNorm:0.00458  GradNormST:0.02861  AvgTextLen:9.8  AvgSpecLen:48.6  StepTime:0.44  LR:0.000100\n",
      "   | > Step:20/68  GlobalStep:76680  TotalLoss:0.00228  PostnetLoss:0.00111  DecoderLoss:0.00117  StopLoss:0.05239  GradNorm:0.00503  GradNormST:0.01313  AvgTextLen:16.4  AvgSpecLen:64.8  StepTime:0.52  LR:0.000100\n",
      "   | > Step:30/68  GlobalStep:76690  TotalLoss:0.00248  PostnetLoss:0.00120  DecoderLoss:0.00128  StopLoss:0.06129  GradNorm:0.00535  GradNormST:0.01863  AvgTextLen:22.9  AvgSpecLen:85.8  StepTime:0.57  LR:0.000100\n",
      "   | > Step:40/68  GlobalStep:76700  TotalLoss:0.00265  PostnetLoss:0.00128  DecoderLoss:0.00137  StopLoss:0.04735  GradNorm:0.00438  GradNormST:0.00954  AvgTextLen:30.9  AvgSpecLen:105.3  StepTime:0.59  LR:0.000100\n",
      "   | > Step:50/68  GlobalStep:76710  TotalLoss:0.00284  PostnetLoss:0.00136  DecoderLoss:0.00148  StopLoss:0.05611  GradNorm:0.00316  GradNormST:0.01212  AvgTextLen:38.8  AvgSpecLen:132.0  StepTime:0.85  LR:0.000100\n",
      "   | > Step:60/68  GlobalStep:76720  TotalLoss:0.00312  PostnetLoss:0.00150  DecoderLoss:0.00163  StopLoss:0.04886  GradNorm:0.00322  GradNormST:0.00939  AvgTextLen:52.0  AvgSpecLen:172.7  StepTime:0.96  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:76728  AvgTotalLoss:0.06964  AvgPostnetLoss:0.00121  AvgDecoderLoss:0.00130  AvgStopLoss:0.06713  EpochTime:42.93  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.10782   PostnetLoss: 0.00434   DecoderLoss:0.00483  StopLoss: 0.09865  \n",
      "   | > TotalLoss: 0.07642   PostnetLoss: 0.00627   DecoderLoss:0.00697  StopLoss: 0.06318  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00121   Validation Loss: 0.00579\n",
      "\n",
      " > Epoch 112/1000\n",
      "   | > Step:1/68  GlobalStep:76730  TotalLoss:0.00171  PostnetLoss:0.00083  DecoderLoss:0.00089  StopLoss:0.07584  GradNorm:0.00537  GradNormST:0.02278  AvgTextLen:3.7  AvgSpecLen:28.6  StepTime:0.31  LR:0.000100\n",
      "   | > Step:11/68  GlobalStep:76740  TotalLoss:0.00195  PostnetLoss:0.00095  DecoderLoss:0.00100  StopLoss:0.06885  GradNorm:0.00563  GradNormST:0.02523  AvgTextLen:10.3  AvgSpecLen:49.4  StepTime:0.38  LR:0.000100\n",
      "   | > Step:21/68  GlobalStep:76750  TotalLoss:0.00226  PostnetLoss:0.00110  DecoderLoss:0.00116  StopLoss:0.05742  GradNorm:0.00568  GradNormST:0.01535  AvgTextLen:16.6  AvgSpecLen:66.3  StepTime:0.34  LR:0.000100\n",
      "   | > Step:31/68  GlobalStep:76760  TotalLoss:0.00246  PostnetLoss:0.00119  DecoderLoss:0.00127  StopLoss:0.06387  GradNorm:0.00476  GradNormST:0.01608  AvgTextLen:24.1  AvgSpecLen:82.7  StepTime:0.54  LR:0.000100\n",
      "   | > Step:41/68  GlobalStep:76770  TotalLoss:0.00275  PostnetLoss:0.00133  DecoderLoss:0.00142  StopLoss:0.04302  GradNorm:0.00468  GradNormST:0.00845  AvgTextLen:32.3  AvgSpecLen:109.4  StepTime:0.68  LR:0.000100\n",
      "   | > Step:51/68  GlobalStep:76780  TotalLoss:0.00285  PostnetLoss:0.00137  DecoderLoss:0.00148  StopLoss:0.05858  GradNorm:0.00382  GradNormST:0.01245  AvgTextLen:39.2  AvgSpecLen:126.9  StepTime:0.78  LR:0.000100\n",
      "   | > Step:61/68  GlobalStep:76790  TotalLoss:0.00304  PostnetLoss:0.00146  DecoderLoss:0.00158  StopLoss:0.05781  GradNorm:0.00313  GradNormST:0.02393  AvgTextLen:54.6  AvgSpecLen:165.5  StepTime:0.90  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:76797  AvgTotalLoss:0.06651  AvgPostnetLoss:0.00121  AvgDecoderLoss:0.00130  AvgStopLoss:0.06399  EpochTime:42.00  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.10876   PostnetLoss: 0.00438   DecoderLoss:0.00485  StopLoss: 0.09953  \n",
      "   | > TotalLoss: 0.07390   PostnetLoss: 0.00610   DecoderLoss:0.00676  StopLoss: 0.06105  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00121   Validation Loss: 0.00575\n",
      "\n",
      " > Epoch 113/1000\n",
      "   | > Step:2/68  GlobalStep:76800  TotalLoss:0.00160  PostnetLoss:0.00077  DecoderLoss:0.00082  StopLoss:0.07728  GradNorm:0.00415  GradNormST:0.02213  AvgTextLen:4.0  AvgSpecLen:31.3  StepTime:0.34  LR:0.000100\n",
      "   | > Step:12/68  GlobalStep:76810  TotalLoss:0.00205  PostnetLoss:0.00099  DecoderLoss:0.00105  StopLoss:0.06366  GradNorm:0.00827  GradNormST:0.01831  AvgTextLen:11.0  AvgSpecLen:49.3  StepTime:0.42  LR:0.000100\n",
      "   | > Step:22/68  GlobalStep:76820  TotalLoss:0.00236  PostnetLoss:0.00115  DecoderLoss:0.00122  StopLoss:0.10879  GradNorm:0.00736  GradNormST:0.05855  AvgTextLen:18.0  AvgSpecLen:72.0  StepTime:0.42  LR:0.000100\n",
      "   | > Step:32/68  GlobalStep:76830  TotalLoss:0.00261  PostnetLoss:0.00126  DecoderLoss:0.00135  StopLoss:0.10397  GradNorm:0.00629  GradNormST:0.04412  AvgTextLen:24.4  AvgSpecLen:91.5  StepTime:0.56  LR:0.000100\n",
      "   | > Step:42/68  GlobalStep:76840  TotalLoss:0.00279  PostnetLoss:0.00134  DecoderLoss:0.00144  StopLoss:0.09540  GradNorm:0.00588  GradNormST:0.02292  AvgTextLen:33.1  AvgSpecLen:109.7  StepTime:0.60  LR:0.000100\n",
      "   | > Step:52/68  GlobalStep:76850  TotalLoss:0.00292  PostnetLoss:0.00141  DecoderLoss:0.00151  StopLoss:0.07836  GradNorm:0.00426  GradNormST:0.01525  AvgTextLen:42.1  AvgSpecLen:137.5  StepTime:0.80  LR:0.000100\n",
      "   | > Step:62/68  GlobalStep:76860  TotalLoss:0.00317  PostnetLoss:0.00152  DecoderLoss:0.00165  StopLoss:0.07335  GradNorm:0.00300  GradNormST:0.01679  AvgTextLen:54.0  AvgSpecLen:174.7  StepTime:0.75  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:76866  AvgTotalLoss:0.08495  AvgPostnetLoss:0.00123  AvgDecoderLoss:0.00132  AvgStopLoss:0.08239  EpochTime:42.63  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.11319   PostnetLoss: 0.00433   DecoderLoss:0.00479  StopLoss: 0.10407  \n",
      "   | > TotalLoss: 0.07850   PostnetLoss: 0.00614   DecoderLoss:0.00679  StopLoss: 0.06558  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00123   Validation Loss: 0.00564\n",
      "\n",
      " > Epoch 114/1000\n",
      "   | > Step:3/68  GlobalStep:76870  TotalLoss:0.00168  PostnetLoss:0.00081  DecoderLoss:0.00086  StopLoss:0.09309  GradNorm:0.00418  GradNormST:0.02837  AvgTextLen:4.7  AvgSpecLen:29.7  StepTime:0.32  LR:0.000100\n",
      "   | > Step:13/68  GlobalStep:76880  TotalLoss:0.00198  PostnetLoss:0.00096  DecoderLoss:0.00102  StopLoss:0.11952  GradNorm:0.00416  GradNormST:0.03645  AvgTextLen:11.5  AvgSpecLen:55.2  StepTime:0.42  LR:0.000100\n",
      "   | > Step:23/68  GlobalStep:76890  TotalLoss:0.00238  PostnetLoss:0.00115  DecoderLoss:0.00123  StopLoss:0.09333  GradNorm:0.00481  GradNormST:0.02565  AvgTextLen:18.3  AvgSpecLen:72.2  StepTime:0.43  LR:0.000100\n",
      "   | > Step:33/68  GlobalStep:76900  TotalLoss:0.00261  PostnetLoss:0.00126  DecoderLoss:0.00135  StopLoss:0.07526  GradNorm:0.00583  GradNormST:0.03290  AvgTextLen:25.9  AvgSpecLen:88.9  StepTime:0.54  LR:0.000100\n",
      "   | > Step:43/68  GlobalStep:76910  TotalLoss:0.00278  PostnetLoss:0.00134  DecoderLoss:0.00143  StopLoss:0.07578  GradNorm:0.00582  GradNormST:0.01745  AvgTextLen:33.4  AvgSpecLen:115.7  StepTime:0.62  LR:0.000100\n",
      "   | > Step:53/68  GlobalStep:76920  TotalLoss:0.00290  PostnetLoss:0.00139  DecoderLoss:0.00150  StopLoss:0.05993  GradNorm:0.00437  GradNormST:0.01144  AvgTextLen:42.4  AvgSpecLen:137.6  StepTime:0.74  LR:0.000100\n",
      "   | > Step:63/68  GlobalStep:76930  TotalLoss:0.00325  PostnetLoss:0.00155  DecoderLoss:0.00169  StopLoss:0.05600  GradNorm:0.00329  GradNormST:0.01897  AvgTextLen:58.6  AvgSpecLen:185.1  StepTime:0.91  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:76935  AvgTotalLoss:0.08408  AvgPostnetLoss:0.00122  AvgDecoderLoss:0.00131  AvgStopLoss:0.08155  EpochTime:43.09  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.11046   PostnetLoss: 0.00445   DecoderLoss:0.00494  StopLoss: 0.10108  \n",
      "   | > TotalLoss: 0.07720   PostnetLoss: 0.00607   DecoderLoss:0.00672  StopLoss: 0.06441  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00122   Validation Loss: 0.00565\n",
      "\n",
      " > Epoch 115/1000\n",
      "   | > Step:4/68  GlobalStep:76940  TotalLoss:0.00161  PostnetLoss:0.00078  DecoderLoss:0.00083  StopLoss:0.13613  GradNorm:0.00407  GradNormST:0.05568  AvgTextLen:4.8  AvgSpecLen:31.1  StepTime:0.27  LR:0.000100\n",
      "   | > Step:14/68  GlobalStep:76950  TotalLoss:0.00202  PostnetLoss:0.00098  DecoderLoss:0.00104  StopLoss:0.09406  GradNorm:0.00427  GradNormST:0.02423  AvgTextLen:12.2  AvgSpecLen:52.0  StepTime:0.35  LR:0.000100\n",
      "   | > Step:24/68  GlobalStep:76960  TotalLoss:0.00239  PostnetLoss:0.00116  DecoderLoss:0.00123  StopLoss:0.07455  GradNorm:0.00434  GradNormST:0.01514  AvgTextLen:19.2  AvgSpecLen:79.0  StepTime:0.43  LR:0.000100\n",
      "   | > Step:34/68  GlobalStep:76970  TotalLoss:0.00261  PostnetLoss:0.00126  DecoderLoss:0.00135  StopLoss:0.06173  GradNorm:0.00493  GradNormST:0.01451  AvgTextLen:26.2  AvgSpecLen:95.9  StepTime:0.59  LR:0.000100\n",
      "   | > Step:44/68  GlobalStep:76980  TotalLoss:0.00279  PostnetLoss:0.00135  DecoderLoss:0.00144  StopLoss:0.06814  GradNorm:0.00514  GradNormST:0.02396  AvgTextLen:33.8  AvgSpecLen:116.3  StepTime:0.69  LR:0.000100\n",
      "   | > Step:54/68  GlobalStep:76990  TotalLoss:0.00296  PostnetLoss:0.00142  DecoderLoss:0.00154  StopLoss:0.06899  GradNorm:0.00484  GradNormST:0.01347  AvgTextLen:42.2  AvgSpecLen:144.9  StepTime:0.82  LR:0.000100\n",
      "   | > Step:64/68  GlobalStep:77000  TotalLoss:0.00305  PostnetLoss:0.00146  DecoderLoss:0.00159  StopLoss:0.07012  GradNorm:0.00296  GradNormST:0.01823  AvgTextLen:60.6  AvgSpecLen:177.6  StepTime:0.81  LR:0.000100\n",
      " | | > Checkpoint saving : /vm/TTS/models/cartman_models/mozilla-no-loc-fattn-stopnet-sigmoid-loss_masking-August-15-2019_11+26AM-de6aeb9/checkpoint_77000.pth.tar\n",
      "warning: audio amplitude out of range, auto clipped.\n",
      "   | > EPOCH END -- GlobalStep:77004  AvgTotalLoss:0.08129  AvgPostnetLoss:0.00121  AvgDecoderLoss:0.00130  AvgStopLoss:0.07878  EpochTime:42.86  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.11910   PostnetLoss: 0.00438   DecoderLoss:0.00482  StopLoss: 0.10990  \n",
      "   | > TotalLoss: 0.07982   PostnetLoss: 0.00614   DecoderLoss:0.00677  StopLoss: 0.06692  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00121   Validation Loss: 0.00566\n",
      "\n",
      " > Epoch 116/1000\n",
      "   | > Step:5/68  GlobalStep:77010  TotalLoss:0.00175  PostnetLoss:0.00084  DecoderLoss:0.00091  StopLoss:0.08561  GradNorm:0.00473  GradNormST:0.02190  AvgTextLen:5.9  AvgSpecLen:33.2  StepTime:0.32  LR:0.000100\n",
      "   | > Step:15/68  GlobalStep:77020  TotalLoss:0.00213  PostnetLoss:0.00103  DecoderLoss:0.00110  StopLoss:0.06620  GradNorm:0.00417  GradNormST:0.02867  AvgTextLen:12.8  AvgSpecLen:59.1  StepTime:0.56  LR:0.000100\n",
      "   | > Step:25/68  GlobalStep:77030  TotalLoss:0.00234  PostnetLoss:0.00113  DecoderLoss:0.00121  StopLoss:0.07925  GradNorm:0.00516  GradNormST:0.01784  AvgTextLen:19.8  AvgSpecLen:73.3  StepTime:0.39  LR:0.000100\n",
      "   | > Step:35/68  GlobalStep:77040  TotalLoss:0.00263  PostnetLoss:0.00127  DecoderLoss:0.00136  StopLoss:0.06919  GradNorm:0.00645  GradNormST:0.02467  AvgTextLen:27.1  AvgSpecLen:104.7  StepTime:0.75  LR:0.000100\n",
      "   | > Step:45/68  GlobalStep:77050  TotalLoss:0.00274  PostnetLoss:0.00132  DecoderLoss:0.00142  StopLoss:0.07389  GradNorm:0.00550  GradNormST:0.02624  AvgTextLen:34.4  AvgSpecLen:118.6  StepTime:0.66  LR:0.000100\n",
      "   | > Step:55/68  GlobalStep:77060  TotalLoss:0.00302  PostnetLoss:0.00145  DecoderLoss:0.00157  StopLoss:0.05768  GradNorm:0.00444  GradNormST:0.01463  AvgTextLen:43.9  AvgSpecLen:145.2  StepTime:0.80  LR:0.000100\n",
      "   | > Step:65/68  GlobalStep:77070  TotalLoss:0.00328  PostnetLoss:0.00157  DecoderLoss:0.00171  StopLoss:0.05534  GradNorm:0.00352  GradNormST:0.01583  AvgTextLen:64.2  AvgSpecLen:207.4  StepTime:1.18  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:77073  AvgTotalLoss:0.07609  AvgPostnetLoss:0.00121  AvgDecoderLoss:0.00130  AvgStopLoss:0.07358  EpochTime:42.04  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.12237   PostnetLoss: 0.00435   DecoderLoss:0.00481  StopLoss: 0.11321  \n",
      "   | > TotalLoss: 0.07520   PostnetLoss: 0.00612   DecoderLoss:0.00677  StopLoss: 0.06231  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00121   Validation Loss: 0.00566\n",
      "\n",
      " > Epoch 117/1000\n",
      "   | > Step:6/68  GlobalStep:77080  TotalLoss:0.00179  PostnetLoss:0.00086  DecoderLoss:0.00092  StopLoss:0.05966  GradNorm:0.00466  GradNormST:0.01671  AvgTextLen:6.0  AvgSpecLen:34.8  StepTime:0.35  LR:0.000100\n",
      "   | > Step:16/68  GlobalStep:77090  TotalLoss:0.00215  PostnetLoss:0.00104  DecoderLoss:0.00111  StopLoss:0.06165  GradNorm:0.00404  GradNormST:0.01671  AvgTextLen:13.8  AvgSpecLen:60.6  StepTime:0.29  LR:0.000100\n",
      "   | > Step:26/68  GlobalStep:77100  TotalLoss:0.00237  PostnetLoss:0.00115  DecoderLoss:0.00122  StopLoss:0.05540  GradNorm:0.00435  GradNormST:0.01249  AvgTextLen:20.3  AvgSpecLen:77.5  StepTime:0.38  LR:0.000100\n",
      "   | > Step:36/68  GlobalStep:77110  TotalLoss:0.00266  PostnetLoss:0.00128  DecoderLoss:0.00138  StopLoss:0.09422  GradNorm:0.00572  GradNormST:0.01863  AvgTextLen:28.5  AvgSpecLen:100.2  StepTime:0.46  LR:0.000100\n",
      "   | > Step:46/68  GlobalStep:77120  TotalLoss:0.00280  PostnetLoss:0.00135  DecoderLoss:0.00145  StopLoss:0.06209  GradNorm:0.00545  GradNormST:0.01290  AvgTextLen:35.4  AvgSpecLen:122.6  StepTime:0.78  LR:0.000100\n",
      "   | > Step:56/68  GlobalStep:77130  TotalLoss:0.00300  PostnetLoss:0.00144  DecoderLoss:0.00156  StopLoss:0.05331  GradNorm:0.00451  GradNormST:0.01541  AvgTextLen:45.8  AvgSpecLen:145.4  StepTime:1.04  LR:0.000100\n",
      "   | > Step:66/68  GlobalStep:77140  TotalLoss:0.00343  PostnetLoss:0.00164  DecoderLoss:0.00179  StopLoss:0.07351  GradNorm:0.00278  GradNormST:0.03861  AvgTextLen:71.2  AvgSpecLen:220.8  StepTime:1.04  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:77142  AvgTotalLoss:0.07404  AvgPostnetLoss:0.00121  AvgDecoderLoss:0.00130  AvgStopLoss:0.07152  EpochTime:42.34  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.11817   PostnetLoss: 0.00436   DecoderLoss:0.00482  StopLoss: 0.10900  \n",
      "   | > TotalLoss: 0.07602   PostnetLoss: 0.00623   DecoderLoss:0.00689  StopLoss: 0.06290  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00121   Validation Loss: 0.00570\n",
      "\n",
      " > Epoch 118/1000\n",
      "   | > Step:7/68  GlobalStep:77150  TotalLoss:0.00181  PostnetLoss:0.00088  DecoderLoss:0.00093  StopLoss:0.05348  GradNorm:0.00401  GradNormST:0.01659  AvgTextLen:7.4  AvgSpecLen:44.6  StepTime:0.30  LR:0.000100\n",
      "   | > Step:17/68  GlobalStep:77160  TotalLoss:0.00213  PostnetLoss:0.00103  DecoderLoss:0.00110  StopLoss:0.09532  GradNorm:0.00368  GradNormST:0.02071  AvgTextLen:14.7  AvgSpecLen:66.0  StepTime:0.43  LR:0.000100\n",
      "   | > Step:27/68  GlobalStep:77170  TotalLoss:0.00242  PostnetLoss:0.00117  DecoderLoss:0.00125  StopLoss:0.06737  GradNorm:0.00452  GradNormST:0.02591  AvgTextLen:21.0  AvgSpecLen:80.2  StepTime:0.62  LR:0.000100\n",
      "   | > Step:37/68  GlobalStep:77180  TotalLoss:0.00266  PostnetLoss:0.00129  DecoderLoss:0.00137  StopLoss:0.06527  GradNorm:0.00534  GradNormST:0.01361  AvgTextLen:29.3  AvgSpecLen:102.4  StepTime:0.61  LR:0.000100\n",
      "   | > Step:47/68  GlobalStep:77190  TotalLoss:0.00285  PostnetLoss:0.00137  DecoderLoss:0.00148  StopLoss:0.07849  GradNorm:0.00478  GradNormST:0.01931  AvgTextLen:36.7  AvgSpecLen:126.9  StepTime:0.74  LR:0.000100\n",
      "   | > Step:57/68  GlobalStep:77200  TotalLoss:0.00307  PostnetLoss:0.00148  DecoderLoss:0.00159  StopLoss:0.05858  GradNorm:0.00443  GradNormST:0.01224  AvgTextLen:47.1  AvgSpecLen:161.0  StepTime:0.81  LR:0.000100\n",
      "   | > Step:67/68  GlobalStep:77210  TotalLoss:0.00344  PostnetLoss:0.00164  DecoderLoss:0.00180  StopLoss:0.06973  GradNorm:0.00311  GradNormST:0.03538  AvgTextLen:80.6  AvgSpecLen:246.5  StepTime:1.21  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:77211  AvgTotalLoss:0.07954  AvgPostnetLoss:0.00120  AvgDecoderLoss:0.00129  AvgStopLoss:0.07705  EpochTime:42.62  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.12174   PostnetLoss: 0.00436   DecoderLoss:0.00484  StopLoss: 0.11254  \n",
      "   | > TotalLoss: 0.07819   PostnetLoss: 0.00633   DecoderLoss:0.00699  StopLoss: 0.06487  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00120   Validation Loss: 0.00577\n",
      "\n",
      " > Epoch 119/1000\n",
      "   | > Step:8/68  GlobalStep:77220  TotalLoss:0.00191  PostnetLoss:0.00092  DecoderLoss:0.00099  StopLoss:0.08296  GradNorm:0.00435  GradNormST:0.01610  AvgTextLen:8.3  AvgSpecLen:44.0  StepTime:0.31  LR:0.000100\n",
      "   | > Step:18/68  GlobalStep:77230  TotalLoss:0.00213  PostnetLoss:0.00103  DecoderLoss:0.00110  StopLoss:0.08184  GradNorm:0.00371  GradNormST:0.01438  AvgTextLen:15.2  AvgSpecLen:64.5  StepTime:0.42  LR:0.000100\n",
      "   | > Step:28/68  GlobalStep:77240  TotalLoss:0.00236  PostnetLoss:0.00114  DecoderLoss:0.00122  StopLoss:0.09815  GradNorm:0.00340  GradNormST:0.02499  AvgTextLen:22.1  AvgSpecLen:85.4  StepTime:0.54  LR:0.000100\n",
      "   | > Step:38/68  GlobalStep:77250  TotalLoss:0.00260  PostnetLoss:0.00126  DecoderLoss:0.00134  StopLoss:0.06271  GradNorm:0.00449  GradNormST:0.01886  AvgTextLen:29.4  AvgSpecLen:104.1  StepTime:0.50  LR:0.000100\n",
      "   | > Step:48/68  GlobalStep:77260  TotalLoss:0.00286  PostnetLoss:0.00138  DecoderLoss:0.00148  StopLoss:0.06781  GradNorm:0.00471  GradNormST:0.01683  AvgTextLen:37.5  AvgSpecLen:123.8  StepTime:0.67  LR:0.000100\n",
      "   | > Step:58/68  GlobalStep:77270  TotalLoss:0.00304  PostnetLoss:0.00146  DecoderLoss:0.00158  StopLoss:0.06178  GradNorm:0.00422  GradNormST:0.01668  AvgTextLen:47.8  AvgSpecLen:158.0  StepTime:0.85  LR:0.000100\n",
      "   | > Step:68/68  GlobalStep:77280  TotalLoss:0.00333  PostnetLoss:0.00159  DecoderLoss:0.00174  StopLoss:0.07102  GradNorm:0.00286  GradNormST:0.04577  AvgTextLen:103.0  AvgSpecLen:308.9  StepTime:1.97  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:77280  AvgTotalLoss:0.07345  AvgPostnetLoss:0.00120  AvgDecoderLoss:0.00129  AvgStopLoss:0.07096  EpochTime:42.38  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.11594   PostnetLoss: 0.00445   DecoderLoss:0.00493  StopLoss: 0.10656  \n",
      "   | > TotalLoss: 0.07795   PostnetLoss: 0.00657   DecoderLoss:0.00725  StopLoss: 0.06413  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00120   Validation Loss: 0.00599\n",
      "\n",
      " > Epoch 120/1000\n",
      "   | > Step:9/68  GlobalStep:77290  TotalLoss:0.00178  PostnetLoss:0.00087  DecoderLoss:0.00092  StopLoss:0.05422  GradNorm:0.00361  GradNormST:0.01647  AvgTextLen:9.0  AvgSpecLen:42.1  StepTime:0.45  LR:0.000100\n",
      "   | > Step:19/68  GlobalStep:77300  TotalLoss:0.00207  PostnetLoss:0.00100  DecoderLoss:0.00107  StopLoss:0.08245  GradNorm:0.00348  GradNormST:0.02989  AvgTextLen:15.5  AvgSpecLen:63.2  StepTime:0.43  LR:0.000100\n",
      "   | > Step:29/68  GlobalStep:77310  TotalLoss:0.00232  PostnetLoss:0.00112  DecoderLoss:0.00120  StopLoss:0.08914  GradNorm:0.00309  GradNormST:0.01812  AvgTextLen:21.8  AvgSpecLen:83.4  StepTime:0.54  LR:0.000100\n",
      "   | > Step:39/68  GlobalStep:77320  TotalLoss:0.00266  PostnetLoss:0.00128  DecoderLoss:0.00138  StopLoss:0.05804  GradNorm:0.00377  GradNormST:0.01569  AvgTextLen:30.2  AvgSpecLen:110.4  StepTime:0.61  LR:0.000100\n",
      "   | > Step:49/68  GlobalStep:77330  TotalLoss:0.00286  PostnetLoss:0.00138  DecoderLoss:0.00148  StopLoss:0.06117  GradNorm:0.00454  GradNormST:0.02239  AvgTextLen:38.2  AvgSpecLen:127.7  StepTime:0.77  LR:0.000100\n",
      "   | > Step:59/68  GlobalStep:77340  TotalLoss:0.00301  PostnetLoss:0.00145  DecoderLoss:0.00157  StopLoss:0.05453  GradNorm:0.00380  GradNormST:0.01145  AvgTextLen:50.5  AvgSpecLen:155.9  StepTime:0.89  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:77349  AvgTotalLoss:0.07017  AvgPostnetLoss:0.00120  AvgDecoderLoss:0.00129  AvgStopLoss:0.06769  EpochTime:41.38  AvgStepTime:0.60\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.12530   PostnetLoss: 0.00446   DecoderLoss:0.00493  StopLoss: 0.11591  \n",
      "   | > TotalLoss: 0.07918   PostnetLoss: 0.00671   DecoderLoss:0.00740  StopLoss: 0.06508  \n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00120   Validation Loss: 0.00605\n",
      "\n",
      " > Epoch 121/1000\n",
      "   | > Step:0/68  GlobalStep:77350  TotalLoss:0.00174  PostnetLoss:0.00083  DecoderLoss:0.00091  StopLoss:0.12628  GradNorm:0.00644  GradNormST:0.04680  AvgTextLen:3.0  AvgSpecLen:25.3  StepTime:0.45  LR:0.000100\n",
      "   | > Step:10/68  GlobalStep:77360  TotalLoss:0.00190  PostnetLoss:0.00092  DecoderLoss:0.00098  StopLoss:0.06780  GradNorm:0.00382  GradNormST:0.02259  AvgTextLen:9.8  AvgSpecLen:48.6  StepTime:0.69  LR:0.000100\n",
      "   | > Step:20/68  GlobalStep:77370  TotalLoss:0.00221  PostnetLoss:0.00107  DecoderLoss:0.00114  StopLoss:0.05695  GradNorm:0.00343  GradNormST:0.01279  AvgTextLen:16.4  AvgSpecLen:64.8  StepTime:0.53  LR:0.000100\n",
      "   | > Step:30/68  GlobalStep:77380  TotalLoss:0.00241  PostnetLoss:0.00116  DecoderLoss:0.00125  StopLoss:0.05856  GradNorm:0.00343  GradNormST:0.01200  AvgTextLen:22.9  AvgSpecLen:85.8  StepTime:0.59  LR:0.000100\n",
      "   | > Step:40/68  GlobalStep:77390  TotalLoss:0.00263  PostnetLoss:0.00127  DecoderLoss:0.00136  StopLoss:0.05479  GradNorm:0.00387  GradNormST:0.01482  AvgTextLen:30.9  AvgSpecLen:105.3  StepTime:0.57  LR:0.000100\n",
      "   | > Step:50/68  GlobalStep:77400  TotalLoss:0.00286  PostnetLoss:0.00137  DecoderLoss:0.00149  StopLoss:0.05498  GradNorm:0.00409  GradNormST:0.01579  AvgTextLen:38.8  AvgSpecLen:132.0  StepTime:0.80  LR:0.000100\n",
      "   | > Step:60/68  GlobalStep:77410  TotalLoss:0.00312  PostnetLoss:0.00150  DecoderLoss:0.00163  StopLoss:0.04842  GradNorm:0.00386  GradNormST:0.01230  AvgTextLen:52.0  AvgSpecLen:172.7  StepTime:0.91  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:77418  AvgTotalLoss:0.07028  AvgPostnetLoss:0.00120  AvgDecoderLoss:0.00129  AvgStopLoss:0.06780  EpochTime:42.86  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.11589   PostnetLoss: 0.00449   DecoderLoss:0.00499  StopLoss: 0.10640  \n",
      "   | > TotalLoss: 0.07659   PostnetLoss: 0.00671   DecoderLoss:0.00743  StopLoss: 0.06245  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00120   Validation Loss: 0.00605\n",
      "\n",
      " > Epoch 122/1000\n",
      "   | > Step:1/68  GlobalStep:77420  TotalLoss:0.00170  PostnetLoss:0.00082  DecoderLoss:0.00087  StopLoss:0.07243  GradNorm:0.00459  GradNormST:0.02815  AvgTextLen:3.7  AvgSpecLen:28.6  StepTime:0.36  LR:0.000100\n",
      "   | > Step:11/68  GlobalStep:77430  TotalLoss:0.00191  PostnetLoss:0.00093  DecoderLoss:0.00098  StopLoss:0.06509  GradNorm:0.00364  GradNormST:0.01837  AvgTextLen:10.3  AvgSpecLen:49.4  StepTime:0.40  LR:0.000100\n",
      "   | > Step:21/68  GlobalStep:77440  TotalLoss:0.00220  PostnetLoss:0.00106  DecoderLoss:0.00113  StopLoss:0.08200  GradNorm:0.00353  GradNormST:0.03222  AvgTextLen:16.6  AvgSpecLen:66.3  StepTime:0.44  LR:0.000100\n",
      "   | > Step:31/68  GlobalStep:77450  TotalLoss:0.00240  PostnetLoss:0.00116  DecoderLoss:0.00124  StopLoss:0.09064  GradNorm:0.00336  GradNormST:0.03543  AvgTextLen:24.1  AvgSpecLen:82.7  StepTime:0.53  LR:0.000100\n",
      "   | > Step:41/68  GlobalStep:77460  TotalLoss:0.00266  PostnetLoss:0.00129  DecoderLoss:0.00137  StopLoss:0.04420  GradNorm:0.00328  GradNormST:0.00916  AvgTextLen:32.3  AvgSpecLen:109.4  StepTime:0.56  LR:0.000100\n",
      "   | > Step:51/68  GlobalStep:77470  TotalLoss:0.00279  PostnetLoss:0.00134  DecoderLoss:0.00145  StopLoss:0.05234  GradNorm:0.00350  GradNormST:0.01311  AvgTextLen:39.2  AvgSpecLen:126.9  StepTime:0.79  LR:0.000100\n",
      "   | > Step:61/68  GlobalStep:77480  TotalLoss:0.00307  PostnetLoss:0.00147  DecoderLoss:0.00160  StopLoss:0.05853  GradNorm:0.00330  GradNormST:0.02194  AvgTextLen:54.6  AvgSpecLen:165.5  StepTime:0.91  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:77487  AvgTotalLoss:0.07113  AvgPostnetLoss:0.00120  AvgDecoderLoss:0.00129  AvgStopLoss:0.06865  EpochTime:41.81  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.12000   PostnetLoss: 0.00452   DecoderLoss:0.00499  StopLoss: 0.11049  \n",
      "   | > TotalLoss: 0.08014   PostnetLoss: 0.00687   DecoderLoss:0.00757  StopLoss: 0.06570  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00120   Validation Loss: 0.00618\n",
      "\n",
      " > Epoch 123/1000\n",
      "   | > Step:2/68  GlobalStep:77490  TotalLoss:0.00157  PostnetLoss:0.00076  DecoderLoss:0.00081  StopLoss:0.11389  GradNorm:0.00392  GradNormST:0.03571  AvgTextLen:4.0  AvgSpecLen:31.3  StepTime:0.49  LR:0.000100\n",
      "   | > Step:12/68  GlobalStep:77500  TotalLoss:0.00191  PostnetLoss:0.00093  DecoderLoss:0.00098  StopLoss:0.06266  GradNorm:0.00367  GradNormST:0.01276  AvgTextLen:11.0  AvgSpecLen:49.3  StepTime:0.43  LR:0.000100\n",
      "   | > Step:22/68  GlobalStep:77510  TotalLoss:0.00222  PostnetLoss:0.00108  DecoderLoss:0.00114  StopLoss:0.07783  GradNorm:0.00349  GradNormST:0.01521  AvgTextLen:18.0  AvgSpecLen:72.0  StepTime:0.46  LR:0.000100\n",
      "   | > Step:32/68  GlobalStep:77520  TotalLoss:0.00246  PostnetLoss:0.00119  DecoderLoss:0.00127  StopLoss:0.07227  GradNorm:0.00319  GradNormST:0.01946  AvgTextLen:24.4  AvgSpecLen:91.5  StepTime:0.46  LR:0.000100\n",
      "   | > Step:42/68  GlobalStep:77530  TotalLoss:0.00266  PostnetLoss:0.00128  DecoderLoss:0.00138  StopLoss:0.06685  GradNorm:0.00306  GradNormST:0.02260  AvgTextLen:33.1  AvgSpecLen:109.7  StepTime:0.60  LR:0.000100\n",
      "   | > Step:52/68  GlobalStep:77540  TotalLoss:0.00290  PostnetLoss:0.00139  DecoderLoss:0.00150  StopLoss:0.06417  GradNorm:0.00344  GradNormST:0.01909  AvgTextLen:42.1  AvgSpecLen:137.5  StepTime:0.77  LR:0.000100\n",
      "   | > Step:62/68  GlobalStep:77550  TotalLoss:0.00309  PostnetLoss:0.00148  DecoderLoss:0.00161  StopLoss:0.05716  GradNorm:0.00328  GradNormST:0.02310  AvgTextLen:54.0  AvgSpecLen:174.7  StepTime:0.85  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:77556  AvgTotalLoss:0.06656  AvgPostnetLoss:0.00119  AvgDecoderLoss:0.00128  AvgStopLoss:0.06409  EpochTime:42.17  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.11222   PostnetLoss: 0.00455   DecoderLoss:0.00503  StopLoss: 0.10264  \n",
      "   | > TotalLoss: 0.07724   PostnetLoss: 0.00688   DecoderLoss:0.00758  StopLoss: 0.06279  \n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00119   Validation Loss: 0.00617\n",
      "\n",
      " > Epoch 124/1000\n",
      "   | > Step:3/68  GlobalStep:77560  TotalLoss:0.00164  PostnetLoss:0.00079  DecoderLoss:0.00084  StopLoss:0.07793  GradNorm:0.00399  GradNormST:0.02398  AvgTextLen:4.7  AvgSpecLen:29.7  StepTime:0.32  LR:0.000100\n",
      "   | > Step:13/68  GlobalStep:77570  TotalLoss:0.00195  PostnetLoss:0.00094  DecoderLoss:0.00100  StopLoss:0.09170  GradNorm:0.00380  GradNormST:0.03160  AvgTextLen:11.5  AvgSpecLen:55.2  StepTime:0.43  LR:0.000100\n",
      "   | > Step:23/68  GlobalStep:77580  TotalLoss:0.00229  PostnetLoss:0.00111  DecoderLoss:0.00119  StopLoss:0.06619  GradNorm:0.00349  GradNormST:0.01490  AvgTextLen:18.3  AvgSpecLen:72.2  StepTime:0.42  LR:0.000100\n",
      "   | > Step:33/68  GlobalStep:77590  TotalLoss:0.00249  PostnetLoss:0.00121  DecoderLoss:0.00129  StopLoss:0.05712  GradNorm:0.00377  GradNormST:0.01258  AvgTextLen:25.9  AvgSpecLen:88.9  StepTime:0.54  LR:0.000100\n",
      "   | > Step:43/68  GlobalStep:77600  TotalLoss:0.00265  PostnetLoss:0.00128  DecoderLoss:0.00137  StopLoss:0.06561  GradNorm:0.00318  GradNormST:0.01699  AvgTextLen:33.4  AvgSpecLen:115.7  StepTime:0.61  LR:0.000100\n",
      "   | > Step:53/68  GlobalStep:77610  TotalLoss:0.00282  PostnetLoss:0.00135  DecoderLoss:0.00147  StopLoss:0.05922  GradNorm:0.00320  GradNormST:0.01797  AvgTextLen:42.4  AvgSpecLen:137.6  StepTime:0.81  LR:0.000100\n",
      "   | > Step:63/68  GlobalStep:77620  TotalLoss:0.00319  PostnetLoss:0.00152  DecoderLoss:0.00166  StopLoss:0.03767  GradNorm:0.00308  GradNormST:0.00676  AvgTextLen:58.6  AvgSpecLen:185.1  StepTime:0.91  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:77625  AvgTotalLoss:0.06760  AvgPostnetLoss:0.00120  AvgDecoderLoss:0.00129  AvgStopLoss:0.06511  EpochTime:43.02  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.11754   PostnetLoss: 0.00457   DecoderLoss:0.00506  StopLoss: 0.10791  \n",
      "   | > TotalLoss: 0.07676   PostnetLoss: 0.00693   DecoderLoss:0.00764  StopLoss: 0.06220  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00120   Validation Loss: 0.00621\n",
      "\n",
      " > Epoch 125/1000\n",
      "   | > Step:4/68  GlobalStep:77630  TotalLoss:0.00166  PostnetLoss:0.00080  DecoderLoss:0.00086  StopLoss:0.12817  GradNorm:0.00437  GradNormST:0.04809  AvgTextLen:4.8  AvgSpecLen:31.1  StepTime:0.20  LR:0.000100\n",
      "   | > Step:14/68  GlobalStep:77640  TotalLoss:0.00201  PostnetLoss:0.00097  DecoderLoss:0.00103  StopLoss:0.08247  GradNorm:0.00368  GradNormST:0.02066  AvgTextLen:12.2  AvgSpecLen:52.0  StepTime:0.32  LR:0.000100\n",
      "   | > Step:24/68  GlobalStep:77650  TotalLoss:0.00238  PostnetLoss:0.00115  DecoderLoss:0.00123  StopLoss:0.06339  GradNorm:0.00356  GradNormST:0.01674  AvgTextLen:19.2  AvgSpecLen:79.0  StepTime:0.39  LR:0.000100\n",
      "   | > Step:34/68  GlobalStep:77660  TotalLoss:0.00254  PostnetLoss:0.00123  DecoderLoss:0.00132  StopLoss:0.05608  GradNorm:0.00335  GradNormST:0.00891  AvgTextLen:26.2  AvgSpecLen:95.9  StepTime:0.48  LR:0.000100\n",
      "   | > Step:44/68  GlobalStep:77670  TotalLoss:0.00269  PostnetLoss:0.00130  DecoderLoss:0.00140  StopLoss:0.04698  GradNorm:0.00335  GradNormST:0.01297  AvgTextLen:33.8  AvgSpecLen:116.3  StepTime:0.58  LR:0.000100\n",
      "   | > Step:54/68  GlobalStep:77680  TotalLoss:0.00293  PostnetLoss:0.00141  DecoderLoss:0.00153  StopLoss:0.05664  GradNorm:0.00317  GradNormST:0.01382  AvgTextLen:42.2  AvgSpecLen:144.9  StepTime:0.82  LR:0.000100\n",
      "   | > Step:64/68  GlobalStep:77690  TotalLoss:0.00302  PostnetLoss:0.00144  DecoderLoss:0.00157  StopLoss:0.05941  GradNorm:0.00296  GradNormST:0.02281  AvgTextLen:60.6  AvgSpecLen:177.6  StepTime:0.80  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:77694  AvgTotalLoss:0.06700  AvgPostnetLoss:0.00119  AvgDecoderLoss:0.00128  AvgStopLoss:0.06452  EpochTime:42.39  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.10974   PostnetLoss: 0.00457   DecoderLoss:0.00506  StopLoss: 0.10011  \n",
      "   | > TotalLoss: 0.08177   PostnetLoss: 0.00698   DecoderLoss:0.00770  StopLoss: 0.06709  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00119   Validation Loss: 0.00626\n",
      "\n",
      " > Epoch 126/1000\n",
      "   | > Step:5/68  GlobalStep:77700  TotalLoss:0.00172  PostnetLoss:0.00083  DecoderLoss:0.00089  StopLoss:0.06878  GradNorm:0.00377  GradNormST:0.01612  AvgTextLen:5.9  AvgSpecLen:33.2  StepTime:0.30  LR:0.000100\n",
      "   | > Step:15/68  GlobalStep:77710  TotalLoss:0.00207  PostnetLoss:0.00100  DecoderLoss:0.00107  StopLoss:0.05798  GradNorm:0.00382  GradNormST:0.01676  AvgTextLen:12.8  AvgSpecLen:59.1  StepTime:0.52  LR:0.000100\n",
      "   | > Step:25/68  GlobalStep:77720  TotalLoss:0.00229  PostnetLoss:0.00111  DecoderLoss:0.00118  StopLoss:0.05781  GradNorm:0.00353  GradNormST:0.01825  AvgTextLen:19.8  AvgSpecLen:73.3  StepTime:0.38  LR:0.000100\n",
      "   | > Step:35/68  GlobalStep:77730  TotalLoss:0.00257  PostnetLoss:0.00124  DecoderLoss:0.00133  StopLoss:0.06697  GradNorm:0.00347  GradNormST:0.02140  AvgTextLen:27.1  AvgSpecLen:104.7  StepTime:0.75  LR:0.000100\n",
      "   | > Step:45/68  GlobalStep:77740  TotalLoss:0.00264  PostnetLoss:0.00127  DecoderLoss:0.00137  StopLoss:0.06574  GradNorm:0.00326  GradNormST:0.02273  AvgTextLen:34.4  AvgSpecLen:118.6  StepTime:0.65  LR:0.000100\n",
      "   | > Step:55/68  GlobalStep:77750  TotalLoss:0.00297  PostnetLoss:0.00143  DecoderLoss:0.00155  StopLoss:0.04666  GradNorm:0.00322  GradNormST:0.00874  AvgTextLen:43.9  AvgSpecLen:145.2  StepTime:0.80  LR:0.000100\n",
      "   | > Step:65/68  GlobalStep:77760  TotalLoss:0.00326  PostnetLoss:0.00156  DecoderLoss:0.00170  StopLoss:0.04983  GradNorm:0.00347  GradNormST:0.01439  AvgTextLen:64.2  AvgSpecLen:207.4  StepTime:1.18  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:77763  AvgTotalLoss:0.06474  AvgPostnetLoss:0.00119  AvgDecoderLoss:0.00128  AvgStopLoss:0.06227  EpochTime:43.48  AvgStepTime:0.63\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.11359   PostnetLoss: 0.00454   DecoderLoss:0.00500  StopLoss: 0.10404  \n",
      "   | > TotalLoss: 0.07792   PostnetLoss: 0.00697   DecoderLoss:0.00768  StopLoss: 0.06326  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00119   Validation Loss: 0.00616\n",
      "\n",
      " > Epoch 127/1000\n",
      "   | > Step:6/68  GlobalStep:77770  TotalLoss:0.00172  PostnetLoss:0.00083  DecoderLoss:0.00088  StopLoss:0.06023  GradNorm:0.00367  GradNormST:0.02769  AvgTextLen:6.0  AvgSpecLen:34.8  StepTime:0.42  LR:0.000100\n",
      "   | > Step:16/68  GlobalStep:77780  TotalLoss:0.00206  PostnetLoss:0.00100  DecoderLoss:0.00107  StopLoss:0.05586  GradNorm:0.00376  GradNormST:0.01241  AvgTextLen:13.8  AvgSpecLen:60.6  StepTime:0.39  LR:0.000100\n",
      "   | > Step:26/68  GlobalStep:77790  TotalLoss:0.00232  PostnetLoss:0.00112  DecoderLoss:0.00120  StopLoss:0.05691  GradNorm:0.00348  GradNormST:0.01234  AvgTextLen:20.3  AvgSpecLen:77.5  StepTime:0.51  LR:0.000100\n",
      "   | > Step:36/68  GlobalStep:77800  TotalLoss:0.00253  PostnetLoss:0.00122  DecoderLoss:0.00131  StopLoss:0.06549  GradNorm:0.00344  GradNormST:0.01923  AvgTextLen:28.5  AvgSpecLen:100.2  StepTime:0.57  LR:0.000100\n",
      "   | > Step:46/68  GlobalStep:77810  TotalLoss:0.00268  PostnetLoss:0.00129  DecoderLoss:0.00139  StopLoss:0.05464  GradNorm:0.00325  GradNormST:0.00966  AvgTextLen:35.4  AvgSpecLen:122.6  StepTime:0.82  LR:0.000100\n",
      "   | > Step:56/68  GlobalStep:77820  TotalLoss:0.00294  PostnetLoss:0.00141  DecoderLoss:0.00153  StopLoss:0.04138  GradNorm:0.00331  GradNormST:0.01338  AvgTextLen:45.8  AvgSpecLen:145.4  StepTime:0.81  LR:0.000100\n",
      "   | > Step:66/68  GlobalStep:77830  TotalLoss:0.00343  PostnetLoss:0.00164  DecoderLoss:0.00179  StopLoss:0.04384  GradNorm:0.00394  GradNormST:0.01839  AvgTextLen:71.2  AvgSpecLen:220.8  StepTime:1.07  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:77832  AvgTotalLoss:0.06550  AvgPostnetLoss:0.00119  AvgDecoderLoss:0.00128  AvgStopLoss:0.06303  EpochTime:41.62  AvgStepTime:0.60\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.11024   PostnetLoss: 0.00451   DecoderLoss:0.00497  StopLoss: 0.10076  \n",
      "   | > TotalLoss: 0.08192   PostnetLoss: 0.00713   DecoderLoss:0.00781  StopLoss: 0.06699  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00119   Validation Loss: 0.00634\n",
      "\n",
      " > Epoch 128/1000\n",
      "   | > Step:7/68  GlobalStep:77840  TotalLoss:0.00184  PostnetLoss:0.00089  DecoderLoss:0.00094  StopLoss:0.06996  GradNorm:0.00385  GradNormST:0.02593  AvgTextLen:7.4  AvgSpecLen:44.6  StepTime:0.33  LR:0.000100\n",
      "   | > Step:17/68  GlobalStep:77850  TotalLoss:0.00204  PostnetLoss:0.00099  DecoderLoss:0.00105  StopLoss:0.06937  GradNorm:0.00339  GradNormST:0.01316  AvgTextLen:14.7  AvgSpecLen:66.0  StepTime:0.43  LR:0.000100\n",
      "   | > Step:27/68  GlobalStep:77860  TotalLoss:0.00237  PostnetLoss:0.00115  DecoderLoss:0.00123  StopLoss:0.07563  GradNorm:0.00349  GradNormST:0.02430  AvgTextLen:21.0  AvgSpecLen:80.2  StepTime:0.65  LR:0.000100\n",
      "   | > Step:37/68  GlobalStep:77870  TotalLoss:0.00260  PostnetLoss:0.00125  DecoderLoss:0.00134  StopLoss:0.05859  GradNorm:0.00359  GradNormST:0.01067  AvgTextLen:29.3  AvgSpecLen:102.4  StepTime:0.60  LR:0.000100\n",
      "   | > Step:47/68  GlobalStep:77880  TotalLoss:0.00279  PostnetLoss:0.00134  DecoderLoss:0.00145  StopLoss:0.05643  GradNorm:0.00427  GradNormST:0.01618  AvgTextLen:36.7  AvgSpecLen:126.9  StepTime:0.74  LR:0.000100\n",
      "   | > Step:57/68  GlobalStep:77890  TotalLoss:0.00304  PostnetLoss:0.00146  DecoderLoss:0.00158  StopLoss:0.04614  GradNorm:0.00379  GradNormST:0.00845  AvgTextLen:47.1  AvgSpecLen:161.0  StepTime:0.85  LR:0.000100\n",
      "   | > Step:67/68  GlobalStep:77900  TotalLoss:0.00340  PostnetLoss:0.00163  DecoderLoss:0.00178  StopLoss:0.05409  GradNorm:0.00436  GradNormST:0.02587  AvgTextLen:80.6  AvgSpecLen:246.5  StepTime:1.22  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:77901  AvgTotalLoss:0.06688  AvgPostnetLoss:0.00120  AvgDecoderLoss:0.00128  AvgStopLoss:0.06440  EpochTime:42.71  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.11072   PostnetLoss: 0.00455   DecoderLoss:0.00502  StopLoss: 0.10115  \n",
      "   | > TotalLoss: 0.07794   PostnetLoss: 0.00708   DecoderLoss:0.00779  StopLoss: 0.06307  \n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00120   Validation Loss: 0.00634\n",
      "\n",
      " > Epoch 129/1000\n",
      "   | > Step:8/68  GlobalStep:77910  TotalLoss:0.00184  PostnetLoss:0.00090  DecoderLoss:0.00095  StopLoss:0.08819  GradNorm:0.00421  GradNormST:0.02180  AvgTextLen:8.3  AvgSpecLen:44.0  StepTime:0.31  LR:0.000100\n",
      "   | > Step:18/68  GlobalStep:77920  TotalLoss:0.00205  PostnetLoss:0.00099  DecoderLoss:0.00106  StopLoss:0.07527  GradNorm:0.00330  GradNormST:0.01629  AvgTextLen:15.2  AvgSpecLen:64.5  StepTime:0.41  LR:0.000100\n",
      "   | > Step:28/68  GlobalStep:77930  TotalLoss:0.00234  PostnetLoss:0.00114  DecoderLoss:0.00121  StopLoss:0.08899  GradNorm:0.00341  GradNormST:0.01951  AvgTextLen:22.1  AvgSpecLen:85.4  StepTime:0.48  LR:0.000100\n",
      "   | > Step:38/68  GlobalStep:77940  TotalLoss:0.00255  PostnetLoss:0.00123  DecoderLoss:0.00132  StopLoss:0.05406  GradNorm:0.00407  GradNormST:0.01081  AvgTextLen:29.4  AvgSpecLen:104.1  StepTime:0.61  LR:0.000100\n",
      "   | > Step:48/68  GlobalStep:77950  TotalLoss:0.00282  PostnetLoss:0.00136  DecoderLoss:0.00146  StopLoss:0.05756  GradNorm:0.00412  GradNormST:0.01202  AvgTextLen:37.5  AvgSpecLen:123.8  StepTime:0.78  LR:0.000100\n",
      "   | > Step:58/68  GlobalStep:77960  TotalLoss:0.00308  PostnetLoss:0.00148  DecoderLoss:0.00160  StopLoss:0.06907  GradNorm:0.00567  GradNormST:0.01816  AvgTextLen:47.8  AvgSpecLen:158.0  StepTime:0.85  LR:0.000100\n",
      "   | > Step:68/68  GlobalStep:77970  TotalLoss:0.00334  PostnetLoss:0.00160  DecoderLoss:0.00174  StopLoss:0.05635  GradNorm:0.00332  GradNormST:0.03015  AvgTextLen:103.0  AvgSpecLen:308.9  StepTime:1.87  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:77970  AvgTotalLoss:0.06680  AvgPostnetLoss:0.00120  AvgDecoderLoss:0.00129  AvgStopLoss:0.06432  EpochTime:41.94  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.11475   PostnetLoss: 0.00452   DecoderLoss:0.00498  StopLoss: 0.10525  \n",
      "   | > TotalLoss: 0.08255   PostnetLoss: 0.00718   DecoderLoss:0.00789  StopLoss: 0.06748  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00120   Validation Loss: 0.00641\n",
      "\n",
      " > Epoch 130/1000\n",
      "   | > Step:9/68  GlobalStep:77980  TotalLoss:0.00184  PostnetLoss:0.00089  DecoderLoss:0.00095  StopLoss:0.05126  GradNorm:0.00467  GradNormST:0.02030  AvgTextLen:9.0  AvgSpecLen:42.1  StepTime:0.33  LR:0.000100\n",
      "   | > Step:19/68  GlobalStep:77990  TotalLoss:0.00206  PostnetLoss:0.00100  DecoderLoss:0.00106  StopLoss:0.09275  GradNorm:0.00319  GradNormST:0.02356  AvgTextLen:15.5  AvgSpecLen:63.2  StepTime:0.43  LR:0.000100\n",
      "   | > Step:29/68  GlobalStep:78000  TotalLoss:0.00235  PostnetLoss:0.00114  DecoderLoss:0.00121  StopLoss:0.07866  GradNorm:0.00327  GradNormST:0.01361  AvgTextLen:21.8  AvgSpecLen:83.4  StepTime:0.56  LR:0.000100\n",
      " | | > Checkpoint saving : /vm/TTS/models/cartman_models/mozilla-no-loc-fattn-stopnet-sigmoid-loss_masking-August-15-2019_11+26AM-de6aeb9/checkpoint_78000.pth.tar\n",
      "warning: audio amplitude out of range, auto clipped.\n",
      "   | > Step:39/68  GlobalStep:78010  TotalLoss:0.00266  PostnetLoss:0.00128  DecoderLoss:0.00138  StopLoss:0.04846  GradNorm:0.00423  GradNormST:0.00914  AvgTextLen:30.2  AvgSpecLen:110.4  StepTime:0.65  LR:0.000100\n",
      "   | > Step:49/68  GlobalStep:78020  TotalLoss:0.00285  PostnetLoss:0.00137  DecoderLoss:0.00148  StopLoss:0.05649  GradNorm:0.00452  GradNormST:0.01174  AvgTextLen:38.2  AvgSpecLen:127.7  StepTime:0.77  LR:0.000100\n",
      "   | > Step:59/68  GlobalStep:78030  TotalLoss:0.00307  PostnetLoss:0.00148  DecoderLoss:0.00160  StopLoss:0.05206  GradNorm:0.00531  GradNormST:0.01088  AvgTextLen:50.5  AvgSpecLen:155.9  StepTime:0.92  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:78039  AvgTotalLoss:0.06587  AvgPostnetLoss:0.00120  AvgDecoderLoss:0.00128  AvgStopLoss:0.06339  EpochTime:41.63  AvgStepTime:0.60\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.11073   PostnetLoss: 0.00457   DecoderLoss:0.00504  StopLoss: 0.10112  \n",
      "   | > TotalLoss: 0.08173   PostnetLoss: 0.00706   DecoderLoss:0.00776  StopLoss: 0.06692  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00120   Validation Loss: 0.00630\n",
      "\n",
      " > Epoch 131/1000\n",
      "   | > Step:0/68  GlobalStep:78040  TotalLoss:0.00173  PostnetLoss:0.00083  DecoderLoss:0.00090  StopLoss:0.08975  GradNorm:0.00678  GradNormST:0.02821  AvgTextLen:3.0  AvgSpecLen:25.3  StepTime:0.48  LR:0.000100\n",
      "   | > Step:10/68  GlobalStep:78050  TotalLoss:0.00188  PostnetLoss:0.00091  DecoderLoss:0.00097  StopLoss:0.07763  GradNorm:0.00383  GradNormST:0.03532  AvgTextLen:9.8  AvgSpecLen:48.6  StepTime:0.43  LR:0.000100\n",
      "   | > Step:20/68  GlobalStep:78060  TotalLoss:0.00219  PostnetLoss:0.00106  DecoderLoss:0.00113  StopLoss:0.06736  GradNorm:0.00353  GradNormST:0.03045  AvgTextLen:16.4  AvgSpecLen:64.8  StepTime:0.48  LR:0.000100\n",
      "   | > Step:30/68  GlobalStep:78070  TotalLoss:0.00239  PostnetLoss:0.00116  DecoderLoss:0.00123  StopLoss:0.06555  GradNorm:0.00343  GradNormST:0.01164  AvgTextLen:22.9  AvgSpecLen:85.8  StepTime:0.55  LR:0.000100\n",
      "   | > Step:40/68  GlobalStep:78080  TotalLoss:0.00260  PostnetLoss:0.00126  DecoderLoss:0.00134  StopLoss:0.05672  GradNorm:0.00415  GradNormST:0.01245  AvgTextLen:30.9  AvgSpecLen:105.3  StepTime:0.60  LR:0.000100\n",
      "   | > Step:50/68  GlobalStep:78090  TotalLoss:0.00285  PostnetLoss:0.00137  DecoderLoss:0.00148  StopLoss:0.05106  GradNorm:0.00474  GradNormST:0.00972  AvgTextLen:38.8  AvgSpecLen:132.0  StepTime:0.80  LR:0.000100\n",
      "   | > Step:60/68  GlobalStep:78100  TotalLoss:0.00315  PostnetLoss:0.00151  DecoderLoss:0.00164  StopLoss:0.04492  GradNorm:0.00546  GradNormST:0.00810  AvgTextLen:52.0  AvgSpecLen:172.7  StepTime:1.01  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:78108  AvgTotalLoss:0.06714  AvgPostnetLoss:0.00119  AvgDecoderLoss:0.00128  AvgStopLoss:0.06466  EpochTime:42.59  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.11149   PostnetLoss: 0.00451   DecoderLoss:0.00496  StopLoss: 0.10202  \n",
      "   | > TotalLoss: 0.08037   PostnetLoss: 0.00715   DecoderLoss:0.00783  StopLoss: 0.06539  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00119   Validation Loss: 0.00637\n",
      "\n",
      " > Epoch 132/1000\n",
      "   | > Step:1/68  GlobalStep:78110  TotalLoss:0.00169  PostnetLoss:0.00082  DecoderLoss:0.00087  StopLoss:0.07530  GradNorm:0.00483  GradNormST:0.02349  AvgTextLen:3.7  AvgSpecLen:28.6  StepTime:0.26  LR:0.000100\n",
      "   | > Step:11/68  GlobalStep:78120  TotalLoss:0.00188  PostnetLoss:0.00091  DecoderLoss:0.00097  StopLoss:0.06385  GradNorm:0.00437  GradNormST:0.01924  AvgTextLen:10.3  AvgSpecLen:49.4  StepTime:0.38  LR:0.000100\n",
      "   | > Step:21/68  GlobalStep:78130  TotalLoss:0.00218  PostnetLoss:0.00106  DecoderLoss:0.00112  StopLoss:0.08972  GradNorm:0.00368  GradNormST:0.02845  AvgTextLen:16.6  AvgSpecLen:66.3  StepTime:0.37  LR:0.000100\n",
      "   | > Step:31/68  GlobalStep:78140  TotalLoss:0.00241  PostnetLoss:0.00116  DecoderLoss:0.00124  StopLoss:0.06412  GradNorm:0.00350  GradNormST:0.01889  AvgTextLen:24.1  AvgSpecLen:82.7  StepTime:0.57  LR:0.000100\n",
      "   | > Step:41/68  GlobalStep:78150  TotalLoss:0.00267  PostnetLoss:0.00129  DecoderLoss:0.00138  StopLoss:0.04199  GradNorm:0.00326  GradNormST:0.00816  AvgTextLen:32.3  AvgSpecLen:109.4  StepTime:0.52  LR:0.000100\n",
      "   | > Step:51/68  GlobalStep:78160  TotalLoss:0.00284  PostnetLoss:0.00137  DecoderLoss:0.00148  StopLoss:0.05738  GradNorm:0.00486  GradNormST:0.01244  AvgTextLen:39.2  AvgSpecLen:126.9  StepTime:0.79  LR:0.000100\n",
      "   | > Step:61/68  GlobalStep:78170  TotalLoss:0.00308  PostnetLoss:0.00148  DecoderLoss:0.00160  StopLoss:0.04234  GradNorm:0.00567  GradNormST:0.01373  AvgTextLen:54.6  AvgSpecLen:165.5  StepTime:0.89  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:78177  AvgTotalLoss:0.06550  AvgPostnetLoss:0.00119  AvgDecoderLoss:0.00128  AvgStopLoss:0.06302  EpochTime:42.41  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.10305   PostnetLoss: 0.00450   DecoderLoss:0.00497  StopLoss: 0.09358  \n",
      "   | > TotalLoss: 0.07803   PostnetLoss: 0.00701   DecoderLoss:0.00770  StopLoss: 0.06332  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00119   Validation Loss: 0.00629\n",
      "\n",
      " > Epoch 133/1000\n",
      "   | > Step:2/68  GlobalStep:78180  TotalLoss:0.00160  PostnetLoss:0.00077  DecoderLoss:0.00083  StopLoss:0.09698  GradNorm:0.00417  GradNormST:0.02251  AvgTextLen:4.0  AvgSpecLen:31.3  StepTime:0.48  LR:0.000100\n",
      "   | > Step:12/68  GlobalStep:78190  TotalLoss:0.00190  PostnetLoss:0.00092  DecoderLoss:0.00098  StopLoss:0.06669  GradNorm:0.00352  GradNormST:0.01221  AvgTextLen:11.0  AvgSpecLen:49.3  StepTime:0.44  LR:0.000100\n",
      "   | > Step:22/68  GlobalStep:78200  TotalLoss:0.00216  PostnetLoss:0.00104  DecoderLoss:0.00112  StopLoss:0.07969  GradNorm:0.00315  GradNormST:0.01325  AvgTextLen:18.0  AvgSpecLen:72.0  StepTime:0.36  LR:0.000100\n",
      "   | > Step:32/68  GlobalStep:78210  TotalLoss:0.00240  PostnetLoss:0.00116  DecoderLoss:0.00124  StopLoss:0.07506  GradNorm:0.00349  GradNormST:0.01773  AvgTextLen:24.4  AvgSpecLen:91.5  StepTime:0.56  LR:0.000100\n",
      "   | > Step:42/68  GlobalStep:78220  TotalLoss:0.00263  PostnetLoss:0.00127  DecoderLoss:0.00137  StopLoss:0.06532  GradNorm:0.00361  GradNormST:0.01886  AvgTextLen:33.1  AvgSpecLen:109.7  StepTime:0.52  LR:0.000100\n",
      "   | > Step:52/68  GlobalStep:78230  TotalLoss:0.00292  PostnetLoss:0.00141  DecoderLoss:0.00151  StopLoss:0.06784  GradNorm:0.00527  GradNormST:0.02124  AvgTextLen:42.1  AvgSpecLen:137.5  StepTime:0.67  LR:0.000100\n",
      "   | > Step:62/68  GlobalStep:78240  TotalLoss:0.00320  PostnetLoss:0.00154  DecoderLoss:0.00167  StopLoss:0.05750  GradNorm:0.00581  GradNormST:0.01188  AvgTextLen:54.0  AvgSpecLen:174.7  StepTime:0.75  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:78246  AvgTotalLoss:0.06639  AvgPostnetLoss:0.00119  AvgDecoderLoss:0.00128  AvgStopLoss:0.06392  EpochTime:42.23  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.10632   PostnetLoss: 0.00442   DecoderLoss:0.00488  StopLoss: 0.09702  \n",
      "   | > TotalLoss: 0.07885   PostnetLoss: 0.00673   DecoderLoss:0.00740  StopLoss: 0.06472  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00119   Validation Loss: 0.00605\n",
      "\n",
      " > Epoch 134/1000\n",
      "   | > Step:3/68  GlobalStep:78250  TotalLoss:0.00163  PostnetLoss:0.00079  DecoderLoss:0.00083  StopLoss:0.06854  GradNorm:0.00397  GradNormST:0.01887  AvgTextLen:4.7  AvgSpecLen:29.7  StepTime:0.28  LR:0.000100\n",
      "   | > Step:13/68  GlobalStep:78260  TotalLoss:0.00193  PostnetLoss:0.00094  DecoderLoss:0.00099  StopLoss:0.07275  GradNorm:0.00493  GradNormST:0.03072  AvgTextLen:11.5  AvgSpecLen:55.2  StepTime:0.41  LR:0.000100\n",
      "   | > Step:23/68  GlobalStep:78270  TotalLoss:0.00229  PostnetLoss:0.00111  DecoderLoss:0.00118  StopLoss:0.06217  GradNorm:0.00356  GradNormST:0.01452  AvgTextLen:18.3  AvgSpecLen:72.2  StepTime:0.34  LR:0.000100\n",
      "   | > Step:33/68  GlobalStep:78280  TotalLoss:0.00249  PostnetLoss:0.00121  DecoderLoss:0.00129  StopLoss:0.06898  GradNorm:0.00402  GradNormST:0.01697  AvgTextLen:25.9  AvgSpecLen:88.9  StepTime:0.46  LR:0.000100\n",
      "   | > Step:43/68  GlobalStep:78290  TotalLoss:0.00263  PostnetLoss:0.00127  DecoderLoss:0.00136  StopLoss:0.06039  GradNorm:0.00368  GradNormST:0.01591  AvgTextLen:33.4  AvgSpecLen:115.7  StepTime:0.55  LR:0.000100\n",
      "   | > Step:53/68  GlobalStep:78300  TotalLoss:0.00290  PostnetLoss:0.00139  DecoderLoss:0.00150  StopLoss:0.05754  GradNorm:0.00464  GradNormST:0.01564  AvgTextLen:42.4  AvgSpecLen:137.6  StepTime:0.74  LR:0.000100\n",
      "   | > Step:63/68  GlobalStep:78310  TotalLoss:0.00321  PostnetLoss:0.00154  DecoderLoss:0.00167  StopLoss:0.04746  GradNorm:0.00391  GradNormST:0.02145  AvgTextLen:58.6  AvgSpecLen:185.1  StepTime:0.95  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:78315  AvgTotalLoss:0.07001  AvgPostnetLoss:0.00119  AvgDecoderLoss:0.00128  AvgStopLoss:0.06754  EpochTime:42.13  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.10756   PostnetLoss: 0.00440   DecoderLoss:0.00485  StopLoss: 0.09832  \n",
      "   | > TotalLoss: 0.08247   PostnetLoss: 0.00684   DecoderLoss:0.00752  StopLoss: 0.06810  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00119   Validation Loss: 0.00614\n",
      "\n",
      " > Epoch 135/1000\n",
      "   | > Step:4/68  GlobalStep:78320  TotalLoss:0.00167  PostnetLoss:0.00081  DecoderLoss:0.00086  StopLoss:0.13213  GradNorm:0.00426  GradNormST:0.04616  AvgTextLen:4.8  AvgSpecLen:31.1  StepTime:0.31  LR:0.000100\n",
      "   | > Step:14/68  GlobalStep:78330  TotalLoss:0.00201  PostnetLoss:0.00097  DecoderLoss:0.00104  StopLoss:0.08633  GradNorm:0.00381  GradNormST:0.02423  AvgTextLen:12.2  AvgSpecLen:52.0  StepTime:0.35  LR:0.000100\n",
      "   | > Step:24/68  GlobalStep:78340  TotalLoss:0.00236  PostnetLoss:0.00114  DecoderLoss:0.00121  StopLoss:0.07972  GradNorm:0.00383  GradNormST:0.02035  AvgTextLen:19.2  AvgSpecLen:79.0  StepTime:0.40  LR:0.000100\n",
      "   | > Step:34/68  GlobalStep:78350  TotalLoss:0.00257  PostnetLoss:0.00124  DecoderLoss:0.00133  StopLoss:0.05344  GradNorm:0.00349  GradNormST:0.00904  AvgTextLen:26.2  AvgSpecLen:95.9  StepTime:0.59  LR:0.000100\n",
      "   | > Step:44/68  GlobalStep:78360  TotalLoss:0.00275  PostnetLoss:0.00133  DecoderLoss:0.00142  StopLoss:0.05700  GradNorm:0.00385  GradNormST:0.02440  AvgTextLen:33.8  AvgSpecLen:116.3  StepTime:0.66  LR:0.000100\n",
      "   | > Step:54/68  GlobalStep:78370  TotalLoss:0.00296  PostnetLoss:0.00142  DecoderLoss:0.00154  StopLoss:0.05696  GradNorm:0.00536  GradNormST:0.01251  AvgTextLen:42.2  AvgSpecLen:144.9  StepTime:0.87  LR:0.000100\n",
      "   | > Step:64/68  GlobalStep:78380  TotalLoss:0.00302  PostnetLoss:0.00145  DecoderLoss:0.00158  StopLoss:0.06065  GradNorm:0.00453  GradNormST:0.01502  AvgTextLen:60.6  AvgSpecLen:177.6  StepTime:0.81  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:78384  AvgTotalLoss:0.07212  AvgPostnetLoss:0.00119  AvgDecoderLoss:0.00128  AvgStopLoss:0.06965  EpochTime:43.30  AvgStepTime:0.63\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.10520   PostnetLoss: 0.00438   DecoderLoss:0.00483  StopLoss: 0.09598  \n",
      "   | > TotalLoss: 0.07826   PostnetLoss: 0.00650   DecoderLoss:0.00715  StopLoss: 0.06461  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00119   Validation Loss: 0.00599\n",
      "\n",
      " > Epoch 136/1000\n",
      "   | > Step:5/68  GlobalStep:78390  TotalLoss:0.00166  PostnetLoss:0.00080  DecoderLoss:0.00086  StopLoss:0.08874  GradNorm:0.00375  GradNormST:0.02371  AvgTextLen:5.9  AvgSpecLen:33.2  StepTime:0.34  LR:0.000100\n",
      "   | > Step:15/68  GlobalStep:78400  TotalLoss:0.00211  PostnetLoss:0.00102  DecoderLoss:0.00109  StopLoss:0.07495  GradNorm:0.00405  GradNormST:0.02890  AvgTextLen:12.8  AvgSpecLen:59.1  StepTime:0.54  LR:0.000100\n",
      "   | > Step:25/68  GlobalStep:78410  TotalLoss:0.00228  PostnetLoss:0.00110  DecoderLoss:0.00118  StopLoss:0.06980  GradNorm:0.00438  GradNormST:0.01989  AvgTextLen:19.8  AvgSpecLen:73.3  StepTime:0.48  LR:0.000100\n",
      "   | > Step:35/68  GlobalStep:78420  TotalLoss:0.00253  PostnetLoss:0.00123  DecoderLoss:0.00131  StopLoss:0.06850  GradNorm:0.00399  GradNormST:0.01999  AvgTextLen:27.1  AvgSpecLen:104.7  StepTime:0.80  LR:0.000100\n",
      "   | > Step:45/68  GlobalStep:78430  TotalLoss:0.00267  PostnetLoss:0.00129  DecoderLoss:0.00139  StopLoss:0.07051  GradNorm:0.00342  GradNormST:0.02299  AvgTextLen:34.4  AvgSpecLen:118.6  StepTime:0.64  LR:0.000100\n",
      "   | > Step:55/68  GlobalStep:78440  TotalLoss:0.00303  PostnetLoss:0.00146  DecoderLoss:0.00157  StopLoss:0.06413  GradNorm:0.00557  GradNormST:0.00979  AvgTextLen:43.9  AvgSpecLen:145.2  StepTime:0.86  LR:0.000100\n",
      "   | > Step:65/68  GlobalStep:78450  TotalLoss:0.00329  PostnetLoss:0.00157  DecoderLoss:0.00172  StopLoss:0.05161  GradNorm:0.00422  GradNormST:0.02078  AvgTextLen:64.2  AvgSpecLen:207.4  StepTime:1.19  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:78453  AvgTotalLoss:0.07105  AvgPostnetLoss:0.00120  AvgDecoderLoss:0.00128  AvgStopLoss:0.06857  EpochTime:43.30  AvgStepTime:0.63\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.10506   PostnetLoss: 0.00451   DecoderLoss:0.00494  StopLoss: 0.09561  \n",
      "   | > TotalLoss: 0.08238   PostnetLoss: 0.00673   DecoderLoss:0.00736  StopLoss: 0.06829  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00120   Validation Loss: 0.00610\n",
      "\n",
      " > Epoch 137/1000\n",
      "   | > Step:6/68  GlobalStep:78460  TotalLoss:0.00177  PostnetLoss:0.00086  DecoderLoss:0.00091  StopLoss:0.08532  GradNorm:0.00511  GradNormST:0.04538  AvgTextLen:6.0  AvgSpecLen:34.8  StepTime:0.43  LR:0.000100\n",
      "   | > Step:16/68  GlobalStep:78470  TotalLoss:0.00208  PostnetLoss:0.00101  DecoderLoss:0.00107  StopLoss:0.09073  GradNorm:0.00394  GradNormST:0.02661  AvgTextLen:13.8  AvgSpecLen:60.6  StepTime:0.36  LR:0.000100\n",
      "   | > Step:26/68  GlobalStep:78480  TotalLoss:0.00231  PostnetLoss:0.00112  DecoderLoss:0.00119  StopLoss:0.05889  GradNorm:0.00362  GradNormST:0.01545  AvgTextLen:20.3  AvgSpecLen:77.5  StepTime:0.48  LR:0.000100\n",
      "   | > Step:36/68  GlobalStep:78490  TotalLoss:0.00252  PostnetLoss:0.00122  DecoderLoss:0.00130  StopLoss:0.07397  GradNorm:0.00353  GradNormST:0.01385  AvgTextLen:28.5  AvgSpecLen:100.2  StepTime:0.75  LR:0.000100\n",
      "   | > Step:46/68  GlobalStep:78500  TotalLoss:0.00270  PostnetLoss:0.00130  DecoderLoss:0.00140  StopLoss:0.06273  GradNorm:0.00441  GradNormST:0.01177  AvgTextLen:35.4  AvgSpecLen:122.6  StepTime:0.77  LR:0.000100\n",
      "   | > Step:56/68  GlobalStep:78510  TotalLoss:0.00297  PostnetLoss:0.00143  DecoderLoss:0.00154  StopLoss:0.04618  GradNorm:0.00515  GradNormST:0.01597  AvgTextLen:45.8  AvgSpecLen:145.4  StepTime:0.82  LR:0.000100\n",
      "   | > Step:66/68  GlobalStep:78520  TotalLoss:0.00341  PostnetLoss:0.00163  DecoderLoss:0.00179  StopLoss:0.05783  GradNorm:0.00337  GradNormST:0.02670  AvgTextLen:71.2  AvgSpecLen:220.8  StepTime:1.04  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:78522  AvgTotalLoss:0.07269  AvgPostnetLoss:0.00119  AvgDecoderLoss:0.00128  AvgStopLoss:0.07022  EpochTime:42.03  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.10160   PostnetLoss: 0.00436   DecoderLoss:0.00480  StopLoss: 0.09244  \n",
      "   | > TotalLoss: 0.07777   PostnetLoss: 0.00639   DecoderLoss:0.00702  StopLoss: 0.06436  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00119   Validation Loss: 0.00593\n",
      "\n",
      " > Epoch 138/1000\n",
      "   | > Step:7/68  GlobalStep:78530  TotalLoss:0.00181  PostnetLoss:0.00088  DecoderLoss:0.00093  StopLoss:0.06156  GradNorm:0.00408  GradNormST:0.02664  AvgTextLen:7.4  AvgSpecLen:44.6  StepTime:0.40  LR:0.000100\n",
      "   | > Step:17/68  GlobalStep:78540  TotalLoss:0.00215  PostnetLoss:0.00104  DecoderLoss:0.00111  StopLoss:0.07941  GradNorm:0.00448  GradNormST:0.01740  AvgTextLen:14.7  AvgSpecLen:66.0  StepTime:0.44  LR:0.000100\n",
      "   | > Step:27/68  GlobalStep:78550  TotalLoss:0.00238  PostnetLoss:0.00115  DecoderLoss:0.00123  StopLoss:0.06919  GradNorm:0.00346  GradNormST:0.02333  AvgTextLen:21.0  AvgSpecLen:80.2  StepTime:0.66  LR:0.000100\n",
      "   | > Step:37/68  GlobalStep:78560  TotalLoss:0.00259  PostnetLoss:0.00125  DecoderLoss:0.00134  StopLoss:0.06628  GradNorm:0.00399  GradNormST:0.02096  AvgTextLen:29.3  AvgSpecLen:102.4  StepTime:0.63  LR:0.000100\n",
      "   | > Step:47/68  GlobalStep:78570  TotalLoss:0.00279  PostnetLoss:0.00135  DecoderLoss:0.00145  StopLoss:0.05657  GradNorm:0.00464  GradNormST:0.01091  AvgTextLen:36.7  AvgSpecLen:126.9  StepTime:0.58  LR:0.000100\n",
      "   | > Step:57/68  GlobalStep:78580  TotalLoss:0.00307  PostnetLoss:0.00147  DecoderLoss:0.00160  StopLoss:0.04885  GradNorm:0.01304  GradNormST:0.01298  AvgTextLen:47.1  AvgSpecLen:161.0  StepTime:0.84  LR:0.000100\n",
      "   | > Step:67/68  GlobalStep:78590  TotalLoss:0.00340  PostnetLoss:0.00162  DecoderLoss:0.00177  StopLoss:0.06177  GradNorm:0.00324  GradNormST:0.02547  AvgTextLen:80.6  AvgSpecLen:246.5  StepTime:1.21  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:78591  AvgTotalLoss:0.06970  AvgPostnetLoss:0.00119  AvgDecoderLoss:0.00128  AvgStopLoss:0.06723  EpochTime:42.59  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.10058   PostnetLoss: 0.00438   DecoderLoss:0.00482  StopLoss: 0.09138  \n",
      "   | > TotalLoss: 0.07435   PostnetLoss: 0.00648   DecoderLoss:0.00714  StopLoss: 0.06073  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00119   Validation Loss: 0.00601\n",
      "\n",
      " > Epoch 139/1000\n",
      "   | > Step:8/68  GlobalStep:78600  TotalLoss:0.00184  PostnetLoss:0.00090  DecoderLoss:0.00094  StopLoss:0.08837  GradNorm:0.00557  GradNormST:0.02130  AvgTextLen:8.3  AvgSpecLen:44.0  StepTime:0.33  LR:0.000100\n",
      "   | > Step:18/68  GlobalStep:78610  TotalLoss:0.00212  PostnetLoss:0.00103  DecoderLoss:0.00109  StopLoss:0.06457  GradNorm:0.00504  GradNormST:0.01724  AvgTextLen:15.2  AvgSpecLen:64.5  StepTime:0.44  LR:0.000100\n",
      "   | > Step:28/68  GlobalStep:78620  TotalLoss:0.00240  PostnetLoss:0.00116  DecoderLoss:0.00124  StopLoss:0.09084  GradNorm:0.00383  GradNormST:0.02616  AvgTextLen:22.1  AvgSpecLen:85.4  StepTime:0.55  LR:0.000100\n",
      "   | > Step:38/68  GlobalStep:78630  TotalLoss:0.00253  PostnetLoss:0.00123  DecoderLoss:0.00131  StopLoss:0.06332  GradNorm:0.00389  GradNormST:0.01560  AvgTextLen:29.4  AvgSpecLen:104.1  StepTime:0.61  LR:0.000100\n",
      "   | > Step:48/68  GlobalStep:78640  TotalLoss:0.00276  PostnetLoss:0.00133  DecoderLoss:0.00143  StopLoss:0.06326  GradNorm:0.00349  GradNormST:0.01858  AvgTextLen:37.5  AvgSpecLen:123.8  StepTime:0.55  LR:0.000100\n",
      "   | > Step:58/68  GlobalStep:78650  TotalLoss:0.00303  PostnetLoss:0.00146  DecoderLoss:0.00157  StopLoss:0.06168  GradNorm:0.00515  GradNormST:0.02234  AvgTextLen:47.8  AvgSpecLen:158.0  StepTime:0.82  LR:0.000100\n",
      "   | > Step:68/68  GlobalStep:78660  TotalLoss:0.00333  PostnetLoss:0.00159  DecoderLoss:0.00174  StopLoss:0.07425  GradNorm:0.00433  GradNormST:0.05590  AvgTextLen:103.0  AvgSpecLen:308.9  StepTime:1.87  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:78660  AvgTotalLoss:0.07125  AvgPostnetLoss:0.00119  AvgDecoderLoss:0.00128  AvgStopLoss:0.06877  EpochTime:42.91  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.10385   PostnetLoss: 0.00441   DecoderLoss:0.00485  StopLoss: 0.09459  \n",
      "   | > TotalLoss: 0.07140   PostnetLoss: 0.00628   DecoderLoss:0.00692  StopLoss: 0.05821  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00119   Validation Loss: 0.00592\n",
      "\n",
      " > Epoch 140/1000\n",
      "   | > Step:9/68  GlobalStep:78670  TotalLoss:0.00181  PostnetLoss:0.00088  DecoderLoss:0.00093  StopLoss:0.05557  GradNorm:0.00520  GradNormST:0.01392  AvgTextLen:9.0  AvgSpecLen:42.1  StepTime:0.38  LR:0.000100\n",
      "   | > Step:19/68  GlobalStep:78680  TotalLoss:0.00213  PostnetLoss:0.00104  DecoderLoss:0.00110  StopLoss:0.07314  GradNorm:0.00531  GradNormST:0.02424  AvgTextLen:15.5  AvgSpecLen:63.2  StepTime:0.43  LR:0.000100\n",
      "   | > Step:29/68  GlobalStep:78690  TotalLoss:0.00237  PostnetLoss:0.00114  DecoderLoss:0.00122  StopLoss:0.08570  GradNorm:0.00506  GradNormST:0.01698  AvgTextLen:21.8  AvgSpecLen:83.4  StepTime:0.58  LR:0.000100\n",
      "   | > Step:39/68  GlobalStep:78700  TotalLoss:0.00263  PostnetLoss:0.00127  DecoderLoss:0.00136  StopLoss:0.04656  GradNorm:0.00419  GradNormST:0.01105  AvgTextLen:30.2  AvgSpecLen:110.4  StepTime:0.58  LR:0.000100\n",
      "   | > Step:49/68  GlobalStep:78710  TotalLoss:0.00279  PostnetLoss:0.00134  DecoderLoss:0.00144  StopLoss:0.04487  GradNorm:0.00395  GradNormST:0.01474  AvgTextLen:38.2  AvgSpecLen:127.7  StepTime:0.72  LR:0.000100\n",
      "   | > Step:59/68  GlobalStep:78720  TotalLoss:0.00302  PostnetLoss:0.00145  DecoderLoss:0.00157  StopLoss:0.05761  GradNorm:0.00379  GradNormST:0.01761  AvgTextLen:50.5  AvgSpecLen:155.9  StepTime:0.74  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:78729  AvgTotalLoss:0.06795  AvgPostnetLoss:0.00119  AvgDecoderLoss:0.00128  AvgStopLoss:0.06547  EpochTime:42.50  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.10088   PostnetLoss: 0.00445   DecoderLoss:0.00490  StopLoss: 0.09152  \n",
      "   | > TotalLoss: 0.06944   PostnetLoss: 0.00632   DecoderLoss:0.00695  StopLoss: 0.05616  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00119   Validation Loss: 0.00591\n",
      "\n",
      " > Epoch 141/1000\n",
      "   | > Step:0/68  GlobalStep:78730  TotalLoss:0.00170  PostnetLoss:0.00082  DecoderLoss:0.00089  StopLoss:0.11689  GradNorm:0.00696  GradNormST:0.03440  AvgTextLen:3.0  AvgSpecLen:25.3  StepTime:0.46  LR:0.000100\n",
      "   | > Step:10/68  GlobalStep:78740  TotalLoss:0.00194  PostnetLoss:0.00094  DecoderLoss:0.00100  StopLoss:0.04967  GradNorm:0.00576  GradNormST:0.01362  AvgTextLen:9.8  AvgSpecLen:48.6  StepTime:0.62  LR:0.000100\n",
      "   | > Step:20/68  GlobalStep:78750  TotalLoss:0.00225  PostnetLoss:0.00110  DecoderLoss:0.00116  StopLoss:0.05352  GradNorm:0.00530  GradNormST:0.01072  AvgTextLen:16.4  AvgSpecLen:64.8  StepTime:0.49  LR:0.000100\n",
      "   | > Step:30/68  GlobalStep:78760  TotalLoss:0.00241  PostnetLoss:0.00117  DecoderLoss:0.00124  StopLoss:0.05872  GradNorm:0.00480  GradNormST:0.01172  AvgTextLen:22.9  AvgSpecLen:85.8  StepTime:0.60  LR:0.000100\n",
      "   | > Step:40/68  GlobalStep:78770  TotalLoss:0.00260  PostnetLoss:0.00126  DecoderLoss:0.00134  StopLoss:0.05559  GradNorm:0.00442  GradNormST:0.01116  AvgTextLen:30.9  AvgSpecLen:105.3  StepTime:0.64  LR:0.000100\n",
      "   | > Step:50/68  GlobalStep:78780  TotalLoss:0.00279  PostnetLoss:0.00134  DecoderLoss:0.00145  StopLoss:0.04656  GradNorm:0.00421  GradNormST:0.00998  AvgTextLen:38.8  AvgSpecLen:132.0  StepTime:0.67  LR:0.000100\n",
      "   | > Step:60/68  GlobalStep:78790  TotalLoss:0.00308  PostnetLoss:0.00148  DecoderLoss:0.00160  StopLoss:0.05179  GradNorm:0.00488  GradNormST:0.01553  AvgTextLen:52.0  AvgSpecLen:172.7  StepTime:0.82  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:78798  AvgTotalLoss:0.06987  AvgPostnetLoss:0.00119  AvgDecoderLoss:0.00128  AvgStopLoss:0.06740  EpochTime:43.27  AvgStepTime:0.63\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.09810   PostnetLoss: 0.00439   DecoderLoss:0.00483  StopLoss: 0.08888  \n",
      "   | > TotalLoss: 0.07234   PostnetLoss: 0.00641   DecoderLoss:0.00703  StopLoss: 0.05890  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00119   Validation Loss: 0.00599\n",
      "\n",
      " > Epoch 142/1000\n",
      "   | > Step:1/68  GlobalStep:78800  TotalLoss:0.00164  PostnetLoss:0.00079  DecoderLoss:0.00084  StopLoss:0.06717  GradNorm:0.00474  GradNormST:0.02155  AvgTextLen:3.7  AvgSpecLen:28.6  StepTime:0.35  LR:0.000100\n",
      "   | > Step:11/68  GlobalStep:78810  TotalLoss:0.00191  PostnetLoss:0.00093  DecoderLoss:0.00098  StopLoss:0.05199  GradNorm:0.00545  GradNormST:0.01676  AvgTextLen:10.3  AvgSpecLen:49.4  StepTime:0.41  LR:0.000100\n",
      "   | > Step:21/68  GlobalStep:78820  TotalLoss:0.00219  PostnetLoss:0.00107  DecoderLoss:0.00113  StopLoss:0.07993  GradNorm:0.00541  GradNormST:0.02248  AvgTextLen:16.6  AvgSpecLen:66.3  StepTime:0.46  LR:0.000100\n",
      "   | > Step:31/68  GlobalStep:78830  TotalLoss:0.00240  PostnetLoss:0.00116  DecoderLoss:0.00124  StopLoss:0.06276  GradNorm:0.00559  GradNormST:0.01628  AvgTextLen:24.1  AvgSpecLen:82.7  StepTime:0.54  LR:0.000100\n",
      "   | > Step:41/68  GlobalStep:78840  TotalLoss:0.00271  PostnetLoss:0.00131  DecoderLoss:0.00140  StopLoss:0.04184  GradNorm:0.00471  GradNormST:0.00794  AvgTextLen:32.3  AvgSpecLen:109.4  StepTime:0.56  LR:0.000100\n",
      "   | > Step:51/68  GlobalStep:78850  TotalLoss:0.00279  PostnetLoss:0.00134  DecoderLoss:0.00145  StopLoss:0.05050  GradNorm:0.00378  GradNormST:0.01341  AvgTextLen:39.2  AvgSpecLen:126.9  StepTime:0.72  LR:0.000100\n",
      "   | > Step:61/68  GlobalStep:78860  TotalLoss:0.00304  PostnetLoss:0.00146  DecoderLoss:0.00158  StopLoss:0.07920  GradNorm:0.00358  GradNormST:0.07045  AvgTextLen:54.6  AvgSpecLen:165.5  StepTime:0.91  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:78867  AvgTotalLoss:0.07031  AvgPostnetLoss:0.00119  AvgDecoderLoss:0.00128  AvgStopLoss:0.06784  EpochTime:42.82  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.09383   PostnetLoss: 0.00448   DecoderLoss:0.00493  StopLoss: 0.08442  \n",
      "   | > TotalLoss: 0.07163   PostnetLoss: 0.00661   DecoderLoss:0.00726  StopLoss: 0.05776  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00119   Validation Loss: 0.00619\n",
      "\n",
      " > Epoch 143/1000\n",
      "   | > Step:2/68  GlobalStep:78870  TotalLoss:0.00152  PostnetLoss:0.00074  DecoderLoss:0.00078  StopLoss:0.07311  GradNorm:0.00358  GradNormST:0.02703  AvgTextLen:4.0  AvgSpecLen:31.3  StepTime:0.54  LR:0.000100\n",
      "   | > Step:12/68  GlobalStep:78880  TotalLoss:0.00186  PostnetLoss:0.00090  DecoderLoss:0.00096  StopLoss:0.05169  GradNorm:0.00403  GradNormST:0.01150  AvgTextLen:11.0  AvgSpecLen:49.3  StepTime:0.44  LR:0.000100\n",
      "   | > Step:22/68  GlobalStep:78890  TotalLoss:0.00228  PostnetLoss:0.00111  DecoderLoss:0.00117  StopLoss:0.08318  GradNorm:0.00613  GradNormST:0.01680  AvgTextLen:18.0  AvgSpecLen:72.0  StepTime:0.40  LR:0.000100\n",
      "   | > Step:32/68  GlobalStep:78900  TotalLoss:0.00252  PostnetLoss:0.00122  DecoderLoss:0.00130  StopLoss:0.08403  GradNorm:0.00538  GradNormST:0.01605  AvgTextLen:24.4  AvgSpecLen:91.5  StepTime:0.57  LR:0.000100\n",
      "   | > Step:42/68  GlobalStep:78910  TotalLoss:0.00275  PostnetLoss:0.00132  DecoderLoss:0.00143  StopLoss:0.07277  GradNorm:0.00605  GradNormST:0.01492  AvgTextLen:33.1  AvgSpecLen:109.7  StepTime:0.63  LR:0.000100\n",
      "   | > Step:52/68  GlobalStep:78920  TotalLoss:0.00289  PostnetLoss:0.00140  DecoderLoss:0.00150  StopLoss:0.06471  GradNorm:0.00452  GradNormST:0.01756  AvgTextLen:42.1  AvgSpecLen:137.5  StepTime:0.67  LR:0.000100\n",
      "   | > Step:62/68  GlobalStep:78930  TotalLoss:0.00315  PostnetLoss:0.00154  DecoderLoss:0.00161  StopLoss:0.12316  GradNorm:0.00506  GradNormST:0.07628  AvgTextLen:54.0  AvgSpecLen:174.7  StepTime:0.75  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:78936  AvgTotalLoss:0.08062  AvgPostnetLoss:0.00121  AvgDecoderLoss:0.00129  AvgStopLoss:0.07811  EpochTime:41.66  AvgStepTime:0.60\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.09880   PostnetLoss: 0.00442   DecoderLoss:0.00486  StopLoss: 0.08953  \n",
      "   | > TotalLoss: 0.06955   PostnetLoss: 0.00607   DecoderLoss:0.00668  StopLoss: 0.05680  \n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00121   Validation Loss: 0.00571\n",
      "\n",
      " > Epoch 144/1000\n",
      "   | > Step:3/68  GlobalStep:78940  TotalLoss:0.00171  PostnetLoss:0.00083  DecoderLoss:0.00088  StopLoss:0.08440  GradNorm:0.00419  GradNormST:0.01935  AvgTextLen:4.7  AvgSpecLen:29.7  StepTime:0.35  LR:0.000100\n",
      "   | > Step:13/68  GlobalStep:78950  TotalLoss:0.00216  PostnetLoss:0.00105  DecoderLoss:0.00111  StopLoss:0.11547  GradNorm:0.00468  GradNormST:0.03148  AvgTextLen:11.5  AvgSpecLen:55.2  StepTime:0.41  LR:0.000100\n",
      "   | > Step:23/68  GlobalStep:78960  TotalLoss:0.00263  PostnetLoss:0.00127  DecoderLoss:0.00135  StopLoss:0.07047  GradNorm:0.00588  GradNormST:0.02085  AvgTextLen:18.3  AvgSpecLen:72.2  StepTime:0.38  LR:0.000100\n",
      "   | > Step:33/68  GlobalStep:78970  TotalLoss:0.00277  PostnetLoss:0.00134  DecoderLoss:0.00143  StopLoss:0.07012  GradNorm:0.00542  GradNormST:0.02157  AvgTextLen:25.9  AvgSpecLen:88.9  StepTime:0.46  LR:0.000100\n",
      "   | > Step:43/68  GlobalStep:78980  TotalLoss:0.00297  PostnetLoss:0.00142  DecoderLoss:0.00154  StopLoss:0.08118  GradNorm:0.00453  GradNormST:0.02596  AvgTextLen:33.4  AvgSpecLen:115.7  StepTime:0.55  LR:0.000100\n",
      "   | > Step:53/68  GlobalStep:78990  TotalLoss:0.00330  PostnetLoss:0.00159  DecoderLoss:0.00172  StopLoss:0.07898  GradNorm:0.00570  GradNormST:0.03016  AvgTextLen:42.4  AvgSpecLen:137.6  StepTime:0.76  LR:0.000100\n",
      "   | > Step:63/68  GlobalStep:79000  TotalLoss:0.00342  PostnetLoss:0.00164  DecoderLoss:0.00179  StopLoss:0.06326  GradNorm:0.00332  GradNormST:0.02482  AvgTextLen:58.6  AvgSpecLen:185.1  StepTime:0.90  LR:0.000100\n",
      " | | > Checkpoint saving : /vm/TTS/models/cartman_models/mozilla-no-loc-fattn-stopnet-sigmoid-loss_masking-August-15-2019_11+26AM-de6aeb9/checkpoint_79000.pth.tar\n",
      "   | > EPOCH END -- GlobalStep:79005  AvgTotalLoss:0.08526  AvgPostnetLoss:0.00134  AvgDecoderLoss:0.00144  AvgStopLoss:0.08249  EpochTime:42.63  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.10338   PostnetLoss: 0.00416   DecoderLoss:0.00462  StopLoss: 0.09459  \n",
      "   | > TotalLoss: 0.07522   PostnetLoss: 0.00577   DecoderLoss:0.00642  StopLoss: 0.06303  \n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00134   Validation Loss: 0.00540\n",
      "\n",
      " > Epoch 145/1000\n",
      "   | > Step:4/68  GlobalStep:79010  TotalLoss:0.00180  PostnetLoss:0.00087  DecoderLoss:0.00093  StopLoss:0.10788  GradNorm:0.00631  GradNormST:0.03667  AvgTextLen:4.8  AvgSpecLen:31.1  StepTime:0.22  LR:0.000100\n",
      "   | > Step:14/68  GlobalStep:79020  TotalLoss:0.00207  PostnetLoss:0.00100  DecoderLoss:0.00107  StopLoss:0.09114  GradNorm:0.00371  GradNormST:0.01972  AvgTextLen:12.2  AvgSpecLen:52.0  StepTime:0.36  LR:0.000100\n",
      "   | > Step:24/68  GlobalStep:79030  TotalLoss:0.00249  PostnetLoss:0.00120  DecoderLoss:0.00129  StopLoss:0.08153  GradNorm:0.00502  GradNormST:0.02100  AvgTextLen:19.2  AvgSpecLen:79.0  StepTime:0.52  LR:0.000100\n",
      "   | > Step:34/68  GlobalStep:79040  TotalLoss:0.00269  PostnetLoss:0.00130  DecoderLoss:0.00139  StopLoss:0.06205  GradNorm:0.00586  GradNormST:0.02085  AvgTextLen:26.2  AvgSpecLen:95.9  StepTime:0.62  LR:0.000100\n",
      "   | > Step:44/68  GlobalStep:79050  TotalLoss:0.00296  PostnetLoss:0.00143  DecoderLoss:0.00153  StopLoss:0.04862  GradNorm:0.00678  GradNormST:0.01501  AvgTextLen:33.8  AvgSpecLen:116.3  StepTime:0.64  LR:0.000100\n",
      "   | > Step:54/68  GlobalStep:79060  TotalLoss:0.00310  PostnetLoss:0.00149  DecoderLoss:0.00161  StopLoss:0.06255  GradNorm:0.00556  GradNormST:0.01595  AvgTextLen:42.2  AvgSpecLen:144.9  StepTime:0.69  LR:0.000100\n",
      "   | > Step:64/68  GlobalStep:79070  TotalLoss:0.00313  PostnetLoss:0.00149  DecoderLoss:0.00163  StopLoss:0.05858  GradNorm:0.00360  GradNormST:0.01860  AvgTextLen:60.6  AvgSpecLen:177.6  StepTime:0.80  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:79074  AvgTotalLoss:0.06979  AvgPostnetLoss:0.00126  AvgDecoderLoss:0.00135  AvgStopLoss:0.06718  EpochTime:42.46  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.10311   PostnetLoss: 0.00430   DecoderLoss:0.00474  StopLoss: 0.09407  \n",
      "   | > TotalLoss: 0.07342   PostnetLoss: 0.00600   DecoderLoss:0.00661  StopLoss: 0.06081  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00126   Validation Loss: 0.00562\n",
      "\n",
      " > Epoch 146/1000\n",
      "   | > Step:5/68  GlobalStep:79080  TotalLoss:0.00176  PostnetLoss:0.00085  DecoderLoss:0.00091  StopLoss:0.08272  GradNorm:0.00409  GradNormST:0.02476  AvgTextLen:5.9  AvgSpecLen:33.2  StepTime:0.34  LR:0.000100\n",
      "   | > Step:15/68  GlobalStep:79090  TotalLoss:0.00213  PostnetLoss:0.00103  DecoderLoss:0.00110  StopLoss:0.05251  GradNorm:0.00371  GradNormST:0.01202  AvgTextLen:12.8  AvgSpecLen:59.1  StepTime:0.43  LR:0.000100\n",
      "   | > Step:25/68  GlobalStep:79100  TotalLoss:0.00231  PostnetLoss:0.00112  DecoderLoss:0.00119  StopLoss:0.06466  GradNorm:0.00403  GradNormST:0.01625  AvgTextLen:19.8  AvgSpecLen:73.3  StepTime:0.46  LR:0.000100\n",
      "   | > Step:35/68  GlobalStep:79110  TotalLoss:0.00267  PostnetLoss:0.00129  DecoderLoss:0.00138  StopLoss:0.06919  GradNorm:0.00560  GradNormST:0.02456  AvgTextLen:27.1  AvgSpecLen:104.7  StepTime:0.74  LR:0.000100\n",
      "   | > Step:45/68  GlobalStep:79120  TotalLoss:0.00274  PostnetLoss:0.00132  DecoderLoss:0.00142  StopLoss:0.06504  GradNorm:0.00472  GradNormST:0.02128  AvgTextLen:34.4  AvgSpecLen:118.6  StepTime:0.65  LR:0.000100\n",
      "   | > Step:55/68  GlobalStep:79130  TotalLoss:0.00305  PostnetLoss:0.00146  DecoderLoss:0.00158  StopLoss:0.05418  GradNorm:0.00459  GradNormST:0.01143  AvgTextLen:43.9  AvgSpecLen:145.2  StepTime:0.66  LR:0.000100\n",
      "   | > Step:65/68  GlobalStep:79140  TotalLoss:0.00336  PostnetLoss:0.00161  DecoderLoss:0.00175  StopLoss:0.06705  GradNorm:0.00418  GradNormST:0.04074  AvgTextLen:64.2  AvgSpecLen:207.4  StepTime:1.19  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:79143  AvgTotalLoss:0.06615  AvgPostnetLoss:0.00122  AvgDecoderLoss:0.00131  AvgStopLoss:0.06362  EpochTime:42.22  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.10859   PostnetLoss: 0.00430   DecoderLoss:0.00473  StopLoss: 0.09956  \n",
      "   | > TotalLoss: 0.07482   PostnetLoss: 0.00609   DecoderLoss:0.00672  StopLoss: 0.06201  \n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00122   Validation Loss: 0.00570\n",
      "\n",
      " > Epoch 147/1000\n",
      "   | > Step:6/68  GlobalStep:79150  TotalLoss:0.00171  PostnetLoss:0.00083  DecoderLoss:0.00088  StopLoss:0.04937  GradNorm:0.00396  GradNormST:0.01610  AvgTextLen:6.0  AvgSpecLen:34.8  StepTime:0.37  LR:0.000100\n",
      "   | > Step:16/68  GlobalStep:79160  TotalLoss:0.00208  PostnetLoss:0.00101  DecoderLoss:0.00107  StopLoss:0.05720  GradNorm:0.00354  GradNormST:0.01875  AvgTextLen:13.8  AvgSpecLen:60.6  StepTime:0.37  LR:0.000100\n",
      "   | > Step:26/68  GlobalStep:79170  TotalLoss:0.00234  PostnetLoss:0.00113  DecoderLoss:0.00121  StopLoss:0.06160  GradNorm:0.00383  GradNormST:0.01394  AvgTextLen:20.3  AvgSpecLen:77.5  StepTime:0.48  LR:0.000100\n",
      "   | > Step:36/68  GlobalStep:79180  TotalLoss:0.00260  PostnetLoss:0.00126  DecoderLoss:0.00135  StopLoss:0.08014  GradNorm:0.00486  GradNormST:0.02050  AvgTextLen:28.5  AvgSpecLen:100.2  StepTime:0.57  LR:0.000100\n",
      "   | > Step:46/68  GlobalStep:79190  TotalLoss:0.00279  PostnetLoss:0.00134  DecoderLoss:0.00144  StopLoss:0.06300  GradNorm:0.00447  GradNormST:0.01871  AvgTextLen:35.4  AvgSpecLen:122.6  StepTime:0.84  LR:0.000100\n",
      "   | > Step:56/68  GlobalStep:79200  TotalLoss:0.00305  PostnetLoss:0.00146  DecoderLoss:0.00159  StopLoss:0.04995  GradNorm:0.00394  GradNormST:0.01120  AvgTextLen:45.8  AvgSpecLen:145.4  StepTime:0.85  LR:0.000100\n",
      "   | > Step:66/68  GlobalStep:79210  TotalLoss:0.00345  PostnetLoss:0.00165  DecoderLoss:0.00181  StopLoss:0.10017  GradNorm:0.00357  GradNormST:0.05856  AvgTextLen:71.2  AvgSpecLen:220.8  StepTime:1.04  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:79212  AvgTotalLoss:0.07178  AvgPostnetLoss:0.00121  AvgDecoderLoss:0.00130  AvgStopLoss:0.06927  EpochTime:42.43  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.11322   PostnetLoss: 0.00425   DecoderLoss:0.00470  StopLoss: 0.10426  \n",
      "   | > TotalLoss: 0.07645   PostnetLoss: 0.00625   DecoderLoss:0.00692  StopLoss: 0.06328  \n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00121   Validation Loss: 0.00577\n",
      "\n",
      " > Epoch 148/1000\n",
      "   | > Step:7/68  GlobalStep:79220  TotalLoss:0.00185  PostnetLoss:0.00090  DecoderLoss:0.00095  StopLoss:0.05435  GradNorm:0.00404  GradNormST:0.02028  AvgTextLen:7.4  AvgSpecLen:44.6  StepTime:0.35  LR:0.000100\n",
      "   | > Step:17/68  GlobalStep:79230  TotalLoss:0.00215  PostnetLoss:0.00104  DecoderLoss:0.00111  StopLoss:0.08287  GradNorm:0.00369  GradNormST:0.02361  AvgTextLen:14.7  AvgSpecLen:66.0  StepTime:0.45  LR:0.000100\n",
      "   | > Step:27/68  GlobalStep:79240  TotalLoss:0.00242  PostnetLoss:0.00117  DecoderLoss:0.00125  StopLoss:0.05323  GradNorm:0.00379  GradNormST:0.00848  AvgTextLen:21.0  AvgSpecLen:80.2  StepTime:0.56  LR:0.000100\n",
      "   | > Step:37/68  GlobalStep:79250  TotalLoss:0.00265  PostnetLoss:0.00128  DecoderLoss:0.00137  StopLoss:0.05544  GradNorm:0.00434  GradNormST:0.01047  AvgTextLen:29.3  AvgSpecLen:102.4  StepTime:0.64  LR:0.000100\n",
      "   | > Step:47/68  GlobalStep:79260  TotalLoss:0.00282  PostnetLoss:0.00136  DecoderLoss:0.00146  StopLoss:0.05053  GradNorm:0.00400  GradNormST:0.01467  AvgTextLen:36.7  AvgSpecLen:126.9  StepTime:0.62  LR:0.000100\n",
      "   | > Step:57/68  GlobalStep:79270  TotalLoss:0.00306  PostnetLoss:0.00147  DecoderLoss:0.00159  StopLoss:0.04383  GradNorm:0.00398  GradNormST:0.00815  AvgTextLen:47.1  AvgSpecLen:161.0  StepTime:0.69  LR:0.000100\n",
      "   | > Step:67/68  GlobalStep:79280  TotalLoss:0.00342  PostnetLoss:0.00163  DecoderLoss:0.00178  StopLoss:0.07142  GradNorm:0.00298  GradNormST:0.04579  AvgTextLen:80.6  AvgSpecLen:246.5  StepTime:1.22  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:79281  AvgTotalLoss:0.06524  AvgPostnetLoss:0.00120  AvgDecoderLoss:0.00129  AvgStopLoss:0.06274  EpochTime:43.04  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.10575   PostnetLoss: 0.00437   DecoderLoss:0.00486  StopLoss: 0.09652  \n",
      "   | > TotalLoss: 0.07455   PostnetLoss: 0.00608   DecoderLoss:0.00672  StopLoss: 0.06176  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00120   Validation Loss: 0.00565\n",
      "\n",
      " > Epoch 149/1000\n",
      "   | > Step:8/68  GlobalStep:79290  TotalLoss:0.00188  PostnetLoss:0.00091  DecoderLoss:0.00097  StopLoss:0.09055  GradNorm:0.00524  GradNormST:0.02793  AvgTextLen:8.3  AvgSpecLen:44.0  StepTime:0.27  LR:0.000100\n",
      "   | > Step:18/68  GlobalStep:79300  TotalLoss:0.00204  PostnetLoss:0.00099  DecoderLoss:0.00105  StopLoss:0.06522  GradNorm:0.00365  GradNormST:0.01702  AvgTextLen:15.2  AvgSpecLen:64.5  StepTime:0.41  LR:0.000100\n",
      "   | > Step:28/68  GlobalStep:79310  TotalLoss:0.00238  PostnetLoss:0.00115  DecoderLoss:0.00123  StopLoss:0.05826  GradNorm:0.00352  GradNormST:0.01450  AvgTextLen:22.1  AvgSpecLen:85.4  StepTime:0.44  LR:0.000100\n",
      "   | > Step:38/68  GlobalStep:79320  TotalLoss:0.00257  PostnetLoss:0.00124  DecoderLoss:0.00133  StopLoss:0.04793  GradNorm:0.00364  GradNormST:0.01292  AvgTextLen:29.4  AvgSpecLen:104.1  StepTime:0.49  LR:0.000100\n",
      "   | > Step:48/68  GlobalStep:79330  TotalLoss:0.00276  PostnetLoss:0.00133  DecoderLoss:0.00143  StopLoss:0.05011  GradNorm:0.00319  GradNormST:0.01145  AvgTextLen:37.5  AvgSpecLen:123.8  StepTime:0.60  LR:0.000100\n",
      "   | > Step:58/68  GlobalStep:79340  TotalLoss:0.00303  PostnetLoss:0.00146  DecoderLoss:0.00157  StopLoss:0.05900  GradNorm:0.00301  GradNormST:0.01519  AvgTextLen:47.8  AvgSpecLen:158.0  StepTime:0.75  LR:0.000100\n",
      "   | > Step:68/68  GlobalStep:79350  TotalLoss:0.00345  PostnetLoss:0.00165  DecoderLoss:0.00180  StopLoss:0.07423  GradNorm:0.10871  GradNormST:0.07967  AvgTextLen:103.0  AvgSpecLen:308.9  StepTime:1.86  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:79350  AvgTotalLoss:0.06441  AvgPostnetLoss:0.00120  AvgDecoderLoss:0.00129  AvgStopLoss:0.06193  EpochTime:43.06  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.10505   PostnetLoss: 0.00421   DecoderLoss:0.00467  StopLoss: 0.09618  \n",
      "   | > TotalLoss: 0.07309   PostnetLoss: 0.00635   DecoderLoss:0.00699  StopLoss: 0.05975  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00120   Validation Loss: 0.00576\n",
      "\n",
      " > Epoch 150/1000\n",
      "   | > Step:9/68  GlobalStep:79360  TotalLoss:0.00190  PostnetLoss:0.00092  DecoderLoss:0.00098  StopLoss:0.09161  GradNorm:0.00471  GradNormST:0.04777  AvgTextLen:9.0  AvgSpecLen:42.1  StepTime:0.49  LR:0.000100\n",
      "   | > Step:19/68  GlobalStep:79370  TotalLoss:0.00219  PostnetLoss:0.00106  DecoderLoss:0.00113  StopLoss:0.11561  GradNorm:0.00419  GradNormST:0.04281  AvgTextLen:15.5  AvgSpecLen:63.2  StepTime:0.42  LR:0.000100\n",
      "   | > Step:29/68  GlobalStep:79380  TotalLoss:0.00240  PostnetLoss:0.00116  DecoderLoss:0.00124  StopLoss:0.10140  GradNorm:0.00379  GradNormST:0.04135  AvgTextLen:21.8  AvgSpecLen:83.4  StepTime:0.59  LR:0.000100\n",
      "   | > Step:39/68  GlobalStep:79390  TotalLoss:0.00267  PostnetLoss:0.00129  DecoderLoss:0.00138  StopLoss:0.08918  GradNorm:0.00340  GradNormST:0.04784  AvgTextLen:30.2  AvgSpecLen:110.4  StepTime:0.68  LR:0.000100\n",
      "   | > Step:49/68  GlobalStep:79400  TotalLoss:0.00284  PostnetLoss:0.00137  DecoderLoss:0.00147  StopLoss:0.07206  GradNorm:0.00358  GradNormST:0.03320  AvgTextLen:38.2  AvgSpecLen:127.7  StepTime:0.60  LR:0.000100\n",
      "   | > Step:59/68  GlobalStep:79410  TotalLoss:0.00303  PostnetLoss:0.00145  DecoderLoss:0.00158  StopLoss:0.06556  GradNorm:0.00299  GradNormST:0.04371  AvgTextLen:50.5  AvgSpecLen:155.9  StepTime:0.72  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:79419  AvgTotalLoss:0.10126  AvgPostnetLoss:0.00123  AvgDecoderLoss:0.00132  AvgStopLoss:0.09870  EpochTime:41.87  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.10760   PostnetLoss: 0.00419   DecoderLoss:0.00461  StopLoss: 0.09880  \n",
      "   | > TotalLoss: 0.07506   PostnetLoss: 0.00609   DecoderLoss:0.00669  StopLoss: 0.06228  \n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00123   Validation Loss: 0.00563\n",
      "\n",
      " > Epoch 151/1000\n",
      "   | > Step:0/68  GlobalStep:79420  TotalLoss:0.00178  PostnetLoss:0.00085  DecoderLoss:0.00092  StopLoss:0.11852  GradNorm:0.00572  GradNormST:0.03977  AvgTextLen:3.0  AvgSpecLen:25.3  StepTime:0.47  LR:0.000100\n",
      "   | > Step:10/68  GlobalStep:79430  TotalLoss:0.00192  PostnetLoss:0.00093  DecoderLoss:0.00099  StopLoss:0.09079  GradNorm:0.00408  GradNormST:0.03975  AvgTextLen:9.8  AvgSpecLen:48.6  StepTime:0.53  LR:0.000100\n",
      "   | > Step:20/68  GlobalStep:79440  TotalLoss:0.00225  PostnetLoss:0.00109  DecoderLoss:0.00116  StopLoss:0.08209  GradNorm:0.00382  GradNormST:0.05114  AvgTextLen:16.4  AvgSpecLen:64.8  StepTime:0.50  LR:0.000100\n",
      "   | > Step:30/68  GlobalStep:79450  TotalLoss:0.00242  PostnetLoss:0.00117  DecoderLoss:0.00125  StopLoss:0.07254  GradNorm:0.00352  GradNormST:0.02924  AvgTextLen:22.9  AvgSpecLen:85.8  StepTime:0.57  LR:0.000100\n",
      "   | > Step:40/68  GlobalStep:79460  TotalLoss:0.00260  PostnetLoss:0.00125  DecoderLoss:0.00134  StopLoss:0.06263  GradNorm:0.00345  GradNormST:0.01707  AvgTextLen:30.9  AvgSpecLen:105.3  StepTime:0.60  LR:0.000100\n",
      "   | > Step:50/68  GlobalStep:79470  TotalLoss:0.00282  PostnetLoss:0.00136  DecoderLoss:0.00147  StopLoss:0.06562  GradNorm:0.00335  GradNormST:0.02822  AvgTextLen:38.8  AvgSpecLen:132.0  StepTime:0.67  LR:0.000100\n",
      "   | > Step:60/68  GlobalStep:79480  TotalLoss:0.00307  PostnetLoss:0.00147  DecoderLoss:0.00159  StopLoss:0.06175  GradNorm:0.00295  GradNormST:0.03262  AvgTextLen:52.0  AvgSpecLen:172.7  StepTime:1.00  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:79488  AvgTotalLoss:0.08219  AvgPostnetLoss:0.00121  AvgDecoderLoss:0.00130  AvgStopLoss:0.07969  EpochTime:43.05  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.10840   PostnetLoss: 0.00425   DecoderLoss:0.00467  StopLoss: 0.09949  \n",
      "   | > TotalLoss: 0.07422   PostnetLoss: 0.00617   DecoderLoss:0.00675  StopLoss: 0.06130  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00121   Validation Loss: 0.00570\n",
      "\n",
      " > Epoch 152/1000\n",
      "   | > Step:1/68  GlobalStep:79490  TotalLoss:0.00162  PostnetLoss:0.00078  DecoderLoss:0.00084  StopLoss:0.07528  GradNorm:0.00451  GradNormST:0.02385  AvgTextLen:3.7  AvgSpecLen:28.6  StepTime:0.27  LR:0.000100\n",
      "   | > Step:11/68  GlobalStep:79500  TotalLoss:0.00191  PostnetLoss:0.00093  DecoderLoss:0.00099  StopLoss:0.06471  GradNorm:0.00372  GradNormST:0.02410  AvgTextLen:10.3  AvgSpecLen:49.4  StepTime:0.31  LR:0.000100\n",
      "   | > Step:21/68  GlobalStep:79510  TotalLoss:0.00219  PostnetLoss:0.00106  DecoderLoss:0.00113  StopLoss:0.10928  GradNorm:0.00408  GradNormST:0.05354  AvgTextLen:16.6  AvgSpecLen:66.3  StepTime:0.46  LR:0.000100\n",
      "   | > Step:31/68  GlobalStep:79520  TotalLoss:0.00236  PostnetLoss:0.00114  DecoderLoss:0.00122  StopLoss:0.09087  GradNorm:0.00358  GradNormST:0.04054  AvgTextLen:24.1  AvgSpecLen:82.7  StepTime:0.54  LR:0.000100\n",
      "   | > Step:41/68  GlobalStep:79530  TotalLoss:0.00268  PostnetLoss:0.00130  DecoderLoss:0.00139  StopLoss:0.08240  GradNorm:0.00370  GradNormST:0.03712  AvgTextLen:32.3  AvgSpecLen:109.4  StepTime:0.63  LR:0.000100\n",
      "   | > Step:51/68  GlobalStep:79540  TotalLoss:0.00276  PostnetLoss:0.00133  DecoderLoss:0.00143  StopLoss:0.07679  GradNorm:0.00298  GradNormST:0.03094  AvgTextLen:39.2  AvgSpecLen:126.9  StepTime:0.66  LR:0.000100\n",
      "   | > Step:61/68  GlobalStep:79550  TotalLoss:0.00304  PostnetLoss:0.00146  DecoderLoss:0.00158  StopLoss:0.06655  GradNorm:0.00295  GradNormST:0.03272  AvgTextLen:54.6  AvgSpecLen:165.5  StepTime:0.91  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:79557  AvgTotalLoss:0.08161  AvgPostnetLoss:0.00120  AvgDecoderLoss:0.00128  AvgStopLoss:0.07913  EpochTime:41.98  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.11066   PostnetLoss: 0.00424   DecoderLoss:0.00466  StopLoss: 0.10176  \n",
      "   | > TotalLoss: 0.08318   PostnetLoss: 0.00626   DecoderLoss:0.00685  StopLoss: 0.07007  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00120   Validation Loss: 0.00574\n",
      "\n",
      " > Epoch 153/1000\n",
      "   | > Step:2/68  GlobalStep:79560  TotalLoss:0.00153  PostnetLoss:0.00074  DecoderLoss:0.00079  StopLoss:0.08777  GradNorm:0.00362  GradNormST:0.03563  AvgTextLen:4.0  AvgSpecLen:31.3  StepTime:0.32  LR:0.000100\n",
      "   | > Step:12/68  GlobalStep:79570  TotalLoss:0.00190  PostnetLoss:0.00092  DecoderLoss:0.00098  StopLoss:0.09475  GradNorm:0.00393  GradNormST:0.06131  AvgTextLen:11.0  AvgSpecLen:49.3  StepTime:0.33  LR:0.000100\n",
      "   | > Step:22/68  GlobalStep:79580  TotalLoss:0.00220  PostnetLoss:0.00107  DecoderLoss:0.00114  StopLoss:0.10975  GradNorm:0.00335  GradNormST:0.05461  AvgTextLen:18.0  AvgSpecLen:72.0  StepTime:0.48  LR:0.000100\n",
      "   | > Step:32/68  GlobalStep:79590  TotalLoss:0.00241  PostnetLoss:0.00117  DecoderLoss:0.00125  StopLoss:0.09326  GradNorm:0.00380  GradNormST:0.03623  AvgTextLen:24.4  AvgSpecLen:91.5  StepTime:0.59  LR:0.000100\n",
      "   | > Step:42/68  GlobalStep:79600  TotalLoss:0.00265  PostnetLoss:0.00128  DecoderLoss:0.00137  StopLoss:0.07396  GradNorm:0.00350  GradNormST:0.02479  AvgTextLen:33.1  AvgSpecLen:109.7  StepTime:0.60  LR:0.000100\n",
      "   | > Step:52/68  GlobalStep:79610  TotalLoss:0.00290  PostnetLoss:0.00140  DecoderLoss:0.00150  StopLoss:0.08682  GradNorm:0.00344  GradNormST:0.02747  AvgTextLen:42.1  AvgSpecLen:137.5  StepTime:0.67  LR:0.000100\n",
      "   | > Step:62/68  GlobalStep:79620  TotalLoss:0.00311  PostnetLoss:0.00149  DecoderLoss:0.00162  StopLoss:0.06149  GradNorm:0.00319  GradNormST:0.02655  AvgTextLen:54.0  AvgSpecLen:174.7  StepTime:0.75  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:79626  AvgTotalLoss:0.08332  AvgPostnetLoss:0.00119  AvgDecoderLoss:0.00128  AvgStopLoss:0.08085  EpochTime:41.76  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.11530   PostnetLoss: 0.00439   DecoderLoss:0.00478  StopLoss: 0.10613  \n",
      "   | > TotalLoss: 0.08266   PostnetLoss: 0.00650   DecoderLoss:0.00707  StopLoss: 0.06908  \n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00119   Validation Loss: 0.00597\n",
      "\n",
      " > Epoch 154/1000\n",
      "   | > Step:3/68  GlobalStep:79630  TotalLoss:0.00164  PostnetLoss:0.00080  DecoderLoss:0.00084  StopLoss:0.07194  GradNorm:0.00378  GradNormST:0.02980  AvgTextLen:4.7  AvgSpecLen:29.7  StepTime:0.31  LR:0.000100\n",
      "   | > Step:13/68  GlobalStep:79640  TotalLoss:0.00192  PostnetLoss:0.00093  DecoderLoss:0.00099  StopLoss:0.10541  GradNorm:0.00384  GradNormST:0.03935  AvgTextLen:11.5  AvgSpecLen:55.2  StepTime:0.33  LR:0.000100\n",
      "   | > Step:23/68  GlobalStep:79650  TotalLoss:0.00231  PostnetLoss:0.00111  DecoderLoss:0.00119  StopLoss:0.07442  GradNorm:0.01339  GradNormST:0.02424  AvgTextLen:18.3  AvgSpecLen:72.2  StepTime:0.38  LR:0.000100\n",
      "   | > Step:33/68  GlobalStep:79660  TotalLoss:0.00250  PostnetLoss:0.00121  DecoderLoss:0.00129  StopLoss:0.08024  GradNorm:0.00367  GradNormST:0.04648  AvgTextLen:25.9  AvgSpecLen:88.9  StepTime:0.53  LR:0.000100\n",
      "   | > Step:43/68  GlobalStep:79670  TotalLoss:0.00264  PostnetLoss:0.00127  DecoderLoss:0.00137  StopLoss:0.08012  GradNorm:0.00311  GradNormST:0.02297  AvgTextLen:33.4  AvgSpecLen:115.7  StepTime:0.61  LR:0.000100\n",
      "   | > Step:53/68  GlobalStep:79680  TotalLoss:0.00283  PostnetLoss:0.00136  DecoderLoss:0.00147  StopLoss:0.08118  GradNorm:0.00320  GradNormST:0.03796  AvgTextLen:42.4  AvgSpecLen:137.6  StepTime:0.74  LR:0.000100\n",
      "   | > Step:63/68  GlobalStep:79690  TotalLoss:0.00319  PostnetLoss:0.00153  DecoderLoss:0.00166  StopLoss:0.04716  GradNorm:0.00309  GradNormST:0.01871  AvgTextLen:58.6  AvgSpecLen:185.1  StepTime:0.94  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:79695  AvgTotalLoss:0.07959  AvgPostnetLoss:0.00119  AvgDecoderLoss:0.00127  AvgStopLoss:0.07713  EpochTime:42.85  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.10859   PostnetLoss: 0.00440   DecoderLoss:0.00480  StopLoss: 0.09939  \n",
      "   | > TotalLoss: 0.07899   PostnetLoss: 0.00618   DecoderLoss:0.00672  StopLoss: 0.06610  \n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00119   Validation Loss: 0.00572\n",
      "\n",
      " > Epoch 155/1000\n",
      "   | > Step:4/68  GlobalStep:79700  TotalLoss:0.00166  PostnetLoss:0.00081  DecoderLoss:0.00086  StopLoss:0.12195  GradNorm:0.00375  GradNormST:0.04848  AvgTextLen:4.8  AvgSpecLen:31.1  StepTime:0.26  LR:0.000100\n",
      "   | > Step:14/68  GlobalStep:79710  TotalLoss:0.00204  PostnetLoss:0.00099  DecoderLoss:0.00106  StopLoss:0.06972  GradNorm:0.00383  GradNormST:0.02124  AvgTextLen:12.2  AvgSpecLen:52.0  StepTime:0.33  LR:0.000100\n",
      "   | > Step:24/68  GlobalStep:79720  TotalLoss:0.00242  PostnetLoss:0.00117  DecoderLoss:0.00125  StopLoss:0.08071  GradNorm:0.00381  GradNormST:0.02512  AvgTextLen:19.2  AvgSpecLen:79.0  StepTime:0.50  LR:0.000100\n",
      "   | > Step:34/68  GlobalStep:79730  TotalLoss:0.00257  PostnetLoss:0.00124  DecoderLoss:0.00133  StopLoss:0.06557  GradNorm:0.00335  GradNormST:0.02623  AvgTextLen:26.2  AvgSpecLen:95.9  StepTime:0.56  LR:0.000100\n",
      "   | > Step:44/68  GlobalStep:79740  TotalLoss:0.00281  PostnetLoss:0.00136  DecoderLoss:0.00146  StopLoss:0.05964  GradNorm:0.00380  GradNormST:0.01907  AvgTextLen:33.8  AvgSpecLen:116.3  StepTime:0.54  LR:0.000100\n",
      "   | > Step:54/68  GlobalStep:79750  TotalLoss:0.00300  PostnetLoss:0.00144  DecoderLoss:0.00156  StopLoss:0.07781  GradNorm:0.00349  GradNormST:0.02859  AvgTextLen:42.2  AvgSpecLen:144.9  StepTime:0.87  LR:0.000100\n",
      "   | > Step:64/68  GlobalStep:79760  TotalLoss:0.00305  PostnetLoss:0.00146  DecoderLoss:0.00159  StopLoss:0.06034  GradNorm:0.00343  GradNormST:0.03292  AvgTextLen:60.6  AvgSpecLen:177.6  StepTime:0.80  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:79764  AvgTotalLoss:0.07314  AvgPostnetLoss:0.00121  AvgDecoderLoss:0.00130  AvgStopLoss:0.07062  EpochTime:41.81  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.11062   PostnetLoss: 0.00443   DecoderLoss:0.00488  StopLoss: 0.10131  \n",
      "   | > TotalLoss: 0.07642   PostnetLoss: 0.00618   DecoderLoss:0.00679  StopLoss: 0.06344  \n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00121   Validation Loss: 0.00569\n",
      "\n",
      " > Epoch 156/1000\n",
      "   | > Step:5/68  GlobalStep:79770  TotalLoss:0.00170  PostnetLoss:0.00083  DecoderLoss:0.00088  StopLoss:0.07934  GradNorm:0.00356  GradNormST:0.02227  AvgTextLen:5.9  AvgSpecLen:33.2  StepTime:0.35  LR:0.000100\n",
      "   | > Step:15/68  GlobalStep:79780  TotalLoss:0.00208  PostnetLoss:0.00101  DecoderLoss:0.00107  StopLoss:0.05568  GradNorm:0.00338  GradNormST:0.02384  AvgTextLen:12.8  AvgSpecLen:59.1  StepTime:0.50  LR:0.000100\n",
      "   | > Step:25/68  GlobalStep:79790  TotalLoss:0.00228  PostnetLoss:0.00110  DecoderLoss:0.00118  StopLoss:0.06202  GradNorm:0.00324  GradNormST:0.01612  AvgTextLen:19.8  AvgSpecLen:73.3  StepTime:0.40  LR:0.000100\n",
      "   | > Step:35/68  GlobalStep:79800  TotalLoss:0.00250  PostnetLoss:0.00121  DecoderLoss:0.00130  StopLoss:0.11680  GradNorm:0.00325  GradNormST:0.07819  AvgTextLen:27.1  AvgSpecLen:104.7  StepTime:0.80  LR:0.000100\n",
      "   | > Step:45/68  GlobalStep:79810  TotalLoss:0.00266  PostnetLoss:0.00128  DecoderLoss:0.00138  StopLoss:0.07428  GradNorm:0.00318  GradNormST:0.02257  AvgTextLen:34.4  AvgSpecLen:118.6  StepTime:0.64  LR:0.000100\n",
      "   | > Step:55/68  GlobalStep:79820  TotalLoss:0.00295  PostnetLoss:0.00142  DecoderLoss:0.00153  StopLoss:0.07809  GradNorm:0.00311  GradNormST:0.04134  AvgTextLen:43.9  AvgSpecLen:145.2  StepTime:0.67  LR:0.000100\n",
      "   | > Step:65/68  GlobalStep:79830  TotalLoss:0.00324  PostnetLoss:0.00155  DecoderLoss:0.00169  StopLoss:0.06788  GradNorm:0.00312  GradNormST:0.04866  AvgTextLen:64.2  AvgSpecLen:207.4  StepTime:1.19  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:79833  AvgTotalLoss:0.07349  AvgPostnetLoss:0.00120  AvgDecoderLoss:0.00128  AvgStopLoss:0.07101  EpochTime:42.64  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.10817   PostnetLoss: 0.00435   DecoderLoss:0.00477  StopLoss: 0.09906  \n",
      "   | > TotalLoss: 0.07704   PostnetLoss: 0.00650   DecoderLoss:0.00709  StopLoss: 0.06345  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00120   Validation Loss: 0.00586\n",
      "\n",
      " > Epoch 157/1000\n",
      "   | > Step:6/68  GlobalStep:79840  TotalLoss:0.00194  PostnetLoss:0.00094  DecoderLoss:0.00100  StopLoss:0.07087  GradNorm:0.00540  GradNormST:0.03060  AvgTextLen:6.0  AvgSpecLen:34.8  StepTime:0.37  LR:0.000100\n",
      "   | > Step:16/68  GlobalStep:79850  TotalLoss:0.00255  PostnetLoss:0.00123  DecoderLoss:0.00132  StopLoss:0.10748  GradNorm:0.00842  GradNormST:0.04575  AvgTextLen:13.8  AvgSpecLen:60.6  StepTime:0.29  LR:0.000100\n",
      "   | > Step:26/68  GlobalStep:79860  TotalLoss:0.00298  PostnetLoss:0.00144  DecoderLoss:0.00154  StopLoss:0.10382  GradNorm:0.00825  GradNormST:0.04740  AvgTextLen:20.3  AvgSpecLen:77.5  StepTime:0.47  LR:0.000100\n",
      "   | > Step:36/68  GlobalStep:79870  TotalLoss:0.00299  PostnetLoss:0.00144  DecoderLoss:0.00155  StopLoss:0.09248  GradNorm:0.00490  GradNormST:0.03087  AvgTextLen:28.5  AvgSpecLen:100.2  StepTime:0.61  LR:0.000100\n",
      "   | > Step:46/68  GlobalStep:79880  TotalLoss:0.00308  PostnetLoss:0.00148  DecoderLoss:0.00160  StopLoss:0.07671  GradNorm:0.00400  GradNormST:0.04786  AvgTextLen:35.4  AvgSpecLen:122.6  StepTime:0.84  LR:0.000100\n",
      "   | > Step:56/68  GlobalStep:79890  TotalLoss:0.00339  PostnetLoss:0.00163  DecoderLoss:0.00176  StopLoss:0.05302  GradNorm:0.01087  GradNormST:0.04147  AvgTextLen:45.8  AvgSpecLen:145.4  StepTime:0.82  LR:0.000100\n",
      "   | > Step:66/68  GlobalStep:79900  TotalLoss:0.00384  PostnetLoss:0.00183  DecoderLoss:0.00201  StopLoss:0.06576  GradNorm:0.00380  GradNormST:0.04669  AvgTextLen:71.2  AvgSpecLen:220.8  StepTime:1.03  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:79902  AvgTotalLoss:0.08524  AvgPostnetLoss:0.00140  AvgDecoderLoss:0.00151  AvgStopLoss:0.08233  EpochTime:41.32  AvgStepTime:0.60\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.10626   PostnetLoss: 0.00448   DecoderLoss:0.00495  StopLoss: 0.09683  \n",
      "   | > TotalLoss: 0.07589   PostnetLoss: 0.00570   DecoderLoss:0.00629  StopLoss: 0.06390  \n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00140   Validation Loss: 0.00534\n",
      "\n",
      " > Epoch 158/1000\n",
      "   | > Step:7/68  GlobalStep:79910  TotalLoss:0.00200  PostnetLoss:0.00097  DecoderLoss:0.00103  StopLoss:0.07001  GradNorm:0.00463  GradNormST:0.02902  AvgTextLen:7.4  AvgSpecLen:44.6  StepTime:0.39  LR:0.000100\n",
      "   | > Step:17/68  GlobalStep:79920  TotalLoss:0.00218  PostnetLoss:0.00106  DecoderLoss:0.00112  StopLoss:0.09290  GradNorm:0.00364  GradNormST:0.02198  AvgTextLen:14.7  AvgSpecLen:66.0  StepTime:0.43  LR:0.000100\n",
      "   | > Step:27/68  GlobalStep:79930  TotalLoss:0.00252  PostnetLoss:0.00122  DecoderLoss:0.00130  StopLoss:0.06641  GradNorm:0.00336  GradNormST:0.02958  AvgTextLen:21.0  AvgSpecLen:80.2  StepTime:0.65  LR:0.000100\n",
      "   | > Step:37/68  GlobalStep:79940  TotalLoss:0.00278  PostnetLoss:0.00134  DecoderLoss:0.00144  StopLoss:0.07070  GradNorm:0.00356  GradNormST:0.02657  AvgTextLen:29.3  AvgSpecLen:102.4  StepTime:0.59  LR:0.000100\n",
      "   | > Step:47/68  GlobalStep:79950  TotalLoss:0.00295  PostnetLoss:0.00142  DecoderLoss:0.00153  StopLoss:0.06162  GradNorm:0.00347  GradNormST:0.02067  AvgTextLen:36.7  AvgSpecLen:126.9  StepTime:0.75  LR:0.000100\n",
      "   | > Step:57/68  GlobalStep:79960  TotalLoss:0.00318  PostnetLoss:0.00153  DecoderLoss:0.00165  StopLoss:0.05480  GradNorm:0.00325  GradNormST:0.02401  AvgTextLen:47.1  AvgSpecLen:161.0  StepTime:0.84  LR:0.000100\n",
      "   | > Step:67/68  GlobalStep:79970  TotalLoss:0.00367  PostnetLoss:0.00175  DecoderLoss:0.00192  StopLoss:0.06844  GradNorm:0.00334  GradNormST:0.06200  AvgTextLen:80.6  AvgSpecLen:246.5  StepTime:1.22  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:79971  AvgTotalLoss:0.07400  AvgPostnetLoss:0.00128  AvgDecoderLoss:0.00137  AvgStopLoss:0.07135  EpochTime:42.71  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.11131   PostnetLoss: 0.00440   DecoderLoss:0.00483  StopLoss: 0.10207  \n",
      "   | > TotalLoss: 0.06850   PostnetLoss: 0.00588   DecoderLoss:0.00644  StopLoss: 0.05618  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00128   Validation Loss: 0.00541\n",
      "\n",
      " > Epoch 159/1000\n",
      "   | > Step:8/68  GlobalStep:79980  TotalLoss:0.00192  PostnetLoss:0.00093  DecoderLoss:0.00099  StopLoss:0.08877  GradNorm:0.00400  GradNormST:0.02398  AvgTextLen:8.3  AvgSpecLen:44.0  StepTime:0.34  LR:0.000100\n",
      "   | > Step:18/68  GlobalStep:79990  TotalLoss:0.00215  PostnetLoss:0.00104  DecoderLoss:0.00111  StopLoss:0.07852  GradNorm:0.00347  GradNormST:0.02244  AvgTextLen:15.2  AvgSpecLen:64.5  StepTime:0.46  LR:0.000100\n",
      "   | > Step:28/68  GlobalStep:80000  TotalLoss:0.00242  PostnetLoss:0.00117  DecoderLoss:0.00125  StopLoss:0.07197  GradNorm:0.00309  GradNormST:0.02068  AvgTextLen:22.1  AvgSpecLen:85.4  StepTime:0.48  LR:0.000100\n",
      " | | > Checkpoint saving : /vm/TTS/models/cartman_models/mozilla-no-loc-fattn-stopnet-sigmoid-loss_masking-August-15-2019_11+26AM-de6aeb9/checkpoint_80000.pth.tar\n",
      "warning: audio amplitude out of range, auto clipped.\n",
      "   | > Step:38/68  GlobalStep:80010  TotalLoss:0.00259  PostnetLoss:0.00125  DecoderLoss:0.00134  StopLoss:0.07140  GradNorm:0.00309  GradNormST:0.02672  AvgTextLen:29.4  AvgSpecLen:104.1  StepTime:0.49  LR:0.000100\n",
      "   | > Step:48/68  GlobalStep:80020  TotalLoss:0.00285  PostnetLoss:0.00137  DecoderLoss:0.00148  StopLoss:0.05694  GradNorm:0.00319  GradNormST:0.03039  AvgTextLen:37.5  AvgSpecLen:123.8  StepTime:0.71  LR:0.000100\n",
      "   | > Step:58/68  GlobalStep:80030  TotalLoss:0.00313  PostnetLoss:0.00150  DecoderLoss:0.00162  StopLoss:0.04586  GradNorm:0.00306  GradNormST:0.02028  AvgTextLen:47.8  AvgSpecLen:158.0  StepTime:0.87  LR:0.000100\n",
      "   | > Step:68/68  GlobalStep:80040  TotalLoss:0.00396  PostnetLoss:0.00191  DecoderLoss:0.00206  StopLoss:0.04552  GradNorm:0.00658  GradNormST:0.03189  AvgTextLen:103.0  AvgSpecLen:308.9  StepTime:1.86  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:80040  AvgTotalLoss:0.07170  AvgPostnetLoss:0.00123  AvgDecoderLoss:0.00132  AvgStopLoss:0.06915  EpochTime:42.05  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.10773   PostnetLoss: 0.00444   DecoderLoss:0.00488  StopLoss: 0.09840  \n",
      "   | > TotalLoss: 0.07093   PostnetLoss: 0.00600   DecoderLoss:0.00656  StopLoss: 0.05838  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00123   Validation Loss: 0.00551\n",
      "\n",
      " > Epoch 160/1000\n",
      "   | > Step:9/68  GlobalStep:80050  TotalLoss:0.00180  PostnetLoss:0.00087  DecoderLoss:0.00093  StopLoss:0.09833  GradNorm:0.00360  GradNormST:0.04799  AvgTextLen:9.0  AvgSpecLen:42.1  StepTime:0.43  LR:0.000100\n",
      "   | > Step:19/68  GlobalStep:80060  TotalLoss:0.00210  PostnetLoss:0.00102  DecoderLoss:0.00108  StopLoss:0.07424  GradNorm:0.00336  GradNormST:0.01431  AvgTextLen:15.5  AvgSpecLen:63.2  StepTime:0.35  LR:0.000100\n",
      "   | > Step:29/68  GlobalStep:80070  TotalLoss:0.00238  PostnetLoss:0.00115  DecoderLoss:0.00123  StopLoss:0.07599  GradNorm:0.00312  GradNormST:0.01920  AvgTextLen:21.8  AvgSpecLen:83.4  StepTime:0.47  LR:0.000100\n",
      "   | > Step:39/68  GlobalStep:80080  TotalLoss:0.00265  PostnetLoss:0.00128  DecoderLoss:0.00137  StopLoss:0.05402  GradNorm:0.00315  GradNormST:0.02313  AvgTextLen:30.2  AvgSpecLen:110.4  StepTime:0.62  LR:0.000100\n",
      "   | > Step:49/68  GlobalStep:80090  TotalLoss:0.00284  PostnetLoss:0.00137  DecoderLoss:0.00147  StopLoss:0.04800  GradNorm:0.00313  GradNormST:0.01766  AvgTextLen:38.2  AvgSpecLen:127.7  StepTime:0.60  LR:0.000100\n",
      "   | > Step:59/68  GlobalStep:80100  TotalLoss:0.00308  PostnetLoss:0.00148  DecoderLoss:0.00161  StopLoss:0.05038  GradNorm:0.00294  GradNormST:0.02094  AvgTextLen:50.5  AvgSpecLen:155.9  StepTime:0.92  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:80109  AvgTotalLoss:0.07243  AvgPostnetLoss:0.00122  AvgDecoderLoss:0.00131  AvgStopLoss:0.06990  EpochTime:42.11  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.10670   PostnetLoss: 0.00437   DecoderLoss:0.00479  StopLoss: 0.09753  \n",
      "   | > TotalLoss: 0.07028   PostnetLoss: 0.00610   DecoderLoss:0.00667  StopLoss: 0.05751  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00122   Validation Loss: 0.00556\n",
      "\n",
      " > Epoch 161/1000\n",
      "   | > Step:0/68  GlobalStep:80110  TotalLoss:0.00170  PostnetLoss:0.00081  DecoderLoss:0.00088  StopLoss:0.10985  GradNorm:0.00571  GradNormST:0.02963  AvgTextLen:3.0  AvgSpecLen:25.3  StepTime:0.28  LR:0.000100\n",
      "   | > Step:10/68  GlobalStep:80120  TotalLoss:0.00188  PostnetLoss:0.00091  DecoderLoss:0.00097  StopLoss:0.10406  GradNorm:0.00356  GradNormST:0.05968  AvgTextLen:9.8  AvgSpecLen:48.6  StepTime:0.43  LR:0.000100\n",
      "   | > Step:20/68  GlobalStep:80130  TotalLoss:0.00225  PostnetLoss:0.00109  DecoderLoss:0.00116  StopLoss:0.06558  GradNorm:0.00331  GradNormST:0.03472  AvgTextLen:16.4  AvgSpecLen:64.8  StepTime:0.39  LR:0.000100\n",
      "   | > Step:30/68  GlobalStep:80140  TotalLoss:0.00242  PostnetLoss:0.00117  DecoderLoss:0.00125  StopLoss:0.09592  GradNorm:0.00326  GradNormST:0.04269  AvgTextLen:22.9  AvgSpecLen:85.8  StepTime:0.62  LR:0.000100\n",
      "   | > Step:40/68  GlobalStep:80150  TotalLoss:0.00263  PostnetLoss:0.00127  DecoderLoss:0.00136  StopLoss:0.05720  GradNorm:0.00329  GradNormST:0.01687  AvgTextLen:30.9  AvgSpecLen:105.3  StepTime:0.63  LR:0.000100\n",
      "   | > Step:50/68  GlobalStep:80160  TotalLoss:0.00286  PostnetLoss:0.00137  DecoderLoss:0.00148  StopLoss:0.05956  GradNorm:0.00313  GradNormST:0.02132  AvgTextLen:38.8  AvgSpecLen:132.0  StepTime:0.80  LR:0.000100\n",
      "   | > Step:60/68  GlobalStep:80170  TotalLoss:0.00310  PostnetLoss:0.00149  DecoderLoss:0.00161  StopLoss:0.05774  GradNorm:0.00292  GradNormST:0.04521  AvgTextLen:52.0  AvgSpecLen:172.7  StepTime:0.78  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:80178  AvgTotalLoss:0.07407  AvgPostnetLoss:0.00121  AvgDecoderLoss:0.00130  AvgStopLoss:0.07156  EpochTime:42.56  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.09892   PostnetLoss: 0.00439   DecoderLoss:0.00482  StopLoss: 0.08971  \n",
      "   | > TotalLoss: 0.06764   PostnetLoss: 0.00603   DecoderLoss:0.00661  StopLoss: 0.05500  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00121   Validation Loss: 0.00554\n",
      "\n",
      " > Epoch 162/1000\n",
      "   | > Step:1/68  GlobalStep:80180  TotalLoss:0.00165  PostnetLoss:0.00080  DecoderLoss:0.00085  StopLoss:0.07371  GradNorm:0.00394  GradNormST:0.02447  AvgTextLen:3.7  AvgSpecLen:28.6  StepTime:0.21  LR:0.000100\n",
      "   | > Step:11/68  GlobalStep:80190  TotalLoss:0.00185  PostnetLoss:0.00090  DecoderLoss:0.00095  StopLoss:0.05688  GradNorm:0.00332  GradNormST:0.02010  AvgTextLen:10.3  AvgSpecLen:49.4  StepTime:0.30  LR:0.000100\n",
      "   | > Step:21/68  GlobalStep:80200  TotalLoss:0.00215  PostnetLoss:0.00104  DecoderLoss:0.00111  StopLoss:0.09234  GradNorm:0.00317  GradNormST:0.04589  AvgTextLen:16.6  AvgSpecLen:66.3  StepTime:0.38  LR:0.000100\n",
      "   | > Step:31/68  GlobalStep:80210  TotalLoss:0.00236  PostnetLoss:0.00114  DecoderLoss:0.00122  StopLoss:0.06149  GradNorm:0.00317  GradNormST:0.02152  AvgTextLen:24.1  AvgSpecLen:82.7  StepTime:0.53  LR:0.000100\n",
      "   | > Step:41/68  GlobalStep:80220  TotalLoss:0.00267  PostnetLoss:0.00129  DecoderLoss:0.00138  StopLoss:0.04970  GradNorm:0.00324  GradNormST:0.01743  AvgTextLen:32.3  AvgSpecLen:109.4  StepTime:0.63  LR:0.000100\n",
      "   | > Step:51/68  GlobalStep:80230  TotalLoss:0.00281  PostnetLoss:0.00135  DecoderLoss:0.00146  StopLoss:0.04549  GradNorm:0.00340  GradNormST:0.01498  AvgTextLen:39.2  AvgSpecLen:126.9  StepTime:0.74  LR:0.000100\n",
      "   | > Step:61/68  GlobalStep:80240  TotalLoss:0.00305  PostnetLoss:0.00147  DecoderLoss:0.00159  StopLoss:0.04896  GradNorm:0.00279  GradNormST:0.03256  AvgTextLen:54.6  AvgSpecLen:165.5  StepTime:0.95  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:80247  AvgTotalLoss:0.07022  AvgPostnetLoss:0.00120  AvgDecoderLoss:0.00129  AvgStopLoss:0.06773  EpochTime:43.01  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.10326   PostnetLoss: 0.00443   DecoderLoss:0.00486  StopLoss: 0.09397  \n",
      "   | > TotalLoss: 0.06815   PostnetLoss: 0.00607   DecoderLoss:0.00665  StopLoss: 0.05542  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00120   Validation Loss: 0.00557\n",
      "\n",
      " > Epoch 163/1000\n",
      "   | > Step:2/68  GlobalStep:80250  TotalLoss:0.00158  PostnetLoss:0.00076  DecoderLoss:0.00082  StopLoss:0.09175  GradNorm:0.00350  GradNormST:0.02632  AvgTextLen:4.0  AvgSpecLen:31.3  StepTime:0.36  LR:0.000100\n",
      "   | > Step:12/68  GlobalStep:80260  TotalLoss:0.00186  PostnetLoss:0.00090  DecoderLoss:0.00096  StopLoss:0.07338  GradNorm:0.00339  GradNormST:0.02905  AvgTextLen:11.0  AvgSpecLen:49.3  StepTime:0.43  LR:0.000100\n",
      "   | > Step:22/68  GlobalStep:80270  TotalLoss:0.00222  PostnetLoss:0.00107  DecoderLoss:0.00114  StopLoss:0.07543  GradNorm:0.00336  GradNormST:0.01686  AvgTextLen:18.0  AvgSpecLen:72.0  StepTime:0.48  LR:0.000100\n",
      "   | > Step:32/68  GlobalStep:80280  TotalLoss:0.00243  PostnetLoss:0.00117  DecoderLoss:0.00126  StopLoss:0.07128  GradNorm:0.00309  GradNormST:0.02385  AvgTextLen:24.4  AvgSpecLen:91.5  StepTime:0.57  LR:0.000100\n",
      "   | > Step:42/68  GlobalStep:80290  TotalLoss:0.00263  PostnetLoss:0.00127  DecoderLoss:0.00136  StopLoss:0.06557  GradNorm:0.00331  GradNormST:0.02452  AvgTextLen:33.1  AvgSpecLen:109.7  StepTime:0.58  LR:0.000100\n",
      "   | > Step:52/68  GlobalStep:80300  TotalLoss:0.00286  PostnetLoss:0.00138  DecoderLoss:0.00149  StopLoss:0.06822  GradNorm:0.00300  GradNormST:0.02683  AvgTextLen:42.1  AvgSpecLen:137.5  StepTime:0.85  LR:0.000100\n",
      "   | > Step:62/68  GlobalStep:80310  TotalLoss:0.00314  PostnetLoss:0.00151  DecoderLoss:0.00163  StopLoss:0.05219  GradNorm:0.00324  GradNormST:0.02332  AvgTextLen:54.0  AvgSpecLen:174.7  StepTime:0.75  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:80316  AvgTotalLoss:0.06958  AvgPostnetLoss:0.00119  AvgDecoderLoss:0.00128  AvgStopLoss:0.06711  EpochTime:42.42  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.10413   PostnetLoss: 0.00445   DecoderLoss:0.00487  StopLoss: 0.09481  \n",
      "   | > TotalLoss: 0.07001   PostnetLoss: 0.00616   DecoderLoss:0.00673  StopLoss: 0.05712  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00119   Validation Loss: 0.00566\n",
      "\n",
      " > Epoch 164/1000\n",
      "   | > Step:3/68  GlobalStep:80320  TotalLoss:0.00165  PostnetLoss:0.00080  DecoderLoss:0.00085  StopLoss:0.07273  GradNorm:0.00391  GradNormST:0.02495  AvgTextLen:4.7  AvgSpecLen:29.7  StepTime:0.33  LR:0.000100\n",
      "   | > Step:13/68  GlobalStep:80330  TotalLoss:0.00190  PostnetLoss:0.00092  DecoderLoss:0.00098  StopLoss:0.08875  GradNorm:0.00373  GradNormST:0.02404  AvgTextLen:11.5  AvgSpecLen:55.2  StepTime:0.41  LR:0.000100\n",
      "   | > Step:23/68  GlobalStep:80340  TotalLoss:0.00227  PostnetLoss:0.00110  DecoderLoss:0.00118  StopLoss:0.06410  GradNorm:0.00350  GradNormST:0.01600  AvgTextLen:18.3  AvgSpecLen:72.2  StepTime:0.34  LR:0.000100\n",
      "   | > Step:33/68  GlobalStep:80350  TotalLoss:0.00246  PostnetLoss:0.00119  DecoderLoss:0.00127  StopLoss:0.05642  GradNorm:0.00309  GradNormST:0.01142  AvgTextLen:25.9  AvgSpecLen:88.9  StepTime:0.53  LR:0.000100\n",
      "   | > Step:43/68  GlobalStep:80360  TotalLoss:0.00266  PostnetLoss:0.00128  DecoderLoss:0.00138  StopLoss:0.05803  GradNorm:0.01098  GradNormST:0.02322  AvgTextLen:33.4  AvgSpecLen:115.7  StepTime:0.54  LR:0.000100\n",
      "   | > Step:53/68  GlobalStep:80370  TotalLoss:0.00286  PostnetLoss:0.00137  DecoderLoss:0.00149  StopLoss:0.06043  GradNorm:0.00310  GradNormST:0.01385  AvgTextLen:42.4  AvgSpecLen:137.6  StepTime:0.63  LR:0.000100\n",
      "   | > Step:63/68  GlobalStep:80380  TotalLoss:0.00323  PostnetLoss:0.00155  DecoderLoss:0.00168  StopLoss:0.04310  GradNorm:0.00323  GradNormST:0.02091  AvgTextLen:58.6  AvgSpecLen:185.1  StepTime:0.91  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:80385  AvgTotalLoss:0.06806  AvgPostnetLoss:0.00119  AvgDecoderLoss:0.00128  AvgStopLoss:0.06560  EpochTime:42.37  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.09999   PostnetLoss: 0.00448   DecoderLoss:0.00491  StopLoss: 0.09061  \n",
      "   | > TotalLoss: 0.06956   PostnetLoss: 0.00624   DecoderLoss:0.00681  StopLoss: 0.05651  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00119   Validation Loss: 0.00567\n",
      "\n",
      " > Epoch 165/1000\n",
      "   | > Step:4/68  GlobalStep:80390  TotalLoss:0.00162  PostnetLoss:0.00078  DecoderLoss:0.00084  StopLoss:0.12250  GradNorm:0.00359  GradNormST:0.05401  AvgTextLen:4.8  AvgSpecLen:31.1  StepTime:0.27  LR:0.000100\n",
      "   | > Step:14/68  GlobalStep:80400  TotalLoss:0.00194  PostnetLoss:0.00094  DecoderLoss:0.00100  StopLoss:0.07807  GradNorm:0.00347  GradNormST:0.01911  AvgTextLen:12.2  AvgSpecLen:52.0  StepTime:0.35  LR:0.000100\n",
      "   | > Step:24/68  GlobalStep:80410  TotalLoss:0.00230  PostnetLoss:0.00111  DecoderLoss:0.00119  StopLoss:0.05773  GradNorm:0.00308  GradNormST:0.01688  AvgTextLen:19.2  AvgSpecLen:79.0  StepTime:0.50  LR:0.000100\n",
      "   | > Step:34/68  GlobalStep:80420  TotalLoss:0.00249  PostnetLoss:0.00120  DecoderLoss:0.00129  StopLoss:0.05145  GradNorm:0.00305  GradNormST:0.01934  AvgTextLen:26.2  AvgSpecLen:95.9  StepTime:0.61  LR:0.000100\n",
      "   | > Step:44/68  GlobalStep:80430  TotalLoss:0.00272  PostnetLoss:0.00131  DecoderLoss:0.00141  StopLoss:0.05202  GradNorm:0.00303  GradNormST:0.01857  AvgTextLen:33.8  AvgSpecLen:116.3  StepTime:0.65  LR:0.000100\n",
      "   | > Step:54/68  GlobalStep:80440  TotalLoss:0.00295  PostnetLoss:0.00141  DecoderLoss:0.00154  StopLoss:0.05513  GradNorm:0.00331  GradNormST:0.01973  AvgTextLen:42.2  AvgSpecLen:144.9  StepTime:0.68  LR:0.000100\n",
      "   | > Step:64/68  GlobalStep:80450  TotalLoss:0.00304  PostnetLoss:0.00145  DecoderLoss:0.00158  StopLoss:0.04557  GradNorm:0.00270  GradNormST:0.02164  AvgTextLen:60.6  AvgSpecLen:177.6  StepTime:0.80  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:80454  AvgTotalLoss:0.06711  AvgPostnetLoss:0.00118  AvgDecoderLoss:0.00127  AvgStopLoss:0.06465  EpochTime:41.97  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.09799   PostnetLoss: 0.00441   DecoderLoss:0.00484  StopLoss: 0.08874  \n",
      "   | > TotalLoss: 0.06845   PostnetLoss: 0.00620   DecoderLoss:0.00680  StopLoss: 0.05545  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00118   Validation Loss: 0.00567\n",
      "\n",
      " > Epoch 166/1000\n",
      "   | > Step:5/68  GlobalStep:80460  TotalLoss:0.00164  PostnetLoss:0.00079  DecoderLoss:0.00085  StopLoss:0.08971  GradNorm:0.00322  GradNormST:0.02907  AvgTextLen:5.9  AvgSpecLen:33.2  StepTime:0.25  LR:0.000100\n",
      "   | > Step:15/68  GlobalStep:80470  TotalLoss:0.00206  PostnetLoss:0.00100  DecoderLoss:0.00106  StopLoss:0.09150  GradNorm:0.00332  GradNormST:0.05943  AvgTextLen:12.8  AvgSpecLen:59.1  StepTime:0.59  LR:0.000100\n",
      "   | > Step:25/68  GlobalStep:80480  TotalLoss:0.00222  PostnetLoss:0.00108  DecoderLoss:0.00115  StopLoss:0.07024  GradNorm:0.00309  GradNormST:0.02027  AvgTextLen:19.8  AvgSpecLen:73.3  StepTime:0.46  LR:0.000100\n",
      "   | > Step:35/68  GlobalStep:80490  TotalLoss:0.00250  PostnetLoss:0.00121  DecoderLoss:0.00129  StopLoss:0.08965  GradNorm:0.00294  GradNormST:0.04799  AvgTextLen:27.1  AvgSpecLen:104.7  StepTime:0.75  LR:0.000100\n",
      "   | > Step:45/68  GlobalStep:80500  TotalLoss:0.00261  PostnetLoss:0.00125  DecoderLoss:0.00135  StopLoss:0.06408  GradNorm:0.00281  GradNormST:0.01794  AvgTextLen:34.4  AvgSpecLen:118.6  StepTime:0.57  LR:0.000100\n",
      "   | > Step:55/68  GlobalStep:80510  TotalLoss:0.00295  PostnetLoss:0.00142  DecoderLoss:0.00153  StopLoss:0.05550  GradNorm:0.00309  GradNormST:0.01375  AvgTextLen:43.9  AvgSpecLen:145.2  StepTime:0.67  LR:0.000100\n",
      "   | > Step:65/68  GlobalStep:80520  TotalLoss:0.00327  PostnetLoss:0.00157  DecoderLoss:0.00170  StopLoss:0.06618  GradNorm:0.00452  GradNormST:0.04103  AvgTextLen:64.2  AvgSpecLen:207.4  StepTime:1.19  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:80523  AvgTotalLoss:0.07445  AvgPostnetLoss:0.00118  AvgDecoderLoss:0.00127  AvgStopLoss:0.07201  EpochTime:42.00  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.09883   PostnetLoss: 0.00447   DecoderLoss:0.00492  StopLoss: 0.08944  \n",
      "   | > TotalLoss: 0.06768   PostnetLoss: 0.00625   DecoderLoss:0.00684  StopLoss: 0.05459  \n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00118   Validation Loss: 0.00566\n",
      "\n",
      " > Epoch 167/1000\n",
      "   | > Step:6/68  GlobalStep:80530  TotalLoss:0.00169  PostnetLoss:0.00082  DecoderLoss:0.00087  StopLoss:0.05809  GradNorm:0.00387  GradNormST:0.03032  AvgTextLen:6.0  AvgSpecLen:34.8  StepTime:0.30  LR:0.000100\n",
      "   | > Step:16/68  GlobalStep:80540  TotalLoss:0.00204  PostnetLoss:0.00099  DecoderLoss:0.00105  StopLoss:0.06904  GradNorm:0.00331  GradNormST:0.02493  AvgTextLen:13.8  AvgSpecLen:60.6  StepTime:0.29  LR:0.000100\n",
      "   | > Step:26/68  GlobalStep:80550  TotalLoss:0.00229  PostnetLoss:0.00111  DecoderLoss:0.00118  StopLoss:0.05389  GradNorm:0.00323  GradNormST:0.01790  AvgTextLen:20.3  AvgSpecLen:77.5  StepTime:0.47  LR:0.000100\n",
      "   | > Step:36/68  GlobalStep:80560  TotalLoss:0.00250  PostnetLoss:0.00120  DecoderLoss:0.00130  StopLoss:0.06976  GradNorm:0.00307  GradNormST:0.01699  AvgTextLen:28.5  AvgSpecLen:100.2  StepTime:0.68  LR:0.000100\n",
      "   | > Step:46/68  GlobalStep:80570  TotalLoss:0.00267  PostnetLoss:0.00128  DecoderLoss:0.00138  StopLoss:0.07571  GradNorm:0.00295  GradNormST:0.03087  AvgTextLen:35.4  AvgSpecLen:122.6  StepTime:0.81  LR:0.000100\n",
      "   | > Step:56/68  GlobalStep:80580  TotalLoss:0.00293  PostnetLoss:0.00141  DecoderLoss:0.00152  StopLoss:0.05964  GradNorm:0.00312  GradNormST:0.03291  AvgTextLen:45.8  AvgSpecLen:145.4  StepTime:1.04  LR:0.000100\n",
      "   | > Step:66/68  GlobalStep:80590  TotalLoss:0.00340  PostnetLoss:0.00162  DecoderLoss:0.00178  StopLoss:0.05759  GradNorm:0.00266  GradNormST:0.02189  AvgTextLen:71.2  AvgSpecLen:220.8  StepTime:1.05  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:80592  AvgTotalLoss:0.07067  AvgPostnetLoss:0.00117  AvgDecoderLoss:0.00126  AvgStopLoss:0.06823  EpochTime:42.96  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.09888   PostnetLoss: 0.00448   DecoderLoss:0.00492  StopLoss: 0.08947  \n",
      "   | > TotalLoss: 0.06943   PostnetLoss: 0.00618   DecoderLoss:0.00677  StopLoss: 0.05648  \n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00117   Validation Loss: 0.00566\n",
      "\n",
      " > Epoch 168/1000\n",
      "   | > Step:7/68  GlobalStep:80600  TotalLoss:0.00182  PostnetLoss:0.00088  DecoderLoss:0.00094  StopLoss:0.05689  GradNorm:0.00402  GradNormST:0.02105  AvgTextLen:7.4  AvgSpecLen:44.6  StepTime:0.40  LR:0.000100\n",
      "   | > Step:17/68  GlobalStep:80610  TotalLoss:0.00200  PostnetLoss:0.00097  DecoderLoss:0.00103  StopLoss:0.08869  GradNorm:0.00334  GradNormST:0.01875  AvgTextLen:14.7  AvgSpecLen:66.0  StepTime:0.48  LR:0.000100\n",
      "   | > Step:27/68  GlobalStep:80620  TotalLoss:0.00236  PostnetLoss:0.00114  DecoderLoss:0.00122  StopLoss:0.08769  GradNorm:0.00334  GradNormST:0.04630  AvgTextLen:21.0  AvgSpecLen:80.2  StepTime:0.53  LR:0.000100\n",
      "   | > Step:37/68  GlobalStep:80630  TotalLoss:0.00255  PostnetLoss:0.00123  DecoderLoss:0.00132  StopLoss:0.05459  GradNorm:0.00298  GradNormST:0.01376  AvgTextLen:29.3  AvgSpecLen:102.4  StepTime:0.60  LR:0.000100\n",
      "   | > Step:47/68  GlobalStep:80640  TotalLoss:0.00278  PostnetLoss:0.00134  DecoderLoss:0.00144  StopLoss:0.06456  GradNorm:0.00279  GradNormST:0.01790  AvgTextLen:36.7  AvgSpecLen:126.9  StepTime:0.70  LR:0.000100\n",
      "   | > Step:57/68  GlobalStep:80650  TotalLoss:0.00299  PostnetLoss:0.00144  DecoderLoss:0.00155  StopLoss:0.04743  GradNorm:0.00283  GradNormST:0.01272  AvgTextLen:47.1  AvgSpecLen:161.0  StepTime:0.73  LR:0.000100\n",
      "   | > Step:67/68  GlobalStep:80660  TotalLoss:0.00341  PostnetLoss:0.00163  DecoderLoss:0.00178  StopLoss:0.04813  GradNorm:0.00243  GradNormST:0.02497  AvgTextLen:80.6  AvgSpecLen:246.5  StepTime:1.22  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:80661  AvgTotalLoss:0.06855  AvgPostnetLoss:0.00117  AvgDecoderLoss:0.00126  AvgStopLoss:0.06612  EpochTime:42.12  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.10218   PostnetLoss: 0.00448   DecoderLoss:0.00492  StopLoss: 0.09278  \n",
      "   | > TotalLoss: 0.06963   PostnetLoss: 0.00632   DecoderLoss:0.00690  StopLoss: 0.05641  \n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00117   Validation Loss: 0.00572\n",
      "\n",
      " > Epoch 169/1000\n",
      "   | > Step:8/68  GlobalStep:80670  TotalLoss:0.00183  PostnetLoss:0.00089  DecoderLoss:0.00094  StopLoss:0.09144  GradNorm:0.00378  GradNormST:0.01870  AvgTextLen:8.3  AvgSpecLen:44.0  StepTime:0.24  LR:0.000100\n",
      "   | > Step:18/68  GlobalStep:80680  TotalLoss:0.00203  PostnetLoss:0.00098  DecoderLoss:0.00105  StopLoss:0.07817  GradNorm:0.00325  GradNormST:0.01989  AvgTextLen:15.2  AvgSpecLen:64.5  StepTime:0.32  LR:0.000100\n",
      "   | > Step:28/68  GlobalStep:80690  TotalLoss:0.00232  PostnetLoss:0.00112  DecoderLoss:0.00120  StopLoss:0.07408  GradNorm:0.00310  GradNormST:0.01596  AvgTextLen:22.1  AvgSpecLen:85.4  StepTime:0.47  LR:0.000100\n",
      "   | > Step:38/68  GlobalStep:80700  TotalLoss:0.00248  PostnetLoss:0.00120  DecoderLoss:0.00128  StopLoss:0.06927  GradNorm:0.00296  GradNormST:0.02236  AvgTextLen:29.4  AvgSpecLen:104.1  StepTime:0.61  LR:0.000100\n",
      "   | > Step:48/68  GlobalStep:80710  TotalLoss:0.00273  PostnetLoss:0.00132  DecoderLoss:0.00142  StopLoss:0.05232  GradNorm:0.00295  GradNormST:0.01405  AvgTextLen:37.5  AvgSpecLen:123.8  StepTime:0.70  LR:0.000100\n",
      "   | > Step:58/68  GlobalStep:80720  TotalLoss:0.00296  PostnetLoss:0.00143  DecoderLoss:0.00154  StopLoss:0.06006  GradNorm:0.00272  GradNormST:0.01617  AvgTextLen:47.8  AvgSpecLen:158.0  StepTime:0.76  LR:0.000100\n",
      "   | > Step:68/68  GlobalStep:80730  TotalLoss:0.00350  PostnetLoss:0.00168  DecoderLoss:0.00183  StopLoss:0.04659  GradNorm:0.00859  GradNormST:0.02323  AvgTextLen:103.0  AvgSpecLen:308.9  StepTime:1.86  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:80730  AvgTotalLoss:0.06576  AvgPostnetLoss:0.00117  AvgDecoderLoss:0.00126  AvgStopLoss:0.06333  EpochTime:42.51  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.10374   PostnetLoss: 0.00453   DecoderLoss:0.00497  StopLoss: 0.09424  \n",
      "   | > TotalLoss: 0.06908   PostnetLoss: 0.00628   DecoderLoss:0.00688  StopLoss: 0.05593  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00117   Validation Loss: 0.00574\n",
      "\n",
      " > Epoch 170/1000\n",
      "   | > Step:9/68  GlobalStep:80740  TotalLoss:0.00175  PostnetLoss:0.00085  DecoderLoss:0.00090  StopLoss:0.05710  GradNorm:0.00329  GradNormST:0.02431  AvgTextLen:9.0  AvgSpecLen:42.1  StepTime:0.34  LR:0.000100\n",
      "   | > Step:19/68  GlobalStep:80750  TotalLoss:0.00206  PostnetLoss:0.00100  DecoderLoss:0.00106  StopLoss:0.10659  GradNorm:0.00322  GradNormST:0.02292  AvgTextLen:15.5  AvgSpecLen:63.2  StepTime:0.43  LR:0.000100\n",
      "   | > Step:29/68  GlobalStep:80760  TotalLoss:0.00228  PostnetLoss:0.00110  DecoderLoss:0.00118  StopLoss:0.07422  GradNorm:0.00300  GradNormST:0.01778  AvgTextLen:21.8  AvgSpecLen:83.4  StepTime:0.58  LR:0.000100\n",
      "   | > Step:39/68  GlobalStep:80770  TotalLoss:0.00257  PostnetLoss:0.00124  DecoderLoss:0.00133  StopLoss:0.05225  GradNorm:0.00314  GradNormST:0.02151  AvgTextLen:30.2  AvgSpecLen:110.4  StepTime:0.58  LR:0.000100\n",
      "   | > Step:49/68  GlobalStep:80780  TotalLoss:0.00276  PostnetLoss:0.00133  DecoderLoss:0.00143  StopLoss:0.04563  GradNorm:0.00303  GradNormST:0.01328  AvgTextLen:38.2  AvgSpecLen:127.7  StepTime:0.70  LR:0.000100\n",
      "   | > Step:59/68  GlobalStep:80790  TotalLoss:0.00297  PostnetLoss:0.00142  DecoderLoss:0.00155  StopLoss:0.05225  GradNorm:0.00266  GradNormST:0.01524  AvgTextLen:50.5  AvgSpecLen:155.9  StepTime:0.98  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:80799  AvgTotalLoss:0.06653  AvgPostnetLoss:0.00117  AvgDecoderLoss:0.00125  AvgStopLoss:0.06411  EpochTime:42.96  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.10352   PostnetLoss: 0.00449   DecoderLoss:0.00493  StopLoss: 0.09409  \n",
      "   | > TotalLoss: 0.06936   PostnetLoss: 0.00634   DecoderLoss:0.00694  StopLoss: 0.05608  \n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00117   Validation Loss: 0.00580\n",
      "\n",
      " > Epoch 171/1000\n",
      "   | > Step:0/68  GlobalStep:80800  TotalLoss:0.00164  PostnetLoss:0.00079  DecoderLoss:0.00085  StopLoss:0.11347  GradNorm:0.00484  GradNormST:0.03698  AvgTextLen:3.0  AvgSpecLen:25.3  StepTime:0.47  LR:0.000100\n",
      "   | > Step:10/68  GlobalStep:80810  TotalLoss:0.00183  PostnetLoss:0.00089  DecoderLoss:0.00094  StopLoss:0.08420  GradNorm:0.00346  GradNormST:0.03259  AvgTextLen:9.8  AvgSpecLen:48.6  StepTime:0.43  LR:0.000100\n",
      "   | > Step:20/68  GlobalStep:80820  TotalLoss:0.00216  PostnetLoss:0.00105  DecoderLoss:0.00111  StopLoss:0.06304  GradNorm:0.00344  GradNormST:0.03398  AvgTextLen:16.4  AvgSpecLen:64.8  StepTime:0.50  LR:0.000100\n",
      "   | > Step:30/68  GlobalStep:80830  TotalLoss:0.00233  PostnetLoss:0.00113  DecoderLoss:0.00121  StopLoss:0.06371  GradNorm:0.00302  GradNormST:0.01300  AvgTextLen:22.9  AvgSpecLen:85.8  StepTime:0.50  LR:0.000100\n",
      "   | > Step:40/68  GlobalStep:80840  TotalLoss:0.00249  PostnetLoss:0.00120  DecoderLoss:0.00129  StopLoss:0.04934  GradNorm:0.00303  GradNormST:0.01056  AvgTextLen:30.9  AvgSpecLen:105.3  StepTime:0.63  LR:0.000100\n",
      "   | > Step:50/68  GlobalStep:80850  TotalLoss:0.00272  PostnetLoss:0.00131  DecoderLoss:0.00141  StopLoss:0.04974  GradNorm:0.00287  GradNormST:0.01395  AvgTextLen:38.8  AvgSpecLen:132.0  StepTime:0.67  LR:0.000100\n",
      "   | > Step:60/68  GlobalStep:80860  TotalLoss:0.00302  PostnetLoss:0.00145  DecoderLoss:0.00157  StopLoss:0.04479  GradNorm:0.00279  GradNormST:0.01246  AvgTextLen:52.0  AvgSpecLen:172.7  StepTime:0.79  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:80868  AvgTotalLoss:0.06572  AvgPostnetLoss:0.00116  AvgDecoderLoss:0.00125  AvgStopLoss:0.06331  EpochTime:41.36  AvgStepTime:0.60\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.10100   PostnetLoss: 0.00455   DecoderLoss:0.00500  StopLoss: 0.09144  \n",
      "   | > TotalLoss: 0.06827   PostnetLoss: 0.00627   DecoderLoss:0.00687  StopLoss: 0.05512  \n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00116   Validation Loss: 0.00575\n",
      "\n",
      " > Epoch 172/1000\n",
      "   | > Step:1/68  GlobalStep:80870  TotalLoss:0.00166  PostnetLoss:0.00080  DecoderLoss:0.00086  StopLoss:0.06753  GradNorm:0.00412  GradNormST:0.02409  AvgTextLen:3.7  AvgSpecLen:28.6  StepTime:0.28  LR:0.000100\n",
      "   | > Step:11/68  GlobalStep:80880  TotalLoss:0.00182  PostnetLoss:0.00089  DecoderLoss:0.00094  StopLoss:0.05874  GradNorm:0.00324  GradNormST:0.01827  AvgTextLen:10.3  AvgSpecLen:49.4  StepTime:0.34  LR:0.000100\n",
      "   | > Step:21/68  GlobalStep:80890  TotalLoss:0.00210  PostnetLoss:0.00102  DecoderLoss:0.00109  StopLoss:0.07760  GradNorm:0.00312  GradNormST:0.02979  AvgTextLen:16.6  AvgSpecLen:66.3  StepTime:0.43  LR:0.000100\n",
      "   | > Step:31/68  GlobalStep:80900  TotalLoss:0.00232  PostnetLoss:0.00112  DecoderLoss:0.00120  StopLoss:0.05693  GradNorm:0.00316  GradNormST:0.01589  AvgTextLen:24.1  AvgSpecLen:82.7  StepTime:0.53  LR:0.000100\n",
      "   | > Step:41/68  GlobalStep:80910  TotalLoss:0.00265  PostnetLoss:0.00128  DecoderLoss:0.00137  StopLoss:0.04187  GradNorm:0.00292  GradNormST:0.00963  AvgTextLen:32.3  AvgSpecLen:109.4  StepTime:0.64  LR:0.000100\n",
      "   | > Step:51/68  GlobalStep:80920  TotalLoss:0.00272  PostnetLoss:0.00131  DecoderLoss:0.00141  StopLoss:0.05189  GradNorm:0.00284  GradNormST:0.01378  AvgTextLen:39.2  AvgSpecLen:126.9  StepTime:0.72  LR:0.000100\n",
      "   | > Step:61/68  GlobalStep:80930  TotalLoss:0.00299  PostnetLoss:0.00143  DecoderLoss:0.00156  StopLoss:0.03559  GradNorm:0.00272  GradNormST:0.01013  AvgTextLen:54.6  AvgSpecLen:165.5  StepTime:0.90  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:80937  AvgTotalLoss:0.06395  AvgPostnetLoss:0.00116  AvgDecoderLoss:0.00125  AvgStopLoss:0.06153  EpochTime:42.64  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.09748   PostnetLoss: 0.00452   DecoderLoss:0.00497  StopLoss: 0.08798  \n",
      "   | > TotalLoss: 0.06814   PostnetLoss: 0.00633   DecoderLoss:0.00694  StopLoss: 0.05488  \n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00116   Validation Loss: 0.00574\n",
      "\n",
      " > Epoch 173/1000\n",
      "   | > Step:2/68  GlobalStep:80940  TotalLoss:0.00154  PostnetLoss:0.00075  DecoderLoss:0.00080  StopLoss:0.07812  GradNorm:0.00396  GradNormST:0.02266  AvgTextLen:4.0  AvgSpecLen:31.3  StepTime:0.46  LR:0.000100\n",
      "   | > Step:12/68  GlobalStep:80950  TotalLoss:0.00208  PostnetLoss:0.00100  DecoderLoss:0.00108  StopLoss:0.06722  GradNorm:0.00424  GradNormST:0.01794  AvgTextLen:11.0  AvgSpecLen:49.3  StepTime:0.44  LR:0.000100\n",
      "   | > Step:22/68  GlobalStep:80960  TotalLoss:0.00240  PostnetLoss:0.00116  DecoderLoss:0.00124  StopLoss:0.08493  GradNorm:0.00438  GradNormST:0.03609  AvgTextLen:18.0  AvgSpecLen:72.0  StepTime:0.46  LR:0.000100\n",
      "   | > Step:32/68  GlobalStep:80970  TotalLoss:0.00266  PostnetLoss:0.00128  DecoderLoss:0.00138  StopLoss:0.08596  GradNorm:0.00380  GradNormST:0.02671  AvgTextLen:24.4  AvgSpecLen:91.5  StepTime:0.57  LR:0.000100\n",
      "   | > Step:42/68  GlobalStep:80980  TotalLoss:0.00290  PostnetLoss:0.00139  DecoderLoss:0.00151  StopLoss:0.06878  GradNorm:0.00362  GradNormST:0.01829  AvgTextLen:33.1  AvgSpecLen:109.7  StepTime:0.59  LR:0.000100\n",
      "   | > Step:52/68  GlobalStep:80990  TotalLoss:0.00303  PostnetLoss:0.00146  DecoderLoss:0.00157  StopLoss:0.06082  GradNorm:0.00376  GradNormST:0.01647  AvgTextLen:42.1  AvgSpecLen:137.5  StepTime:0.83  LR:0.000100\n",
      "   | > Step:62/68  GlobalStep:81000  TotalLoss:0.00326  PostnetLoss:0.00157  DecoderLoss:0.00170  StopLoss:0.04901  GradNorm:0.00318  GradNormST:0.01489  AvgTextLen:54.0  AvgSpecLen:174.7  StepTime:0.75  LR:0.000100\n",
      " | | > Checkpoint saving : /vm/TTS/models/cartman_models/mozilla-no-loc-fattn-stopnet-sigmoid-loss_masking-August-15-2019_11+26AM-de6aeb9/checkpoint_81000.pth.tar\n",
      "   | > EPOCH END -- GlobalStep:81006  AvgTotalLoss:0.06935  AvgPostnetLoss:0.00128  AvgDecoderLoss:0.00138  AvgStopLoss:0.06669  EpochTime:42.69  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.09456   PostnetLoss: 0.00443   DecoderLoss:0.00491  StopLoss: 0.08522  \n",
      "   | > TotalLoss: 0.06797   PostnetLoss: 0.00623   DecoderLoss:0.00688  StopLoss: 0.05486  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00128   Validation Loss: 0.00565\n",
      "\n",
      " > Epoch 174/1000\n",
      "   | > Step:3/68  GlobalStep:81010  TotalLoss:0.00171  PostnetLoss:0.00083  DecoderLoss:0.00088  StopLoss:0.08796  GradNorm:0.00456  GradNormST:0.03205  AvgTextLen:4.7  AvgSpecLen:29.7  StepTime:0.34  LR:0.000100\n",
      "   | > Step:13/68  GlobalStep:81020  TotalLoss:0.00201  PostnetLoss:0.00098  DecoderLoss:0.00104  StopLoss:0.09399  GradNorm:0.00401  GradNormST:0.02578  AvgTextLen:11.5  AvgSpecLen:55.2  StepTime:0.33  LR:0.000100\n",
      "   | > Step:23/68  GlobalStep:81030  TotalLoss:0.00228  PostnetLoss:0.00110  DecoderLoss:0.00118  StopLoss:0.06326  GradNorm:0.00349  GradNormST:0.01371  AvgTextLen:18.3  AvgSpecLen:72.2  StepTime:0.45  LR:0.000100\n",
      "   | > Step:33/68  GlobalStep:81040  TotalLoss:0.00250  PostnetLoss:0.00121  DecoderLoss:0.00129  StopLoss:0.05347  GradNorm:0.00333  GradNormST:0.01684  AvgTextLen:25.9  AvgSpecLen:88.9  StepTime:0.54  LR:0.000100\n",
      "   | > Step:43/68  GlobalStep:81050  TotalLoss:0.00270  PostnetLoss:0.00130  DecoderLoss:0.00140  StopLoss:0.06436  GradNorm:0.00310  GradNormST:0.02006  AvgTextLen:33.4  AvgSpecLen:115.7  StepTime:0.61  LR:0.000100\n",
      "   | > Step:53/68  GlobalStep:81060  TotalLoss:0.00289  PostnetLoss:0.00139  DecoderLoss:0.00150  StopLoss:0.04939  GradNorm:0.00315  GradNormST:0.01230  AvgTextLen:42.4  AvgSpecLen:137.6  StepTime:0.65  LR:0.000100\n",
      "   | > Step:63/68  GlobalStep:81070  TotalLoss:0.00325  PostnetLoss:0.00156  DecoderLoss:0.00169  StopLoss:0.04331  GradNorm:0.00297  GradNormST:0.02265  AvgTextLen:58.6  AvgSpecLen:185.1  StepTime:0.91  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:81075  AvgTotalLoss:0.06803  AvgPostnetLoss:0.00122  AvgDecoderLoss:0.00131  AvgStopLoss:0.06549  EpochTime:40.98  AvgStepTime:0.59\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.10119   PostnetLoss: 0.00446   DecoderLoss:0.00494  StopLoss: 0.09180  \n",
      "   | > TotalLoss: 0.06976   PostnetLoss: 0.00625   DecoderLoss:0.00687  StopLoss: 0.05664  \n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00122   Validation Loss: 0.00567\n",
      "\n",
      " > Epoch 175/1000\n",
      "   | > Step:4/68  GlobalStep:81080  TotalLoss:0.00161  PostnetLoss:0.00078  DecoderLoss:0.00083  StopLoss:0.12413  GradNorm:0.00368  GradNormST:0.03976  AvgTextLen:4.8  AvgSpecLen:31.1  StepTime:0.25  LR:0.000100\n",
      "   | > Step:14/68  GlobalStep:81090  TotalLoss:0.00195  PostnetLoss:0.00095  DecoderLoss:0.00101  StopLoss:0.07905  GradNorm:0.00339  GradNormST:0.02463  AvgTextLen:12.2  AvgSpecLen:52.0  StepTime:0.32  LR:0.000100\n",
      "   | > Step:24/68  GlobalStep:81100  TotalLoss:0.00239  PostnetLoss:0.00116  DecoderLoss:0.00124  StopLoss:0.06701  GradNorm:0.00324  GradNormST:0.01744  AvgTextLen:19.2  AvgSpecLen:79.0  StepTime:0.51  LR:0.000100\n",
      "   | > Step:34/68  GlobalStep:81110  TotalLoss:0.00255  PostnetLoss:0.00123  DecoderLoss:0.00132  StopLoss:0.04913  GradNorm:0.00340  GradNormST:0.01483  AvgTextLen:26.2  AvgSpecLen:95.9  StepTime:0.49  LR:0.000100\n",
      "   | > Step:44/68  GlobalStep:81120  TotalLoss:0.00272  PostnetLoss:0.00131  DecoderLoss:0.00141  StopLoss:0.04813  GradNorm:0.00309  GradNormST:0.01244  AvgTextLen:33.8  AvgSpecLen:116.3  StepTime:0.69  LR:0.000100\n",
      "   | > Step:54/68  GlobalStep:81130  TotalLoss:0.00296  PostnetLoss:0.00142  DecoderLoss:0.00154  StopLoss:0.05806  GradNorm:0.00323  GradNormST:0.01846  AvgTextLen:42.2  AvgSpecLen:144.9  StepTime:0.81  LR:0.000100\n",
      "   | > Step:64/68  GlobalStep:81140  TotalLoss:0.00304  PostnetLoss:0.00146  DecoderLoss:0.00159  StopLoss:0.05053  GradNorm:0.00294  GradNormST:0.01605  AvgTextLen:60.6  AvgSpecLen:177.6  StepTime:0.81  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:81144  AvgTotalLoss:0.06272  AvgPostnetLoss:0.00120  AvgDecoderLoss:0.00129  AvgStopLoss:0.06023  EpochTime:42.39  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.10039   PostnetLoss: 0.00442   DecoderLoss:0.00490  StopLoss: 0.09106  \n",
      "   | > TotalLoss: 0.07191   PostnetLoss: 0.00629   DecoderLoss:0.00693  StopLoss: 0.05868  \n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00120   Validation Loss: 0.00570\n",
      "\n",
      " > Epoch 176/1000\n",
      "   | > Step:5/68  GlobalStep:81150  TotalLoss:0.00169  PostnetLoss:0.00082  DecoderLoss:0.00087  StopLoss:0.07096  GradNorm:0.00343  GradNormST:0.02049  AvgTextLen:5.9  AvgSpecLen:33.2  StepTime:0.38  LR:0.000100\n",
      "   | > Step:15/68  GlobalStep:81160  TotalLoss:0.00206  PostnetLoss:0.00100  DecoderLoss:0.00106  StopLoss:0.06680  GradNorm:0.00339  GradNormST:0.03224  AvgTextLen:12.8  AvgSpecLen:59.1  StepTime:0.54  LR:0.000100\n",
      "   | > Step:25/68  GlobalStep:81170  TotalLoss:0.00225  PostnetLoss:0.00109  DecoderLoss:0.00116  StopLoss:0.05957  GradNorm:0.00330  GradNormST:0.01698  AvgTextLen:19.8  AvgSpecLen:73.3  StepTime:0.47  LR:0.000100\n",
      "   | > Step:35/68  GlobalStep:81180  TotalLoss:0.00257  PostnetLoss:0.00124  DecoderLoss:0.00133  StopLoss:0.07062  GradNorm:0.00313  GradNormST:0.02619  AvgTextLen:27.1  AvgSpecLen:104.7  StepTime:0.80  LR:0.000100\n",
      "   | > Step:45/68  GlobalStep:81190  TotalLoss:0.00265  PostnetLoss:0.00127  DecoderLoss:0.00138  StopLoss:0.06225  GradNorm:0.00279  GradNormST:0.02287  AvgTextLen:34.4  AvgSpecLen:118.6  StepTime:0.54  LR:0.000100\n",
      "   | > Step:55/68  GlobalStep:81200  TotalLoss:0.00299  PostnetLoss:0.00144  DecoderLoss:0.00155  StopLoss:0.04626  GradNorm:0.00308  GradNormST:0.01226  AvgTextLen:43.9  AvgSpecLen:145.2  StepTime:0.66  LR:0.000100\n",
      "   | > Step:65/68  GlobalStep:81210  TotalLoss:0.00327  PostnetLoss:0.00157  DecoderLoss:0.00170  StopLoss:0.04061  GradNorm:0.00289  GradNormST:0.01550  AvgTextLen:64.2  AvgSpecLen:207.4  StepTime:1.19  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:81213  AvgTotalLoss:0.06311  AvgPostnetLoss:0.00119  AvgDecoderLoss:0.00127  AvgStopLoss:0.06065  EpochTime:42.00  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.10599   PostnetLoss: 0.00449   DecoderLoss:0.00496  StopLoss: 0.09654  \n",
      "   | > TotalLoss: 0.07003   PostnetLoss: 0.00633   DecoderLoss:0.00696  StopLoss: 0.05673  \n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00119   Validation Loss: 0.00575\n",
      "\n",
      " > Epoch 177/1000\n",
      "   | > Step:6/68  GlobalStep:81220  TotalLoss:0.00170  PostnetLoss:0.00082  DecoderLoss:0.00087  StopLoss:0.05392  GradNorm:0.00414  GradNormST:0.01904  AvgTextLen:6.0  AvgSpecLen:34.8  StepTime:0.40  LR:0.000100\n",
      "   | > Step:16/68  GlobalStep:81230  TotalLoss:0.00206  PostnetLoss:0.00100  DecoderLoss:0.00107  StopLoss:0.04594  GradNorm:0.00342  GradNormST:0.01336  AvgTextLen:13.8  AvgSpecLen:60.6  StepTime:0.29  LR:0.000100\n",
      "   | > Step:26/68  GlobalStep:81240  TotalLoss:0.00230  PostnetLoss:0.00111  DecoderLoss:0.00119  StopLoss:0.05570  GradNorm:0.00313  GradNormST:0.01241  AvgTextLen:20.3  AvgSpecLen:77.5  StepTime:0.50  LR:0.000100\n",
      "   | > Step:36/68  GlobalStep:81250  TotalLoss:0.00247  PostnetLoss:0.00119  DecoderLoss:0.00128  StopLoss:0.06867  GradNorm:0.00296  GradNormST:0.01502  AvgTextLen:28.5  AvgSpecLen:100.2  StepTime:0.57  LR:0.000100\n",
      "   | > Step:46/68  GlobalStep:81260  TotalLoss:0.00268  PostnetLoss:0.00129  DecoderLoss:0.00139  StopLoss:0.05620  GradNorm:0.00296  GradNormST:0.02325  AvgTextLen:35.4  AvgSpecLen:122.6  StepTime:0.77  LR:0.000100\n",
      "   | > Step:56/68  GlobalStep:81270  TotalLoss:0.00293  PostnetLoss:0.00141  DecoderLoss:0.00152  StopLoss:0.04759  GradNorm:0.00292  GradNormST:0.01932  AvgTextLen:45.8  AvgSpecLen:145.4  StepTime:0.96  LR:0.000100\n",
      "   | > Step:66/68  GlobalStep:81280  TotalLoss:0.00345  PostnetLoss:0.00165  DecoderLoss:0.00180  StopLoss:0.05136  GradNorm:0.00278  GradNormST:0.01813  AvgTextLen:71.2  AvgSpecLen:220.8  StepTime:1.10  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:81282  AvgTotalLoss:0.06217  AvgPostnetLoss:0.00118  AvgDecoderLoss:0.00127  AvgStopLoss:0.05972  EpochTime:43.02  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.10411   PostnetLoss: 0.00452   DecoderLoss:0.00499  StopLoss: 0.09460  \n",
      "   | > TotalLoss: 0.07259   PostnetLoss: 0.00640   DecoderLoss:0.00703  StopLoss: 0.05917  \n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00118   Validation Loss: 0.00580\n",
      "\n",
      " > Epoch 178/1000\n",
      "   | > Step:7/68  GlobalStep:81290  TotalLoss:0.00177  PostnetLoss:0.00085  DecoderLoss:0.00091  StopLoss:0.05205  GradNorm:0.00349  GradNormST:0.01451  AvgTextLen:7.4  AvgSpecLen:44.6  StepTime:0.38  LR:0.000100\n",
      "   | > Step:17/68  GlobalStep:81300  TotalLoss:0.00203  PostnetLoss:0.00098  DecoderLoss:0.00105  StopLoss:0.07304  GradNorm:0.00323  GradNormST:0.01626  AvgTextLen:14.7  AvgSpecLen:66.0  StepTime:0.45  LR:0.000100\n",
      "   | > Step:27/68  GlobalStep:81310  TotalLoss:0.00233  PostnetLoss:0.00113  DecoderLoss:0.00121  StopLoss:0.05895  GradNorm:0.00300  GradNormST:0.01569  AvgTextLen:21.0  AvgSpecLen:80.2  StepTime:0.62  LR:0.000100\n",
      "   | > Step:37/68  GlobalStep:81320  TotalLoss:0.00256  PostnetLoss:0.00123  DecoderLoss:0.00132  StopLoss:0.05606  GradNorm:0.00302  GradNormST:0.01398  AvgTextLen:29.3  AvgSpecLen:102.4  StepTime:0.49  LR:0.000100\n",
      "   | > Step:47/68  GlobalStep:81330  TotalLoss:0.00277  PostnetLoss:0.00133  DecoderLoss:0.00144  StopLoss:0.05109  GradNorm:0.00296  GradNormST:0.02147  AvgTextLen:36.7  AvgSpecLen:126.9  StepTime:0.70  LR:0.000100\n",
      "   | > Step:57/68  GlobalStep:81340  TotalLoss:0.00299  PostnetLoss:0.00144  DecoderLoss:0.00156  StopLoss:0.04146  GradNorm:0.00333  GradNormST:0.01005  AvgTextLen:47.1  AvgSpecLen:161.0  StepTime:0.83  LR:0.000100\n",
      "   | > Step:67/68  GlobalStep:81350  TotalLoss:0.00346  PostnetLoss:0.00166  DecoderLoss:0.00181  StopLoss:0.04654  GradNorm:0.00307  GradNormST:0.02495  AvgTextLen:80.6  AvgSpecLen:246.5  StepTime:1.22  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:81351  AvgTotalLoss:0.06154  AvgPostnetLoss:0.00118  AvgDecoderLoss:0.00127  AvgStopLoss:0.05910  EpochTime:42.84  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.10400   PostnetLoss: 0.00452   DecoderLoss:0.00499  StopLoss: 0.09449  \n",
      "   | > TotalLoss: 0.07082   PostnetLoss: 0.00630   DecoderLoss:0.00692  StopLoss: 0.05760  \n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00118   Validation Loss: 0.00574\n",
      "\n",
      " > Epoch 179/1000\n",
      "   | > Step:8/68  GlobalStep:81360  TotalLoss:0.00181  PostnetLoss:0.00088  DecoderLoss:0.00093  StopLoss:0.07859  GradNorm:0.00354  GradNormST:0.02220  AvgTextLen:8.3  AvgSpecLen:44.0  StepTime:0.25  LR:0.000100\n",
      "   | > Step:18/68  GlobalStep:81370  TotalLoss:0.00200  PostnetLoss:0.00097  DecoderLoss:0.00103  StopLoss:0.06822  GradNorm:0.00324  GradNormST:0.02186  AvgTextLen:15.2  AvgSpecLen:64.5  StepTime:0.40  LR:0.000100\n",
      "   | > Step:28/68  GlobalStep:81380  TotalLoss:0.00235  PostnetLoss:0.00114  DecoderLoss:0.00121  StopLoss:0.06791  GradNorm:0.00317  GradNormST:0.01745  AvgTextLen:22.1  AvgSpecLen:85.4  StepTime:0.52  LR:0.000100\n",
      "   | > Step:38/68  GlobalStep:81390  TotalLoss:0.00250  PostnetLoss:0.00120  DecoderLoss:0.00130  StopLoss:0.04258  GradNorm:0.00295  GradNormST:0.01333  AvgTextLen:29.4  AvgSpecLen:104.1  StepTime:0.50  LR:0.000100\n",
      "   | > Step:48/68  GlobalStep:81400  TotalLoss:0.00275  PostnetLoss:0.00133  DecoderLoss:0.00143  StopLoss:0.04407  GradNorm:0.00285  GradNormST:0.01121  AvgTextLen:37.5  AvgSpecLen:123.8  StepTime:0.81  LR:0.000100\n",
      "   | > Step:58/68  GlobalStep:81410  TotalLoss:0.00298  PostnetLoss:0.00143  DecoderLoss:0.00154  StopLoss:0.05781  GradNorm:0.00271  GradNormST:0.01900  AvgTextLen:47.8  AvgSpecLen:158.0  StepTime:0.82  LR:0.000100\n",
      "   | > Step:68/68  GlobalStep:81420  TotalLoss:0.00357  PostnetLoss:0.00170  DecoderLoss:0.00187  StopLoss:0.04648  GradNorm:0.00346  GradNormST:0.02106  AvgTextLen:103.0  AvgSpecLen:308.9  StepTime:1.87  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:81420  AvgTotalLoss:0.06113  AvgPostnetLoss:0.00117  AvgDecoderLoss:0.00126  AvgStopLoss:0.05870  EpochTime:41.90  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.10271   PostnetLoss: 0.00457   DecoderLoss:0.00504  StopLoss: 0.09310  \n",
      "   | > TotalLoss: 0.07127   PostnetLoss: 0.00630   DecoderLoss:0.00693  StopLoss: 0.05803  \n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00117   Validation Loss: 0.00573\n",
      "\n",
      " > Epoch 180/1000\n",
      "   | > Step:9/68  GlobalStep:81430  TotalLoss:0.00175  PostnetLoss:0.00085  DecoderLoss:0.00090  StopLoss:0.04953  GradNorm:0.00350  GradNormST:0.01780  AvgTextLen:9.0  AvgSpecLen:42.1  StepTime:0.43  LR:0.000100\n",
      "   | > Step:19/68  GlobalStep:81440  TotalLoss:0.00204  PostnetLoss:0.00099  DecoderLoss:0.00105  StopLoss:0.08577  GradNorm:0.00328  GradNormST:0.01949  AvgTextLen:15.5  AvgSpecLen:63.2  StepTime:0.43  LR:0.000100\n",
      "   | > Step:29/68  GlobalStep:81450  TotalLoss:0.00228  PostnetLoss:0.00110  DecoderLoss:0.00118  StopLoss:0.07236  GradNorm:0.00295  GradNormST:0.01479  AvgTextLen:21.8  AvgSpecLen:83.4  StepTime:0.59  LR:0.000100\n",
      "   | > Step:39/68  GlobalStep:81460  TotalLoss:0.00256  PostnetLoss:0.00124  DecoderLoss:0.00132  StopLoss:0.03713  GradNorm:0.00294  GradNormST:0.01058  AvgTextLen:30.2  AvgSpecLen:110.4  StepTime:0.54  LR:0.000100\n",
      "   | > Step:49/68  GlobalStep:81470  TotalLoss:0.00275  PostnetLoss:0.00132  DecoderLoss:0.00143  StopLoss:0.03858  GradNorm:0.00278  GradNormST:0.01174  AvgTextLen:38.2  AvgSpecLen:127.7  StepTime:0.76  LR:0.000100\n",
      "   | > Step:59/68  GlobalStep:81480  TotalLoss:0.00298  PostnetLoss:0.00143  DecoderLoss:0.00155  StopLoss:0.05859  GradNorm:0.00274  GradNormST:0.01805  AvgTextLen:50.5  AvgSpecLen:155.9  StepTime:0.74  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:81489  AvgTotalLoss:0.06145  AvgPostnetLoss:0.00117  AvgDecoderLoss:0.00126  AvgStopLoss:0.05902  EpochTime:42.61  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.10408   PostnetLoss: 0.00450   DecoderLoss:0.00496  StopLoss: 0.09461  \n",
      "   | > TotalLoss: 0.07229   PostnetLoss: 0.00638   DecoderLoss:0.00700  StopLoss: 0.05890  \n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00117   Validation Loss: 0.00580\n",
      "\n",
      " > Epoch 181/1000\n",
      "   | > Step:0/68  GlobalStep:81490  TotalLoss:0.00163  PostnetLoss:0.00078  DecoderLoss:0.00084  StopLoss:0.09616  GradNorm:0.00456  GradNormST:0.02805  AvgTextLen:3.0  AvgSpecLen:25.3  StepTime:0.41  LR:0.000100\n",
      "   | > Step:10/68  GlobalStep:81500  TotalLoss:0.00185  PostnetLoss:0.00090  DecoderLoss:0.00095  StopLoss:0.06009  GradNorm:0.00353  GradNormST:0.01880  AvgTextLen:9.8  AvgSpecLen:48.6  StepTime:0.51  LR:0.000100\n",
      "   | > Step:20/68  GlobalStep:81510  TotalLoss:0.00218  PostnetLoss:0.00105  DecoderLoss:0.00112  StopLoss:0.04980  GradNorm:0.00333  GradNormST:0.01631  AvgTextLen:16.4  AvgSpecLen:64.8  StepTime:0.40  LR:0.000100\n",
      "   | > Step:30/68  GlobalStep:81520  TotalLoss:0.00234  PostnetLoss:0.00113  DecoderLoss:0.00121  StopLoss:0.05736  GradNorm:0.00316  GradNormST:0.01141  AvgTextLen:22.9  AvgSpecLen:85.8  StepTime:0.60  LR:0.000100\n",
      "   | > Step:40/68  GlobalStep:81530  TotalLoss:0.00255  PostnetLoss:0.00123  DecoderLoss:0.00132  StopLoss:0.05011  GradNorm:0.00315  GradNormST:0.00950  AvgTextLen:30.9  AvgSpecLen:105.3  StepTime:0.58  LR:0.000100\n",
      "   | > Step:50/68  GlobalStep:81540  TotalLoss:0.00276  PostnetLoss:0.00132  DecoderLoss:0.00143  StopLoss:0.04471  GradNorm:0.00314  GradNormST:0.01252  AvgTextLen:38.8  AvgSpecLen:132.0  StepTime:0.80  LR:0.000100\n",
      "   | > Step:60/68  GlobalStep:81550  TotalLoss:0.00305  PostnetLoss:0.00146  DecoderLoss:0.00159  StopLoss:0.03895  GradNorm:0.00269  GradNormST:0.01041  AvgTextLen:52.0  AvgSpecLen:172.7  StepTime:0.93  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:81558  AvgTotalLoss:0.06014  AvgPostnetLoss:0.00117  AvgDecoderLoss:0.00125  AvgStopLoss:0.05773  EpochTime:42.60  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.10356   PostnetLoss: 0.00452   DecoderLoss:0.00498  StopLoss: 0.09407  \n",
      "   | > TotalLoss: 0.07145   PostnetLoss: 0.00644   DecoderLoss:0.00707  StopLoss: 0.05794  \n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00117   Validation Loss: 0.00582\n",
      "\n",
      " > Epoch 182/1000\n",
      "   | > Step:1/68  GlobalStep:81560  TotalLoss:0.00164  PostnetLoss:0.00079  DecoderLoss:0.00085  StopLoss:0.06314  GradNorm:0.00403  GradNormST:0.02248  AvgTextLen:3.7  AvgSpecLen:28.6  StepTime:0.22  LR:0.000100\n",
      "   | > Step:11/68  GlobalStep:81570  TotalLoss:0.00183  PostnetLoss:0.00089  DecoderLoss:0.00094  StopLoss:0.05868  GradNorm:0.00326  GradNormST:0.01693  AvgTextLen:10.3  AvgSpecLen:49.4  StepTime:0.38  LR:0.000100\n",
      "   | > Step:21/68  GlobalStep:81580  TotalLoss:0.00212  PostnetLoss:0.00103  DecoderLoss:0.00110  StopLoss:0.06764  GradNorm:0.00337  GradNormST:0.02934  AvgTextLen:16.6  AvgSpecLen:66.3  StepTime:0.43  LR:0.000100\n",
      "   | > Step:31/68  GlobalStep:81590  TotalLoss:0.00230  PostnetLoss:0.00111  DecoderLoss:0.00119  StopLoss:0.05072  GradNorm:0.00307  GradNormST:0.01669  AvgTextLen:24.1  AvgSpecLen:82.7  StepTime:0.44  LR:0.000100\n",
      "   | > Step:41/68  GlobalStep:81600  TotalLoss:0.00264  PostnetLoss:0.00127  DecoderLoss:0.00137  StopLoss:0.04168  GradNorm:0.00299  GradNormST:0.01478  AvgTextLen:32.3  AvgSpecLen:109.4  StepTime:0.53  LR:0.000100\n",
      "   | > Step:51/68  GlobalStep:81610  TotalLoss:0.00273  PostnetLoss:0.00131  DecoderLoss:0.00142  StopLoss:0.04666  GradNorm:0.00311  GradNormST:0.01399  AvgTextLen:39.2  AvgSpecLen:126.9  StepTime:0.75  LR:0.000100\n",
      "   | > Step:61/68  GlobalStep:81620  TotalLoss:0.00298  PostnetLoss:0.00143  DecoderLoss:0.00156  StopLoss:0.03603  GradNorm:0.00267  GradNormST:0.01170  AvgTextLen:54.6  AvgSpecLen:165.5  StepTime:0.89  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:81627  AvgTotalLoss:0.05913  AvgPostnetLoss:0.00117  AvgDecoderLoss:0.00125  AvgStopLoss:0.05671  EpochTime:41.77  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.09802   PostnetLoss: 0.00458   DecoderLoss:0.00504  StopLoss: 0.08840  \n",
      "   | > TotalLoss: 0.06457   PostnetLoss: 0.00641   DecoderLoss:0.00704  StopLoss: 0.05112  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00117   Validation Loss: 0.00580\n",
      "\n",
      " > Epoch 183/1000\n",
      "   | > Step:2/68  GlobalStep:81630  TotalLoss:0.00153  PostnetLoss:0.00074  DecoderLoss:0.00079  StopLoss:0.07323  GradNorm:0.00365  GradNormST:0.02396  AvgTextLen:4.0  AvgSpecLen:31.3  StepTime:0.38  LR:0.000100\n",
      "   | > Step:12/68  GlobalStep:81640  TotalLoss:0.00184  PostnetLoss:0.00089  DecoderLoss:0.00095  StopLoss:0.05468  GradNorm:0.00328  GradNormST:0.01459  AvgTextLen:11.0  AvgSpecLen:49.3  StepTime:0.36  LR:0.000100\n",
      "   | > Step:22/68  GlobalStep:81650  TotalLoss:0.00213  PostnetLoss:0.00103  DecoderLoss:0.00110  StopLoss:0.06740  GradNorm:0.00321  GradNormST:0.01336  AvgTextLen:18.0  AvgSpecLen:72.0  StepTime:0.48  LR:0.000100\n",
      "   | > Step:32/68  GlobalStep:81660  TotalLoss:0.00236  PostnetLoss:0.00114  DecoderLoss:0.00122  StopLoss:0.06137  GradNorm:0.00317  GradNormST:0.01688  AvgTextLen:24.4  AvgSpecLen:91.5  StepTime:0.59  LR:0.000100\n",
      "   | > Step:42/68  GlobalStep:81670  TotalLoss:0.00262  PostnetLoss:0.00126  DecoderLoss:0.00136  StopLoss:0.05250  GradNorm:0.00309  GradNormST:0.02193  AvgTextLen:33.1  AvgSpecLen:109.7  StepTime:0.63  LR:0.000100\n",
      "   | > Step:52/68  GlobalStep:81680  TotalLoss:0.00284  PostnetLoss:0.00137  DecoderLoss:0.00147  StopLoss:0.06014  GradNorm:0.00298  GradNormST:0.01738  AvgTextLen:42.1  AvgSpecLen:137.5  StepTime:0.86  LR:0.000100\n",
      "   | > Step:62/68  GlobalStep:81690  TotalLoss:0.00307  PostnetLoss:0.00147  DecoderLoss:0.00159  StopLoss:0.04483  GradNorm:0.00278  GradNormST:0.02131  AvgTextLen:54.0  AvgSpecLen:174.7  StepTime:0.76  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:81696  AvgTotalLoss:0.05912  AvgPostnetLoss:0.00116  AvgDecoderLoss:0.00125  AvgStopLoss:0.05671  EpochTime:42.72  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.10028   PostnetLoss: 0.00458   DecoderLoss:0.00505  StopLoss: 0.09065  \n",
      "   | > TotalLoss: 0.06735   PostnetLoss: 0.00649   DecoderLoss:0.00712  StopLoss: 0.05374  \n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00116   Validation Loss: 0.00588\n",
      "\n",
      " > Epoch 184/1000\n",
      "   | > Step:3/68  GlobalStep:81700  TotalLoss:0.00163  PostnetLoss:0.00079  DecoderLoss:0.00084  StopLoss:0.07894  GradNorm:0.00370  GradNormST:0.02293  AvgTextLen:4.7  AvgSpecLen:29.7  StepTime:0.30  LR:0.000100\n",
      "   | > Step:13/68  GlobalStep:81710  TotalLoss:0.00185  PostnetLoss:0.00090  DecoderLoss:0.00096  StopLoss:0.08454  GradNorm:0.00300  GradNormST:0.03109  AvgTextLen:11.5  AvgSpecLen:55.2  StepTime:0.42  LR:0.000100\n",
      "   | > Step:23/68  GlobalStep:81720  TotalLoss:0.00221  PostnetLoss:0.00107  DecoderLoss:0.00114  StopLoss:0.06012  GradNorm:0.00314  GradNormST:0.01164  AvgTextLen:18.3  AvgSpecLen:72.2  StepTime:0.43  LR:0.000100\n",
      "   | > Step:33/68  GlobalStep:81730  TotalLoss:0.00240  PostnetLoss:0.00116  DecoderLoss:0.00124  StopLoss:0.04710  GradNorm:0.00328  GradNormST:0.01052  AvgTextLen:25.9  AvgSpecLen:88.9  StepTime:0.53  LR:0.000100\n",
      "   | > Step:43/68  GlobalStep:81740  TotalLoss:0.00263  PostnetLoss:0.00127  DecoderLoss:0.00136  StopLoss:0.05153  GradNorm:0.00318  GradNormST:0.01914  AvgTextLen:33.4  AvgSpecLen:115.7  StepTime:0.66  LR:0.000100\n",
      "   | > Step:53/68  GlobalStep:81750  TotalLoss:0.00278  PostnetLoss:0.00134  DecoderLoss:0.00144  StopLoss:0.04422  GradNorm:0.00301  GradNormST:0.01305  AvgTextLen:42.4  AvgSpecLen:137.6  StepTime:0.73  LR:0.000100\n",
      "   | > Step:63/68  GlobalStep:81760  TotalLoss:0.00314  PostnetLoss:0.00151  DecoderLoss:0.00163  StopLoss:0.04684  GradNorm:0.00272  GradNormST:0.02251  AvgTextLen:58.6  AvgSpecLen:185.1  StepTime:0.91  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:81765  AvgTotalLoss:0.05935  AvgPostnetLoss:0.00116  AvgDecoderLoss:0.00125  AvgStopLoss:0.05694  EpochTime:42.97  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.10048   PostnetLoss: 0.00455   DecoderLoss:0.00500  StopLoss: 0.09092  \n",
      "   | > TotalLoss: 0.07122   PostnetLoss: 0.00646   DecoderLoss:0.00707  StopLoss: 0.05768  \n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00116   Validation Loss: 0.00586\n",
      "\n",
      " > Epoch 185/1000\n",
      "   | > Step:4/68  GlobalStep:81770  TotalLoss:0.00162  PostnetLoss:0.00078  DecoderLoss:0.00083  StopLoss:0.10512  GradNorm:0.00367  GradNormST:0.04752  AvgTextLen:4.8  AvgSpecLen:31.1  StepTime:0.29  LR:0.000100\n",
      "   | > Step:14/68  GlobalStep:81780  TotalLoss:0.00192  PostnetLoss:0.00093  DecoderLoss:0.00099  StopLoss:0.06316  GradNorm:0.00350  GradNormST:0.01607  AvgTextLen:12.2  AvgSpecLen:52.0  StepTime:0.33  LR:0.000100\n",
      "   | > Step:24/68  GlobalStep:81790  TotalLoss:0.00227  PostnetLoss:0.00110  DecoderLoss:0.00117  StopLoss:0.05748  GradNorm:0.00301  GradNormST:0.01582  AvgTextLen:19.2  AvgSpecLen:79.0  StepTime:0.53  LR:0.000100\n",
      "   | > Step:34/68  GlobalStep:81800  TotalLoss:0.00242  PostnetLoss:0.00117  DecoderLoss:0.00125  StopLoss:0.05576  GradNorm:0.00291  GradNormST:0.01311  AvgTextLen:26.2  AvgSpecLen:95.9  StepTime:0.62  LR:0.000100\n",
      "   | > Step:44/68  GlobalStep:81810  TotalLoss:0.00266  PostnetLoss:0.00128  DecoderLoss:0.00138  StopLoss:0.04265  GradNorm:0.00289  GradNormST:0.01511  AvgTextLen:33.8  AvgSpecLen:116.3  StepTime:0.54  LR:0.000100\n",
      "   | > Step:54/68  GlobalStep:81820  TotalLoss:0.00286  PostnetLoss:0.00137  DecoderLoss:0.00149  StopLoss:0.05325  GradNorm:0.00291  GradNormST:0.01545  AvgTextLen:42.2  AvgSpecLen:144.9  StepTime:0.81  LR:0.000100\n",
      "   | > Step:64/68  GlobalStep:81830  TotalLoss:0.00296  PostnetLoss:0.00142  DecoderLoss:0.00154  StopLoss:0.04732  GradNorm:0.00291  GradNormST:0.01604  AvgTextLen:60.6  AvgSpecLen:177.6  StepTime:0.80  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:81834  AvgTotalLoss:0.05779  AvgPostnetLoss:0.00116  AvgDecoderLoss:0.00125  AvgStopLoss:0.05539  EpochTime:42.83  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.10134   PostnetLoss: 0.00459   DecoderLoss:0.00505  StopLoss: 0.09170  \n",
      "   | > TotalLoss: 0.07169   PostnetLoss: 0.00653   DecoderLoss:0.00716  StopLoss: 0.05800  \n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00116   Validation Loss: 0.00590\n",
      "\n",
      " > Epoch 186/1000\n",
      "   | > Step:5/68  GlobalStep:81840  TotalLoss:0.00168  PostnetLoss:0.00081  DecoderLoss:0.00087  StopLoss:0.06322  GradNorm:0.00362  GradNormST:0.02190  AvgTextLen:5.9  AvgSpecLen:33.2  StepTime:0.36  LR:0.000100\n",
      "   | > Step:15/68  GlobalStep:81850  TotalLoss:0.00203  PostnetLoss:0.00099  DecoderLoss:0.00105  StopLoss:0.06145  GradNorm:0.00328  GradNormST:0.02279  AvgTextLen:12.8  AvgSpecLen:59.1  StepTime:0.51  LR:0.000100\n",
      "   | > Step:25/68  GlobalStep:81860  TotalLoss:0.00219  PostnetLoss:0.00106  DecoderLoss:0.00113  StopLoss:0.05407  GradNorm:0.00309  GradNormST:0.01327  AvgTextLen:19.8  AvgSpecLen:73.3  StepTime:0.46  LR:0.000100\n",
      "   | > Step:35/68  GlobalStep:81870  TotalLoss:0.00246  PostnetLoss:0.00119  DecoderLoss:0.00127  StopLoss:0.04929  GradNorm:0.00297  GradNormST:0.01537  AvgTextLen:27.1  AvgSpecLen:104.7  StepTime:0.76  LR:0.000100\n",
      "   | > Step:45/68  GlobalStep:81880  TotalLoss:0.00260  PostnetLoss:0.00125  DecoderLoss:0.00135  StopLoss:0.05906  GradNorm:0.00287  GradNormST:0.02407  AvgTextLen:34.4  AvgSpecLen:118.6  StepTime:0.64  LR:0.000100\n",
      "   | > Step:55/68  GlobalStep:81890  TotalLoss:0.00295  PostnetLoss:0.00142  DecoderLoss:0.00153  StopLoss:0.04281  GradNorm:0.00311  GradNormST:0.01009  AvgTextLen:43.9  AvgSpecLen:145.2  StepTime:0.78  LR:0.000100\n",
      "   | > Step:65/68  GlobalStep:81900  TotalLoss:0.00317  PostnetLoss:0.00152  DecoderLoss:0.00165  StopLoss:0.04077  GradNorm:0.00280  GradNormST:0.01165  AvgTextLen:64.2  AvgSpecLen:207.4  StepTime:1.18  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:81903  AvgTotalLoss:0.05910  AvgPostnetLoss:0.00116  AvgDecoderLoss:0.00125  AvgStopLoss:0.05669  EpochTime:42.98  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.09374   PostnetLoss: 0.00457   DecoderLoss:0.00502  StopLoss: 0.08415  \n",
      "   | > TotalLoss: 0.07047   PostnetLoss: 0.00652   DecoderLoss:0.00713  StopLoss: 0.05682  \n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00116   Validation Loss: 0.00590\n",
      "\n",
      " > Epoch 187/1000\n",
      "   | > Step:6/68  GlobalStep:81910  TotalLoss:0.00166  PostnetLoss:0.00081  DecoderLoss:0.00085  StopLoss:0.04470  GradNorm:0.00361  GradNormST:0.01419  AvgTextLen:6.0  AvgSpecLen:34.8  StepTime:0.32  LR:0.000100\n",
      "   | > Step:16/68  GlobalStep:81920  TotalLoss:0.00202  PostnetLoss:0.00098  DecoderLoss:0.00104  StopLoss:0.05246  GradNorm:0.00317  GradNormST:0.01397  AvgTextLen:13.8  AvgSpecLen:60.6  StepTime:0.32  LR:0.000100\n",
      "   | > Step:26/68  GlobalStep:81930  TotalLoss:0.00222  PostnetLoss:0.00107  DecoderLoss:0.00114  StopLoss:0.05204  GradNorm:0.00305  GradNormST:0.01243  AvgTextLen:20.3  AvgSpecLen:77.5  StepTime:0.41  LR:0.000100\n",
      "   | > Step:36/68  GlobalStep:81940  TotalLoss:0.00248  PostnetLoss:0.00119  DecoderLoss:0.00128  StopLoss:0.07623  GradNorm:0.00299  GradNormST:0.01812  AvgTextLen:28.5  AvgSpecLen:100.2  StepTime:0.47  LR:0.000100\n",
      "   | > Step:46/68  GlobalStep:81950  TotalLoss:0.00262  PostnetLoss:0.00126  DecoderLoss:0.00135  StopLoss:0.05339  GradNorm:0.00285  GradNormST:0.02030  AvgTextLen:35.4  AvgSpecLen:122.6  StepTime:0.82  LR:0.000100\n",
      "   | > Step:56/68  GlobalStep:81960  TotalLoss:0.00288  PostnetLoss:0.00138  DecoderLoss:0.00150  StopLoss:0.03791  GradNorm:0.00285  GradNormST:0.01463  AvgTextLen:45.8  AvgSpecLen:145.4  StepTime:0.83  LR:0.000100\n",
      "   | > Step:66/68  GlobalStep:81970  TotalLoss:0.00336  PostnetLoss:0.00160  DecoderLoss:0.00175  StopLoss:0.04110  GradNorm:0.00261  GradNormST:0.01437  AvgTextLen:71.2  AvgSpecLen:220.8  StepTime:1.04  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:81972  AvgTotalLoss:0.05886  AvgPostnetLoss:0.00116  AvgDecoderLoss:0.00124  AvgStopLoss:0.05646  EpochTime:41.87  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.09759   PostnetLoss: 0.00456   DecoderLoss:0.00501  StopLoss: 0.08802  \n",
      "   | > TotalLoss: 0.07216   PostnetLoss: 0.00645   DecoderLoss:0.00706  StopLoss: 0.05865  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00116   Validation Loss: 0.00587\n",
      "\n",
      " > Epoch 188/1000\n",
      "   | > Step:7/68  GlobalStep:81980  TotalLoss:0.00176  PostnetLoss:0.00085  DecoderLoss:0.00090  StopLoss:0.05453  GradNorm:0.00350  GradNormST:0.01724  AvgTextLen:7.4  AvgSpecLen:44.6  StepTime:0.38  LR:0.000100\n",
      "   | > Step:17/68  GlobalStep:81990  TotalLoss:0.00200  PostnetLoss:0.00097  DecoderLoss:0.00103  StopLoss:0.08294  GradNorm:0.00313  GradNormST:0.01784  AvgTextLen:14.7  AvgSpecLen:66.0  StepTime:0.42  LR:0.000100\n",
      "   | > Step:27/68  GlobalStep:82000  TotalLoss:0.00231  PostnetLoss:0.00112  DecoderLoss:0.00119  StopLoss:0.05956  GradNorm:0.00314  GradNormST:0.02280  AvgTextLen:21.0  AvgSpecLen:80.2  StepTime:0.63  LR:0.000100\n",
      " | | > Checkpoint saving : /vm/TTS/models/cartman_models/mozilla-no-loc-fattn-stopnet-sigmoid-loss_masking-August-15-2019_11+26AM-de6aeb9/checkpoint_82000.pth.tar\n",
      "warning: audio amplitude out of range, auto clipped.\n",
      "   | > Step:37/68  GlobalStep:82010  TotalLoss:0.00252  PostnetLoss:0.00122  DecoderLoss:0.00130  StopLoss:0.05755  GradNorm:0.00303  GradNormST:0.01434  AvgTextLen:29.3  AvgSpecLen:102.4  StepTime:0.59  LR:0.000100\n",
      "   | > Step:47/68  GlobalStep:82020  TotalLoss:0.00272  PostnetLoss:0.00131  DecoderLoss:0.00141  StopLoss:0.04667  GradNorm:0.00272  GradNormST:0.01461  AvgTextLen:36.7  AvgSpecLen:126.9  StepTime:0.58  LR:0.000100\n",
      "   | > Step:57/68  GlobalStep:82030  TotalLoss:0.00296  PostnetLoss:0.00142  DecoderLoss:0.00154  StopLoss:0.03843  GradNorm:0.00277  GradNormST:0.00748  AvgTextLen:47.1  AvgSpecLen:161.0  StepTime:0.90  LR:0.000100\n",
      "   | > Step:67/68  GlobalStep:82040  TotalLoss:0.00337  PostnetLoss:0.00161  DecoderLoss:0.00175  StopLoss:0.04069  GradNorm:0.00278  GradNormST:0.02426  AvgTextLen:80.6  AvgSpecLen:246.5  StepTime:1.22  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:82041  AvgTotalLoss:0.05886  AvgPostnetLoss:0.00116  AvgDecoderLoss:0.00124  AvgStopLoss:0.05646  EpochTime:42.95  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.09759   PostnetLoss: 0.00458   DecoderLoss:0.00502  StopLoss: 0.08799  \n",
      "   | > TotalLoss: 0.07131   PostnetLoss: 0.00654   DecoderLoss:0.00716  StopLoss: 0.05761  \n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00116   Validation Loss: 0.00590\n",
      "\n",
      " > Epoch 189/1000\n",
      "   | > Step:8/68  GlobalStep:82050  TotalLoss:0.00180  PostnetLoss:0.00087  DecoderLoss:0.00093  StopLoss:0.06975  GradNorm:0.00358  GradNormST:0.02239  AvgTextLen:8.3  AvgSpecLen:44.0  StepTime:0.31  LR:0.000100\n",
      "   | > Step:18/68  GlobalStep:82060  TotalLoss:0.00200  PostnetLoss:0.00097  DecoderLoss:0.00103  StopLoss:0.05984  GradNorm:0.00317  GradNormST:0.01193  AvgTextLen:15.2  AvgSpecLen:64.5  StepTime:0.42  LR:0.000100\n",
      "   | > Step:28/68  GlobalStep:82070  TotalLoss:0.00231  PostnetLoss:0.00112  DecoderLoss:0.00119  StopLoss:0.06927  GradNorm:0.00306  GradNormST:0.01324  AvgTextLen:22.1  AvgSpecLen:85.4  StepTime:0.57  LR:0.000100\n",
      "   | > Step:38/68  GlobalStep:82080  TotalLoss:0.00245  PostnetLoss:0.00118  DecoderLoss:0.00127  StopLoss:0.05044  GradNorm:0.00576  GradNormST:0.00984  AvgTextLen:29.4  AvgSpecLen:104.1  StepTime:0.64  LR:0.000100\n",
      "   | > Step:48/68  GlobalStep:82090  TotalLoss:0.00272  PostnetLoss:0.00131  DecoderLoss:0.00141  StopLoss:0.04781  GradNorm:0.00285  GradNormST:0.01282  AvgTextLen:37.5  AvgSpecLen:123.8  StepTime:0.56  LR:0.000100\n",
      "   | > Step:58/68  GlobalStep:82100  TotalLoss:0.00294  PostnetLoss:0.00142  DecoderLoss:0.00153  StopLoss:0.04910  GradNorm:0.00265  GradNormST:0.01406  AvgTextLen:47.8  AvgSpecLen:158.0  StepTime:0.74  LR:0.000100\n",
      "   | > Step:68/68  GlobalStep:82110  TotalLoss:0.00342  PostnetLoss:0.00164  DecoderLoss:0.00179  StopLoss:0.03658  GradNorm:0.00303  GradNormST:0.01799  AvgTextLen:103.0  AvgSpecLen:308.9  StepTime:1.90  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:82110  AvgTotalLoss:0.05831  AvgPostnetLoss:0.00116  AvgDecoderLoss:0.00124  AvgStopLoss:0.05592  EpochTime:42.04  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.09697   PostnetLoss: 0.00455   DecoderLoss:0.00500  StopLoss: 0.08741  \n",
      "   | > TotalLoss: 0.06531   PostnetLoss: 0.00649   DecoderLoss:0.00710  StopLoss: 0.05173  \n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00116   Validation Loss: 0.00587\n",
      "\n",
      " > Epoch 190/1000\n",
      "   | > Step:9/68  GlobalStep:82120  TotalLoss:0.00173  PostnetLoss:0.00084  DecoderLoss:0.00089  StopLoss:0.06177  GradNorm:0.00348  GradNormST:0.02768  AvgTextLen:9.0  AvgSpecLen:42.1  StepTime:0.43  LR:0.000100\n",
      "   | > Step:19/68  GlobalStep:82130  TotalLoss:0.00200  PostnetLoss:0.00097  DecoderLoss:0.00103  StopLoss:0.07231  GradNorm:0.00308  GradNormST:0.01656  AvgTextLen:15.5  AvgSpecLen:63.2  StepTime:0.44  LR:0.000100\n",
      "   | > Step:29/68  GlobalStep:82140  TotalLoss:0.00226  PostnetLoss:0.00109  DecoderLoss:0.00117  StopLoss:0.06488  GradNorm:0.00299  GradNormST:0.01315  AvgTextLen:21.8  AvgSpecLen:83.4  StepTime:0.59  LR:0.000100\n",
      "   | > Step:39/68  GlobalStep:82150  TotalLoss:0.00253  PostnetLoss:0.00122  DecoderLoss:0.00131  StopLoss:0.03497  GradNorm:0.00290  GradNormST:0.00814  AvgTextLen:30.2  AvgSpecLen:110.4  StepTime:0.64  LR:0.000100\n",
      "   | > Step:49/68  GlobalStep:82160  TotalLoss:0.00272  PostnetLoss:0.00131  DecoderLoss:0.00141  StopLoss:0.03365  GradNorm:0.00285  GradNormST:0.01322  AvgTextLen:38.2  AvgSpecLen:127.7  StepTime:0.72  LR:0.000100\n",
      "   | > Step:59/68  GlobalStep:82170  TotalLoss:0.00291  PostnetLoss:0.00140  DecoderLoss:0.00151  StopLoss:0.05378  GradNorm:0.00275  GradNormST:0.01793  AvgTextLen:50.5  AvgSpecLen:155.9  StepTime:0.87  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:82179  AvgTotalLoss:0.05821  AvgPostnetLoss:0.00115  AvgDecoderLoss:0.00124  AvgStopLoss:0.05582  EpochTime:43.07  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.09687   PostnetLoss: 0.00452   DecoderLoss:0.00498  StopLoss: 0.08737  \n",
      "   | > TotalLoss: 0.06519   PostnetLoss: 0.00658   DecoderLoss:0.00720  StopLoss: 0.05140  \n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00115   Validation Loss: 0.00592\n",
      "\n",
      " > Epoch 191/1000\n",
      "   | > Step:0/68  GlobalStep:82180  TotalLoss:0.00164  PostnetLoss:0.00079  DecoderLoss:0.00085  StopLoss:0.08250  GradNorm:0.00461  GradNormST:0.02483  AvgTextLen:3.0  AvgSpecLen:25.3  StepTime:0.62  LR:0.000100\n",
      "   | > Step:10/68  GlobalStep:82190  TotalLoss:0.00180  PostnetLoss:0.00087  DecoderLoss:0.00093  StopLoss:0.06256  GradNorm:0.00353  GradNormST:0.02867  AvgTextLen:9.8  AvgSpecLen:48.6  StepTime:0.52  LR:0.000100\n",
      "   | > Step:20/68  GlobalStep:82200  TotalLoss:0.00212  PostnetLoss:0.00103  DecoderLoss:0.00109  StopLoss:0.04525  GradNorm:0.00321  GradNormST:0.01696  AvgTextLen:16.4  AvgSpecLen:64.8  StepTime:0.43  LR:0.000100\n",
      "   | > Step:30/68  GlobalStep:82210  TotalLoss:0.00234  PostnetLoss:0.00113  DecoderLoss:0.00121  StopLoss:0.06080  GradNorm:0.00319  GradNormST:0.01194  AvgTextLen:22.9  AvgSpecLen:85.8  StepTime:0.58  LR:0.000100\n",
      "   | > Step:40/68  GlobalStep:82220  TotalLoss:0.00252  PostnetLoss:0.00122  DecoderLoss:0.00130  StopLoss:0.03802  GradNorm:0.00298  GradNormST:0.00841  AvgTextLen:30.9  AvgSpecLen:105.3  StepTime:0.50  LR:0.000100\n",
      "   | > Step:50/68  GlobalStep:82230  TotalLoss:0.00270  PostnetLoss:0.00130  DecoderLoss:0.00140  StopLoss:0.04261  GradNorm:0.00281  GradNormST:0.01594  AvgTextLen:38.8  AvgSpecLen:132.0  StepTime:0.81  LR:0.000100\n",
      "   | > Step:60/68  GlobalStep:82240  TotalLoss:0.00302  PostnetLoss:0.00145  DecoderLoss:0.00157  StopLoss:0.03362  GradNorm:0.00277  GradNormST:0.00784  AvgTextLen:52.0  AvgSpecLen:172.7  StepTime:0.93  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:82248  AvgTotalLoss:0.05773  AvgPostnetLoss:0.00115  AvgDecoderLoss:0.00124  AvgStopLoss:0.05534  EpochTime:42.04  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.09708   PostnetLoss: 0.00460   DecoderLoss:0.00505  StopLoss: 0.08743  \n",
      "   | > TotalLoss: 0.07157   PostnetLoss: 0.00644   DecoderLoss:0.00704  StopLoss: 0.05810  \n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00115   Validation Loss: 0.00591\n",
      "\n",
      " > Epoch 192/1000\n",
      "   | > Step:1/68  GlobalStep:82250  TotalLoss:0.00165  PostnetLoss:0.00080  DecoderLoss:0.00086  StopLoss:0.05862  GradNorm:0.00485  GradNormST:0.01975  AvgTextLen:3.7  AvgSpecLen:28.6  StepTime:0.35  LR:0.000100\n",
      "   | > Step:11/68  GlobalStep:82260  TotalLoss:0.00180  PostnetLoss:0.00087  DecoderLoss:0.00093  StopLoss:0.05452  GradNorm:0.00340  GradNormST:0.01924  AvgTextLen:10.3  AvgSpecLen:49.4  StepTime:0.40  LR:0.000100\n",
      "   | > Step:21/68  GlobalStep:82270  TotalLoss:0.00209  PostnetLoss:0.00101  DecoderLoss:0.00108  StopLoss:0.07526  GradNorm:0.00321  GradNormST:0.03198  AvgTextLen:16.6  AvgSpecLen:66.3  StepTime:0.42  LR:0.000100\n",
      "   | > Step:31/68  GlobalStep:82280  TotalLoss:0.00232  PostnetLoss:0.00112  DecoderLoss:0.00120  StopLoss:0.04351  GradNorm:0.00301  GradNormST:0.01241  AvgTextLen:24.1  AvgSpecLen:82.7  StepTime:0.54  LR:0.000100\n",
      "   | > Step:41/68  GlobalStep:82290  TotalLoss:0.00259  PostnetLoss:0.00125  DecoderLoss:0.00134  StopLoss:0.03897  GradNorm:0.00325  GradNormST:0.01067  AvgTextLen:32.3  AvgSpecLen:109.4  StepTime:0.68  LR:0.000100\n",
      "   | > Step:51/68  GlobalStep:82300  TotalLoss:0.00273  PostnetLoss:0.00131  DecoderLoss:0.00141  StopLoss:0.04640  GradNorm:0.00285  GradNormST:0.01251  AvgTextLen:39.2  AvgSpecLen:126.9  StepTime:0.74  LR:0.000100\n",
      "   | > Step:61/68  GlobalStep:82310  TotalLoss:0.00295  PostnetLoss:0.00142  DecoderLoss:0.00154  StopLoss:0.03705  GradNorm:0.00266  GradNormST:0.01217  AvgTextLen:54.6  AvgSpecLen:165.5  StepTime:0.90  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:82317  AvgTotalLoss:0.05719  AvgPostnetLoss:0.00115  AvgDecoderLoss:0.00124  AvgStopLoss:0.05480  EpochTime:42.85  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.09791   PostnetLoss: 0.00454   DecoderLoss:0.00499  StopLoss: 0.08838  \n",
      "   | > TotalLoss: 0.06476   PostnetLoss: 0.00650   DecoderLoss:0.00712  StopLoss: 0.05114  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00115   Validation Loss: 0.00591\n",
      "\n",
      " > Epoch 193/1000\n",
      "   | > Step:2/68  GlobalStep:82320  TotalLoss:0.00154  PostnetLoss:0.00075  DecoderLoss:0.00079  StopLoss:0.07950  GradNorm:0.00380  GradNormST:0.02180  AvgTextLen:4.0  AvgSpecLen:31.3  StepTime:0.33  LR:0.000100\n",
      "   | > Step:12/68  GlobalStep:82330  TotalLoss:0.00183  PostnetLoss:0.00088  DecoderLoss:0.00094  StopLoss:0.05460  GradNorm:0.00319  GradNormST:0.01443  AvgTextLen:11.0  AvgSpecLen:49.3  StepTime:0.42  LR:0.000100\n",
      "   | > Step:22/68  GlobalStep:82340  TotalLoss:0.00216  PostnetLoss:0.00105  DecoderLoss:0.00111  StopLoss:0.06251  GradNorm:0.00315  GradNormST:0.01517  AvgTextLen:18.0  AvgSpecLen:72.0  StepTime:0.45  LR:0.000100\n",
      "   | > Step:32/68  GlobalStep:82350  TotalLoss:0.00233  PostnetLoss:0.00113  DecoderLoss:0.00120  StopLoss:0.05969  GradNorm:0.00298  GradNormST:0.01428  AvgTextLen:24.4  AvgSpecLen:91.5  StepTime:0.57  LR:0.000100\n",
      "   | > Step:42/68  GlobalStep:82360  TotalLoss:0.00255  PostnetLoss:0.00123  DecoderLoss:0.00133  StopLoss:0.06146  GradNorm:0.00281  GradNormST:0.01705  AvgTextLen:33.1  AvgSpecLen:109.7  StepTime:0.64  LR:0.000100\n",
      "   | > Step:52/68  GlobalStep:82370  TotalLoss:0.00278  PostnetLoss:0.00134  DecoderLoss:0.00144  StopLoss:0.05148  GradNorm:0.00292  GradNormST:0.01475  AvgTextLen:42.1  AvgSpecLen:137.5  StepTime:0.78  LR:0.000100\n",
      "   | > Step:62/68  GlobalStep:82380  TotalLoss:0.00304  PostnetLoss:0.00146  DecoderLoss:0.00158  StopLoss:0.04774  GradNorm:0.00281  GradNormST:0.01763  AvgTextLen:54.0  AvgSpecLen:174.7  StepTime:0.75  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:82386  AvgTotalLoss:0.05832  AvgPostnetLoss:0.00115  AvgDecoderLoss:0.00124  AvgStopLoss:0.05593  EpochTime:42.03  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.09740   PostnetLoss: 0.00459   DecoderLoss:0.00504  StopLoss: 0.08777  \n",
      "   | > TotalLoss: 0.06996   PostnetLoss: 0.00652   DecoderLoss:0.00713  StopLoss: 0.05631  \n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00115   Validation Loss: 0.00593\n",
      "\n",
      " > Epoch 194/1000\n",
      "   | > Step:3/68  GlobalStep:82390  TotalLoss:0.00160  PostnetLoss:0.00077  DecoderLoss:0.00082  StopLoss:0.08090  GradNorm:0.00392  GradNormST:0.02802  AvgTextLen:4.7  AvgSpecLen:29.7  StepTime:0.36  LR:0.000100\n",
      "   | > Step:13/68  GlobalStep:82400  TotalLoss:0.00178  PostnetLoss:0.00086  DecoderLoss:0.00091  StopLoss:0.07510  GradNorm:0.00316  GradNormST:0.02290  AvgTextLen:11.5  AvgSpecLen:55.2  StepTime:0.36  LR:0.000100\n",
      "   | > Step:23/68  GlobalStep:82410  TotalLoss:0.00222  PostnetLoss:0.00107  DecoderLoss:0.00114  StopLoss:0.04848  GradNorm:0.00326  GradNormST:0.01276  AvgTextLen:18.3  AvgSpecLen:72.2  StepTime:0.43  LR:0.000100\n",
      "   | > Step:33/68  GlobalStep:82420  TotalLoss:0.00241  PostnetLoss:0.00117  DecoderLoss:0.00125  StopLoss:0.05725  GradNorm:0.00305  GradNormST:0.01228  AvgTextLen:25.9  AvgSpecLen:88.9  StepTime:0.54  LR:0.000100\n",
      "   | > Step:43/68  GlobalStep:82430  TotalLoss:0.00256  PostnetLoss:0.00123  DecoderLoss:0.00133  StopLoss:0.05225  GradNorm:0.00291  GradNormST:0.01700  AvgTextLen:33.4  AvgSpecLen:115.7  StepTime:0.63  LR:0.000100\n",
      "   | > Step:53/68  GlobalStep:82440  TotalLoss:0.00275  PostnetLoss:0.00132  DecoderLoss:0.00143  StopLoss:0.04244  GradNorm:0.00278  GradNormST:0.00931  AvgTextLen:42.4  AvgSpecLen:137.6  StepTime:0.76  LR:0.000100\n",
      "   | > Step:63/68  GlobalStep:82450  TotalLoss:0.00311  PostnetLoss:0.00149  DecoderLoss:0.00162  StopLoss:0.03097  GradNorm:0.00272  GradNormST:0.01376  AvgTextLen:58.6  AvgSpecLen:185.1  StepTime:0.91  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:82455  AvgTotalLoss:0.05546  AvgPostnetLoss:0.00115  AvgDecoderLoss:0.00124  AvgStopLoss:0.05308  EpochTime:42.01  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.09440   PostnetLoss: 0.00458   DecoderLoss:0.00504  StopLoss: 0.08479  \n",
      "   | > TotalLoss: 0.06913   PostnetLoss: 0.00653   DecoderLoss:0.00715  StopLoss: 0.05545  \n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00115   Validation Loss: 0.00593\n",
      "\n",
      " > Epoch 195/1000\n",
      "   | > Step:4/68  GlobalStep:82460  TotalLoss:0.00161  PostnetLoss:0.00078  DecoderLoss:0.00083  StopLoss:0.11238  GradNorm:0.00375  GradNormST:0.03187  AvgTextLen:4.8  AvgSpecLen:31.1  StepTime:0.28  LR:0.000100\n",
      "   | > Step:14/68  GlobalStep:82470  TotalLoss:0.00193  PostnetLoss:0.00094  DecoderLoss:0.00100  StopLoss:0.06637  GradNorm:0.00349  GradNormST:0.01885  AvgTextLen:12.2  AvgSpecLen:52.0  StepTime:0.33  LR:0.000100\n",
      "   | > Step:24/68  GlobalStep:82480  TotalLoss:0.00227  PostnetLoss:0.00110  DecoderLoss:0.00117  StopLoss:0.05526  GradNorm:0.00320  GradNormST:0.01042  AvgTextLen:19.2  AvgSpecLen:79.0  StepTime:0.49  LR:0.000100\n",
      "   | > Step:34/68  GlobalStep:82490  TotalLoss:0.00242  PostnetLoss:0.00117  DecoderLoss:0.00126  StopLoss:0.05277  GradNorm:0.00285  GradNormST:0.01376  AvgTextLen:26.2  AvgSpecLen:95.9  StepTime:0.59  LR:0.000100\n",
      "   | > Step:44/68  GlobalStep:82500  TotalLoss:0.00263  PostnetLoss:0.00127  DecoderLoss:0.00136  StopLoss:0.04990  GradNorm:0.00300  GradNormST:0.01896  AvgTextLen:33.8  AvgSpecLen:116.3  StepTime:0.65  LR:0.000100\n",
      "   | > Step:54/68  GlobalStep:82510  TotalLoss:0.00285  PostnetLoss:0.00137  DecoderLoss:0.00149  StopLoss:0.04193  GradNorm:0.00307  GradNormST:0.01288  AvgTextLen:42.2  AvgSpecLen:144.9  StepTime:0.79  LR:0.000100\n",
      "   | > Step:64/68  GlobalStep:82520  TotalLoss:0.00292  PostnetLoss:0.00140  DecoderLoss:0.00152  StopLoss:0.03180  GradNorm:0.00290  GradNormST:0.00794  AvgTextLen:60.6  AvgSpecLen:177.6  StepTime:0.81  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:82524  AvgTotalLoss:0.05593  AvgPostnetLoss:0.00115  AvgDecoderLoss:0.00123  AvgStopLoss:0.05355  EpochTime:42.95  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.09718   PostnetLoss: 0.00460   DecoderLoss:0.00505  StopLoss: 0.08753  \n",
      "   | > TotalLoss: 0.07203   PostnetLoss: 0.00658   DecoderLoss:0.00720  StopLoss: 0.05826  \n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00115   Validation Loss: 0.00599\n",
      "\n",
      " > Epoch 196/1000\n",
      "   | > Step:5/68  GlobalStep:82530  TotalLoss:0.00167  PostnetLoss:0.00080  DecoderLoss:0.00086  StopLoss:0.06842  GradNorm:0.00373  GradNormST:0.01961  AvgTextLen:5.9  AvgSpecLen:33.2  StepTime:0.36  LR:0.000100\n",
      "   | > Step:15/68  GlobalStep:82540  TotalLoss:0.00200  PostnetLoss:0.00097  DecoderLoss:0.00103  StopLoss:0.04988  GradNorm:0.00320  GradNormST:0.01649  AvgTextLen:12.8  AvgSpecLen:59.1  StepTime:0.53  LR:0.000100\n",
      "   | > Step:25/68  GlobalStep:82550  TotalLoss:0.00218  PostnetLoss:0.00106  DecoderLoss:0.00113  StopLoss:0.06397  GradNorm:0.00321  GradNormST:0.01595  AvgTextLen:19.8  AvgSpecLen:73.3  StepTime:0.48  LR:0.000100\n",
      "   | > Step:35/68  GlobalStep:82560  TotalLoss:0.00247  PostnetLoss:0.00119  DecoderLoss:0.00128  StopLoss:0.05675  GradNorm:0.00306  GradNormST:0.01756  AvgTextLen:27.1  AvgSpecLen:104.7  StepTime:0.63  LR:0.000100\n",
      "   | > Step:45/68  GlobalStep:82570  TotalLoss:0.00257  PostnetLoss:0.00124  DecoderLoss:0.00133  StopLoss:0.04635  GradNorm:0.00281  GradNormST:0.01845  AvgTextLen:34.4  AvgSpecLen:118.6  StepTime:0.66  LR:0.000100\n",
      "   | > Step:55/68  GlobalStep:82580  TotalLoss:0.00289  PostnetLoss:0.00139  DecoderLoss:0.00150  StopLoss:0.04653  GradNorm:0.00294  GradNormST:0.01289  AvgTextLen:43.9  AvgSpecLen:145.2  StepTime:0.70  LR:0.000100\n",
      "   | > Step:65/68  GlobalStep:82590  TotalLoss:0.00317  PostnetLoss:0.00152  DecoderLoss:0.00165  StopLoss:0.03655  GradNorm:0.00268  GradNormST:0.00972  AvgTextLen:64.2  AvgSpecLen:207.4  StepTime:1.18  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:82593  AvgTotalLoss:0.05432  AvgPostnetLoss:0.00115  AvgDecoderLoss:0.00123  AvgStopLoss:0.05194  EpochTime:41.60  AvgStepTime:0.60\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.09694   PostnetLoss: 0.00462   DecoderLoss:0.00507  StopLoss: 0.08724  \n",
      "   | > TotalLoss: 0.07119   PostnetLoss: 0.00654   DecoderLoss:0.00716  StopLoss: 0.05749  \n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00115   Validation Loss: 0.00597\n",
      "\n",
      " > Epoch 197/1000\n",
      "   | > Step:6/68  GlobalStep:82600  TotalLoss:0.00167  PostnetLoss:0.00081  DecoderLoss:0.00086  StopLoss:0.04661  GradNorm:0.00355  GradNormST:0.01600  AvgTextLen:6.0  AvgSpecLen:34.8  StepTime:0.40  LR:0.000100\n",
      "   | > Step:16/68  GlobalStep:82610  TotalLoss:0.00201  PostnetLoss:0.00097  DecoderLoss:0.00104  StopLoss:0.06006  GradNorm:0.00334  GradNormST:0.01807  AvgTextLen:13.8  AvgSpecLen:60.6  StepTime:0.36  LR:0.000100\n",
      "   | > Step:26/68  GlobalStep:82620  TotalLoss:0.00219  PostnetLoss:0.00106  DecoderLoss:0.00113  StopLoss:0.04812  GradNorm:0.00316  GradNormST:0.01289  AvgTextLen:20.3  AvgSpecLen:77.5  StepTime:0.46  LR:0.000100\n",
      "   | > Step:36/68  GlobalStep:82630  TotalLoss:0.00244  PostnetLoss:0.00118  DecoderLoss:0.00126  StopLoss:0.07256  GradNorm:0.00310  GradNormST:0.01268  AvgTextLen:28.5  AvgSpecLen:100.2  StepTime:0.68  LR:0.000100\n",
      "   | > Step:46/68  GlobalStep:82640  TotalLoss:0.00262  PostnetLoss:0.00127  DecoderLoss:0.00136  StopLoss:0.05009  GradNorm:0.00344  GradNormST:0.01255  AvgTextLen:35.4  AvgSpecLen:122.6  StepTime:0.83  LR:0.000100\n",
      "   | > Step:56/68  GlobalStep:82650  TotalLoss:0.00285  PostnetLoss:0.00137  DecoderLoss:0.00148  StopLoss:0.03380  GradNorm:0.00319  GradNormST:0.00987  AvgTextLen:45.8  AvgSpecLen:145.4  StepTime:0.96  LR:0.000100\n",
      "   | > Step:66/68  GlobalStep:82660  TotalLoss:0.00336  PostnetLoss:0.00161  DecoderLoss:0.00175  StopLoss:0.04105  GradNorm:0.00283  GradNormST:0.01599  AvgTextLen:71.2  AvgSpecLen:220.8  StepTime:1.04  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:82662  AvgTotalLoss:0.05456  AvgPostnetLoss:0.00115  AvgDecoderLoss:0.00123  AvgStopLoss:0.05218  EpochTime:42.68  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.09642   PostnetLoss: 0.00457   DecoderLoss:0.00502  StopLoss: 0.08684  \n",
      "   | > TotalLoss: 0.06657   PostnetLoss: 0.00659   DecoderLoss:0.00723  StopLoss: 0.05275  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00115   Validation Loss: 0.00595\n",
      "\n",
      " > Epoch 198/1000\n",
      "   | > Step:7/68  GlobalStep:82670  TotalLoss:0.00175  PostnetLoss:0.00085  DecoderLoss:0.00090  StopLoss:0.04194  GradNorm:0.00347  GradNormST:0.01844  AvgTextLen:7.4  AvgSpecLen:44.6  StepTime:0.38  LR:0.000100\n",
      "   | > Step:17/68  GlobalStep:82680  TotalLoss:0.00197  PostnetLoss:0.00096  DecoderLoss:0.00102  StopLoss:0.07060  GradNorm:0.00314  GradNormST:0.01287  AvgTextLen:14.7  AvgSpecLen:66.0  StepTime:0.42  LR:0.000100\n",
      "   | > Step:27/68  GlobalStep:82690  TotalLoss:0.00230  PostnetLoss:0.00111  DecoderLoss:0.00119  StopLoss:0.05576  GradNorm:0.00305  GradNormST:0.01914  AvgTextLen:21.0  AvgSpecLen:80.2  StepTime:0.68  LR:0.000100\n",
      "   | > Step:37/68  GlobalStep:82700  TotalLoss:0.00251  PostnetLoss:0.00121  DecoderLoss:0.00130  StopLoss:0.04368  GradNorm:0.00308  GradNormST:0.01105  AvgTextLen:29.3  AvgSpecLen:102.4  StepTime:0.60  LR:0.000100\n",
      "   | > Step:47/68  GlobalStep:82710  TotalLoss:0.00269  PostnetLoss:0.00130  DecoderLoss:0.00139  StopLoss:0.04581  GradNorm:0.00285  GradNormST:0.01216  AvgTextLen:36.7  AvgSpecLen:126.9  StepTime:0.59  LR:0.000100\n",
      "   | > Step:57/68  GlobalStep:82720  TotalLoss:0.00291  PostnetLoss:0.00140  DecoderLoss:0.00151  StopLoss:0.03183  GradNorm:0.00287  GradNormST:0.00709  AvgTextLen:47.1  AvgSpecLen:161.0  StepTime:0.84  LR:0.000100\n",
      "   | > Step:67/68  GlobalStep:82730  TotalLoss:0.00334  PostnetLoss:0.00160  DecoderLoss:0.00174  StopLoss:0.03700  GradNorm:0.00281  GradNormST:0.02120  AvgTextLen:80.6  AvgSpecLen:246.5  StepTime:1.24  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:82731  AvgTotalLoss:0.05639  AvgPostnetLoss:0.00115  AvgDecoderLoss:0.00123  AvgStopLoss:0.05401  EpochTime:41.56  AvgStepTime:0.60\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.09858   PostnetLoss: 0.00463   DecoderLoss:0.00509  StopLoss: 0.08886  \n",
      "   | > TotalLoss: 0.06732   PostnetLoss: 0.00665   DecoderLoss:0.00730  StopLoss: 0.05337  \n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00115   Validation Loss: 0.00603\n",
      "\n",
      " > Epoch 199/1000\n",
      "   | > Step:8/68  GlobalStep:82740  TotalLoss:0.00179  PostnetLoss:0.00087  DecoderLoss:0.00092  StopLoss:0.07583  GradNorm:0.00425  GradNormST:0.01925  AvgTextLen:8.3  AvgSpecLen:44.0  StepTime:0.24  LR:0.000100\n",
      "   | > Step:18/68  GlobalStep:82750  TotalLoss:0.00197  PostnetLoss:0.00096  DecoderLoss:0.00102  StopLoss:0.06190  GradNorm:0.00313  GradNormST:0.01267  AvgTextLen:15.2  AvgSpecLen:64.5  StepTime:0.41  LR:0.000100\n",
      "   | > Step:28/68  GlobalStep:82760  TotalLoss:0.00225  PostnetLoss:0.00109  DecoderLoss:0.00116  StopLoss:0.05854  GradNorm:0.00293  GradNormST:0.01418  AvgTextLen:22.1  AvgSpecLen:85.4  StepTime:0.55  LR:0.000100\n",
      "   | > Step:38/68  GlobalStep:82770  TotalLoss:0.00244  PostnetLoss:0.00118  DecoderLoss:0.00126  StopLoss:0.04169  GradNorm:0.00290  GradNormST:0.00843  AvgTextLen:29.4  AvgSpecLen:104.1  StepTime:0.61  LR:0.000100\n",
      "   | > Step:48/68  GlobalStep:82780  TotalLoss:0.00268  PostnetLoss:0.00129  DecoderLoss:0.00139  StopLoss:0.04302  GradNorm:0.00294  GradNormST:0.01009  AvgTextLen:37.5  AvgSpecLen:123.8  StepTime:0.68  LR:0.000100\n",
      "   | > Step:58/68  GlobalStep:82790  TotalLoss:0.00291  PostnetLoss:0.00140  DecoderLoss:0.00151  StopLoss:0.05461  GradNorm:0.00280  GradNormST:0.01389  AvgTextLen:47.8  AvgSpecLen:158.0  StepTime:0.75  LR:0.000100\n",
      "   | > Step:68/68  GlobalStep:82800  TotalLoss:0.00337  PostnetLoss:0.00161  DecoderLoss:0.00176  StopLoss:0.03336  GradNorm:0.00271  GradNormST:0.01748  AvgTextLen:103.0  AvgSpecLen:308.9  StepTime:1.87  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:82800  AvgTotalLoss:0.05497  AvgPostnetLoss:0.00115  AvgDecoderLoss:0.00123  AvgStopLoss:0.05260  EpochTime:42.67  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.09395   PostnetLoss: 0.00462   DecoderLoss:0.00508  StopLoss: 0.08424  \n",
      "   | > TotalLoss: 0.06959   PostnetLoss: 0.00660   DecoderLoss:0.00722  StopLoss: 0.05578  \n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00115   Validation Loss: 0.00600\n",
      "\n",
      " > Epoch 200/1000\n",
      "   | > Step:9/68  GlobalStep:82810  TotalLoss:0.00172  PostnetLoss:0.00084  DecoderLoss:0.00089  StopLoss:0.05251  GradNorm:0.00365  GradNormST:0.02378  AvgTextLen:9.0  AvgSpecLen:42.1  StepTime:0.44  LR:0.000100\n",
      "   | > Step:19/68  GlobalStep:82820  TotalLoss:0.00201  PostnetLoss:0.00097  DecoderLoss:0.00104  StopLoss:0.06855  GradNorm:0.00315  GradNormST:0.01520  AvgTextLen:15.5  AvgSpecLen:63.2  StepTime:0.43  LR:0.000100\n",
      "   | > Step:29/68  GlobalStep:82830  TotalLoss:0.00223  PostnetLoss:0.00108  DecoderLoss:0.00115  StopLoss:0.06164  GradNorm:0.00309  GradNormST:0.01536  AvgTextLen:21.8  AvgSpecLen:83.4  StepTime:0.56  LR:0.000100\n",
      "   | > Step:39/68  GlobalStep:82840  TotalLoss:0.00250  PostnetLoss:0.00121  DecoderLoss:0.00129  StopLoss:0.03569  GradNorm:0.00284  GradNormST:0.00937  AvgTextLen:30.2  AvgSpecLen:110.4  StepTime:0.63  LR:0.000100\n",
      "   | > Step:49/68  GlobalStep:82850  TotalLoss:0.00270  PostnetLoss:0.00130  DecoderLoss:0.00140  StopLoss:0.03913  GradNorm:0.00302  GradNormST:0.00828  AvgTextLen:38.2  AvgSpecLen:127.7  StepTime:0.76  LR:0.000100\n",
      "   | > Step:59/68  GlobalStep:82860  TotalLoss:0.00292  PostnetLoss:0.00140  DecoderLoss:0.00152  StopLoss:0.04793  GradNorm:0.00284  GradNormST:0.01163  AvgTextLen:50.5  AvgSpecLen:155.9  StepTime:0.75  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:82869  AvgTotalLoss:0.05556  AvgPostnetLoss:0.00115  AvgDecoderLoss:0.00123  AvgStopLoss:0.05319  EpochTime:41.87  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.09286   PostnetLoss: 0.00465   DecoderLoss:0.00510  StopLoss: 0.08310  \n",
      "   | > TotalLoss: 0.06741   PostnetLoss: 0.00659   DecoderLoss:0.00721  StopLoss: 0.05361  \n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00115   Validation Loss: 0.00603\n",
      "\n",
      " > Epoch 201/1000\n",
      "   | > Step:0/68  GlobalStep:82870  TotalLoss:0.00164  PostnetLoss:0.00079  DecoderLoss:0.00085  StopLoss:0.07784  GradNorm:0.00470  GradNormST:0.02394  AvgTextLen:3.0  AvgSpecLen:25.3  StepTime:0.52  LR:0.000100\n",
      "   | > Step:10/68  GlobalStep:82880  TotalLoss:0.00178  PostnetLoss:0.00087  DecoderLoss:0.00092  StopLoss:0.06227  GradNorm:0.00370  GradNormST:0.03003  AvgTextLen:9.8  AvgSpecLen:48.6  StepTime:0.55  LR:0.000100\n",
      "   | > Step:20/68  GlobalStep:82890  TotalLoss:0.00212  PostnetLoss:0.00103  DecoderLoss:0.00109  StopLoss:0.04496  GradNorm:0.00327  GradNormST:0.01359  AvgTextLen:16.4  AvgSpecLen:64.8  StepTime:0.51  LR:0.000100\n",
      "   | > Step:30/68  GlobalStep:82900  TotalLoss:0.00225  PostnetLoss:0.00109  DecoderLoss:0.00116  StopLoss:0.04311  GradNorm:0.00296  GradNormST:0.00817  AvgTextLen:22.9  AvgSpecLen:85.8  StepTime:0.56  LR:0.000100\n",
      "   | > Step:40/68  GlobalStep:82910  TotalLoss:0.00248  PostnetLoss:0.00120  DecoderLoss:0.00128  StopLoss:0.03897  GradNorm:0.00289  GradNormST:0.00986  AvgTextLen:30.9  AvgSpecLen:105.3  StepTime:0.64  LR:0.000100\n",
      "   | > Step:50/68  GlobalStep:82920  TotalLoss:0.00269  PostnetLoss:0.00129  DecoderLoss:0.00140  StopLoss:0.03627  GradNorm:0.00293  GradNormST:0.00911  AvgTextLen:38.8  AvgSpecLen:132.0  StepTime:0.68  LR:0.000100\n",
      "   | > Step:60/68  GlobalStep:82930  TotalLoss:0.00298  PostnetLoss:0.00143  DecoderLoss:0.00155  StopLoss:0.03242  GradNorm:0.00279  GradNormST:0.00676  AvgTextLen:52.0  AvgSpecLen:172.7  StepTime:0.93  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:82938  AvgTotalLoss:0.05431  AvgPostnetLoss:0.00114  AvgDecoderLoss:0.00123  AvgStopLoss:0.05194  EpochTime:42.83  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.09371   PostnetLoss: 0.00475   DecoderLoss:0.00521  StopLoss: 0.08375  \n",
      "   | > TotalLoss: 0.07016   PostnetLoss: 0.00663   DecoderLoss:0.00726  StopLoss: 0.05626  \n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00114   Validation Loss: 0.00603\n",
      "\n",
      " > Epoch 202/1000\n",
      "   | > Step:1/68  GlobalStep:82940  TotalLoss:0.00160  PostnetLoss:0.00078  DecoderLoss:0.00083  StopLoss:0.06319  GradNorm:0.00462  GradNormST:0.02142  AvgTextLen:3.7  AvgSpecLen:28.6  StepTime:0.22  LR:0.000100\n",
      "   | > Step:11/68  GlobalStep:82950  TotalLoss:0.00183  PostnetLoss:0.00089  DecoderLoss:0.00094  StopLoss:0.04999  GradNorm:0.00332  GradNormST:0.01389  AvgTextLen:10.3  AvgSpecLen:49.4  StepTime:0.38  LR:0.000100\n",
      "   | > Step:21/68  GlobalStep:82960  TotalLoss:0.00210  PostnetLoss:0.00102  DecoderLoss:0.00108  StopLoss:0.07188  GradNorm:0.00322  GradNormST:0.02467  AvgTextLen:16.6  AvgSpecLen:66.3  StepTime:0.46  LR:0.000100\n",
      "   | > Step:31/68  GlobalStep:82970  TotalLoss:0.00227  PostnetLoss:0.00110  DecoderLoss:0.00117  StopLoss:0.04438  GradNorm:0.00317  GradNormST:0.01413  AvgTextLen:24.1  AvgSpecLen:82.7  StepTime:0.51  LR:0.000100\n",
      "   | > Step:41/68  GlobalStep:82980  TotalLoss:0.00255  PostnetLoss:0.00123  DecoderLoss:0.00132  StopLoss:0.03796  GradNorm:0.00290  GradNormST:0.01207  AvgTextLen:32.3  AvgSpecLen:109.4  StepTime:0.55  LR:0.000100\n",
      "   | > Step:51/68  GlobalStep:82990  TotalLoss:0.00268  PostnetLoss:0.00129  DecoderLoss:0.00139  StopLoss:0.03748  GradNorm:0.00270  GradNormST:0.00751  AvgTextLen:39.2  AvgSpecLen:126.9  StepTime:0.74  LR:0.000100\n",
      "   | > Step:61/68  GlobalStep:83000  TotalLoss:0.00291  PostnetLoss:0.00140  DecoderLoss:0.00151  StopLoss:0.03005  GradNorm:0.00271  GradNormST:0.00544  AvgTextLen:54.6  AvgSpecLen:165.5  StepTime:0.89  LR:0.000100\n",
      " | | > Checkpoint saving : /vm/TTS/models/cartman_models/mozilla-no-loc-fattn-stopnet-sigmoid-loss_masking-August-15-2019_11+26AM-de6aeb9/checkpoint_83000.pth.tar\n",
      "   | > EPOCH END -- GlobalStep:83007  AvgTotalLoss:0.05337  AvgPostnetLoss:0.00114  AvgDecoderLoss:0.00123  AvgStopLoss:0.05100  EpochTime:41.88  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.09570   PostnetLoss: 0.00462   DecoderLoss:0.00507  StopLoss: 0.08601  \n",
      "   | > TotalLoss: 0.07224   PostnetLoss: 0.00667   DecoderLoss:0.00730  StopLoss: 0.05827  \n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00114   Validation Loss: 0.00607\n",
      "\n",
      " > Epoch 203/1000\n",
      "   | > Step:2/68  GlobalStep:83010  TotalLoss:0.00152  PostnetLoss:0.00074  DecoderLoss:0.00079  StopLoss:0.07429  GradNorm:0.00373  GradNormST:0.02050  AvgTextLen:4.0  AvgSpecLen:31.3  StepTime:0.34  LR:0.000100\n",
      "   | > Step:12/68  GlobalStep:83020  TotalLoss:0.00180  PostnetLoss:0.00087  DecoderLoss:0.00093  StopLoss:0.06012  GradNorm:0.00323  GradNormST:0.02071  AvgTextLen:11.0  AvgSpecLen:49.3  StepTime:0.44  LR:0.000100\n",
      "   | > Step:22/68  GlobalStep:83030  TotalLoss:0.00210  PostnetLoss:0.00102  DecoderLoss:0.00108  StopLoss:0.05409  GradNorm:0.00321  GradNormST:0.01153  AvgTextLen:18.0  AvgSpecLen:72.0  StepTime:0.36  LR:0.000100\n",
      "   | > Step:32/68  GlobalStep:83040  TotalLoss:0.00234  PostnetLoss:0.00113  DecoderLoss:0.00121  StopLoss:0.06017  GradNorm:0.00297  GradNormST:0.01834  AvgTextLen:24.4  AvgSpecLen:91.5  StepTime:0.46  LR:0.000100\n",
      "   | > Step:42/68  GlobalStep:83050  TotalLoss:0.00255  PostnetLoss:0.00123  DecoderLoss:0.00132  StopLoss:0.05075  GradNorm:0.00309  GradNormST:0.01741  AvgTextLen:33.1  AvgSpecLen:109.7  StepTime:0.60  LR:0.000100\n",
      "   | > Step:52/68  GlobalStep:83060  TotalLoss:0.00275  PostnetLoss:0.00132  DecoderLoss:0.00142  StopLoss:0.04512  GradNorm:0.00282  GradNormST:0.01326  AvgTextLen:42.1  AvgSpecLen:137.5  StepTime:0.81  LR:0.000100\n",
      "   | > Step:62/68  GlobalStep:83070  TotalLoss:0.00301  PostnetLoss:0.00144  DecoderLoss:0.00156  StopLoss:0.04352  GradNorm:0.00286  GradNormST:0.01887  AvgTextLen:54.0  AvgSpecLen:174.7  StepTime:0.78  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:83076  AvgTotalLoss:0.05331  AvgPostnetLoss:0.00114  AvgDecoderLoss:0.00123  AvgStopLoss:0.05094  EpochTime:41.86  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.09930   PostnetLoss: 0.00461   DecoderLoss:0.00506  StopLoss: 0.08963  \n",
      "   | > TotalLoss: 0.07103   PostnetLoss: 0.00668   DecoderLoss:0.00731  StopLoss: 0.05704  \n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00114   Validation Loss: 0.00608\n",
      "\n",
      " > Epoch 204/1000\n",
      "   | > Step:3/68  GlobalStep:83080  TotalLoss:0.00157  PostnetLoss:0.00076  DecoderLoss:0.00081  StopLoss:0.08731  GradNorm:0.00377  GradNormST:0.02600  AvgTextLen:4.7  AvgSpecLen:29.7  StepTime:0.28  LR:0.000100\n",
      "   | > Step:13/68  GlobalStep:83090  TotalLoss:0.00179  PostnetLoss:0.00086  DecoderLoss:0.00092  StopLoss:0.06847  GradNorm:0.00307  GradNormST:0.02322  AvgTextLen:11.5  AvgSpecLen:55.2  StepTime:0.42  LR:0.000100\n",
      "   | > Step:23/68  GlobalStep:83100  TotalLoss:0.00220  PostnetLoss:0.00106  DecoderLoss:0.00113  StopLoss:0.05056  GradNorm:0.00339  GradNormST:0.01163  AvgTextLen:18.3  AvgSpecLen:72.2  StepTime:0.38  LR:0.000100\n",
      "   | > Step:33/68  GlobalStep:83110  TotalLoss:0.00238  PostnetLoss:0.00115  DecoderLoss:0.00123  StopLoss:0.04507  GradNorm:0.00314  GradNormST:0.00994  AvgTextLen:25.9  AvgSpecLen:88.9  StepTime:0.52  LR:0.000100\n",
      "   | > Step:43/68  GlobalStep:83120  TotalLoss:0.00256  PostnetLoss:0.00123  DecoderLoss:0.00132  StopLoss:0.05094  GradNorm:0.00311  GradNormST:0.00930  AvgTextLen:33.4  AvgSpecLen:115.7  StepTime:0.66  LR:0.000100\n",
      "   | > Step:53/68  GlobalStep:83130  TotalLoss:0.00271  PostnetLoss:0.00130  DecoderLoss:0.00141  StopLoss:0.04422  GradNorm:0.00267  GradNormST:0.00953  AvgTextLen:42.4  AvgSpecLen:137.6  StepTime:0.81  LR:0.000100\n",
      "   | > Step:63/68  GlobalStep:83140  TotalLoss:0.00308  PostnetLoss:0.00148  DecoderLoss:0.00160  StopLoss:0.03788  GradNorm:0.00268  GradNormST:0.01583  AvgTextLen:58.6  AvgSpecLen:185.1  StepTime:0.91  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:83145  AvgTotalLoss:0.05244  AvgPostnetLoss:0.00114  AvgDecoderLoss:0.00123  AvgStopLoss:0.05007  EpochTime:43.41  AvgStepTime:0.63\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.09708   PostnetLoss: 0.00465   DecoderLoss:0.00510  StopLoss: 0.08733  \n",
      "   | > TotalLoss: 0.07398   PostnetLoss: 0.00667   DecoderLoss:0.00732  StopLoss: 0.05999  \n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00114   Validation Loss: 0.00610\n",
      "\n",
      " > Epoch 205/1000\n",
      "   | > Step:4/68  GlobalStep:83150  TotalLoss:0.00165  PostnetLoss:0.00080  DecoderLoss:0.00085  StopLoss:0.09338  GradNorm:0.00418  GradNormST:0.03253  AvgTextLen:4.8  AvgSpecLen:31.1  StepTime:0.28  LR:0.000100\n",
      "   | > Step:14/68  GlobalStep:83160  TotalLoss:0.00191  PostnetLoss:0.00093  DecoderLoss:0.00099  StopLoss:0.06582  GradNorm:0.00358  GradNormST:0.01739  AvgTextLen:12.2  AvgSpecLen:52.0  StepTime:0.36  LR:0.000100\n",
      "   | > Step:24/68  GlobalStep:83170  TotalLoss:0.00224  PostnetLoss:0.00108  DecoderLoss:0.00116  StopLoss:0.06023  GradNorm:0.00306  GradNormST:0.01337  AvgTextLen:19.2  AvgSpecLen:79.0  StepTime:0.48  LR:0.000100\n",
      "   | > Step:34/68  GlobalStep:83180  TotalLoss:0.00241  PostnetLoss:0.00117  DecoderLoss:0.00125  StopLoss:0.03926  GradNorm:0.00304  GradNormST:0.00853  AvgTextLen:26.2  AvgSpecLen:95.9  StepTime:0.58  LR:0.000100\n",
      "   | > Step:44/68  GlobalStep:83190  TotalLoss:0.00258  PostnetLoss:0.00125  DecoderLoss:0.00134  StopLoss:0.04275  GradNorm:0.00289  GradNormST:0.00978  AvgTextLen:33.8  AvgSpecLen:116.3  StepTime:0.53  LR:0.000100\n",
      "   | > Step:54/68  GlobalStep:83200  TotalLoss:0.00282  PostnetLoss:0.00136  DecoderLoss:0.00147  StopLoss:0.04295  GradNorm:0.00310  GradNormST:0.00985  AvgTextLen:42.2  AvgSpecLen:144.9  StepTime:0.81  LR:0.000100\n",
      "   | > Step:64/68  GlobalStep:83210  TotalLoss:0.00290  PostnetLoss:0.00139  DecoderLoss:0.00151  StopLoss:0.03609  GradNorm:0.00263  GradNormST:0.01128  AvgTextLen:60.6  AvgSpecLen:177.6  StepTime:0.80  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:83214  AvgTotalLoss:0.05302  AvgPostnetLoss:0.00114  AvgDecoderLoss:0.00123  AvgStopLoss:0.05064  EpochTime:42.16  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.09754   PostnetLoss: 0.00465   DecoderLoss:0.00510  StopLoss: 0.08779  \n",
      "   | > TotalLoss: 0.06726   PostnetLoss: 0.00675   DecoderLoss:0.00738  StopLoss: 0.05313  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00114   Validation Loss: 0.00613\n",
      "\n",
      " > Epoch 206/1000\n",
      "   | > Step:5/68  GlobalStep:83220  TotalLoss:0.00165  PostnetLoss:0.00080  DecoderLoss:0.00085  StopLoss:0.08434  GradNorm:0.00414  GradNormST:0.02648  AvgTextLen:5.9  AvgSpecLen:33.2  StepTime:0.36  LR:0.000100\n",
      "   | > Step:15/68  GlobalStep:83230  TotalLoss:0.00196  PostnetLoss:0.00095  DecoderLoss:0.00101  StopLoss:0.05184  GradNorm:0.00339  GradNormST:0.02101  AvgTextLen:12.8  AvgSpecLen:59.1  StepTime:0.52  LR:0.000100\n",
      "   | > Step:25/68  GlobalStep:83240  TotalLoss:0.00218  PostnetLoss:0.00105  DecoderLoss:0.00112  StopLoss:0.04774  GradNorm:0.00321  GradNormST:0.01196  AvgTextLen:19.8  AvgSpecLen:73.3  StepTime:0.48  LR:0.000100\n",
      "   | > Step:35/68  GlobalStep:83250  TotalLoss:0.00243  PostnetLoss:0.00117  DecoderLoss:0.00125  StopLoss:0.05434  GradNorm:0.00303  GradNormST:0.01818  AvgTextLen:27.1  AvgSpecLen:104.7  StepTime:0.65  LR:0.000100\n",
      "   | > Step:45/68  GlobalStep:83260  TotalLoss:0.00256  PostnetLoss:0.00123  DecoderLoss:0.00132  StopLoss:0.05423  GradNorm:0.00280  GradNormST:0.01749  AvgTextLen:34.4  AvgSpecLen:118.6  StepTime:0.54  LR:0.000100\n",
      "   | > Step:55/68  GlobalStep:83270  TotalLoss:0.00285  PostnetLoss:0.00137  DecoderLoss:0.00148  StopLoss:0.03625  GradNorm:0.00279  GradNormST:0.00561  AvgTextLen:43.9  AvgSpecLen:145.2  StepTime:0.67  LR:0.000100\n",
      "   | > Step:65/68  GlobalStep:83280  TotalLoss:0.00313  PostnetLoss:0.00150  DecoderLoss:0.00163  StopLoss:0.03688  GradNorm:0.00296  GradNormST:0.01143  AvgTextLen:64.2  AvgSpecLen:207.4  StepTime:1.19  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:83283  AvgTotalLoss:0.05359  AvgPostnetLoss:0.00114  AvgDecoderLoss:0.00122  AvgStopLoss:0.05123  EpochTime:42.76  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.09611   PostnetLoss: 0.00464   DecoderLoss:0.00509  StopLoss: 0.08638  \n",
      "   | > TotalLoss: 0.06938   PostnetLoss: 0.00675   DecoderLoss:0.00738  StopLoss: 0.05525  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00114   Validation Loss: 0.00612\n",
      "\n",
      " > Epoch 207/1000\n",
      "   | > Step:6/68  GlobalStep:83290  TotalLoss:0.00164  PostnetLoss:0.00079  DecoderLoss:0.00084  StopLoss:0.05792  GradNorm:0.00365  GradNormST:0.01996  AvgTextLen:6.0  AvgSpecLen:34.8  StepTime:0.38  LR:0.000100\n",
      "   | > Step:16/68  GlobalStep:83300  TotalLoss:0.00199  PostnetLoss:0.00096  DecoderLoss:0.00102  StopLoss:0.04328  GradNorm:0.00338  GradNormST:0.01347  AvgTextLen:13.8  AvgSpecLen:60.6  StepTime:0.36  LR:0.000100\n",
      "   | > Step:26/68  GlobalStep:83310  TotalLoss:0.00219  PostnetLoss:0.00106  DecoderLoss:0.00113  StopLoss:0.05245  GradNorm:0.00316  GradNormST:0.01051  AvgTextLen:20.3  AvgSpecLen:77.5  StepTime:0.50  LR:0.000100\n",
      "   | > Step:36/68  GlobalStep:83320  TotalLoss:0.00241  PostnetLoss:0.00116  DecoderLoss:0.00125  StopLoss:0.06389  GradNorm:0.00294  GradNormST:0.01106  AvgTextLen:28.5  AvgSpecLen:100.2  StepTime:0.69  LR:0.000100\n",
      "   | > Step:46/68  GlobalStep:83330  TotalLoss:0.00260  PostnetLoss:0.00126  DecoderLoss:0.00135  StopLoss:0.04122  GradNorm:0.00304  GradNormST:0.01025  AvgTextLen:35.4  AvgSpecLen:122.6  StepTime:0.78  LR:0.000100\n",
      "   | > Step:56/68  GlobalStep:83340  TotalLoss:0.00285  PostnetLoss:0.00137  DecoderLoss:0.00148  StopLoss:0.03027  GradNorm:0.00300  GradNormST:0.00737  AvgTextLen:45.8  AvgSpecLen:145.4  StepTime:0.96  LR:0.000100\n",
      "   | > Step:66/68  GlobalStep:83350  TotalLoss:0.00331  PostnetLoss:0.00158  DecoderLoss:0.00173  StopLoss:0.04194  GradNorm:0.00259  GradNormST:0.01178  AvgTextLen:71.2  AvgSpecLen:220.8  StepTime:1.06  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:83352  AvgTotalLoss:0.05247  AvgPostnetLoss:0.00114  AvgDecoderLoss:0.00122  AvgStopLoss:0.05011  EpochTime:42.68  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.09424   PostnetLoss: 0.00461   DecoderLoss:0.00507  StopLoss: 0.08456  \n",
      "   | > TotalLoss: 0.07528   PostnetLoss: 0.00663   DecoderLoss:0.00727  StopLoss: 0.06138  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00114   Validation Loss: 0.00609\n",
      "\n",
      " > Epoch 208/1000\n",
      "   | > Step:7/68  GlobalStep:83360  TotalLoss:0.00175  PostnetLoss:0.00085  DecoderLoss:0.00090  StopLoss:0.03794  GradNorm:0.00368  GradNormST:0.01633  AvgTextLen:7.4  AvgSpecLen:44.6  StepTime:0.38  LR:0.000100\n",
      "   | > Step:17/68  GlobalStep:83370  TotalLoss:0.00201  PostnetLoss:0.00098  DecoderLoss:0.00104  StopLoss:0.06435  GradNorm:0.00328  GradNormST:0.01208  AvgTextLen:14.7  AvgSpecLen:66.0  StepTime:0.42  LR:0.000100\n",
      "   | > Step:27/68  GlobalStep:83380  TotalLoss:0.00226  PostnetLoss:0.00110  DecoderLoss:0.00117  StopLoss:0.06244  GradNorm:0.00308  GradNormST:0.02735  AvgTextLen:21.0  AvgSpecLen:80.2  StepTime:0.63  LR:0.000100\n",
      "   | > Step:37/68  GlobalStep:83390  TotalLoss:0.00247  PostnetLoss:0.00119  DecoderLoss:0.00128  StopLoss:0.04810  GradNorm:0.00289  GradNormST:0.01227  AvgTextLen:29.3  AvgSpecLen:102.4  StepTime:0.59  LR:0.000100\n",
      "   | > Step:47/68  GlobalStep:83400  TotalLoss:0.00267  PostnetLoss:0.00129  DecoderLoss:0.00138  StopLoss:0.04686  GradNorm:0.00294  GradNormST:0.01196  AvgTextLen:36.7  AvgSpecLen:126.9  StepTime:0.76  LR:0.000100\n",
      "   | > Step:57/68  GlobalStep:83410  TotalLoss:0.00291  PostnetLoss:0.00140  DecoderLoss:0.00151  StopLoss:0.03218  GradNorm:0.00288  GradNormST:0.00489  AvgTextLen:47.1  AvgSpecLen:161.0  StepTime:0.92  LR:0.000100\n",
      "   | > Step:67/68  GlobalStep:83420  TotalLoss:0.00327  PostnetLoss:0.00157  DecoderLoss:0.00171  StopLoss:0.03770  GradNorm:0.00267  GradNormST:0.01672  AvgTextLen:80.6  AvgSpecLen:246.5  StepTime:1.21  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:83421  AvgTotalLoss:0.05314  AvgPostnetLoss:0.00114  AvgDecoderLoss:0.00122  AvgStopLoss:0.05077  EpochTime:42.44  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.09372   PostnetLoss: 0.00461   DecoderLoss:0.00505  StopLoss: 0.08406  \n",
      "   | > TotalLoss: 0.07301   PostnetLoss: 0.00672   DecoderLoss:0.00733  StopLoss: 0.05897  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00114   Validation Loss: 0.00614\n",
      "\n",
      " > Epoch 209/1000\n",
      "   | > Step:8/68  GlobalStep:83430  TotalLoss:0.00177  PostnetLoss:0.00086  DecoderLoss:0.00091  StopLoss:0.05995  GradNorm:0.00383  GradNormST:0.01397  AvgTextLen:8.3  AvgSpecLen:44.0  StepTime:0.30  LR:0.000100\n",
      "   | > Step:18/68  GlobalStep:83440  TotalLoss:0.00195  PostnetLoss:0.00095  DecoderLoss:0.00101  StopLoss:0.06204  GradNorm:0.00312  GradNormST:0.01469  AvgTextLen:15.2  AvgSpecLen:64.5  StepTime:0.32  LR:0.000100\n",
      "   | > Step:28/68  GlobalStep:83450  TotalLoss:0.00229  PostnetLoss:0.00111  DecoderLoss:0.00118  StopLoss:0.05676  GradNorm:0.00314  GradNormST:0.01228  AvgTextLen:22.1  AvgSpecLen:85.4  StepTime:0.58  LR:0.000100\n",
      "   | > Step:38/68  GlobalStep:83460  TotalLoss:0.00248  PostnetLoss:0.00120  DecoderLoss:0.00128  StopLoss:0.04445  GradNorm:0.00311  GradNormST:0.00950  AvgTextLen:29.4  AvgSpecLen:104.1  StepTime:0.49  LR:0.000100\n",
      "   | > Step:48/68  GlobalStep:83470  TotalLoss:0.00267  PostnetLoss:0.00129  DecoderLoss:0.00138  StopLoss:0.03370  GradNorm:0.00297  GradNormST:0.00664  AvgTextLen:37.5  AvgSpecLen:123.8  StepTime:0.60  LR:0.000100\n",
      "   | > Step:58/68  GlobalStep:83480  TotalLoss:0.00290  PostnetLoss:0.00139  DecoderLoss:0.00150  StopLoss:0.04948  GradNorm:0.00277  GradNormST:0.01428  AvgTextLen:47.8  AvgSpecLen:158.0  StepTime:0.70  LR:0.000100\n",
      "   | > Step:68/68  GlobalStep:83490  TotalLoss:0.00330  PostnetLoss:0.00158  DecoderLoss:0.00172  StopLoss:0.03221  GradNorm:0.00275  GradNormST:0.01218  AvgTextLen:103.0  AvgSpecLen:308.9  StepTime:1.88  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:83490  AvgTotalLoss:0.05322  AvgPostnetLoss:0.00114  AvgDecoderLoss:0.00122  AvgStopLoss:0.05086  EpochTime:42.71  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.09769   PostnetLoss: 0.00461   DecoderLoss:0.00507  StopLoss: 0.08801  \n",
      "   | > TotalLoss: 0.07728   PostnetLoss: 0.00684   DecoderLoss:0.00749  StopLoss: 0.06295  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00114   Validation Loss: 0.00619\n",
      "\n",
      " > Epoch 210/1000\n",
      "   | > Step:9/68  GlobalStep:83500  TotalLoss:0.00170  PostnetLoss:0.00083  DecoderLoss:0.00087  StopLoss:0.05538  GradNorm:0.00346  GradNormST:0.02634  AvgTextLen:9.0  AvgSpecLen:42.1  StepTime:0.42  LR:0.000100\n",
      "   | > Step:19/68  GlobalStep:83510  TotalLoss:0.00199  PostnetLoss:0.00096  DecoderLoss:0.00102  StopLoss:0.06397  GradNorm:0.00323  GradNormST:0.02048  AvgTextLen:15.5  AvgSpecLen:63.2  StepTime:0.41  LR:0.000100\n",
      "   | > Step:29/68  GlobalStep:83520  TotalLoss:0.00218  PostnetLoss:0.00105  DecoderLoss:0.00112  StopLoss:0.06285  GradNorm:0.00290  GradNormST:0.01886  AvgTextLen:21.8  AvgSpecLen:83.4  StepTime:0.56  LR:0.000100\n",
      "   | > Step:39/68  GlobalStep:83530  TotalLoss:0.00249  PostnetLoss:0.00120  DecoderLoss:0.00129  StopLoss:0.04074  GradNorm:0.00284  GradNormST:0.00975  AvgTextLen:30.2  AvgSpecLen:110.4  StepTime:0.68  LR:0.000100\n",
      "   | > Step:49/68  GlobalStep:83540  TotalLoss:0.00265  PostnetLoss:0.00128  DecoderLoss:0.00137  StopLoss:0.03371  GradNorm:0.00298  GradNormST:0.00844  AvgTextLen:38.2  AvgSpecLen:127.7  StepTime:0.73  LR:0.000100\n",
      "   | > Step:59/68  GlobalStep:83550  TotalLoss:0.00287  PostnetLoss:0.00138  DecoderLoss:0.00149  StopLoss:0.03523  GradNorm:0.00276  GradNormST:0.00928  AvgTextLen:50.5  AvgSpecLen:155.9  StepTime:0.71  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:83559  AvgTotalLoss:0.05270  AvgPostnetLoss:0.00114  AvgDecoderLoss:0.00122  AvgStopLoss:0.05034  EpochTime:43.14  AvgStepTime:0.63\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.09323   PostnetLoss: 0.00466   DecoderLoss:0.00511  StopLoss: 0.08345  \n",
      "   | > TotalLoss: 0.06966   PostnetLoss: 0.00674   DecoderLoss:0.00737  StopLoss: 0.05555  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00114   Validation Loss: 0.00610\n",
      "\n",
      " > Epoch 211/1000\n",
      "   | > Step:0/68  GlobalStep:83560  TotalLoss:0.00164  PostnetLoss:0.00079  DecoderLoss:0.00086  StopLoss:0.07880  GradNorm:0.00526  GradNormST:0.02242  AvgTextLen:3.0  AvgSpecLen:25.3  StepTime:0.47  LR:0.000100\n",
      "   | > Step:10/68  GlobalStep:83570  TotalLoss:0.00179  PostnetLoss:0.00086  DecoderLoss:0.00092  StopLoss:0.05786  GradNorm:0.00346  GradNormST:0.02530  AvgTextLen:9.8  AvgSpecLen:48.6  StepTime:0.43  LR:0.000100\n",
      "   | > Step:20/68  GlobalStep:83580  TotalLoss:0.00212  PostnetLoss:0.00103  DecoderLoss:0.00110  StopLoss:0.04581  GradNorm:0.00335  GradNormST:0.01244  AvgTextLen:16.4  AvgSpecLen:64.8  StepTime:0.42  LR:0.000100\n",
      "   | > Step:30/68  GlobalStep:83590  TotalLoss:0.00229  PostnetLoss:0.00111  DecoderLoss:0.00118  StopLoss:0.05519  GradNorm:0.00320  GradNormST:0.00953  AvgTextLen:22.9  AvgSpecLen:85.8  StepTime:0.61  LR:0.000100\n",
      "   | > Step:40/68  GlobalStep:83600  TotalLoss:0.00246  PostnetLoss:0.00119  DecoderLoss:0.00127  StopLoss:0.03734  GradNorm:0.00290  GradNormST:0.00694  AvgTextLen:30.9  AvgSpecLen:105.3  StepTime:0.60  LR:0.000100\n",
      "   | > Step:50/68  GlobalStep:83610  TotalLoss:0.00266  PostnetLoss:0.00128  DecoderLoss:0.00138  StopLoss:0.03946  GradNorm:0.00281  GradNormST:0.00827  AvgTextLen:38.8  AvgSpecLen:132.0  StepTime:0.67  LR:0.000100\n",
      "   | > Step:60/68  GlobalStep:83620  TotalLoss:0.00294  PostnetLoss:0.00141  DecoderLoss:0.00153  StopLoss:0.03051  GradNorm:0.00290  GradNormST:0.00613  AvgTextLen:52.0  AvgSpecLen:172.7  StepTime:0.93  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:83628  AvgTotalLoss:0.05224  AvgPostnetLoss:0.00114  AvgDecoderLoss:0.00122  AvgStopLoss:0.04988  EpochTime:42.27  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.09351   PostnetLoss: 0.00479   DecoderLoss:0.00525  StopLoss: 0.08347  \n",
      "   | > TotalLoss: 0.07169   PostnetLoss: 0.00674   DecoderLoss:0.00737  StopLoss: 0.05759  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00114   Validation Loss: 0.00617\n",
      "\n",
      " > Epoch 212/1000\n",
      "   | > Step:1/68  GlobalStep:83630  TotalLoss:0.00161  PostnetLoss:0.00078  DecoderLoss:0.00083  StopLoss:0.06510  GradNorm:0.00455  GradNormST:0.01834  AvgTextLen:3.7  AvgSpecLen:28.6  StepTime:0.39  LR:0.000100\n",
      "   | > Step:11/68  GlobalStep:83640  TotalLoss:0.00177  PostnetLoss:0.00086  DecoderLoss:0.00091  StopLoss:0.04266  GradNorm:0.00325  GradNormST:0.01349  AvgTextLen:10.3  AvgSpecLen:49.4  StepTime:0.39  LR:0.000100\n",
      "   | > Step:21/68  GlobalStep:83650  TotalLoss:0.00210  PostnetLoss:0.00102  DecoderLoss:0.00108  StopLoss:0.05507  GradNorm:0.00324  GradNormST:0.02000  AvgTextLen:16.6  AvgSpecLen:66.3  StepTime:0.34  LR:0.000100\n",
      "   | > Step:31/68  GlobalStep:83660  TotalLoss:0.00231  PostnetLoss:0.00112  DecoderLoss:0.00119  StopLoss:0.04430  GradNorm:0.00317  GradNormST:0.01085  AvgTextLen:24.1  AvgSpecLen:82.7  StepTime:0.57  LR:0.000100\n",
      "   | > Step:41/68  GlobalStep:83670  TotalLoss:0.00258  PostnetLoss:0.00125  DecoderLoss:0.00133  StopLoss:0.04302  GradNorm:0.00295  GradNormST:0.01317  AvgTextLen:32.3  AvgSpecLen:109.4  StepTime:0.68  LR:0.000100\n",
      "   | > Step:51/68  GlobalStep:83680  TotalLoss:0.00271  PostnetLoss:0.00130  DecoderLoss:0.00141  StopLoss:0.04058  GradNorm:0.00284  GradNormST:0.00973  AvgTextLen:39.2  AvgSpecLen:126.9  StepTime:0.72  LR:0.000100\n",
      "   | > Step:61/68  GlobalStep:83690  TotalLoss:0.00291  PostnetLoss:0.00140  DecoderLoss:0.00151  StopLoss:0.03672  GradNorm:0.00271  GradNormST:0.01082  AvgTextLen:54.6  AvgSpecLen:165.5  StepTime:0.89  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:83697  AvgTotalLoss:0.05270  AvgPostnetLoss:0.00114  AvgDecoderLoss:0.00122  AvgStopLoss:0.05034  EpochTime:42.53  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.09470   PostnetLoss: 0.00462   DecoderLoss:0.00507  StopLoss: 0.08501  \n",
      "   | > TotalLoss: 0.06864   PostnetLoss: 0.00678   DecoderLoss:0.00742  StopLoss: 0.05445  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00114   Validation Loss: 0.00616\n",
      "\n",
      " > Epoch 213/1000\n",
      "   | > Step:2/68  GlobalStep:83700  TotalLoss:0.00151  PostnetLoss:0.00073  DecoderLoss:0.00078  StopLoss:0.07330  GradNorm:0.00388  GradNormST:0.02108  AvgTextLen:4.0  AvgSpecLen:31.3  StepTime:0.47  LR:0.000100\n",
      "   | > Step:12/68  GlobalStep:83710  TotalLoss:0.00180  PostnetLoss:0.00087  DecoderLoss:0.00093  StopLoss:0.04789  GradNorm:0.00336  GradNormST:0.00943  AvgTextLen:11.0  AvgSpecLen:49.3  StepTime:0.39  LR:0.000100\n",
      "   | > Step:22/68  GlobalStep:83720  TotalLoss:0.00210  PostnetLoss:0.00102  DecoderLoss:0.00108  StopLoss:0.06131  GradNorm:0.00312  GradNormST:0.01206  AvgTextLen:18.0  AvgSpecLen:72.0  StepTime:0.49  LR:0.000100\n",
      "   | > Step:32/68  GlobalStep:83730  TotalLoss:0.00228  PostnetLoss:0.00110  DecoderLoss:0.00118  StopLoss:0.05706  GradNorm:0.00298  GradNormST:0.01549  AvgTextLen:24.4  AvgSpecLen:91.5  StepTime:0.59  LR:0.000100\n",
      "   | > Step:42/68  GlobalStep:83740  TotalLoss:0.00251  PostnetLoss:0.00121  DecoderLoss:0.00130  StopLoss:0.04727  GradNorm:0.00278  GradNormST:0.01351  AvgTextLen:33.1  AvgSpecLen:109.7  StepTime:0.64  LR:0.000100\n",
      "   | > Step:52/68  GlobalStep:83750  TotalLoss:0.00273  PostnetLoss:0.00132  DecoderLoss:0.00141  StopLoss:0.05271  GradNorm:0.00281  GradNormST:0.01369  AvgTextLen:42.1  AvgSpecLen:137.5  StepTime:0.77  LR:0.000100\n",
      "   | > Step:62/68  GlobalStep:83760  TotalLoss:0.00301  PostnetLoss:0.00145  DecoderLoss:0.00157  StopLoss:0.03810  GradNorm:0.00277  GradNormST:0.01602  AvgTextLen:54.0  AvgSpecLen:174.7  StepTime:0.75  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:83766  AvgTotalLoss:0.05248  AvgPostnetLoss:0.00114  AvgDecoderLoss:0.00122  AvgStopLoss:0.05012  EpochTime:42.63  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.09404   PostnetLoss: 0.00468   DecoderLoss:0.00512  StopLoss: 0.08424  \n",
      "   | > TotalLoss: 0.06878   PostnetLoss: 0.00668   DecoderLoss:0.00730  StopLoss: 0.05480  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00114   Validation Loss: 0.00615\n",
      "\n",
      " > Epoch 214/1000\n",
      "   | > Step:3/68  GlobalStep:83770  TotalLoss:0.00159  PostnetLoss:0.00077  DecoderLoss:0.00082  StopLoss:0.06730  GradNorm:0.00407  GradNormST:0.01721  AvgTextLen:4.7  AvgSpecLen:29.7  StepTime:0.30  LR:0.000100\n",
      "   | > Step:13/68  GlobalStep:83780  TotalLoss:0.00181  PostnetLoss:0.00088  DecoderLoss:0.00093  StopLoss:0.06656  GradNorm:0.00326  GradNormST:0.02406  AvgTextLen:11.5  AvgSpecLen:55.2  StepTime:0.44  LR:0.000100\n",
      "   | > Step:23/68  GlobalStep:83790  TotalLoss:0.00218  PostnetLoss:0.00105  DecoderLoss:0.00112  StopLoss:0.05097  GradNorm:0.00307  GradNormST:0.00896  AvgTextLen:18.3  AvgSpecLen:72.2  StepTime:0.41  LR:0.000100\n",
      "   | > Step:33/68  GlobalStep:83800  TotalLoss:0.00236  PostnetLoss:0.00114  DecoderLoss:0.00122  StopLoss:0.04649  GradNorm:0.00301  GradNormST:0.00809  AvgTextLen:25.9  AvgSpecLen:88.9  StepTime:0.48  LR:0.000100\n",
      "   | > Step:43/68  GlobalStep:83810  TotalLoss:0.00255  PostnetLoss:0.00123  DecoderLoss:0.00132  StopLoss:0.04421  GradNorm:0.00295  GradNormST:0.01514  AvgTextLen:33.4  AvgSpecLen:115.7  StepTime:0.68  LR:0.000100\n",
      "   | > Step:53/68  GlobalStep:83820  TotalLoss:0.00269  PostnetLoss:0.00130  DecoderLoss:0.00140  StopLoss:0.03716  GradNorm:0.00282  GradNormST:0.00802  AvgTextLen:42.4  AvgSpecLen:137.6  StepTime:0.83  LR:0.000100\n",
      "   | > Step:63/68  GlobalStep:83830  TotalLoss:0.00308  PostnetLoss:0.00147  DecoderLoss:0.00160  StopLoss:0.02986  GradNorm:0.00293  GradNormST:0.00882  AvgTextLen:58.6  AvgSpecLen:185.1  StepTime:0.91  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:83835  AvgTotalLoss:0.05161  AvgPostnetLoss:0.00114  AvgDecoderLoss:0.00122  AvgStopLoss:0.04925  EpochTime:42.54  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.09406   PostnetLoss: 0.00462   DecoderLoss:0.00507  StopLoss: 0.08437  \n",
      "   | > TotalLoss: 0.06914   PostnetLoss: 0.00677   DecoderLoss:0.00742  StopLoss: 0.05496  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00114   Validation Loss: 0.00614\n",
      "\n",
      " > Epoch 215/1000\n",
      "   | > Step:4/68  GlobalStep:83840  TotalLoss:0.00156  PostnetLoss:0.00076  DecoderLoss:0.00080  StopLoss:0.09038  GradNorm:0.00410  GradNormST:0.03633  AvgTextLen:4.8  AvgSpecLen:31.1  StepTime:0.23  LR:0.000100\n",
      "   | > Step:14/68  GlobalStep:83850  TotalLoss:0.00190  PostnetLoss:0.00092  DecoderLoss:0.00098  StopLoss:0.06381  GradNorm:0.00352  GradNormST:0.01561  AvgTextLen:12.2  AvgSpecLen:52.0  StepTime:0.29  LR:0.000100\n",
      "   | > Step:24/68  GlobalStep:83860  TotalLoss:0.00225  PostnetLoss:0.00109  DecoderLoss:0.00116  StopLoss:0.05376  GradNorm:0.00320  GradNormST:0.01155  AvgTextLen:19.2  AvgSpecLen:79.0  StepTime:0.41  LR:0.000100\n",
      "   | > Step:34/68  GlobalStep:83870  TotalLoss:0.00240  PostnetLoss:0.00116  DecoderLoss:0.00124  StopLoss:0.04099  GradNorm:0.00297  GradNormST:0.00881  AvgTextLen:26.2  AvgSpecLen:95.9  StepTime:0.48  LR:0.000100\n",
      "   | > Step:44/68  GlobalStep:83880  TotalLoss:0.00260  PostnetLoss:0.00126  DecoderLoss:0.00135  StopLoss:0.03994  GradNorm:0.00306  GradNormST:0.01243  AvgTextLen:33.8  AvgSpecLen:116.3  StepTime:0.70  LR:0.000100\n",
      "   | > Step:54/68  GlobalStep:83890  TotalLoss:0.00277  PostnetLoss:0.00133  DecoderLoss:0.00144  StopLoss:0.04238  GradNorm:0.00286  GradNormST:0.01098  AvgTextLen:42.2  AvgSpecLen:144.9  StepTime:0.83  LR:0.000100\n",
      "   | > Step:64/68  GlobalStep:83900  TotalLoss:0.00290  PostnetLoss:0.00139  DecoderLoss:0.00151  StopLoss:0.03614  GradNorm:0.00308  GradNormST:0.01011  AvgTextLen:60.6  AvgSpecLen:177.6  StepTime:0.80  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:83904  AvgTotalLoss:0.05237  AvgPostnetLoss:0.00114  AvgDecoderLoss:0.00122  AvgStopLoss:0.05001  EpochTime:41.56  AvgStepTime:0.60\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.09412   PostnetLoss: 0.00459   DecoderLoss:0.00504  StopLoss: 0.08449  \n",
      "   | > TotalLoss: 0.07153   PostnetLoss: 0.00677   DecoderLoss:0.00740  StopLoss: 0.05736  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00114   Validation Loss: 0.00619\n",
      "\n",
      " > Epoch 216/1000\n",
      "   | > Step:5/68  GlobalStep:83910  TotalLoss:0.00163  PostnetLoss:0.00079  DecoderLoss:0.00084  StopLoss:0.06136  GradNorm:0.00350  GradNormST:0.01861  AvgTextLen:5.9  AvgSpecLen:33.2  StepTime:0.26  LR:0.000100\n",
      "   | > Step:15/68  GlobalStep:83920  TotalLoss:0.00199  PostnetLoss:0.00097  DecoderLoss:0.00102  StopLoss:0.05492  GradNorm:0.00378  GradNormST:0.01674  AvgTextLen:12.8  AvgSpecLen:59.1  StepTime:0.46  LR:0.000100\n",
      "   | > Step:25/68  GlobalStep:83930  TotalLoss:0.00217  PostnetLoss:0.00105  DecoderLoss:0.00112  StopLoss:0.04727  GradNorm:0.00319  GradNormST:0.01030  AvgTextLen:19.8  AvgSpecLen:73.3  StepTime:0.36  LR:0.000100\n",
      "   | > Step:35/68  GlobalStep:83940  TotalLoss:0.00241  PostnetLoss:0.00117  DecoderLoss:0.00125  StopLoss:0.04961  GradNorm:0.00299  GradNormST:0.01941  AvgTextLen:27.1  AvgSpecLen:104.7  StepTime:0.77  LR:0.000100\n",
      "   | > Step:45/68  GlobalStep:83950  TotalLoss:0.00254  PostnetLoss:0.00122  DecoderLoss:0.00131  StopLoss:0.04923  GradNorm:0.00293  GradNormST:0.02096  AvgTextLen:34.4  AvgSpecLen:118.6  StepTime:0.58  LR:0.000100\n",
      "   | > Step:55/68  GlobalStep:83960  TotalLoss:0.00284  PostnetLoss:0.00137  DecoderLoss:0.00147  StopLoss:0.03723  GradNorm:0.00277  GradNormST:0.00710  AvgTextLen:43.9  AvgSpecLen:145.2  StepTime:0.66  LR:0.000100\n",
      "   | > Step:65/68  GlobalStep:83970  TotalLoss:0.00311  PostnetLoss:0.00149  DecoderLoss:0.00162  StopLoss:0.02989  GradNorm:0.00285  GradNormST:0.01029  AvgTextLen:64.2  AvgSpecLen:207.4  StepTime:1.19  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:83973  AvgTotalLoss:0.05190  AvgPostnetLoss:0.00114  AvgDecoderLoss:0.00122  AvgStopLoss:0.04955  EpochTime:42.12  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.09333   PostnetLoss: 0.00465   DecoderLoss:0.00510  StopLoss: 0.08358  \n",
      "   | > TotalLoss: 0.07295   PostnetLoss: 0.00660   DecoderLoss:0.00722  StopLoss: 0.05913  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00114   Validation Loss: 0.00615\n",
      "\n",
      " > Epoch 217/1000\n",
      "   | > Step:6/68  GlobalStep:83980  TotalLoss:0.00162  PostnetLoss:0.00079  DecoderLoss:0.00083  StopLoss:0.04595  GradNorm:0.00386  GradNormST:0.01482  AvgTextLen:6.0  AvgSpecLen:34.8  StepTime:0.38  LR:0.000100\n",
      "   | > Step:16/68  GlobalStep:83990  TotalLoss:0.00199  PostnetLoss:0.00097  DecoderLoss:0.00103  StopLoss:0.05015  GradNorm:0.00385  GradNormST:0.01297  AvgTextLen:13.8  AvgSpecLen:60.6  StepTime:0.29  LR:0.000100\n",
      "   | > Step:26/68  GlobalStep:84000  TotalLoss:0.00227  PostnetLoss:0.00110  DecoderLoss:0.00117  StopLoss:0.05212  GradNorm:0.00346  GradNormST:0.01371  AvgTextLen:20.3  AvgSpecLen:77.5  StepTime:0.40  LR:0.000100\n",
      " | | > Checkpoint saving : /vm/TTS/models/cartman_models/mozilla-no-loc-fattn-stopnet-sigmoid-loss_masking-August-15-2019_11+26AM-de6aeb9/checkpoint_84000.pth.tar\n",
      "   | > Step:36/68  GlobalStep:84010  TotalLoss:0.00242  PostnetLoss:0.00117  DecoderLoss:0.00126  StopLoss:0.06318  GradNorm:0.00300  GradNormST:0.01187  AvgTextLen:28.5  AvgSpecLen:100.2  StepTime:0.59  LR:0.000100\n",
      "   | > Step:46/68  GlobalStep:84020  TotalLoss:0.00258  PostnetLoss:0.00125  DecoderLoss:0.00134  StopLoss:0.04321  GradNorm:0.00293  GradNormST:0.00863  AvgTextLen:35.4  AvgSpecLen:122.6  StepTime:0.66  LR:0.000100\n",
      "   | > Step:56/68  GlobalStep:84030  TotalLoss:0.00281  PostnetLoss:0.00135  DecoderLoss:0.00146  StopLoss:0.03141  GradNorm:0.00275  GradNormST:0.01224  AvgTextLen:45.8  AvgSpecLen:145.4  StepTime:0.88  LR:0.000100\n",
      "   | > Step:66/68  GlobalStep:84040  TotalLoss:0.00330  PostnetLoss:0.00158  DecoderLoss:0.00172  StopLoss:0.03512  GradNorm:0.00269  GradNormST:0.01150  AvgTextLen:71.2  AvgSpecLen:220.8  StepTime:1.04  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:84042  AvgTotalLoss:0.05216  AvgPostnetLoss:0.00114  AvgDecoderLoss:0.00122  AvgStopLoss:0.04980  EpochTime:41.35  AvgStepTime:0.60\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.09307   PostnetLoss: 0.00461   DecoderLoss:0.00507  StopLoss: 0.08339  \n",
      "   | > TotalLoss: 0.06565   PostnetLoss: 0.00675   DecoderLoss:0.00738  StopLoss: 0.05152  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00114   Validation Loss: 0.00614\n",
      "\n",
      " > Epoch 218/1000\n",
      "   | > Step:7/68  GlobalStep:84050  TotalLoss:0.00173  PostnetLoss:0.00084  DecoderLoss:0.00089  StopLoss:0.04507  GradNorm:0.00359  GradNormST:0.01823  AvgTextLen:7.4  AvgSpecLen:44.6  StepTime:0.33  LR:0.000100\n",
      "   | > Step:17/68  GlobalStep:84060  TotalLoss:0.00196  PostnetLoss:0.00095  DecoderLoss:0.00101  StopLoss:0.05822  GradNorm:0.00310  GradNormST:0.01125  AvgTextLen:14.7  AvgSpecLen:66.0  StepTime:0.42  LR:0.000100\n",
      "   | > Step:27/68  GlobalStep:84070  TotalLoss:0.00225  PostnetLoss:0.00109  DecoderLoss:0.00116  StopLoss:0.04998  GradNorm:0.00329  GradNormST:0.02145  AvgTextLen:21.0  AvgSpecLen:80.2  StepTime:0.64  LR:0.000100\n",
      "   | > Step:37/68  GlobalStep:84080  TotalLoss:0.00248  PostnetLoss:0.00120  DecoderLoss:0.00128  StopLoss:0.04516  GradNorm:0.00309  GradNormST:0.01149  AvgTextLen:29.3  AvgSpecLen:102.4  StepTime:0.59  LR:0.000100\n",
      "   | > Step:47/68  GlobalStep:84090  TotalLoss:0.00265  PostnetLoss:0.00128  DecoderLoss:0.00137  StopLoss:0.04085  GradNorm:0.00274  GradNormST:0.01158  AvgTextLen:36.7  AvgSpecLen:126.9  StepTime:0.59  LR:0.000100\n",
      "   | > Step:57/68  GlobalStep:84100  TotalLoss:0.00286  PostnetLoss:0.00138  DecoderLoss:0.00148  StopLoss:0.03816  GradNorm:0.00269  GradNormST:0.00810  AvgTextLen:47.1  AvgSpecLen:161.0  StepTime:0.74  LR:0.000100\n",
      "   | > Step:67/68  GlobalStep:84110  TotalLoss:0.00326  PostnetLoss:0.00156  DecoderLoss:0.00170  StopLoss:0.03365  GradNorm:0.00268  GradNormST:0.01530  AvgTextLen:80.6  AvgSpecLen:246.5  StepTime:1.25  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:84111  AvgTotalLoss:0.05113  AvgPostnetLoss:0.00113  AvgDecoderLoss:0.00122  AvgStopLoss:0.04878  EpochTime:41.38  AvgStepTime:0.60\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.09153   PostnetLoss: 0.00470   DecoderLoss:0.00515  StopLoss: 0.08169  \n",
      "   | > TotalLoss: 0.06643   PostnetLoss: 0.00677   DecoderLoss:0.00739  StopLoss: 0.05228  \n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00113   Validation Loss: 0.00623\n",
      "\n",
      " > Epoch 219/1000\n",
      "   | > Step:8/68  GlobalStep:84120  TotalLoss:0.00175  PostnetLoss:0.00085  DecoderLoss:0.00090  StopLoss:0.06292  GradNorm:0.00399  GradNormST:0.01296  AvgTextLen:8.3  AvgSpecLen:44.0  StepTime:0.24  LR:0.000100\n",
      "   | > Step:18/68  GlobalStep:84130  TotalLoss:0.00196  PostnetLoss:0.00095  DecoderLoss:0.00101  StopLoss:0.04940  GradNorm:0.00322  GradNormST:0.00939  AvgTextLen:15.2  AvgSpecLen:64.5  StepTime:0.40  LR:0.000100\n",
      "   | > Step:28/68  GlobalStep:84140  TotalLoss:0.00223  PostnetLoss:0.00108  DecoderLoss:0.00115  StopLoss:0.06104  GradNorm:0.00310  GradNormST:0.01361  AvgTextLen:22.1  AvgSpecLen:85.4  StepTime:0.55  LR:0.000100\n",
      "   | > Step:38/68  GlobalStep:84150  TotalLoss:0.00241  PostnetLoss:0.00116  DecoderLoss:0.00124  StopLoss:0.03731  GradNorm:0.00299  GradNormST:0.00732  AvgTextLen:29.4  AvgSpecLen:104.1  StepTime:0.49  LR:0.000100\n",
      "   | > Step:48/68  GlobalStep:84160  TotalLoss:0.00264  PostnetLoss:0.00128  DecoderLoss:0.00137  StopLoss:0.03843  GradNorm:0.00290  GradNormST:0.00858  AvgTextLen:37.5  AvgSpecLen:123.8  StepTime:0.72  LR:0.000100\n",
      "   | > Step:58/68  GlobalStep:84170  TotalLoss:0.00290  PostnetLoss:0.00139  DecoderLoss:0.00150  StopLoss:0.04937  GradNorm:0.00274  GradNormST:0.01232  AvgTextLen:47.8  AvgSpecLen:158.0  StepTime:0.84  LR:0.000100\n",
      "   | > Step:68/68  GlobalStep:84180  TotalLoss:0.00329  PostnetLoss:0.00158  DecoderLoss:0.00171  StopLoss:0.03472  GradNorm:0.00287  GradNormST:0.01266  AvgTextLen:103.0  AvgSpecLen:308.9  StepTime:1.87  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:84180  AvgTotalLoss:0.05135  AvgPostnetLoss:0.00114  AvgDecoderLoss:0.00122  AvgStopLoss:0.04900  EpochTime:42.06  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.09490   PostnetLoss: 0.00469   DecoderLoss:0.00512  StopLoss: 0.08508  \n",
      "   | > TotalLoss: 0.06945   PostnetLoss: 0.00688   DecoderLoss:0.00750  StopLoss: 0.05506  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00114   Validation Loss: 0.00632\n",
      "\n",
      " > Epoch 220/1000\n",
      "   | > Step:9/68  GlobalStep:84190  TotalLoss:0.00173  PostnetLoss:0.00084  DecoderLoss:0.00089  StopLoss:0.04757  GradNorm:0.00356  GradNormST:0.02269  AvgTextLen:9.0  AvgSpecLen:42.1  StepTime:0.34  LR:0.000100\n",
      "   | > Step:19/68  GlobalStep:84200  TotalLoss:0.00197  PostnetLoss:0.00096  DecoderLoss:0.00101  StopLoss:0.07320  GradNorm:0.00321  GradNormST:0.01559  AvgTextLen:15.5  AvgSpecLen:63.2  StepTime:0.44  LR:0.000100\n",
      "   | > Step:29/68  GlobalStep:84210  TotalLoss:0.00223  PostnetLoss:0.00108  DecoderLoss:0.00115  StopLoss:0.06289  GradNorm:0.00301  GradNormST:0.01682  AvgTextLen:21.8  AvgSpecLen:83.4  StepTime:0.59  LR:0.000100\n",
      "   | > Step:39/68  GlobalStep:84220  TotalLoss:0.00249  PostnetLoss:0.00120  DecoderLoss:0.00129  StopLoss:0.04187  GradNorm:0.00291  GradNormST:0.01513  AvgTextLen:30.2  AvgSpecLen:110.4  StepTime:0.68  LR:0.000100\n",
      "   | > Step:49/68  GlobalStep:84230  TotalLoss:0.00270  PostnetLoss:0.00130  DecoderLoss:0.00140  StopLoss:0.04467  GradNorm:0.00285  GradNormST:0.00872  AvgTextLen:38.2  AvgSpecLen:127.7  StepTime:0.72  LR:0.000100\n",
      "   | > Step:59/68  GlobalStep:84240  TotalLoss:0.00286  PostnetLoss:0.00138  DecoderLoss:0.00149  StopLoss:0.04137  GradNorm:0.00304  GradNormST:0.00902  AvgTextLen:50.5  AvgSpecLen:155.9  StepTime:0.85  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:84249  AvgTotalLoss:0.05431  AvgPostnetLoss:0.00114  AvgDecoderLoss:0.00122  AvgStopLoss:0.05195  EpochTime:42.79  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.09618   PostnetLoss: 0.00464   DecoderLoss:0.00507  StopLoss: 0.08647  \n",
      "   | > TotalLoss: 0.06966   PostnetLoss: 0.00678   DecoderLoss:0.00739  StopLoss: 0.05550  \n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00114   Validation Loss: 0.00624\n",
      "\n",
      " > Epoch 221/1000\n",
      "   | > Step:0/68  GlobalStep:84250  TotalLoss:0.00161  PostnetLoss:0.00077  DecoderLoss:0.00083  StopLoss:0.07940  GradNorm:0.00603  GradNormST:0.02447  AvgTextLen:3.0  AvgSpecLen:25.3  StepTime:0.51  LR:0.000100\n",
      "   | > Step:10/68  GlobalStep:84260  TotalLoss:0.00179  PostnetLoss:0.00087  DecoderLoss:0.00092  StopLoss:0.06659  GradNorm:0.00358  GradNormST:0.02671  AvgTextLen:9.8  AvgSpecLen:48.6  StepTime:0.62  LR:0.000100\n",
      "   | > Step:20/68  GlobalStep:84270  TotalLoss:0.00213  PostnetLoss:0.00103  DecoderLoss:0.00110  StopLoss:0.04271  GradNorm:0.00353  GradNormST:0.01120  AvgTextLen:16.4  AvgSpecLen:64.8  StepTime:0.39  LR:0.000100\n",
      "   | > Step:30/68  GlobalStep:84280  TotalLoss:0.00230  PostnetLoss:0.00111  DecoderLoss:0.00119  StopLoss:0.05170  GradNorm:0.00317  GradNormST:0.00749  AvgTextLen:22.9  AvgSpecLen:85.8  StepTime:0.56  LR:0.000100\n",
      "   | > Step:40/68  GlobalStep:84290  TotalLoss:0.00246  PostnetLoss:0.00119  DecoderLoss:0.00127  StopLoss:0.04505  GradNorm:0.00317  GradNormST:0.00793  AvgTextLen:30.9  AvgSpecLen:105.3  StepTime:0.64  LR:0.000100\n",
      "   | > Step:50/68  GlobalStep:84300  TotalLoss:0.00267  PostnetLoss:0.00129  DecoderLoss:0.00139  StopLoss:0.03751  GradNorm:0.00284  GradNormST:0.00923  AvgTextLen:38.8  AvgSpecLen:132.0  StepTime:0.73  LR:0.000100\n",
      "   | > Step:60/68  GlobalStep:84310  TotalLoss:0.00295  PostnetLoss:0.00142  DecoderLoss:0.00153  StopLoss:0.03133  GradNorm:0.00297  GradNormST:0.00701  AvgTextLen:52.0  AvgSpecLen:172.7  StepTime:0.92  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:84318  AvgTotalLoss:0.05130  AvgPostnetLoss:0.00114  AvgDecoderLoss:0.00122  AvgStopLoss:0.04895  EpochTime:42.38  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.09307   PostnetLoss: 0.00469   DecoderLoss:0.00513  StopLoss: 0.08325  \n",
      "   | > TotalLoss: 0.06763   PostnetLoss: 0.00689   DecoderLoss:0.00751  StopLoss: 0.05322  \n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00114   Validation Loss: 0.00629\n",
      "\n",
      " > Epoch 222/1000\n",
      "   | > Step:1/68  GlobalStep:84320  TotalLoss:0.00163  PostnetLoss:0.00079  DecoderLoss:0.00084  StopLoss:0.06453  GradNorm:0.00450  GradNormST:0.02370  AvgTextLen:3.7  AvgSpecLen:28.6  StepTime:0.32  LR:0.000100\n",
      "   | > Step:11/68  GlobalStep:84330  TotalLoss:0.00182  PostnetLoss:0.00088  DecoderLoss:0.00093  StopLoss:0.04733  GradNorm:0.00349  GradNormST:0.01207  AvgTextLen:10.3  AvgSpecLen:49.4  StepTime:0.39  LR:0.000100\n",
      "   | > Step:21/68  GlobalStep:84340  TotalLoss:0.00209  PostnetLoss:0.00101  DecoderLoss:0.00108  StopLoss:0.05834  GradNorm:0.00327  GradNormST:0.01618  AvgTextLen:16.6  AvgSpecLen:66.3  StepTime:0.43  LR:0.000100\n",
      "   | > Step:31/68  GlobalStep:84350  TotalLoss:0.00227  PostnetLoss:0.00110  DecoderLoss:0.00117  StopLoss:0.04142  GradNorm:0.00314  GradNormST:0.01073  AvgTextLen:24.1  AvgSpecLen:82.7  StepTime:0.43  LR:0.000100\n",
      "   | > Step:41/68  GlobalStep:84360  TotalLoss:0.00254  PostnetLoss:0.00122  DecoderLoss:0.00131  StopLoss:0.03437  GradNorm:0.00291  GradNormST:0.01077  AvgTextLen:32.3  AvgSpecLen:109.4  StepTime:0.62  LR:0.000100\n",
      "   | > Step:51/68  GlobalStep:84370  TotalLoss:0.00266  PostnetLoss:0.00128  DecoderLoss:0.00138  StopLoss:0.03967  GradNorm:0.00291  GradNormST:0.00820  AvgTextLen:39.2  AvgSpecLen:126.9  StepTime:0.79  LR:0.000100\n",
      "   | > Step:61/68  GlobalStep:84380  TotalLoss:0.00288  PostnetLoss:0.00138  DecoderLoss:0.00149  StopLoss:0.02950  GradNorm:0.00266  GradNormST:0.00616  AvgTextLen:54.6  AvgSpecLen:165.5  StepTime:0.90  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:84387  AvgTotalLoss:0.05154  AvgPostnetLoss:0.00113  AvgDecoderLoss:0.00121  AvgStopLoss:0.04919  EpochTime:42.87  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.09101   PostnetLoss: 0.00473   DecoderLoss:0.00517  StopLoss: 0.08112  \n",
      "   | > TotalLoss: 0.07217   PostnetLoss: 0.00679   DecoderLoss:0.00740  StopLoss: 0.05798  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00113   Validation Loss: 0.00627\n",
      "\n",
      " > Epoch 223/1000\n",
      "   | > Step:2/68  GlobalStep:84390  TotalLoss:0.00152  PostnetLoss:0.00073  DecoderLoss:0.00078  StopLoss:0.06264  GradNorm:0.00357  GradNormST:0.01966  AvgTextLen:4.0  AvgSpecLen:31.3  StepTime:0.41  LR:0.000100\n",
      "   | > Step:12/68  GlobalStep:84400  TotalLoss:0.00179  PostnetLoss:0.00087  DecoderLoss:0.00092  StopLoss:0.05487  GradNorm:0.00355  GradNormST:0.01692  AvgTextLen:11.0  AvgSpecLen:49.3  StepTime:0.33  LR:0.000100\n",
      "   | > Step:22/68  GlobalStep:84410  TotalLoss:0.00215  PostnetLoss:0.00104  DecoderLoss:0.00111  StopLoss:0.07649  GradNorm:0.00378  GradNormST:0.01571  AvgTextLen:18.0  AvgSpecLen:72.0  StepTime:0.36  LR:0.000100\n",
      "   | > Step:32/68  GlobalStep:84420  TotalLoss:0.00228  PostnetLoss:0.00110  DecoderLoss:0.00118  StopLoss:0.06063  GradNorm:0.00293  GradNormST:0.01566  AvgTextLen:24.4  AvgSpecLen:91.5  StepTime:0.57  LR:0.000100\n",
      "   | > Step:42/68  GlobalStep:84430  TotalLoss:0.00252  PostnetLoss:0.00122  DecoderLoss:0.00131  StopLoss:0.05275  GradNorm:0.00285  GradNormST:0.01282  AvgTextLen:33.1  AvgSpecLen:109.7  StepTime:0.60  LR:0.000100\n",
      "   | > Step:52/68  GlobalStep:84440  TotalLoss:0.00274  PostnetLoss:0.00132  DecoderLoss:0.00142  StopLoss:0.04542  GradNorm:0.00282  GradNormST:0.01450  AvgTextLen:42.1  AvgSpecLen:137.5  StepTime:0.80  LR:0.000100\n",
      "   | > Step:62/68  GlobalStep:84450  TotalLoss:0.00296  PostnetLoss:0.00142  DecoderLoss:0.00154  StopLoss:0.03867  GradNorm:0.00272  GradNormST:0.01418  AvgTextLen:54.0  AvgSpecLen:174.7  StepTime:0.75  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:84456  AvgTotalLoss:0.05046  AvgPostnetLoss:0.00113  AvgDecoderLoss:0.00122  AvgStopLoss:0.04811  EpochTime:42.27  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.08961   PostnetLoss: 0.00466   DecoderLoss:0.00510  StopLoss: 0.07985  \n",
      "   | > TotalLoss: 0.06592   PostnetLoss: 0.00672   DecoderLoss:0.00733  StopLoss: 0.05188  \n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00113   Validation Loss: 0.00618\n",
      "\n",
      " > Epoch 224/1000\n",
      "   | > Step:3/68  GlobalStep:84460  TotalLoss:0.00153  PostnetLoss:0.00075  DecoderLoss:0.00079  StopLoss:0.05647  GradNorm:0.00379  GradNormST:0.02010  AvgTextLen:4.7  AvgSpecLen:29.7  StepTime:0.31  LR:0.000100\n",
      "   | > Step:13/68  GlobalStep:84470  TotalLoss:0.00183  PostnetLoss:0.00089  DecoderLoss:0.00094  StopLoss:0.06044  GradNorm:0.00342  GradNormST:0.01979  AvgTextLen:11.5  AvgSpecLen:55.2  StepTime:0.43  LR:0.000100\n",
      "   | > Step:23/68  GlobalStep:84480  TotalLoss:0.00215  PostnetLoss:0.00104  DecoderLoss:0.00111  StopLoss:0.04179  GradNorm:0.00332  GradNormST:0.00900  AvgTextLen:18.3  AvgSpecLen:72.2  StepTime:0.45  LR:0.000100\n",
      "   | > Step:33/68  GlobalStep:84490  TotalLoss:0.00236  PostnetLoss:0.00114  DecoderLoss:0.00121  StopLoss:0.05314  GradNorm:0.00342  GradNormST:0.00960  AvgTextLen:25.9  AvgSpecLen:88.9  StepTime:0.53  LR:0.000100\n",
      "   | > Step:43/68  GlobalStep:84500  TotalLoss:0.00249  PostnetLoss:0.00120  DecoderLoss:0.00129  StopLoss:0.05269  GradNorm:0.00283  GradNormST:0.01498  AvgTextLen:33.4  AvgSpecLen:115.7  StepTime:0.52  LR:0.000100\n",
      "   | > Step:53/68  GlobalStep:84510  TotalLoss:0.00268  PostnetLoss:0.00129  DecoderLoss:0.00139  StopLoss:0.04457  GradNorm:0.00277  GradNormST:0.01169  AvgTextLen:42.4  AvgSpecLen:137.6  StepTime:0.76  LR:0.000100\n",
      "   | > Step:63/68  GlobalStep:84520  TotalLoss:0.00304  PostnetLoss:0.00146  DecoderLoss:0.00158  StopLoss:0.03068  GradNorm:0.00285  GradNormST:0.01294  AvgTextLen:58.6  AvgSpecLen:185.1  StepTime:0.91  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:84525  AvgTotalLoss:0.05097  AvgPostnetLoss:0.00113  AvgDecoderLoss:0.00121  AvgStopLoss:0.04863  EpochTime:42.48  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.08918   PostnetLoss: 0.00461   DecoderLoss:0.00504  StopLoss: 0.07952  \n",
      "   | > TotalLoss: 0.07177   PostnetLoss: 0.00699   DecoderLoss:0.00762  StopLoss: 0.05716  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00113   Validation Loss: 0.00639\n",
      "\n",
      " > Epoch 225/1000\n",
      "   | > Step:4/68  GlobalStep:84530  TotalLoss:0.00159  PostnetLoss:0.00077  DecoderLoss:0.00082  StopLoss:0.08955  GradNorm:0.00474  GradNormST:0.03984  AvgTextLen:4.8  AvgSpecLen:31.1  StepTime:0.26  LR:0.000100\n",
      "   | > Step:14/68  GlobalStep:84540  TotalLoss:0.00190  PostnetLoss:0.00092  DecoderLoss:0.00097  StopLoss:0.05654  GradNorm:0.00415  GradNormST:0.01286  AvgTextLen:12.2  AvgSpecLen:52.0  StepTime:0.34  LR:0.000100\n",
      "   | > Step:24/68  GlobalStep:84550  TotalLoss:0.00222  PostnetLoss:0.00107  DecoderLoss:0.00115  StopLoss:0.04856  GradNorm:0.00318  GradNormST:0.00918  AvgTextLen:19.2  AvgSpecLen:79.0  StepTime:0.49  LR:0.000100\n",
      "   | > Step:34/68  GlobalStep:84560  TotalLoss:0.00241  PostnetLoss:0.00116  DecoderLoss:0.00124  StopLoss:0.03679  GradNorm:0.00307  GradNormST:0.00823  AvgTextLen:26.2  AvgSpecLen:95.9  StepTime:0.59  LR:0.000100\n",
      "   | > Step:44/68  GlobalStep:84570  TotalLoss:0.00260  PostnetLoss:0.00125  DecoderLoss:0.00135  StopLoss:0.03383  GradNorm:0.00313  GradNormST:0.00886  AvgTextLen:33.8  AvgSpecLen:116.3  StepTime:0.54  LR:0.000100\n",
      "   | > Step:54/68  GlobalStep:84580  TotalLoss:0.00278  PostnetLoss:0.00133  DecoderLoss:0.00144  StopLoss:0.04830  GradNorm:0.00285  GradNormST:0.00909  AvgTextLen:42.2  AvgSpecLen:144.9  StepTime:0.68  LR:0.000100\n",
      "   | > Step:64/68  GlobalStep:84590  TotalLoss:0.00289  PostnetLoss:0.00139  DecoderLoss:0.00150  StopLoss:0.03792  GradNorm:0.00277  GradNormST:0.01166  AvgTextLen:60.6  AvgSpecLen:177.6  StepTime:0.82  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:84594  AvgTotalLoss:0.05110  AvgPostnetLoss:0.00113  AvgDecoderLoss:0.00121  AvgStopLoss:0.04875  EpochTime:42.06  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.08967   PostnetLoss: 0.00469   DecoderLoss:0.00513  StopLoss: 0.07986  \n",
      "   | > TotalLoss: 0.07424   PostnetLoss: 0.00688   DecoderLoss:0.00749  StopLoss: 0.05987  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00113   Validation Loss: 0.00628\n",
      "\n",
      " > Epoch 226/1000\n",
      "   | > Step:5/68  GlobalStep:84600  TotalLoss:0.00162  PostnetLoss:0.00078  DecoderLoss:0.00084  StopLoss:0.06363  GradNorm:0.00361  GradNormST:0.01972  AvgTextLen:5.9  AvgSpecLen:33.2  StepTime:0.35  LR:0.000100\n",
      "   | > Step:15/68  GlobalStep:84610  TotalLoss:0.00197  PostnetLoss:0.00096  DecoderLoss:0.00101  StopLoss:0.05494  GradNorm:0.00333  GradNormST:0.02271  AvgTextLen:12.8  AvgSpecLen:59.1  StepTime:0.54  LR:0.000100\n",
      "   | > Step:25/68  GlobalStep:84620  TotalLoss:0.00215  PostnetLoss:0.00104  DecoderLoss:0.00111  StopLoss:0.05349  GradNorm:0.00313  GradNormST:0.01287  AvgTextLen:19.8  AvgSpecLen:73.3  StepTime:0.47  LR:0.000100\n",
      "   | > Step:35/68  GlobalStep:84630  TotalLoss:0.00240  PostnetLoss:0.00116  DecoderLoss:0.00124  StopLoss:0.04709  GradNorm:0.00302  GradNormST:0.01700  AvgTextLen:27.1  AvgSpecLen:104.7  StepTime:0.79  LR:0.000100\n",
      "   | > Step:45/68  GlobalStep:84640  TotalLoss:0.00254  PostnetLoss:0.00123  DecoderLoss:0.00132  StopLoss:0.05417  GradNorm:0.00333  GradNormST:0.01687  AvgTextLen:34.4  AvgSpecLen:118.6  StepTime:0.64  LR:0.000100\n",
      "   | > Step:55/68  GlobalStep:84650  TotalLoss:0.00284  PostnetLoss:0.00137  DecoderLoss:0.00147  StopLoss:0.03904  GradNorm:0.00287  GradNormST:0.00642  AvgTextLen:43.9  AvgSpecLen:145.2  StepTime:0.87  LR:0.000100\n",
      "   | > Step:65/68  GlobalStep:84660  TotalLoss:0.00310  PostnetLoss:0.00148  DecoderLoss:0.00161  StopLoss:0.03610  GradNorm:0.00297  GradNormST:0.01134  AvgTextLen:64.2  AvgSpecLen:207.4  StepTime:1.20  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:84663  AvgTotalLoss:0.05067  AvgPostnetLoss:0.00113  AvgDecoderLoss:0.00121  AvgStopLoss:0.04833  EpochTime:42.94  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.08748   PostnetLoss: 0.00475   DecoderLoss:0.00519  StopLoss: 0.07754  \n",
      "   | > TotalLoss: 0.06870   PostnetLoss: 0.00693   DecoderLoss:0.00756  StopLoss: 0.05420  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00113   Validation Loss: 0.00635\n",
      "\n",
      " > Epoch 227/1000\n",
      "   | > Step:6/68  GlobalStep:84670  TotalLoss:0.00164  PostnetLoss:0.00080  DecoderLoss:0.00085  StopLoss:0.05571  GradNorm:0.00357  GradNormST:0.02433  AvgTextLen:6.0  AvgSpecLen:34.8  StepTime:0.42  LR:0.000100\n",
      "   | > Step:16/68  GlobalStep:84680  TotalLoss:0.00200  PostnetLoss:0.00097  DecoderLoss:0.00103  StopLoss:0.04379  GradNorm:0.00350  GradNormST:0.01051  AvgTextLen:13.8  AvgSpecLen:60.6  StepTime:0.42  LR:0.000100\n",
      "   | > Step:26/68  GlobalStep:84690  TotalLoss:0.00218  PostnetLoss:0.00106  DecoderLoss:0.00112  StopLoss:0.04847  GradNorm:0.00321  GradNormST:0.01231  AvgTextLen:20.3  AvgSpecLen:77.5  StepTime:0.51  LR:0.000100\n",
      "   | > Step:36/68  GlobalStep:84700  TotalLoss:0.00242  PostnetLoss:0.00116  DecoderLoss:0.00125  StopLoss:0.06176  GradNorm:0.00289  GradNormST:0.01124  AvgTextLen:28.5  AvgSpecLen:100.2  StepTime:0.47  LR:0.000100\n",
      "   | > Step:46/68  GlobalStep:84710  TotalLoss:0.00258  PostnetLoss:0.00125  DecoderLoss:0.00134  StopLoss:0.04364  GradNorm:0.00314  GradNormST:0.00887  AvgTextLen:35.4  AvgSpecLen:122.6  StepTime:0.79  LR:0.000100\n",
      "   | > Step:56/68  GlobalStep:84720  TotalLoss:0.00279  PostnetLoss:0.00134  DecoderLoss:0.00145  StopLoss:0.02944  GradNorm:0.00289  GradNormST:0.00762  AvgTextLen:45.8  AvgSpecLen:145.4  StepTime:0.98  LR:0.000100\n",
      "   | > Step:66/68  GlobalStep:84730  TotalLoss:0.00326  PostnetLoss:0.00156  DecoderLoss:0.00170  StopLoss:0.03637  GradNorm:0.00259  GradNormST:0.01280  AvgTextLen:71.2  AvgSpecLen:220.8  StepTime:1.07  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:84732  AvgTotalLoss:0.05054  AvgPostnetLoss:0.00113  AvgDecoderLoss:0.00121  AvgStopLoss:0.04819  EpochTime:43.69  AvgStepTime:0.63\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.09029   PostnetLoss: 0.00466   DecoderLoss:0.00510  StopLoss: 0.08052  \n",
      "   | > TotalLoss: 0.07102   PostnetLoss: 0.00692   DecoderLoss:0.00756  StopLoss: 0.05654  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00113   Validation Loss: 0.00632\n",
      "\n",
      " > Epoch 228/1000\n",
      "   | > Step:7/68  GlobalStep:84740  TotalLoss:0.00172  PostnetLoss:0.00084  DecoderLoss:0.00088  StopLoss:0.04835  GradNorm:0.00359  GradNormST:0.02147  AvgTextLen:7.4  AvgSpecLen:44.6  StepTime:0.37  LR:0.000100\n",
      "   | > Step:17/68  GlobalStep:84750  TotalLoss:0.00195  PostnetLoss:0.00095  DecoderLoss:0.00100  StopLoss:0.07475  GradNorm:0.00352  GradNormST:0.01751  AvgTextLen:14.7  AvgSpecLen:66.0  StepTime:0.48  LR:0.000100\n",
      "   | > Step:27/68  GlobalStep:84760  TotalLoss:0.00224  PostnetLoss:0.00109  DecoderLoss:0.00115  StopLoss:0.04964  GradNorm:0.00333  GradNormST:0.02639  AvgTextLen:21.0  AvgSpecLen:80.2  StepTime:0.63  LR:0.000100\n",
      "   | > Step:37/68  GlobalStep:84770  TotalLoss:0.00244  PostnetLoss:0.00118  DecoderLoss:0.00126  StopLoss:0.05384  GradNorm:0.00308  GradNormST:0.01218  AvgTextLen:29.3  AvgSpecLen:102.4  StepTime:0.62  LR:0.000100\n",
      "   | > Step:47/68  GlobalStep:84780  TotalLoss:0.00265  PostnetLoss:0.00128  DecoderLoss:0.00137  StopLoss:0.04434  GradNorm:0.00299  GradNormST:0.00877  AvgTextLen:36.7  AvgSpecLen:126.9  StepTime:0.68  LR:0.000100\n",
      "   | > Step:57/68  GlobalStep:84790  TotalLoss:0.00286  PostnetLoss:0.00138  DecoderLoss:0.00148  StopLoss:0.03851  GradNorm:0.00279  GradNormST:0.00910  AvgTextLen:47.1  AvgSpecLen:161.0  StepTime:0.81  LR:0.000100\n",
      "   | > Step:67/68  GlobalStep:84800  TotalLoss:0.00323  PostnetLoss:0.00155  DecoderLoss:0.00169  StopLoss:0.03255  GradNorm:0.00259  GradNormST:0.01698  AvgTextLen:80.6  AvgSpecLen:246.5  StepTime:1.23  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:84801  AvgTotalLoss:0.05142  AvgPostnetLoss:0.00113  AvgDecoderLoss:0.00121  AvgStopLoss:0.04909  EpochTime:43.32  AvgStepTime:0.63\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.08820   PostnetLoss: 0.00472   DecoderLoss:0.00517  StopLoss: 0.07831  \n",
      "   | > TotalLoss: 0.07575   PostnetLoss: 0.00708   DecoderLoss:0.00771  StopLoss: 0.06097  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00113   Validation Loss: 0.00642\n",
      "\n",
      " > Epoch 229/1000\n",
      "   | > Step:8/68  GlobalStep:84810  TotalLoss:0.00180  PostnetLoss:0.00088  DecoderLoss:0.00093  StopLoss:0.06500  GradNorm:0.00405  GradNormST:0.01677  AvgTextLen:8.3  AvgSpecLen:44.0  StepTime:0.24  LR:0.000100\n",
      "   | > Step:18/68  GlobalStep:84820  TotalLoss:0.00195  PostnetLoss:0.00094  DecoderLoss:0.00100  StopLoss:0.05856  GradNorm:0.00320  GradNormST:0.01002  AvgTextLen:15.2  AvgSpecLen:64.5  StepTime:0.33  LR:0.000100\n",
      "   | > Step:28/68  GlobalStep:84830  TotalLoss:0.00225  PostnetLoss:0.00109  DecoderLoss:0.00116  StopLoss:0.05051  GradNorm:0.00304  GradNormST:0.01158  AvgTextLen:22.1  AvgSpecLen:85.4  StepTime:0.57  LR:0.000100\n",
      "   | > Step:38/68  GlobalStep:84840  TotalLoss:0.00242  PostnetLoss:0.00117  DecoderLoss:0.00125  StopLoss:0.04533  GradNorm:0.00301  GradNormST:0.00902  AvgTextLen:29.4  AvgSpecLen:104.1  StepTime:0.50  LR:0.000100\n",
      "   | > Step:48/68  GlobalStep:84850  TotalLoss:0.00262  PostnetLoss:0.00126  DecoderLoss:0.00135  StopLoss:0.03639  GradNorm:0.00290  GradNormST:0.00778  AvgTextLen:37.5  AvgSpecLen:123.8  StepTime:0.72  LR:0.000100\n",
      "   | > Step:58/68  GlobalStep:84860  TotalLoss:0.00288  PostnetLoss:0.00138  DecoderLoss:0.00149  StopLoss:0.04698  GradNorm:0.00289  GradNormST:0.01077  AvgTextLen:47.8  AvgSpecLen:158.0  StepTime:0.83  LR:0.000100\n",
      "   | > Step:68/68  GlobalStep:84870  TotalLoss:0.00323  PostnetLoss:0.00155  DecoderLoss:0.00168  StopLoss:0.03051  GradNorm:0.00280  GradNormST:0.01001  AvgTextLen:103.0  AvgSpecLen:308.9  StepTime:1.96  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:84870  AvgTotalLoss:0.04978  AvgPostnetLoss:0.00113  AvgDecoderLoss:0.00121  AvgStopLoss:0.04745  EpochTime:42.48  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.09208   PostnetLoss: 0.00478   DecoderLoss:0.00524  StopLoss: 0.08207  \n",
      "   | > TotalLoss: 0.07769   PostnetLoss: 0.00684   DecoderLoss:0.00747  StopLoss: 0.06338  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00113   Validation Loss: 0.00628\n",
      "\n",
      " > Epoch 230/1000\n",
      "   | > Step:9/68  GlobalStep:84880  TotalLoss:0.00172  PostnetLoss:0.00083  DecoderLoss:0.00088  StopLoss:0.05142  GradNorm:0.00353  GradNormST:0.02235  AvgTextLen:9.0  AvgSpecLen:42.1  StepTime:0.43  LR:0.000100\n",
      "   | > Step:19/68  GlobalStep:84890  TotalLoss:0.00197  PostnetLoss:0.00096  DecoderLoss:0.00101  StopLoss:0.06764  GradNorm:0.00320  GradNormST:0.01507  AvgTextLen:15.5  AvgSpecLen:63.2  StepTime:0.37  LR:0.000100\n",
      "   | > Step:29/68  GlobalStep:84900  TotalLoss:0.00218  PostnetLoss:0.00106  DecoderLoss:0.00113  StopLoss:0.06435  GradNorm:0.00305  GradNormST:0.01262  AvgTextLen:21.8  AvgSpecLen:83.4  StepTime:0.57  LR:0.000100\n",
      "   | > Step:39/68  GlobalStep:84910  TotalLoss:0.00249  PostnetLoss:0.00120  DecoderLoss:0.00129  StopLoss:0.03451  GradNorm:0.00298  GradNormST:0.00706  AvgTextLen:30.2  AvgSpecLen:110.4  StepTime:0.53  LR:0.000100\n",
      "   | > Step:49/68  GlobalStep:84920  TotalLoss:0.00264  PostnetLoss:0.00127  DecoderLoss:0.00137  StopLoss:0.03077  GradNorm:0.00284  GradNormST:0.00860  AvgTextLen:38.2  AvgSpecLen:127.7  StepTime:0.64  LR:0.000100\n",
      "   | > Step:59/68  GlobalStep:84930  TotalLoss:0.00282  PostnetLoss:0.00136  DecoderLoss:0.00147  StopLoss:0.03863  GradNorm:0.00273  GradNormST:0.00695  AvgTextLen:50.5  AvgSpecLen:155.9  StepTime:0.88  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:84939  AvgTotalLoss:0.05061  AvgPostnetLoss:0.00113  AvgDecoderLoss:0.00121  AvgStopLoss:0.04827  EpochTime:42.69  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.08962   PostnetLoss: 0.00469   DecoderLoss:0.00513  StopLoss: 0.07981  \n",
      "   | > TotalLoss: 0.07562   PostnetLoss: 0.00696   DecoderLoss:0.00759  StopLoss: 0.06107  \n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00113   Validation Loss: 0.00641\n",
      "\n",
      " > Epoch 231/1000\n",
      "   | > Step:0/68  GlobalStep:84940  TotalLoss:0.00163  PostnetLoss:0.00078  DecoderLoss:0.00085  StopLoss:0.08399  GradNorm:0.00533  GradNormST:0.02288  AvgTextLen:3.0  AvgSpecLen:25.3  StepTime:0.44  LR:0.000100\n",
      "   | > Step:10/68  GlobalStep:84950  TotalLoss:0.00181  PostnetLoss:0.00088  DecoderLoss:0.00093  StopLoss:0.05367  GradNorm:0.00365  GradNormST:0.02673  AvgTextLen:9.8  AvgSpecLen:48.6  StepTime:0.45  LR:0.000100\n",
      "   | > Step:20/68  GlobalStep:84960  TotalLoss:0.00208  PostnetLoss:0.00101  DecoderLoss:0.00107  StopLoss:0.04167  GradNorm:0.00335  GradNormST:0.01432  AvgTextLen:16.4  AvgSpecLen:64.8  StepTime:0.51  LR:0.000100\n",
      "   | > Step:30/68  GlobalStep:84970  TotalLoss:0.00226  PostnetLoss:0.00109  DecoderLoss:0.00116  StopLoss:0.04956  GradNorm:0.00303  GradNormST:0.00956  AvgTextLen:22.9  AvgSpecLen:85.8  StepTime:0.59  LR:0.000100\n",
      "   | > Step:40/68  GlobalStep:84980  TotalLoss:0.00245  PostnetLoss:0.00119  DecoderLoss:0.00126  StopLoss:0.04465  GradNorm:0.00317  GradNormST:0.01128  AvgTextLen:30.9  AvgSpecLen:105.3  StepTime:0.60  LR:0.000100\n",
      "   | > Step:50/68  GlobalStep:84990  TotalLoss:0.00263  PostnetLoss:0.00127  DecoderLoss:0.00136  StopLoss:0.03871  GradNorm:0.00286  GradNormST:0.00848  AvgTextLen:38.8  AvgSpecLen:132.0  StepTime:0.87  LR:0.000100\n",
      "   | > Step:60/68  GlobalStep:85000  TotalLoss:0.00292  PostnetLoss:0.00141  DecoderLoss:0.00152  StopLoss:0.03386  GradNorm:0.00271  GradNormST:0.00817  AvgTextLen:52.0  AvgSpecLen:172.7  StepTime:0.97  LR:0.000100\n",
      " | | > Checkpoint saving : /vm/TTS/models/cartman_models/mozilla-no-loc-fattn-stopnet-sigmoid-loss_masking-August-15-2019_11+26AM-de6aeb9/checkpoint_85000.pth.tar\n",
      "warning: audio amplitude out of range, auto clipped.\n",
      "   | > EPOCH END -- GlobalStep:85008  AvgTotalLoss:0.05071  AvgPostnetLoss:0.00113  AvgDecoderLoss:0.00121  AvgStopLoss:0.04838  EpochTime:43.79  AvgStepTime:0.63\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.08829   PostnetLoss: 0.00465   DecoderLoss:0.00508  StopLoss: 0.07856  \n",
      "   | > TotalLoss: 0.07594   PostnetLoss: 0.00692   DecoderLoss:0.00755  StopLoss: 0.06147  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00113   Validation Loss: 0.00630\n",
      "\n",
      " > Epoch 232/1000\n",
      "   | > Step:1/68  GlobalStep:85010  TotalLoss:0.00157  PostnetLoss:0.00076  DecoderLoss:0.00081  StopLoss:0.05253  GradNorm:0.00438  GradNormST:0.02452  AvgTextLen:3.7  AvgSpecLen:28.6  StepTime:0.33  LR:0.000100\n",
      "   | > Step:11/68  GlobalStep:85020  TotalLoss:0.00180  PostnetLoss:0.00087  DecoderLoss:0.00093  StopLoss:0.05171  GradNorm:0.00336  GradNormST:0.01980  AvgTextLen:10.3  AvgSpecLen:49.4  StepTime:0.36  LR:0.000100\n",
      "   | > Step:21/68  GlobalStep:85030  TotalLoss:0.00208  PostnetLoss:0.00101  DecoderLoss:0.00107  StopLoss:0.05340  GradNorm:0.00341  GradNormST:0.01677  AvgTextLen:16.6  AvgSpecLen:66.3  StepTime:0.34  LR:0.000100\n",
      "   | > Step:31/68  GlobalStep:85040  TotalLoss:0.00226  PostnetLoss:0.00109  DecoderLoss:0.00117  StopLoss:0.04017  GradNorm:0.00330  GradNormST:0.01193  AvgTextLen:24.1  AvgSpecLen:82.7  StepTime:0.53  LR:0.000100\n",
      "   | > Step:41/68  GlobalStep:85050  TotalLoss:0.00254  PostnetLoss:0.00123  DecoderLoss:0.00131  StopLoss:0.03790  GradNorm:0.00300  GradNormST:0.01352  AvgTextLen:32.3  AvgSpecLen:109.4  StepTime:0.67  LR:0.000100\n",
      "   | > Step:51/68  GlobalStep:85060  TotalLoss:0.00264  PostnetLoss:0.00127  DecoderLoss:0.00137  StopLoss:0.04190  GradNorm:0.00304  GradNormST:0.00757  AvgTextLen:39.2  AvgSpecLen:126.9  StepTime:0.69  LR:0.000100\n",
      "   | > Step:61/68  GlobalStep:85070  TotalLoss:0.00290  PostnetLoss:0.00139  DecoderLoss:0.00151  StopLoss:0.03198  GradNorm:0.00307  GradNormST:0.00709  AvgTextLen:54.6  AvgSpecLen:165.5  StepTime:0.92  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:85077  AvgTotalLoss:0.05030  AvgPostnetLoss:0.00113  AvgDecoderLoss:0.00121  AvgStopLoss:0.04797  EpochTime:43.06  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.09032   PostnetLoss: 0.00470   DecoderLoss:0.00514  StopLoss: 0.08048  \n",
      "   | > TotalLoss: 0.07423   PostnetLoss: 0.00694   DecoderLoss:0.00757  StopLoss: 0.05972  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00113   Validation Loss: 0.00636\n",
      "\n",
      " > Epoch 233/1000\n",
      "   | > Step:2/68  GlobalStep:85080  TotalLoss:0.00147  PostnetLoss:0.00071  DecoderLoss:0.00076  StopLoss:0.05252  GradNorm:0.00389  GradNormST:0.01746  AvgTextLen:4.0  AvgSpecLen:31.3  StepTime:0.61  LR:0.000100\n",
      "   | > Step:12/68  GlobalStep:85090  TotalLoss:0.00174  PostnetLoss:0.00085  DecoderLoss:0.00090  StopLoss:0.04775  GradNorm:0.00317  GradNormST:0.01042  AvgTextLen:11.0  AvgSpecLen:49.3  StepTime:0.42  LR:0.000100\n",
      "   | > Step:22/68  GlobalStep:85100  TotalLoss:0.00211  PostnetLoss:0.00102  DecoderLoss:0.00109  StopLoss:0.05986  GradNorm:0.00317  GradNormST:0.01305  AvgTextLen:18.0  AvgSpecLen:72.0  StepTime:0.49  LR:0.000100\n",
      "   | > Step:32/68  GlobalStep:85110  TotalLoss:0.00231  PostnetLoss:0.00112  DecoderLoss:0.00119  StopLoss:0.05340  GradNorm:0.00306  GradNormST:0.01613  AvgTextLen:24.4  AvgSpecLen:91.5  StepTime:0.58  LR:0.000100\n",
      "   | > Step:42/68  GlobalStep:85120  TotalLoss:0.00251  PostnetLoss:0.00121  DecoderLoss:0.00130  StopLoss:0.05052  GradNorm:0.00282  GradNormST:0.01514  AvgTextLen:33.1  AvgSpecLen:109.7  StepTime:0.61  LR:0.000100\n",
      "   | > Step:52/68  GlobalStep:85130  TotalLoss:0.00269  PostnetLoss:0.00130  DecoderLoss:0.00139  StopLoss:0.04948  GradNorm:0.00288  GradNormST:0.01704  AvgTextLen:42.1  AvgSpecLen:137.5  StepTime:0.80  LR:0.000100\n",
      "   | > Step:62/68  GlobalStep:85140  TotalLoss:0.00294  PostnetLoss:0.00141  DecoderLoss:0.00153  StopLoss:0.04409  GradNorm:0.00265  GradNormST:0.01936  AvgTextLen:54.0  AvgSpecLen:174.7  StepTime:0.76  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:85146  AvgTotalLoss:0.04929  AvgPostnetLoss:0.00113  AvgDecoderLoss:0.00121  AvgStopLoss:0.04696  EpochTime:44.46  AvgStepTime:0.64\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.08982   PostnetLoss: 0.00478   DecoderLoss:0.00523  StopLoss: 0.07981  \n",
      "   | > TotalLoss: 0.07754   PostnetLoss: 0.00708   DecoderLoss:0.00773  StopLoss: 0.06273  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00113   Validation Loss: 0.00652\n",
      "\n",
      " > Epoch 234/1000\n",
      "   | > Step:3/68  GlobalStep:85150  TotalLoss:0.00157  PostnetLoss:0.00076  DecoderLoss:0.00081  StopLoss:0.06390  GradNorm:0.00401  GradNormST:0.02015  AvgTextLen:4.7  AvgSpecLen:29.7  StepTime:0.37  LR:0.000100\n",
      "   | > Step:13/68  GlobalStep:85160  TotalLoss:0.00183  PostnetLoss:0.00089  DecoderLoss:0.00094  StopLoss:0.06607  GradNorm:0.00355  GradNormST:0.02537  AvgTextLen:11.5  AvgSpecLen:55.2  StepTime:0.43  LR:0.000100\n",
      "   | > Step:23/68  GlobalStep:85170  TotalLoss:0.00215  PostnetLoss:0.00104  DecoderLoss:0.00111  StopLoss:0.04751  GradNorm:0.00323  GradNormST:0.00771  AvgTextLen:18.3  AvgSpecLen:72.2  StepTime:0.44  LR:0.000100\n",
      "   | > Step:33/68  GlobalStep:85180  TotalLoss:0.00236  PostnetLoss:0.00114  DecoderLoss:0.00121  StopLoss:0.05142  GradNorm:0.00321  GradNormST:0.00947  AvgTextLen:25.9  AvgSpecLen:88.9  StepTime:0.58  LR:0.000100\n",
      "   | > Step:43/68  GlobalStep:85190  TotalLoss:0.00250  PostnetLoss:0.00121  DecoderLoss:0.00129  StopLoss:0.04380  GradNorm:0.00316  GradNormST:0.01434  AvgTextLen:33.4  AvgSpecLen:115.7  StepTime:0.64  LR:0.000100\n",
      "   | > Step:53/68  GlobalStep:85200  TotalLoss:0.00270  PostnetLoss:0.00130  DecoderLoss:0.00140  StopLoss:0.04672  GradNorm:0.00298  GradNormST:0.00864  AvgTextLen:42.4  AvgSpecLen:137.6  StepTime:0.75  LR:0.000100\n",
      "   | > Step:63/68  GlobalStep:85210  TotalLoss:0.00302  PostnetLoss:0.00145  DecoderLoss:0.00157  StopLoss:0.02920  GradNorm:0.00302  GradNormST:0.00773  AvgTextLen:58.6  AvgSpecLen:185.1  StepTime:0.91  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:85215  AvgTotalLoss:0.04918  AvgPostnetLoss:0.00113  AvgDecoderLoss:0.00121  AvgStopLoss:0.04684  EpochTime:43.66  AvgStepTime:0.63\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.09217   PostnetLoss: 0.00473   DecoderLoss:0.00517  StopLoss: 0.08227  \n",
      "   | > TotalLoss: 0.07503   PostnetLoss: 0.00696   DecoderLoss:0.00759  StopLoss: 0.06048  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00113   Validation Loss: 0.00638\n",
      "\n",
      " > Epoch 235/1000\n",
      "   | > Step:4/68  GlobalStep:85220  TotalLoss:0.00160  PostnetLoss:0.00077  DecoderLoss:0.00082  StopLoss:0.09150  GradNorm:0.00402  GradNormST:0.03138  AvgTextLen:4.8  AvgSpecLen:31.1  StepTime:0.27  LR:0.000100\n",
      "   | > Step:14/68  GlobalStep:85230  TotalLoss:0.00187  PostnetLoss:0.00091  DecoderLoss:0.00096  StopLoss:0.05517  GradNorm:0.00381  GradNormST:0.01481  AvgTextLen:12.2  AvgSpecLen:52.0  StepTime:0.34  LR:0.000100\n",
      "   | > Step:24/68  GlobalStep:85240  TotalLoss:0.00224  PostnetLoss:0.00109  DecoderLoss:0.00116  StopLoss:0.04975  GradNorm:0.00318  GradNormST:0.01028  AvgTextLen:19.2  AvgSpecLen:79.0  StepTime:0.43  LR:0.000100\n",
      "   | > Step:34/68  GlobalStep:85250  TotalLoss:0.00240  PostnetLoss:0.00116  DecoderLoss:0.00124  StopLoss:0.03090  GradNorm:0.00310  GradNormST:0.00814  AvgTextLen:26.2  AvgSpecLen:95.9  StepTime:0.63  LR:0.000100\n",
      "   | > Step:44/68  GlobalStep:85260  TotalLoss:0.00256  PostnetLoss:0.00124  DecoderLoss:0.00132  StopLoss:0.03280  GradNorm:0.00301  GradNormST:0.00883  AvgTextLen:33.8  AvgSpecLen:116.3  StepTime:0.69  LR:0.000100\n",
      "   | > Step:54/68  GlobalStep:85270  TotalLoss:0.00275  PostnetLoss:0.00133  DecoderLoss:0.00143  StopLoss:0.04467  GradNorm:0.00278  GradNormST:0.00918  AvgTextLen:42.2  AvgSpecLen:144.9  StepTime:0.81  LR:0.000100\n",
      "   | > Step:64/68  GlobalStep:85280  TotalLoss:0.00286  PostnetLoss:0.00137  DecoderLoss:0.00149  StopLoss:0.03583  GradNorm:0.00292  GradNormST:0.00978  AvgTextLen:60.6  AvgSpecLen:177.6  StepTime:0.86  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:85284  AvgTotalLoss:0.04999  AvgPostnetLoss:0.00113  AvgDecoderLoss:0.00121  AvgStopLoss:0.04766  EpochTime:44.24  AvgStepTime:0.64\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.08934   PostnetLoss: 0.00473   DecoderLoss:0.00518  StopLoss: 0.07943  \n",
      "   | > TotalLoss: 0.07610   PostnetLoss: 0.00689   DecoderLoss:0.00752  StopLoss: 0.06168  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00113   Validation Loss: 0.00635\n",
      "\n",
      " > Epoch 236/1000\n",
      "   | > Step:5/68  GlobalStep:85290  TotalLoss:0.00169  PostnetLoss:0.00081  DecoderLoss:0.00087  StopLoss:0.04958  GradNorm:0.00466  GradNormST:0.01120  AvgTextLen:5.9  AvgSpecLen:33.2  StepTime:0.35  LR:0.000100\n",
      "   | > Step:15/68  GlobalStep:85300  TotalLoss:0.00197  PostnetLoss:0.00096  DecoderLoss:0.00101  StopLoss:0.04809  GradNorm:0.00364  GradNormST:0.01790  AvgTextLen:12.8  AvgSpecLen:59.1  StepTime:0.50  LR:0.000100\n",
      "   | > Step:25/68  GlobalStep:85310  TotalLoss:0.00215  PostnetLoss:0.00104  DecoderLoss:0.00111  StopLoss:0.05153  GradNorm:0.00343  GradNormST:0.01098  AvgTextLen:19.8  AvgSpecLen:73.3  StepTime:0.38  LR:0.000100\n",
      "   | > Step:35/68  GlobalStep:85320  TotalLoss:0.00239  PostnetLoss:0.00116  DecoderLoss:0.00123  StopLoss:0.04955  GradNorm:0.00308  GradNormST:0.02375  AvgTextLen:27.1  AvgSpecLen:104.7  StepTime:0.78  LR:0.000100\n",
      "   | > Step:45/68  GlobalStep:85330  TotalLoss:0.00251  PostnetLoss:0.00121  DecoderLoss:0.00130  StopLoss:0.04733  GradNorm:0.00348  GradNormST:0.01514  AvgTextLen:34.4  AvgSpecLen:118.6  StepTime:0.63  LR:0.000100\n",
      "   | > Step:55/68  GlobalStep:85340  TotalLoss:0.00280  PostnetLoss:0.00135  DecoderLoss:0.00145  StopLoss:0.03851  GradNorm:0.00270  GradNormST:0.00628  AvgTextLen:43.9  AvgSpecLen:145.2  StepTime:0.87  LR:0.000100\n",
      "   | > Step:65/68  GlobalStep:85350  TotalLoss:0.00308  PostnetLoss:0.00148  DecoderLoss:0.00160  StopLoss:0.03632  GradNorm:0.00302  GradNormST:0.01217  AvgTextLen:64.2  AvgSpecLen:207.4  StepTime:1.25  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:85353  AvgTotalLoss:0.04916  AvgPostnetLoss:0.00113  AvgDecoderLoss:0.00121  AvgStopLoss:0.04682  EpochTime:44.60  AvgStepTime:0.65\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.09039   PostnetLoss: 0.00482   DecoderLoss:0.00527  StopLoss: 0.08030  \n",
      "   | > TotalLoss: 0.07556   PostnetLoss: 0.00684   DecoderLoss:0.00746  StopLoss: 0.06126  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00113   Validation Loss: 0.00631\n",
      "\n",
      " > Epoch 237/1000\n",
      "   | > Step:6/68  GlobalStep:85360  TotalLoss:0.00165  PostnetLoss:0.00080  DecoderLoss:0.00084  StopLoss:0.04448  GradNorm:0.00470  GradNormST:0.01361  AvgTextLen:6.0  AvgSpecLen:34.8  StepTime:0.42  LR:0.000100\n",
      "   | > Step:16/68  GlobalStep:85370  TotalLoss:0.00198  PostnetLoss:0.00096  DecoderLoss:0.00102  StopLoss:0.04261  GradNorm:0.00351  GradNormST:0.01192  AvgTextLen:13.8  AvgSpecLen:60.6  StepTime:0.32  LR:0.000100\n",
      "   | > Step:26/68  GlobalStep:85380  TotalLoss:0.00217  PostnetLoss:0.00106  DecoderLoss:0.00112  StopLoss:0.04709  GradNorm:0.00327  GradNormST:0.01198  AvgTextLen:20.3  AvgSpecLen:77.5  StepTime:0.52  LR:0.000100\n",
      "   | > Step:36/68  GlobalStep:85390  TotalLoss:0.00239  PostnetLoss:0.00115  DecoderLoss:0.00124  StopLoss:0.05446  GradNorm:0.00308  GradNormST:0.00935  AvgTextLen:28.5  AvgSpecLen:100.2  StepTime:0.63  LR:0.000100\n",
      "   | > Step:46/68  GlobalStep:85400  TotalLoss:0.00256  PostnetLoss:0.00123  DecoderLoss:0.00132  StopLoss:0.04097  GradNorm:0.00302  GradNormST:0.01317  AvgTextLen:35.4  AvgSpecLen:122.6  StepTime:0.81  LR:0.000100\n",
      "   | > Step:56/68  GlobalStep:85410  TotalLoss:0.00279  PostnetLoss:0.00134  DecoderLoss:0.00145  StopLoss:0.03550  GradNorm:0.00281  GradNormST:0.01722  AvgTextLen:45.8  AvgSpecLen:145.4  StepTime:0.98  LR:0.000100\n",
      "   | > Step:66/68  GlobalStep:85420  TotalLoss:0.00328  PostnetLoss:0.00157  DecoderLoss:0.00171  StopLoss:0.03080  GradNorm:0.00278  GradNormST:0.01056  AvgTextLen:71.2  AvgSpecLen:220.8  StepTime:1.07  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:85422  AvgTotalLoss:0.05046  AvgPostnetLoss:0.00113  AvgDecoderLoss:0.00121  AvgStopLoss:0.04813  EpochTime:44.09  AvgStepTime:0.64\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.09073   PostnetLoss: 0.00479   DecoderLoss:0.00524  StopLoss: 0.08069  \n",
      "   | > TotalLoss: 0.07670   PostnetLoss: 0.00686   DecoderLoss:0.00749  StopLoss: 0.06235  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00113   Validation Loss: 0.00631\n",
      "\n",
      " > Epoch 238/1000\n",
      "   | > Step:7/68  GlobalStep:85430  TotalLoss:0.00171  PostnetLoss:0.00083  DecoderLoss:0.00087  StopLoss:0.04641  GradNorm:0.00436  GradNormST:0.01231  AvgTextLen:7.4  AvgSpecLen:44.6  StepTime:0.55  LR:0.000100\n",
      "   | > Step:17/68  GlobalStep:85440  TotalLoss:0.00196  PostnetLoss:0.00095  DecoderLoss:0.00101  StopLoss:0.08487  GradNorm:0.00384  GradNormST:0.01860  AvgTextLen:14.7  AvgSpecLen:66.0  StepTime:0.45  LR:0.000100\n",
      "   | > Step:27/68  GlobalStep:85450  TotalLoss:0.00226  PostnetLoss:0.00109  DecoderLoss:0.00116  StopLoss:0.05563  GradNorm:0.00349  GradNormST:0.02724  AvgTextLen:21.0  AvgSpecLen:80.2  StepTime:0.63  LR:0.000100\n",
      "   | > Step:37/68  GlobalStep:85460  TotalLoss:0.00246  PostnetLoss:0.00119  DecoderLoss:0.00127  StopLoss:0.04865  GradNorm:0.00315  GradNormST:0.00874  AvgTextLen:29.3  AvgSpecLen:102.4  StepTime:0.60  LR:0.000100\n",
      "   | > Step:47/68  GlobalStep:85470  TotalLoss:0.00266  PostnetLoss:0.00128  DecoderLoss:0.00138  StopLoss:0.03882  GradNorm:0.00292  GradNormST:0.00795  AvgTextLen:36.7  AvgSpecLen:126.9  StepTime:0.62  LR:0.000100\n",
      "   | > Step:57/68  GlobalStep:85480  TotalLoss:0.00287  PostnetLoss:0.00138  DecoderLoss:0.00149  StopLoss:0.03844  GradNorm:0.00287  GradNormST:0.00550  AvgTextLen:47.1  AvgSpecLen:161.0  StepTime:0.80  LR:0.000100\n",
      "   | > Step:67/68  GlobalStep:85490  TotalLoss:0.00324  PostnetLoss:0.00155  DecoderLoss:0.00169  StopLoss:0.03174  GradNorm:0.00299  GradNormST:0.01072  AvgTextLen:80.6  AvgSpecLen:246.5  StepTime:1.25  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:85491  AvgTotalLoss:0.05065  AvgPostnetLoss:0.00113  AvgDecoderLoss:0.00121  AvgStopLoss:0.04831  EpochTime:43.37  AvgStepTime:0.63\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.09628   PostnetLoss: 0.00468   DecoderLoss:0.00511  StopLoss: 0.08649  \n",
      "   | > TotalLoss: 0.07076   PostnetLoss: 0.00687   DecoderLoss:0.00751  StopLoss: 0.05638  \n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00113   Validation Loss: 0.00629\n",
      "\n",
      " > Epoch 239/1000\n",
      "   | > Step:8/68  GlobalStep:85500  TotalLoss:0.00171  PostnetLoss:0.00083  DecoderLoss:0.00088  StopLoss:0.06272  GradNorm:0.00387  GradNormST:0.02266  AvgTextLen:8.3  AvgSpecLen:44.0  StepTime:0.31  LR:0.000100\n",
      "   | > Step:18/68  GlobalStep:85510  TotalLoss:0.00196  PostnetLoss:0.00095  DecoderLoss:0.00101  StopLoss:0.06315  GradNorm:0.00409  GradNormST:0.01114  AvgTextLen:15.2  AvgSpecLen:64.5  StepTime:0.41  LR:0.000100\n",
      "   | > Step:28/68  GlobalStep:85520  TotalLoss:0.00224  PostnetLoss:0.00109  DecoderLoss:0.00115  StopLoss:0.05494  GradNorm:0.00302  GradNormST:0.01031  AvgTextLen:22.1  AvgSpecLen:85.4  StepTime:0.60  LR:0.000100\n",
      "   | > Step:38/68  GlobalStep:85530  TotalLoss:0.00239  PostnetLoss:0.00116  DecoderLoss:0.00123  StopLoss:0.04171  GradNorm:0.00328  GradNormST:0.00790  AvgTextLen:29.4  AvgSpecLen:104.1  StepTime:0.60  LR:0.000100\n",
      "   | > Step:48/68  GlobalStep:85540  TotalLoss:0.00263  PostnetLoss:0.00127  DecoderLoss:0.00136  StopLoss:0.03485  GradNorm:0.00284  GradNormST:0.00807  AvgTextLen:37.5  AvgSpecLen:123.8  StepTime:0.62  LR:0.000100\n",
      "   | > Step:58/68  GlobalStep:85550  TotalLoss:0.00287  PostnetLoss:0.00138  DecoderLoss:0.00149  StopLoss:0.04566  GradNorm:0.00276  GradNormST:0.01265  AvgTextLen:47.8  AvgSpecLen:158.0  StepTime:0.73  LR:0.000100\n",
      "   | > Step:68/68  GlobalStep:85560  TotalLoss:0.00322  PostnetLoss:0.00154  DecoderLoss:0.00168  StopLoss:0.03043  GradNorm:0.00294  GradNormST:0.01170  AvgTextLen:103.0  AvgSpecLen:308.9  StepTime:1.98  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:85560  AvgTotalLoss:0.05021  AvgPostnetLoss:0.00113  AvgDecoderLoss:0.00121  AvgStopLoss:0.04788  EpochTime:43.50  AvgStepTime:0.63\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.09416   PostnetLoss: 0.00470   DecoderLoss:0.00512  StopLoss: 0.08434  \n",
      "   | > TotalLoss: 0.07194   PostnetLoss: 0.00676   DecoderLoss:0.00741  StopLoss: 0.05777  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00113   Validation Loss: 0.00626\n",
      "\n",
      " > Epoch 240/1000\n",
      "   | > Step:9/68  GlobalStep:85570  TotalLoss:0.00170  PostnetLoss:0.00082  DecoderLoss:0.00087  StopLoss:0.04080  GradNorm:0.00384  GradNormST:0.01930  AvgTextLen:9.0  AvgSpecLen:42.1  StepTime:0.46  LR:0.000100\n",
      "   | > Step:19/68  GlobalStep:85580  TotalLoss:0.00197  PostnetLoss:0.00096  DecoderLoss:0.00101  StopLoss:0.06232  GradNorm:0.00407  GradNormST:0.01454  AvgTextLen:15.5  AvgSpecLen:63.2  StepTime:0.49  LR:0.000100\n",
      "   | > Step:29/68  GlobalStep:85590  TotalLoss:0.00218  PostnetLoss:0.00106  DecoderLoss:0.00113  StopLoss:0.06690  GradNorm:0.00303  GradNormST:0.01495  AvgTextLen:21.8  AvgSpecLen:83.4  StepTime:0.55  LR:0.000100\n",
      "   | > Step:39/68  GlobalStep:85600  TotalLoss:0.00248  PostnetLoss:0.00120  DecoderLoss:0.00128  StopLoss:0.04055  GradNorm:0.00320  GradNormST:0.00953  AvgTextLen:30.2  AvgSpecLen:110.4  StepTime:0.76  LR:0.000100\n",
      "   | > Step:49/68  GlobalStep:85610  TotalLoss:0.00263  PostnetLoss:0.00127  DecoderLoss:0.00136  StopLoss:0.03738  GradNorm:0.00288  GradNormST:0.00692  AvgTextLen:38.2  AvgSpecLen:127.7  StepTime:0.72  LR:0.000100\n",
      "   | > Step:59/68  GlobalStep:85620  TotalLoss:0.00284  PostnetLoss:0.00137  DecoderLoss:0.00148  StopLoss:0.03849  GradNorm:0.00290  GradNormST:0.00841  AvgTextLen:50.5  AvgSpecLen:155.9  StepTime:0.86  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:85629  AvgTotalLoss:0.05053  AvgPostnetLoss:0.00113  AvgDecoderLoss:0.00121  AvgStopLoss:0.04820  EpochTime:44.36  AvgStepTime:0.64\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.09668   PostnetLoss: 0.00468   DecoderLoss:0.00511  StopLoss: 0.08689  \n",
      "   | > TotalLoss: 0.07453   PostnetLoss: 0.00702   DecoderLoss:0.00766  StopLoss: 0.05986  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00113   Validation Loss: 0.00635\n",
      "\n",
      " > Epoch 241/1000\n",
      "   | > Step:0/68  GlobalStep:85630  TotalLoss:0.00162  PostnetLoss:0.00078  DecoderLoss:0.00084  StopLoss:0.08496  GradNorm:0.00493  GradNormST:0.02465  AvgTextLen:3.0  AvgSpecLen:25.3  StepTime:0.30  LR:0.000100\n",
      "   | > Step:10/68  GlobalStep:85640  TotalLoss:0.00178  PostnetLoss:0.00086  DecoderLoss:0.00092  StopLoss:0.06112  GradNorm:0.00357  GradNormST:0.02791  AvgTextLen:9.8  AvgSpecLen:48.6  StepTime:0.67  LR:0.000100\n",
      "   | > Step:20/68  GlobalStep:85650  TotalLoss:0.00213  PostnetLoss:0.00103  DecoderLoss:0.00110  StopLoss:0.03762  GradNorm:0.00434  GradNormST:0.01526  AvgTextLen:16.4  AvgSpecLen:64.8  StepTime:0.43  LR:0.000100\n",
      "   | > Step:30/68  GlobalStep:85660  TotalLoss:0.00225  PostnetLoss:0.00109  DecoderLoss:0.00116  StopLoss:0.05928  GradNorm:0.00336  GradNormST:0.01116  AvgTextLen:22.9  AvgSpecLen:85.8  StepTime:0.61  LR:0.000100\n",
      "   | > Step:40/68  GlobalStep:85670  TotalLoss:0.00243  PostnetLoss:0.00118  DecoderLoss:0.00126  StopLoss:0.03765  GradNorm:0.00323  GradNormST:0.00810  AvgTextLen:30.9  AvgSpecLen:105.3  StepTime:0.50  LR:0.000100\n",
      "   | > Step:50/68  GlobalStep:85680  TotalLoss:0.00267  PostnetLoss:0.00129  DecoderLoss:0.00138  StopLoss:0.03623  GradNorm:0.00318  GradNormST:0.01118  AvgTextLen:38.8  AvgSpecLen:132.0  StepTime:0.81  LR:0.000100\n",
      "   | > Step:60/68  GlobalStep:85690  TotalLoss:0.00291  PostnetLoss:0.00140  DecoderLoss:0.00151  StopLoss:0.03197  GradNorm:0.00286  GradNormST:0.00609  AvgTextLen:52.0  AvgSpecLen:172.7  StepTime:0.93  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:85698  AvgTotalLoss:0.05018  AvgPostnetLoss:0.00113  AvgDecoderLoss:0.00121  AvgStopLoss:0.04785  EpochTime:43.24  AvgStepTime:0.63\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.09423   PostnetLoss: 0.00456   DecoderLoss:0.00499  StopLoss: 0.08468  \n",
      "   | > TotalLoss: 0.06897   PostnetLoss: 0.00703   DecoderLoss:0.00769  StopLoss: 0.05426  \n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00113   Validation Loss: 0.00623\n",
      "\n",
      " > Epoch 242/1000\n",
      "   | > Step:1/68  GlobalStep:85700  TotalLoss:0.00165  PostnetLoss:0.00080  DecoderLoss:0.00085  StopLoss:0.05282  GradNorm:0.00466  GradNormST:0.01491  AvgTextLen:3.7  AvgSpecLen:28.6  StepTime:0.36  LR:0.000100\n",
      "   | > Step:11/68  GlobalStep:85710  TotalLoss:0.00178  PostnetLoss:0.00087  DecoderLoss:0.00092  StopLoss:0.04085  GradNorm:0.00350  GradNormST:0.01671  AvgTextLen:10.3  AvgSpecLen:49.4  StepTime:0.39  LR:0.000100\n",
      "   | > Step:21/68  GlobalStep:85720  TotalLoss:0.00210  PostnetLoss:0.00102  DecoderLoss:0.00108  StopLoss:0.05953  GradNorm:0.00470  GradNormST:0.02405  AvgTextLen:16.6  AvgSpecLen:66.3  StepTime:0.47  LR:0.000100\n",
      "   | > Step:31/68  GlobalStep:85730  TotalLoss:0.00224  PostnetLoss:0.00109  DecoderLoss:0.00116  StopLoss:0.04033  GradNorm:0.00325  GradNormST:0.01244  AvgTextLen:24.1  AvgSpecLen:82.7  StepTime:0.61  LR:0.000100\n",
      "   | > Step:41/68  GlobalStep:85740  TotalLoss:0.00256  PostnetLoss:0.00124  DecoderLoss:0.00132  StopLoss:0.03281  GradNorm:0.00321  GradNormST:0.01012  AvgTextLen:32.3  AvgSpecLen:109.4  StepTime:0.55  LR:0.000100\n",
      "   | > Step:51/68  GlobalStep:85750  TotalLoss:0.00263  PostnetLoss:0.00127  DecoderLoss:0.00136  StopLoss:0.03634  GradNorm:0.00308  GradNormST:0.00638  AvgTextLen:39.2  AvgSpecLen:126.9  StepTime:0.75  LR:0.000100\n",
      "   | > Step:61/68  GlobalStep:85760  TotalLoss:0.00287  PostnetLoss:0.00138  DecoderLoss:0.00149  StopLoss:0.03072  GradNorm:0.00282  GradNormST:0.00655  AvgTextLen:54.6  AvgSpecLen:165.5  StepTime:0.94  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:85767  AvgTotalLoss:0.05028  AvgPostnetLoss:0.00113  AvgDecoderLoss:0.00121  AvgStopLoss:0.04794  EpochTime:45.22  AvgStepTime:0.66\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.08996   PostnetLoss: 0.00457   DecoderLoss:0.00500  StopLoss: 0.08039  \n",
      "   | > TotalLoss: 0.07389   PostnetLoss: 0.00670   DecoderLoss:0.00730  StopLoss: 0.05989  \n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00113   Validation Loss: 0.00617\n",
      "\n",
      " > Epoch 243/1000\n",
      "   | > Step:2/68  GlobalStep:85770  TotalLoss:0.00149  PostnetLoss:0.00072  DecoderLoss:0.00077  StopLoss:0.06891  GradNorm:0.00383  GradNormST:0.02275  AvgTextLen:4.0  AvgSpecLen:31.3  StepTime:0.38  LR:0.000100\n",
      "   | > Step:12/68  GlobalStep:85780  TotalLoss:0.00178  PostnetLoss:0.00086  DecoderLoss:0.00092  StopLoss:0.04733  GradNorm:0.00405  GradNormST:0.01226  AvgTextLen:11.0  AvgSpecLen:49.3  StepTime:0.37  LR:0.000100\n",
      "   | > Step:22/68  GlobalStep:85790  TotalLoss:0.00211  PostnetLoss:0.00103  DecoderLoss:0.00109  StopLoss:0.06175  GradNorm:0.00393  GradNormST:0.01133  AvgTextLen:18.0  AvgSpecLen:72.0  StepTime:0.52  LR:0.000100\n",
      "   | > Step:32/68  GlobalStep:85800  TotalLoss:0.00230  PostnetLoss:0.00111  DecoderLoss:0.00118  StopLoss:0.05450  GradNorm:0.00368  GradNormST:0.01190  AvgTextLen:24.4  AvgSpecLen:91.5  StepTime:0.58  LR:0.000100\n",
      "   | > Step:42/68  GlobalStep:85810  TotalLoss:0.00253  PostnetLoss:0.00122  DecoderLoss:0.00131  StopLoss:0.04270  GradNorm:0.00302  GradNormST:0.01359  AvgTextLen:33.1  AvgSpecLen:109.7  StepTime:0.52  LR:0.000100\n",
      "   | > Step:52/68  GlobalStep:85820  TotalLoss:0.00275  PostnetLoss:0.00133  DecoderLoss:0.00142  StopLoss:0.04473  GradNorm:0.00396  GradNormST:0.01289  AvgTextLen:42.1  AvgSpecLen:137.5  StepTime:0.87  LR:0.000100\n",
      "   | > Step:62/68  GlobalStep:85830  TotalLoss:0.00297  PostnetLoss:0.00143  DecoderLoss:0.00154  StopLoss:0.03719  GradNorm:0.00291  GradNormST:0.01405  AvgTextLen:54.0  AvgSpecLen:174.7  StepTime:0.77  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:85836  AvgTotalLoss:0.04964  AvgPostnetLoss:0.00113  AvgDecoderLoss:0.00121  AvgStopLoss:0.04730  EpochTime:43.24  AvgStepTime:0.63\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.09649   PostnetLoss: 0.00477   DecoderLoss:0.00519  StopLoss: 0.08652  \n",
      "   | > TotalLoss: 0.07391   PostnetLoss: 0.00708   DecoderLoss:0.00770  StopLoss: 0.05913  \n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00113   Validation Loss: 0.00644\n",
      "\n",
      " > Epoch 244/1000\n",
      "   | > Step:3/68  GlobalStep:85840  TotalLoss:0.00161  PostnetLoss:0.00078  DecoderLoss:0.00083  StopLoss:0.06487  GradNorm:0.00461  GradNormST:0.01646  AvgTextLen:4.7  AvgSpecLen:29.7  StepTime:0.35  LR:0.000100\n",
      "   | > Step:13/68  GlobalStep:85850  TotalLoss:0.00180  PostnetLoss:0.00088  DecoderLoss:0.00093  StopLoss:0.06420  GradNorm:0.00318  GradNormST:0.02426  AvgTextLen:11.5  AvgSpecLen:55.2  StepTime:0.46  LR:0.000100\n",
      "   | > Step:23/68  GlobalStep:85860  TotalLoss:0.00212  PostnetLoss:0.00102  DecoderLoss:0.00109  StopLoss:0.04256  GradNorm:0.00317  GradNormST:0.00816  AvgTextLen:18.3  AvgSpecLen:72.2  StepTime:0.45  LR:0.000100\n",
      "   | > Step:33/68  GlobalStep:85870  TotalLoss:0.00237  PostnetLoss:0.00115  DecoderLoss:0.00123  StopLoss:0.04412  GradNorm:0.00358  GradNormST:0.00927  AvgTextLen:25.9  AvgSpecLen:88.9  StepTime:0.51  LR:0.000100\n",
      "   | > Step:43/68  GlobalStep:85880  TotalLoss:0.00248  PostnetLoss:0.00120  DecoderLoss:0.00128  StopLoss:0.04233  GradNorm:0.00290  GradNormST:0.01164  AvgTextLen:33.4  AvgSpecLen:115.7  StepTime:0.68  LR:0.000100\n",
      "   | > Step:53/68  GlobalStep:85890  TotalLoss:0.00267  PostnetLoss:0.00129  DecoderLoss:0.00138  StopLoss:0.03848  GradNorm:0.00406  GradNormST:0.00795  AvgTextLen:42.4  AvgSpecLen:137.6  StepTime:0.84  LR:0.000100\n",
      "   | > Step:63/68  GlobalStep:85900  TotalLoss:0.00304  PostnetLoss:0.00146  DecoderLoss:0.00158  StopLoss:0.02793  GradNorm:0.00351  GradNormST:0.00889  AvgTextLen:58.6  AvgSpecLen:185.1  StepTime:0.99  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:85905  AvgTotalLoss:0.04922  AvgPostnetLoss:0.00113  AvgDecoderLoss:0.00120  AvgStopLoss:0.04689  EpochTime:45.02  AvgStepTime:0.65\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.09547   PostnetLoss: 0.00477   DecoderLoss:0.00521  StopLoss: 0.08549  \n",
      "   | > TotalLoss: 0.07224   PostnetLoss: 0.00698   DecoderLoss:0.00763  StopLoss: 0.05764  \n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00113   Validation Loss: 0.00632\n",
      "\n",
      " > Epoch 245/1000\n",
      "   | > Step:4/68  GlobalStep:85910  TotalLoss:0.00156  PostnetLoss:0.00075  DecoderLoss:0.00080  StopLoss:0.08235  GradNorm:0.00391  GradNormST:0.03286  AvgTextLen:4.8  AvgSpecLen:31.1  StepTime:0.24  LR:0.000100\n",
      "   | > Step:14/68  GlobalStep:85920  TotalLoss:0.00191  PostnetLoss:0.00092  DecoderLoss:0.00098  StopLoss:0.05448  GradNorm:0.00375  GradNormST:0.01290  AvgTextLen:12.2  AvgSpecLen:52.0  StepTime:0.36  LR:0.000100\n",
      "   | > Step:24/68  GlobalStep:85930  TotalLoss:0.00223  PostnetLoss:0.00108  DecoderLoss:0.00115  StopLoss:0.05209  GradNorm:0.00323  GradNormST:0.00929  AvgTextLen:19.2  AvgSpecLen:79.0  StepTime:0.56  LR:0.000100\n",
      "   | > Step:34/68  GlobalStep:85940  TotalLoss:0.00239  PostnetLoss:0.00115  DecoderLoss:0.00123  StopLoss:0.03576  GradNorm:0.00337  GradNormST:0.00937  AvgTextLen:26.2  AvgSpecLen:95.9  StepTime:0.64  LR:0.000100\n",
      "   | > Step:44/68  GlobalStep:85950  TotalLoss:0.00255  PostnetLoss:0.00123  DecoderLoss:0.00132  StopLoss:0.03392  GradNorm:0.00297  GradNormST:0.00865  AvgTextLen:33.8  AvgSpecLen:116.3  StepTime:0.68  LR:0.000100\n",
      "   | > Step:54/68  GlobalStep:85960  TotalLoss:0.00276  PostnetLoss:0.00133  DecoderLoss:0.00143  StopLoss:0.04499  GradNorm:0.00364  GradNormST:0.00885  AvgTextLen:42.2  AvgSpecLen:144.9  StepTime:0.80  LR:0.000100\n",
      "   | > Step:64/68  GlobalStep:85970  TotalLoss:0.00285  PostnetLoss:0.00137  DecoderLoss:0.00148  StopLoss:0.03429  GradNorm:0.00271  GradNormST:0.00827  AvgTextLen:60.6  AvgSpecLen:177.6  StepTime:0.87  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:85974  AvgTotalLoss:0.05007  AvgPostnetLoss:0.00112  AvgDecoderLoss:0.00120  AvgStopLoss:0.04774  EpochTime:44.39  AvgStepTime:0.64\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.09376   PostnetLoss: 0.00480   DecoderLoss:0.00523  StopLoss: 0.08373  \n",
      "   | > TotalLoss: 0.06965   PostnetLoss: 0.00689   DecoderLoss:0.00750  StopLoss: 0.05526  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00112   Validation Loss: 0.00634\n",
      "\n",
      " > Epoch 246/1000\n",
      "   | > Step:5/68  GlobalStep:85980  TotalLoss:0.00164  PostnetLoss:0.00079  DecoderLoss:0.00085  StopLoss:0.05411  GradNorm:0.00365  GradNormST:0.01384  AvgTextLen:5.9  AvgSpecLen:33.2  StepTime:0.35  LR:0.000100\n",
      "   | > Step:15/68  GlobalStep:85990  TotalLoss:0.00197  PostnetLoss:0.00096  DecoderLoss:0.00101  StopLoss:0.04587  GradNorm:0.00343  GradNormST:0.01646  AvgTextLen:12.8  AvgSpecLen:59.1  StepTime:0.52  LR:0.000100\n",
      "   | > Step:25/68  GlobalStep:86000  TotalLoss:0.00211  PostnetLoss:0.00102  DecoderLoss:0.00109  StopLoss:0.05434  GradNorm:0.00333  GradNormST:0.01004  AvgTextLen:19.8  AvgSpecLen:73.3  StepTime:0.43  LR:0.000100\n",
      " | | > Checkpoint saving : /vm/TTS/models/cartman_models/mozilla-no-loc-fattn-stopnet-sigmoid-loss_masking-August-15-2019_11+26AM-de6aeb9/checkpoint_86000.pth.tar\n",
      "warning: audio amplitude out of range, auto clipped.\n",
      "   | > Step:35/68  GlobalStep:86010  TotalLoss:0.00236  PostnetLoss:0.00115  DecoderLoss:0.00122  StopLoss:0.05519  GradNorm:0.00297  GradNormST:0.01706  AvgTextLen:27.1  AvgSpecLen:104.7  StepTime:0.68  LR:0.000100\n",
      "   | > Step:45/68  GlobalStep:86020  TotalLoss:0.00249  PostnetLoss:0.00120  DecoderLoss:0.00129  StopLoss:0.04417  GradNorm:0.00289  GradNormST:0.01928  AvgTextLen:34.4  AvgSpecLen:118.6  StepTime:0.66  LR:0.000100\n",
      "   | > Step:55/68  GlobalStep:86030  TotalLoss:0.00282  PostnetLoss:0.00136  DecoderLoss:0.00146  StopLoss:0.03873  GradNorm:0.00294  GradNormST:0.00590  AvgTextLen:43.9  AvgSpecLen:145.2  StepTime:0.81  LR:0.000100\n",
      "   | > Step:65/68  GlobalStep:86040  TotalLoss:0.00308  PostnetLoss:0.00148  DecoderLoss:0.00160  StopLoss:0.03338  GradNorm:0.00325  GradNormST:0.01177  AvgTextLen:64.2  AvgSpecLen:207.4  StepTime:1.23  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:86043  AvgTotalLoss:0.04993  AvgPostnetLoss:0.00113  AvgDecoderLoss:0.00120  AvgStopLoss:0.04760  EpochTime:44.78  AvgStepTime:0.65\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.09604   PostnetLoss: 0.00464   DecoderLoss:0.00507  StopLoss: 0.08633  \n",
      "   | > TotalLoss: 0.07148   PostnetLoss: 0.00685   DecoderLoss:0.00749  StopLoss: 0.05715  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00113   Validation Loss: 0.00626\n",
      "\n",
      " > Epoch 247/1000\n",
      "   | > Step:6/68  GlobalStep:86050  TotalLoss:0.00162  PostnetLoss:0.00079  DecoderLoss:0.00083  StopLoss:0.04757  GradNorm:0.00366  GradNormST:0.01606  AvgTextLen:6.0  AvgSpecLen:34.8  StepTime:0.44  LR:0.000100\n",
      "   | > Step:16/68  GlobalStep:86060  TotalLoss:0.00198  PostnetLoss:0.00096  DecoderLoss:0.00102  StopLoss:0.04795  GradNorm:0.00365  GradNormST:0.01257  AvgTextLen:13.8  AvgSpecLen:60.6  StepTime:0.38  LR:0.000100\n",
      "   | > Step:26/68  GlobalStep:86070  TotalLoss:0.00217  PostnetLoss:0.00105  DecoderLoss:0.00112  StopLoss:0.04259  GradNorm:0.00312  GradNormST:0.01139  AvgTextLen:20.3  AvgSpecLen:77.5  StepTime:0.49  LR:0.000100\n",
      "   | > Step:36/68  GlobalStep:86080  TotalLoss:0.00240  PostnetLoss:0.00116  DecoderLoss:0.00124  StopLoss:0.06642  GradNorm:0.00302  GradNormST:0.01299  AvgTextLen:28.5  AvgSpecLen:100.2  StepTime:0.55  LR:0.000100\n",
      "   | > Step:46/68  GlobalStep:86090  TotalLoss:0.00255  PostnetLoss:0.00123  DecoderLoss:0.00132  StopLoss:0.04740  GradNorm:0.00293  GradNormST:0.01249  AvgTextLen:35.4  AvgSpecLen:122.6  StepTime:0.70  LR:0.000100\n",
      "   | > Step:56/68  GlobalStep:86100  TotalLoss:0.00277  PostnetLoss:0.00133  DecoderLoss:0.00144  StopLoss:0.03083  GradNorm:0.00323  GradNormST:0.01130  AvgTextLen:45.8  AvgSpecLen:145.4  StepTime:1.01  LR:0.000100\n",
      "   | > Step:66/68  GlobalStep:86110  TotalLoss:0.00321  PostnetLoss:0.00154  DecoderLoss:0.00167  StopLoss:0.03210  GradNorm:0.00329  GradNormST:0.00935  AvgTextLen:71.2  AvgSpecLen:220.8  StepTime:1.09  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:86112  AvgTotalLoss:0.05012  AvgPostnetLoss:0.00113  AvgDecoderLoss:0.00120  AvgStopLoss:0.04779  EpochTime:45.94  AvgStepTime:0.67\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.09681   PostnetLoss: 0.00464   DecoderLoss:0.00508  StopLoss: 0.08708  \n",
      "   | > TotalLoss: 0.07361   PostnetLoss: 0.00687   DecoderLoss:0.00751  StopLoss: 0.05924  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00113   Validation Loss: 0.00632\n",
      "\n",
      " > Epoch 248/1000\n",
      "   | > Step:7/68  GlobalStep:86120  TotalLoss:0.00175  PostnetLoss:0.00085  DecoderLoss:0.00090  StopLoss:0.04991  GradNorm:0.00408  GradNormST:0.02435  AvgTextLen:7.4  AvgSpecLen:44.6  StepTime:0.39  LR:0.000100\n",
      "   | > Step:17/68  GlobalStep:86130  TotalLoss:0.00195  PostnetLoss:0.00095  DecoderLoss:0.00101  StopLoss:0.06560  GradNorm:0.00354  GradNormST:0.01381  AvgTextLen:14.7  AvgSpecLen:66.0  StepTime:0.45  LR:0.000100\n",
      "   | > Step:27/68  GlobalStep:86140  TotalLoss:0.00223  PostnetLoss:0.00108  DecoderLoss:0.00115  StopLoss:0.05386  GradNorm:0.00304  GradNormST:0.02079  AvgTextLen:21.0  AvgSpecLen:80.2  StepTime:0.58  LR:0.000100\n",
      "   | > Step:37/68  GlobalStep:86150  TotalLoss:0.00246  PostnetLoss:0.00119  DecoderLoss:0.00127  StopLoss:0.04704  GradNorm:0.00300  GradNormST:0.00831  AvgTextLen:29.3  AvgSpecLen:102.4  StepTime:0.50  LR:0.000100\n",
      "   | > Step:47/68  GlobalStep:86160  TotalLoss:0.00263  PostnetLoss:0.00127  DecoderLoss:0.00136  StopLoss:0.04332  GradNorm:0.00294  GradNormST:0.01018  AvgTextLen:36.7  AvgSpecLen:126.9  StepTime:0.70  LR:0.000100\n",
      "   | > Step:57/68  GlobalStep:86170  TotalLoss:0.00285  PostnetLoss:0.00137  DecoderLoss:0.00148  StopLoss:0.03794  GradNorm:0.00325  GradNormST:0.00764  AvgTextLen:47.1  AvgSpecLen:161.0  StepTime:0.91  LR:0.000100\n",
      "   | > Step:67/68  GlobalStep:86180  TotalLoss:0.00323  PostnetLoss:0.00155  DecoderLoss:0.00168  StopLoss:0.03192  GradNorm:0.00263  GradNormST:0.01494  AvgTextLen:80.6  AvgSpecLen:246.5  StepTime:1.24  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:86181  AvgTotalLoss:0.05000  AvgPostnetLoss:0.00112  AvgDecoderLoss:0.00120  AvgStopLoss:0.04768  EpochTime:43.81  AvgStepTime:0.63\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.09585   PostnetLoss: 0.00469   DecoderLoss:0.00512  StopLoss: 0.08604  \n",
      "   | > TotalLoss: 0.07612   PostnetLoss: 0.00678   DecoderLoss:0.00740  StopLoss: 0.06193  \n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00112   Validation Loss: 0.00627\n",
      "\n",
      " > Epoch 249/1000\n",
      "   | > Step:8/68  GlobalStep:86190  TotalLoss:0.00177  PostnetLoss:0.00086  DecoderLoss:0.00091  StopLoss:0.06594  GradNorm:0.00486  GradNormST:0.01446  AvgTextLen:8.3  AvgSpecLen:44.0  StepTime:0.33  LR:0.000100\n",
      "   | > Step:18/68  GlobalStep:86200  TotalLoss:0.00195  PostnetLoss:0.00095  DecoderLoss:0.00101  StopLoss:0.05294  GradNorm:0.00324  GradNormST:0.00905  AvgTextLen:15.2  AvgSpecLen:64.5  StepTime:0.36  LR:0.000100\n",
      "   | > Step:28/68  GlobalStep:86210  TotalLoss:0.00223  PostnetLoss:0.00108  DecoderLoss:0.00115  StopLoss:0.05945  GradNorm:0.00342  GradNormST:0.01427  AvgTextLen:22.1  AvgSpecLen:85.4  StepTime:0.46  LR:0.000100\n",
      "   | > Step:38/68  GlobalStep:86220  TotalLoss:0.00240  PostnetLoss:0.00116  DecoderLoss:0.00124  StopLoss:0.03508  GradNorm:0.00306  GradNormST:0.00717  AvgTextLen:29.4  AvgSpecLen:104.1  StepTime:0.62  LR:0.000100\n",
      "   | > Step:48/68  GlobalStep:86230  TotalLoss:0.00259  PostnetLoss:0.00125  DecoderLoss:0.00134  StopLoss:0.04087  GradNorm:0.00296  GradNormST:0.00978  AvgTextLen:37.5  AvgSpecLen:123.8  StepTime:0.64  LR:0.000100\n",
      "   | > Step:58/68  GlobalStep:86240  TotalLoss:0.00285  PostnetLoss:0.00137  DecoderLoss:0.00148  StopLoss:0.05043  GradNorm:0.00322  GradNormST:0.01283  AvgTextLen:47.8  AvgSpecLen:158.0  StepTime:0.94  LR:0.000100\n",
      "   | > Step:68/68  GlobalStep:86250  TotalLoss:0.00319  PostnetLoss:0.00153  DecoderLoss:0.00166  StopLoss:0.03788  GradNorm:0.00290  GradNormST:0.01220  AvgTextLen:103.0  AvgSpecLen:308.9  StepTime:1.87  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:86250  AvgTotalLoss:0.04927  AvgPostnetLoss:0.00112  AvgDecoderLoss:0.00120  AvgStopLoss:0.04695  EpochTime:43.45  AvgStepTime:0.63\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.09815   PostnetLoss: 0.00474   DecoderLoss:0.00516  StopLoss: 0.08825  \n",
      "   | > TotalLoss: 0.07813   PostnetLoss: 0.00711   DecoderLoss:0.00774  StopLoss: 0.06328  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00112   Validation Loss: 0.00645\n",
      "\n",
      " > Epoch 250/1000\n",
      "   | > Step:9/68  GlobalStep:86260  TotalLoss:0.00171  PostnetLoss:0.00083  DecoderLoss:0.00088  StopLoss:0.03833  GradNorm:0.00354  GradNormST:0.01604  AvgTextLen:9.0  AvgSpecLen:42.1  StepTime:0.33  LR:0.000100\n",
      "   | > Step:19/68  GlobalStep:86270  TotalLoss:0.00199  PostnetLoss:0.00097  DecoderLoss:0.00102  StopLoss:0.07202  GradNorm:0.00533  GradNormST:0.01997  AvgTextLen:15.5  AvgSpecLen:63.2  StepTime:0.41  LR:0.000100\n",
      "   | > Step:29/68  GlobalStep:86280  TotalLoss:0.00220  PostnetLoss:0.00107  DecoderLoss:0.00114  StopLoss:0.06403  GradNorm:0.00327  GradNormST:0.01360  AvgTextLen:21.8  AvgSpecLen:83.4  StepTime:0.54  LR:0.000100\n",
      "   | > Step:39/68  GlobalStep:86290  TotalLoss:0.00243  PostnetLoss:0.00118  DecoderLoss:0.00126  StopLoss:0.03568  GradNorm:0.00298  GradNormST:0.00933  AvgTextLen:30.2  AvgSpecLen:110.4  StepTime:0.58  LR:0.000100\n",
      "   | > Step:49/68  GlobalStep:86300  TotalLoss:0.00263  PostnetLoss:0.00127  DecoderLoss:0.00136  StopLoss:0.03394  GradNorm:0.00317  GradNormST:0.00735  AvgTextLen:38.2  AvgSpecLen:127.7  StepTime:0.78  LR:0.000100\n",
      "   | > Step:59/68  GlobalStep:86310  TotalLoss:0.00282  PostnetLoss:0.00136  DecoderLoss:0.00146  StopLoss:0.03317  GradNorm:0.00278  GradNormST:0.00857  AvgTextLen:50.5  AvgSpecLen:155.9  StepTime:0.87  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:86319  AvgTotalLoss:0.04950  AvgPostnetLoss:0.00112  AvgDecoderLoss:0.00120  AvgStopLoss:0.04717  EpochTime:42.00  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.09738   PostnetLoss: 0.00462   DecoderLoss:0.00505  StopLoss: 0.08770  \n",
      "   | > TotalLoss: 0.06960   PostnetLoss: 0.00691   DecoderLoss:0.00754  StopLoss: 0.05515  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00112   Validation Loss: 0.00635\n",
      "\n",
      " > Epoch 251/1000\n",
      "   | > Step:0/68  GlobalStep:86320  TotalLoss:0.00166  PostnetLoss:0.00080  DecoderLoss:0.00087  StopLoss:0.08567  GradNorm:0.00573  GradNormST:0.02873  AvgTextLen:3.0  AvgSpecLen:25.3  StepTime:0.51  LR:0.000100\n",
      "   | > Step:10/68  GlobalStep:86330  TotalLoss:0.00178  PostnetLoss:0.00086  DecoderLoss:0.00092  StopLoss:0.05838  GradNorm:0.00362  GradNormST:0.02817  AvgTextLen:9.8  AvgSpecLen:48.6  StepTime:0.54  LR:0.000100\n",
      "   | > Step:20/68  GlobalStep:86340  TotalLoss:0.00208  PostnetLoss:0.00101  DecoderLoss:0.00107  StopLoss:0.04098  GradNorm:0.00363  GradNormST:0.00892  AvgTextLen:16.4  AvgSpecLen:64.8  StepTime:0.53  LR:0.000100\n",
      "   | > Step:30/68  GlobalStep:86350  TotalLoss:0.00226  PostnetLoss:0.00109  DecoderLoss:0.00116  StopLoss:0.04630  GradNorm:0.00303  GradNormST:0.01062  AvgTextLen:22.9  AvgSpecLen:85.8  StepTime:0.46  LR:0.000100\n",
      "   | > Step:40/68  GlobalStep:86360  TotalLoss:0.00242  PostnetLoss:0.00117  DecoderLoss:0.00125  StopLoss:0.04356  GradNorm:0.00299  GradNormST:0.00925  AvgTextLen:30.9  AvgSpecLen:105.3  StepTime:0.52  LR:0.000100\n",
      "   | > Step:50/68  GlobalStep:86370  TotalLoss:0.00265  PostnetLoss:0.00128  DecoderLoss:0.00137  StopLoss:0.03769  GradNorm:0.00348  GradNormST:0.00804  AvgTextLen:38.8  AvgSpecLen:132.0  StepTime:0.86  LR:0.000100\n",
      "   | > Step:60/68  GlobalStep:86380  TotalLoss:0.00290  PostnetLoss:0.00139  DecoderLoss:0.00151  StopLoss:0.03347  GradNorm:0.00276  GradNormST:0.00773  AvgTextLen:52.0  AvgSpecLen:172.7  StepTime:0.92  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:86388  AvgTotalLoss:0.04935  AvgPostnetLoss:0.00112  AvgDecoderLoss:0.00120  AvgStopLoss:0.04703  EpochTime:42.50  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.09554   PostnetLoss: 0.00463   DecoderLoss:0.00506  StopLoss: 0.08586  \n",
      "   | > TotalLoss: 0.07376   PostnetLoss: 0.00705   DecoderLoss:0.00766  StopLoss: 0.05905  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00112   Validation Loss: 0.00643\n",
      "\n",
      " > Epoch 252/1000\n",
      "   | > Step:1/68  GlobalStep:86390  TotalLoss:0.00164  PostnetLoss:0.00079  DecoderLoss:0.00085  StopLoss:0.06002  GradNorm:0.00470  GradNormST:0.01893  AvgTextLen:3.7  AvgSpecLen:28.6  StepTime:0.32  LR:0.000100\n",
      "   | > Step:11/68  GlobalStep:86400  TotalLoss:0.00178  PostnetLoss:0.00086  DecoderLoss:0.00092  StopLoss:0.07402  GradNorm:0.00342  GradNormST:0.02655  AvgTextLen:10.3  AvgSpecLen:49.4  StepTime:0.41  LR:0.000100\n",
      "   | > Step:21/68  GlobalStep:86410  TotalLoss:0.00206  PostnetLoss:0.00100  DecoderLoss:0.00106  StopLoss:0.05255  GradNorm:0.00332  GradNormST:0.01762  AvgTextLen:16.6  AvgSpecLen:66.3  StepTime:0.48  LR:0.000100\n",
      "   | > Step:31/68  GlobalStep:86420  TotalLoss:0.00220  PostnetLoss:0.00107  DecoderLoss:0.00114  StopLoss:0.03798  GradNorm:0.00313  GradNormST:0.01480  AvgTextLen:24.1  AvgSpecLen:82.7  StepTime:0.54  LR:0.000100\n",
      "   | > Step:41/68  GlobalStep:86430  TotalLoss:0.00253  PostnetLoss:0.00122  DecoderLoss:0.00130  StopLoss:0.03828  GradNorm:0.00319  GradNormST:0.01275  AvgTextLen:32.3  AvgSpecLen:109.4  StepTime:0.69  LR:0.000100\n",
      "   | > Step:51/68  GlobalStep:86440  TotalLoss:0.00261  PostnetLoss:0.00126  DecoderLoss:0.00135  StopLoss:0.04553  GradNorm:0.00326  GradNormST:0.01056  AvgTextLen:39.2  AvgSpecLen:126.9  StepTime:0.62  LR:0.000100\n",
      "   | > Step:61/68  GlobalStep:86450  TotalLoss:0.00286  PostnetLoss:0.00137  DecoderLoss:0.00149  StopLoss:0.03389  GradNorm:0.00284  GradNormST:0.00758  AvgTextLen:54.6  AvgSpecLen:165.5  StepTime:0.96  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:86457  AvgTotalLoss:0.04930  AvgPostnetLoss:0.00112  AvgDecoderLoss:0.00120  AvgStopLoss:0.04699  EpochTime:43.24  AvgStepTime:0.63\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.09168   PostnetLoss: 0.00469   DecoderLoss:0.00513  StopLoss: 0.08187  \n",
      "   | > TotalLoss: 0.06573   PostnetLoss: 0.00694   DecoderLoss:0.00756  StopLoss: 0.05122  \n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00112   Validation Loss: 0.00641\n",
      "\n",
      " > Epoch 253/1000\n",
      "   | > Step:2/68  GlobalStep:86460  TotalLoss:0.00149  PostnetLoss:0.00073  DecoderLoss:0.00077  StopLoss:0.04922  GradNorm:0.00369  GradNormST:0.01874  AvgTextLen:4.0  AvgSpecLen:31.3  StepTime:0.38  LR:0.000100\n",
      "   | > Step:12/68  GlobalStep:86470  TotalLoss:0.00178  PostnetLoss:0.00086  DecoderLoss:0.00092  StopLoss:0.05049  GradNorm:0.00349  GradNormST:0.01700  AvgTextLen:11.0  AvgSpecLen:49.3  StepTime:0.44  LR:0.000100\n",
      "   | > Step:22/68  GlobalStep:86480  TotalLoss:0.00209  PostnetLoss:0.00101  DecoderLoss:0.00108  StopLoss:0.05739  GradNorm:0.00357  GradNormST:0.01208  AvgTextLen:18.0  AvgSpecLen:72.0  StepTime:0.48  LR:0.000100\n",
      "   | > Step:32/68  GlobalStep:86490  TotalLoss:0.00227  PostnetLoss:0.00110  DecoderLoss:0.00118  StopLoss:0.05879  GradNorm:0.00300  GradNormST:0.01765  AvgTextLen:24.4  AvgSpecLen:91.5  StepTime:0.55  LR:0.000100\n",
      "   | > Step:42/68  GlobalStep:86500  TotalLoss:0.00247  PostnetLoss:0.00119  DecoderLoss:0.00128  StopLoss:0.04013  GradNorm:0.00302  GradNormST:0.01465  AvgTextLen:33.1  AvgSpecLen:109.7  StepTime:0.50  LR:0.000100\n",
      "   | > Step:52/68  GlobalStep:86510  TotalLoss:0.00272  PostnetLoss:0.00131  DecoderLoss:0.00140  StopLoss:0.05140  GradNorm:0.00361  GradNormST:0.01817  AvgTextLen:42.1  AvgSpecLen:137.5  StepTime:0.78  LR:0.000100\n",
      "   | > Step:62/68  GlobalStep:86520  TotalLoss:0.00295  PostnetLoss:0.00142  DecoderLoss:0.00153  StopLoss:0.03894  GradNorm:0.00274  GradNormST:0.01575  AvgTextLen:54.0  AvgSpecLen:174.7  StepTime:0.78  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:86526  AvgTotalLoss:0.04881  AvgPostnetLoss:0.00112  AvgDecoderLoss:0.00120  AvgStopLoss:0.04649  EpochTime:42.23  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.09675   PostnetLoss: 0.00474   DecoderLoss:0.00517  StopLoss: 0.08684  \n",
      "   | > TotalLoss: 0.07133   PostnetLoss: 0.00700   DecoderLoss:0.00763  StopLoss: 0.05671  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00112   Validation Loss: 0.00639\n",
      "\n",
      " > Epoch 254/1000\n",
      "   | > Step:3/68  GlobalStep:86530  TotalLoss:0.00153  PostnetLoss:0.00074  DecoderLoss:0.00079  StopLoss:0.07395  GradNorm:0.00348  GradNormST:0.02030  AvgTextLen:4.7  AvgSpecLen:29.7  StepTime:0.27  LR:0.000100\n",
      "   | > Step:13/68  GlobalStep:86540  TotalLoss:0.00179  PostnetLoss:0.00087  DecoderLoss:0.00092  StopLoss:0.06585  GradNorm:0.00334  GradNormST:0.02242  AvgTextLen:11.5  AvgSpecLen:55.2  StepTime:0.44  LR:0.000100\n",
      "   | > Step:23/68  GlobalStep:86550  TotalLoss:0.00214  PostnetLoss:0.00103  DecoderLoss:0.00110  StopLoss:0.04378  GradNorm:0.00351  GradNormST:0.00971  AvgTextLen:18.3  AvgSpecLen:72.2  StepTime:0.46  LR:0.000100\n",
      "   | > Step:33/68  GlobalStep:86560  TotalLoss:0.00233  PostnetLoss:0.00113  DecoderLoss:0.00120  StopLoss:0.05231  GradNorm:0.00323  GradNormST:0.01101  AvgTextLen:25.9  AvgSpecLen:88.9  StepTime:0.47  LR:0.000100\n",
      "   | > Step:43/68  GlobalStep:86570  TotalLoss:0.00251  PostnetLoss:0.00121  DecoderLoss:0.00130  StopLoss:0.04962  GradNorm:0.00340  GradNormST:0.01222  AvgTextLen:33.4  AvgSpecLen:115.7  StepTime:0.61  LR:0.000100\n",
      "   | > Step:53/68  GlobalStep:86580  TotalLoss:0.00266  PostnetLoss:0.00128  DecoderLoss:0.00138  StopLoss:0.04947  GradNorm:0.00319  GradNormST:0.01096  AvgTextLen:42.4  AvgSpecLen:137.6  StepTime:0.64  LR:0.000100\n",
      "   | > Step:63/68  GlobalStep:86590  TotalLoss:0.00300  PostnetLoss:0.00144  DecoderLoss:0.00156  StopLoss:0.03764  GradNorm:0.00291  GradNormST:0.01347  AvgTextLen:58.6  AvgSpecLen:185.1  StepTime:0.93  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:86595  AvgTotalLoss:0.05022  AvgPostnetLoss:0.00112  AvgDecoderLoss:0.00120  AvgStopLoss:0.04790  EpochTime:43.44  AvgStepTime:0.63\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.09328   PostnetLoss: 0.00470   DecoderLoss:0.00513  StopLoss: 0.08345  \n",
      "   | > TotalLoss: 0.07084   PostnetLoss: 0.00716   DecoderLoss:0.00779  StopLoss: 0.05589  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00112   Validation Loss: 0.00650\n",
      "\n",
      " > Epoch 255/1000\n",
      "   | > Step:4/68  GlobalStep:86600  TotalLoss:0.00156  PostnetLoss:0.00075  DecoderLoss:0.00080  StopLoss:0.09071  GradNorm:0.00362  GradNormST:0.02890  AvgTextLen:4.8  AvgSpecLen:31.1  StepTime:0.20  LR:0.000100\n",
      "   | > Step:14/68  GlobalStep:86610  TotalLoss:0.00187  PostnetLoss:0.00091  DecoderLoss:0.00097  StopLoss:0.05642  GradNorm:0.00342  GradNormST:0.01607  AvgTextLen:12.2  AvgSpecLen:52.0  StepTime:0.27  LR:0.000100\n",
      "   | > Step:24/68  GlobalStep:86620  TotalLoss:0.00217  PostnetLoss:0.00105  DecoderLoss:0.00112  StopLoss:0.04843  GradNorm:0.00331  GradNormST:0.01183  AvgTextLen:19.2  AvgSpecLen:79.0  StepTime:0.50  LR:0.000100\n",
      "   | > Step:34/68  GlobalStep:86630  TotalLoss:0.00241  PostnetLoss:0.00116  DecoderLoss:0.00124  StopLoss:0.03794  GradNorm:0.00336  GradNormST:0.00728  AvgTextLen:26.2  AvgSpecLen:95.9  StepTime:0.63  LR:0.000100\n",
      "   | > Step:44/68  GlobalStep:86640  TotalLoss:0.00254  PostnetLoss:0.00123  DecoderLoss:0.00131  StopLoss:0.04138  GradNorm:0.00305  GradNormST:0.01171  AvgTextLen:33.8  AvgSpecLen:116.3  StepTime:0.66  LR:0.000100\n",
      "   | > Step:54/68  GlobalStep:86650  TotalLoss:0.00278  PostnetLoss:0.00134  DecoderLoss:0.00144  StopLoss:0.04130  GradNorm:0.00452  GradNormST:0.00865  AvgTextLen:42.2  AvgSpecLen:144.9  StepTime:0.68  LR:0.000100\n",
      "   | > Step:64/68  GlobalStep:86660  TotalLoss:0.00283  PostnetLoss:0.00136  DecoderLoss:0.00147  StopLoss:0.03422  GradNorm:0.00269  GradNormST:0.00852  AvgTextLen:60.6  AvgSpecLen:177.6  StepTime:0.81  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:86664  AvgTotalLoss:0.04841  AvgPostnetLoss:0.00112  AvgDecoderLoss:0.00120  AvgStopLoss:0.04609  EpochTime:42.90  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.08830   PostnetLoss: 0.00469   DecoderLoss:0.00511  StopLoss: 0.07849  \n",
      "   | > TotalLoss: 0.06813   PostnetLoss: 0.00700   DecoderLoss:0.00761  StopLoss: 0.05352  \n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00112   Validation Loss: 0.00644\n",
      "\n",
      " > Epoch 256/1000\n",
      "   | > Step:5/68  GlobalStep:86670  TotalLoss:0.00161  PostnetLoss:0.00078  DecoderLoss:0.00083  StopLoss:0.06068  GradNorm:0.00408  GradNormST:0.01490  AvgTextLen:5.9  AvgSpecLen:33.2  StepTime:0.34  LR:0.000100\n",
      "   | > Step:15/68  GlobalStep:86680  TotalLoss:0.00197  PostnetLoss:0.00096  DecoderLoss:0.00101  StopLoss:0.04821  GradNorm:0.00367  GradNormST:0.01470  AvgTextLen:12.8  AvgSpecLen:59.1  StepTime:0.52  LR:0.000100\n",
      "   | > Step:25/68  GlobalStep:86690  TotalLoss:0.00213  PostnetLoss:0.00103  DecoderLoss:0.00110  StopLoss:0.05217  GradNorm:0.00324  GradNormST:0.01087  AvgTextLen:19.8  AvgSpecLen:73.3  StepTime:0.48  LR:0.000100\n",
      "   | > Step:35/68  GlobalStep:86700  TotalLoss:0.00236  PostnetLoss:0.00114  DecoderLoss:0.00122  StopLoss:0.05272  GradNorm:0.00331  GradNormST:0.02501  AvgTextLen:27.1  AvgSpecLen:104.7  StepTime:0.64  LR:0.000100\n",
      "   | > Step:45/68  GlobalStep:86710  TotalLoss:0.00245  PostnetLoss:0.00118  DecoderLoss:0.00127  StopLoss:0.04509  GradNorm:0.00300  GradNormST:0.01449  AvgTextLen:34.4  AvgSpecLen:118.6  StepTime:0.69  LR:0.000100\n",
      "   | > Step:55/68  GlobalStep:86720  TotalLoss:0.00278  PostnetLoss:0.00134  DecoderLoss:0.00144  StopLoss:0.03624  GradNorm:0.00347  GradNormST:0.00613  AvgTextLen:43.9  AvgSpecLen:145.2  StepTime:0.88  LR:0.000100\n",
      "   | > Step:65/68  GlobalStep:86730  TotalLoss:0.00305  PostnetLoss:0.00146  DecoderLoss:0.00159  StopLoss:0.03936  GradNorm:0.00282  GradNormST:0.01585  AvgTextLen:64.2  AvgSpecLen:207.4  StepTime:1.20  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:86733  AvgTotalLoss:0.05025  AvgPostnetLoss:0.00112  AvgDecoderLoss:0.00120  AvgStopLoss:0.04793  EpochTime:43.01  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.09381   PostnetLoss: 0.00469   DecoderLoss:0.00511  StopLoss: 0.08400  \n",
      "   | > TotalLoss: 0.06847   PostnetLoss: 0.00719   DecoderLoss:0.00778  StopLoss: 0.05350  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00112   Validation Loss: 0.00658\n",
      "\n",
      " > Epoch 257/1000\n",
      "   | > Step:6/68  GlobalStep:86740  TotalLoss:0.00162  PostnetLoss:0.00079  DecoderLoss:0.00083  StopLoss:0.03889  GradNorm:0.00385  GradNormST:0.01636  AvgTextLen:6.0  AvgSpecLen:34.8  StepTime:0.35  LR:0.000100\n",
      "   | > Step:16/68  GlobalStep:86750  TotalLoss:0.00193  PostnetLoss:0.00094  DecoderLoss:0.00099  StopLoss:0.04080  GradNorm:0.00340  GradNormST:0.01403  AvgTextLen:13.8  AvgSpecLen:60.6  StepTime:0.39  LR:0.000100\n",
      "   | > Step:26/68  GlobalStep:86760  TotalLoss:0.00218  PostnetLoss:0.00106  DecoderLoss:0.00112  StopLoss:0.03955  GradNorm:0.00332  GradNormST:0.00918  AvgTextLen:20.3  AvgSpecLen:77.5  StepTime:0.47  LR:0.000100\n",
      "   | > Step:36/68  GlobalStep:86770  TotalLoss:0.00236  PostnetLoss:0.00114  DecoderLoss:0.00122  StopLoss:0.05393  GradNorm:0.00297  GradNormST:0.00942  AvgTextLen:28.5  AvgSpecLen:100.2  StepTime:0.61  LR:0.000100\n",
      "   | > Step:46/68  GlobalStep:86780  TotalLoss:0.00254  PostnetLoss:0.00123  DecoderLoss:0.00131  StopLoss:0.04420  GradNorm:0.00286  GradNormST:0.01276  AvgTextLen:35.4  AvgSpecLen:122.6  StepTime:0.78  LR:0.000100\n",
      "   | > Step:56/68  GlobalStep:86790  TotalLoss:0.00274  PostnetLoss:0.00132  DecoderLoss:0.00142  StopLoss:0.03392  GradNorm:0.00303  GradNormST:0.01306  AvgTextLen:45.8  AvgSpecLen:145.4  StepTime:0.89  LR:0.000100\n",
      "   | > Step:66/68  GlobalStep:86800  TotalLoss:0.00321  PostnetLoss:0.00154  DecoderLoss:0.00167  StopLoss:0.03221  GradNorm:0.00265  GradNormST:0.00915  AvgTextLen:71.2  AvgSpecLen:220.8  StepTime:1.07  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:86802  AvgTotalLoss:0.04858  AvgPostnetLoss:0.00112  AvgDecoderLoss:0.00120  AvgStopLoss:0.04626  EpochTime:43.03  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.09263   PostnetLoss: 0.00474   DecoderLoss:0.00515  StopLoss: 0.08274  \n",
      "   | > TotalLoss: 0.06709   PostnetLoss: 0.00691   DecoderLoss:0.00749  StopLoss: 0.05268  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00112   Validation Loss: 0.00645\n",
      "\n",
      " > Epoch 258/1000\n",
      "   | > Step:7/68  GlobalStep:86810  TotalLoss:0.00169  PostnetLoss:0.00082  DecoderLoss:0.00087  StopLoss:0.03698  GradNorm:0.00369  GradNormST:0.01303  AvgTextLen:7.4  AvgSpecLen:44.6  StepTime:0.30  LR:0.000100\n",
      "   | > Step:17/68  GlobalStep:86820  TotalLoss:0.00197  PostnetLoss:0.00095  DecoderLoss:0.00101  StopLoss:0.08745  GradNorm:0.00339  GradNormST:0.01984  AvgTextLen:14.7  AvgSpecLen:66.0  StepTime:0.45  LR:0.000100\n",
      "   | > Step:27/68  GlobalStep:86830  TotalLoss:0.00222  PostnetLoss:0.00107  DecoderLoss:0.00115  StopLoss:0.05046  GradNorm:0.00300  GradNormST:0.01456  AvgTextLen:21.0  AvgSpecLen:80.2  StepTime:0.68  LR:0.000100\n",
      "   | > Step:37/68  GlobalStep:86840  TotalLoss:0.00242  PostnetLoss:0.00117  DecoderLoss:0.00125  StopLoss:0.04944  GradNorm:0.00310  GradNormST:0.00916  AvgTextLen:29.3  AvgSpecLen:102.4  StepTime:0.60  LR:0.000100\n",
      "   | > Step:47/68  GlobalStep:86850  TotalLoss:0.00263  PostnetLoss:0.00127  DecoderLoss:0.00136  StopLoss:0.04082  GradNorm:0.00295  GradNormST:0.00789  AvgTextLen:36.7  AvgSpecLen:126.9  StepTime:0.68  LR:0.000100\n",
      "   | > Step:57/68  GlobalStep:86860  TotalLoss:0.00285  PostnetLoss:0.00137  DecoderLoss:0.00148  StopLoss:0.04051  GradNorm:0.00305  GradNormST:0.00718  AvgTextLen:47.1  AvgSpecLen:161.0  StepTime:0.85  LR:0.000100\n",
      "   | > Step:67/68  GlobalStep:86870  TotalLoss:0.00321  PostnetLoss:0.00153  DecoderLoss:0.00167  StopLoss:0.03276  GradNorm:0.00290  GradNormST:0.01053  AvgTextLen:80.6  AvgSpecLen:246.5  StepTime:1.25  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:86871  AvgTotalLoss:0.04930  AvgPostnetLoss:0.00112  AvgDecoderLoss:0.00120  AvgStopLoss:0.04699  EpochTime:43.22  AvgStepTime:0.63\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.09212   PostnetLoss: 0.00490   DecoderLoss:0.00534  StopLoss: 0.08189  \n",
      "   | > TotalLoss: 0.06894   PostnetLoss: 0.00691   DecoderLoss:0.00751  StopLoss: 0.05452  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00112   Validation Loss: 0.00644\n",
      "\n",
      " > Epoch 259/1000\n",
      "   | > Step:8/68  GlobalStep:86880  TotalLoss:0.00180  PostnetLoss:0.00088  DecoderLoss:0.00092  StopLoss:0.05639  GradNorm:0.00505  GradNormST:0.01489  AvgTextLen:8.3  AvgSpecLen:44.0  StepTime:0.26  LR:0.000100\n",
      "   | > Step:18/68  GlobalStep:86890  TotalLoss:0.00196  PostnetLoss:0.00095  DecoderLoss:0.00101  StopLoss:0.05111  GradNorm:0.00353  GradNormST:0.01058  AvgTextLen:15.2  AvgSpecLen:64.5  StepTime:0.34  LR:0.000100\n",
      "   | > Step:28/68  GlobalStep:86900  TotalLoss:0.00222  PostnetLoss:0.00108  DecoderLoss:0.00115  StopLoss:0.05590  GradNorm:0.00298  GradNormST:0.01426  AvgTextLen:22.1  AvgSpecLen:85.4  StepTime:0.44  LR:0.000100\n",
      "   | > Step:38/68  GlobalStep:86910  TotalLoss:0.00235  PostnetLoss:0.00114  DecoderLoss:0.00121  StopLoss:0.03771  GradNorm:0.00295  GradNormST:0.00826  AvgTextLen:29.4  AvgSpecLen:104.1  StepTime:0.59  LR:0.000100\n",
      "   | > Step:48/68  GlobalStep:86920  TotalLoss:0.00260  PostnetLoss:0.00125  DecoderLoss:0.00134  StopLoss:0.03986  GradNorm:0.00303  GradNormST:0.00892  AvgTextLen:37.5  AvgSpecLen:123.8  StepTime:0.66  LR:0.000100\n",
      "   | > Step:58/68  GlobalStep:86930  TotalLoss:0.00286  PostnetLoss:0.00138  DecoderLoss:0.00148  StopLoss:0.04428  GradNorm:0.00311  GradNormST:0.01294  AvgTextLen:47.8  AvgSpecLen:158.0  StepTime:0.70  LR:0.000100\n",
      "   | > Step:68/68  GlobalStep:86940  TotalLoss:0.00316  PostnetLoss:0.00151  DecoderLoss:0.00164  StopLoss:0.03352  GradNorm:0.00292  GradNormST:0.01251  AvgTextLen:103.0  AvgSpecLen:308.9  StepTime:1.87  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:86940  AvgTotalLoss:0.04845  AvgPostnetLoss:0.00112  AvgDecoderLoss:0.00120  AvgStopLoss:0.04613  EpochTime:42.25  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.09739   PostnetLoss: 0.00484   DecoderLoss:0.00526  StopLoss: 0.08729  \n",
      "   | > TotalLoss: 0.07290   PostnetLoss: 0.00711   DecoderLoss:0.00774  StopLoss: 0.05805  \n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00112   Validation Loss: 0.00649\n",
      "\n",
      " > Epoch 260/1000\n",
      "   | > Step:9/68  GlobalStep:86950  TotalLoss:0.00175  PostnetLoss:0.00085  DecoderLoss:0.00090  StopLoss:0.03598  GradNorm:0.00563  GradNormST:0.01019  AvgTextLen:9.0  AvgSpecLen:42.1  StepTime:0.44  LR:0.000100\n",
      "   | > Step:19/68  GlobalStep:86960  TotalLoss:0.00197  PostnetLoss:0.00096  DecoderLoss:0.00101  StopLoss:0.05767  GradNorm:0.00327  GradNormST:0.01741  AvgTextLen:15.5  AvgSpecLen:63.2  StepTime:0.38  LR:0.000100\n",
      "   | > Step:29/68  GlobalStep:86970  TotalLoss:0.00217  PostnetLoss:0.00106  DecoderLoss:0.00112  StopLoss:0.05769  GradNorm:0.00298  GradNormST:0.01371  AvgTextLen:21.8  AvgSpecLen:83.4  StepTime:0.55  LR:0.000100\n",
      "   | > Step:39/68  GlobalStep:86980  TotalLoss:0.00244  PostnetLoss:0.00118  DecoderLoss:0.00126  StopLoss:0.03345  GradNorm:0.00317  GradNormST:0.00859  AvgTextLen:30.2  AvgSpecLen:110.4  StepTime:0.53  LR:0.000100\n",
      "   | > Step:49/68  GlobalStep:86990  TotalLoss:0.00260  PostnetLoss:0.00126  DecoderLoss:0.00135  StopLoss:0.03445  GradNorm:0.00325  GradNormST:0.01035  AvgTextLen:38.2  AvgSpecLen:127.7  StepTime:0.73  LR:0.000100\n",
      "   | > Step:59/68  GlobalStep:87000  TotalLoss:0.00279  PostnetLoss:0.00134  DecoderLoss:0.00145  StopLoss:0.04448  GradNorm:0.00283  GradNormST:0.01392  AvgTextLen:50.5  AvgSpecLen:155.9  StepTime:0.88  LR:0.000100\n",
      " | | > Checkpoint saving : /vm/TTS/models/cartman_models/mozilla-no-loc-fattn-stopnet-sigmoid-loss_masking-August-15-2019_11+26AM-de6aeb9/checkpoint_87000.pth.tar\n",
      "warning: audio amplitude out of range, auto clipped.\n",
      "   | > EPOCH END -- GlobalStep:87009  AvgTotalLoss:0.04833  AvgPostnetLoss:0.00112  AvgDecoderLoss:0.00120  AvgStopLoss:0.04601  EpochTime:42.87  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.09761   PostnetLoss: 0.00476   DecoderLoss:0.00518  StopLoss: 0.08766  \n",
      "   | > TotalLoss: 0.07636   PostnetLoss: 0.00731   DecoderLoss:0.00791  StopLoss: 0.06114  \n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00112   Validation Loss: 0.00667\n",
      "\n",
      " > Epoch 261/1000\n",
      "   | > Step:0/68  GlobalStep:87010  TotalLoss:0.00166  PostnetLoss:0.00079  DecoderLoss:0.00086  StopLoss:0.08392  GradNorm:0.00682  GradNormST:0.03386  AvgTextLen:3.0  AvgSpecLen:25.3  StepTime:0.60  LR:0.000100\n",
      "   | > Step:10/68  GlobalStep:87020  TotalLoss:0.00182  PostnetLoss:0.00088  DecoderLoss:0.00094  StopLoss:0.04498  GradNorm:0.00497  GradNormST:0.01192  AvgTextLen:9.8  AvgSpecLen:48.6  StepTime:0.56  LR:0.000100\n",
      "   | > Step:20/68  GlobalStep:87030  TotalLoss:0.00213  PostnetLoss:0.00104  DecoderLoss:0.00109  StopLoss:0.04088  GradNorm:0.00488  GradNormST:0.01211  AvgTextLen:16.4  AvgSpecLen:64.8  StepTime:0.44  LR:0.000100\n",
      "   | > Step:30/68  GlobalStep:87040  TotalLoss:0.00225  PostnetLoss:0.00109  DecoderLoss:0.00116  StopLoss:0.03926  GradNorm:0.00314  GradNormST:0.00592  AvgTextLen:22.9  AvgSpecLen:85.8  StepTime:0.58  LR:0.000100\n",
      "   | > Step:40/68  GlobalStep:87050  TotalLoss:0.00241  PostnetLoss:0.00117  DecoderLoss:0.00124  StopLoss:0.03925  GradNorm:0.00298  GradNormST:0.00819  AvgTextLen:30.9  AvgSpecLen:105.3  StepTime:0.51  LR:0.000100\n",
      "   | > Step:50/68  GlobalStep:87060  TotalLoss:0.00263  PostnetLoss:0.00126  DecoderLoss:0.00136  StopLoss:0.04043  GradNorm:0.00344  GradNormST:0.01036  AvgTextLen:38.8  AvgSpecLen:132.0  StepTime:0.68  LR:0.000100\n",
      "   | > Step:60/68  GlobalStep:87070  TotalLoss:0.00287  PostnetLoss:0.00138  DecoderLoss:0.00149  StopLoss:0.02635  GradNorm:0.00351  GradNormST:0.00569  AvgTextLen:52.0  AvgSpecLen:172.7  StepTime:1.00  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:87078  AvgTotalLoss:0.04765  AvgPostnetLoss:0.00112  AvgDecoderLoss:0.00120  AvgStopLoss:0.04532  EpochTime:42.63  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.09025   PostnetLoss: 0.00489   DecoderLoss:0.00531  StopLoss: 0.08005  \n",
      "   | > TotalLoss: 0.07177   PostnetLoss: 0.00690   DecoderLoss:0.00750  StopLoss: 0.05737  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00112   Validation Loss: 0.00640\n",
      "\n",
      " > Epoch 262/1000\n",
      "   | > Step:1/68  GlobalStep:87080  TotalLoss:0.00163  PostnetLoss:0.00079  DecoderLoss:0.00084  StopLoss:0.06599  GradNorm:0.00539  GradNormST:0.01926  AvgTextLen:3.7  AvgSpecLen:28.6  StepTime:0.45  LR:0.000100\n",
      "   | > Step:11/68  GlobalStep:87090  TotalLoss:0.00183  PostnetLoss:0.00089  DecoderLoss:0.00094  StopLoss:0.06026  GradNorm:0.00355  GradNormST:0.02253  AvgTextLen:10.3  AvgSpecLen:49.4  StepTime:0.34  LR:0.000100\n",
      "   | > Step:21/68  GlobalStep:87100  TotalLoss:0.00209  PostnetLoss:0.00101  DecoderLoss:0.00107  StopLoss:0.05612  GradNorm:0.00469  GradNormST:0.01758  AvgTextLen:16.6  AvgSpecLen:66.3  StepTime:0.43  LR:0.000100\n",
      "   | > Step:31/68  GlobalStep:87110  TotalLoss:0.00227  PostnetLoss:0.00110  DecoderLoss:0.00117  StopLoss:0.04061  GradNorm:0.00441  GradNormST:0.01003  AvgTextLen:24.1  AvgSpecLen:82.7  StepTime:0.43  LR:0.000100\n",
      "   | > Step:41/68  GlobalStep:87120  TotalLoss:0.00250  PostnetLoss:0.00121  DecoderLoss:0.00129  StopLoss:0.03098  GradNorm:0.00327  GradNormST:0.00900  AvgTextLen:32.3  AvgSpecLen:109.4  StepTime:0.64  LR:0.000100\n",
      "   | > Step:51/68  GlobalStep:87130  TotalLoss:0.00261  PostnetLoss:0.00126  DecoderLoss:0.00135  StopLoss:0.03693  GradNorm:0.00355  GradNormST:0.00956  AvgTextLen:39.2  AvgSpecLen:126.9  StepTime:0.81  LR:0.000100\n",
      "   | > Step:61/68  GlobalStep:87140  TotalLoss:0.00283  PostnetLoss:0.00136  DecoderLoss:0.00147  StopLoss:0.03335  GradNorm:0.00328  GradNormST:0.00961  AvgTextLen:54.6  AvgSpecLen:165.5  StepTime:0.89  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:87147  AvgTotalLoss:0.05025  AvgPostnetLoss:0.00113  AvgDecoderLoss:0.00120  AvgStopLoss:0.04792  EpochTime:42.59  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.09461   PostnetLoss: 0.00482   DecoderLoss:0.00523  StopLoss: 0.08456  \n",
      "   | > TotalLoss: 0.07119   PostnetLoss: 0.00695   DecoderLoss:0.00753  StopLoss: 0.05672  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00113   Validation Loss: 0.00641\n",
      "\n",
      " > Epoch 263/1000\n",
      "   | > Step:2/68  GlobalStep:87150  TotalLoss:0.00148  PostnetLoss:0.00072  DecoderLoss:0.00076  StopLoss:0.05106  GradNorm:0.00355  GradNormST:0.01886  AvgTextLen:4.0  AvgSpecLen:31.3  StepTime:0.38  LR:0.000100\n",
      "   | > Step:12/68  GlobalStep:87160  TotalLoss:0.00177  PostnetLoss:0.00086  DecoderLoss:0.00091  StopLoss:0.04160  GradNorm:0.00368  GradNormST:0.00904  AvgTextLen:11.0  AvgSpecLen:49.3  StepTime:0.34  LR:0.000100\n",
      "   | > Step:22/68  GlobalStep:87170  TotalLoss:0.00204  PostnetLoss:0.00099  DecoderLoss:0.00105  StopLoss:0.05612  GradNorm:0.00335  GradNormST:0.01099  AvgTextLen:18.0  AvgSpecLen:72.0  StepTime:0.48  LR:0.000100\n",
      "   | > Step:32/68  GlobalStep:87180  TotalLoss:0.00229  PostnetLoss:0.00111  DecoderLoss:0.00118  StopLoss:0.06043  GradNorm:0.00449  GradNormST:0.01529  AvgTextLen:24.4  AvgSpecLen:91.5  StepTime:0.56  LR:0.000100\n",
      "   | > Step:42/68  GlobalStep:87190  TotalLoss:0.00246  PostnetLoss:0.00119  DecoderLoss:0.00128  StopLoss:0.03992  GradNorm:0.00313  GradNormST:0.01126  AvgTextLen:33.1  AvgSpecLen:109.7  StepTime:0.62  LR:0.000100\n",
      "   | > Step:52/68  GlobalStep:87200  TotalLoss:0.00271  PostnetLoss:0.00131  DecoderLoss:0.00140  StopLoss:0.04895  GradNorm:0.00346  GradNormST:0.01094  AvgTextLen:42.1  AvgSpecLen:137.5  StepTime:0.78  LR:0.000100\n",
      "   | > Step:62/68  GlobalStep:87210  TotalLoss:0.00295  PostnetLoss:0.00142  DecoderLoss:0.00153  StopLoss:0.03426  GradNorm:0.00342  GradNormST:0.01121  AvgTextLen:54.0  AvgSpecLen:174.7  StepTime:0.75  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:87216  AvgTotalLoss:0.05027  AvgPostnetLoss:0.00112  AvgDecoderLoss:0.00120  AvgStopLoss:0.04795  EpochTime:42.80  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.09710   PostnetLoss: 0.00473   DecoderLoss:0.00514  StopLoss: 0.08724  \n",
      "   | > TotalLoss: 0.07352   PostnetLoss: 0.00675   DecoderLoss:0.00736  StopLoss: 0.05941  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00112   Validation Loss: 0.00636\n",
      "\n",
      " > Epoch 264/1000\n",
      "   | > Step:3/68  GlobalStep:87220  TotalLoss:0.00155  PostnetLoss:0.00075  DecoderLoss:0.00080  StopLoss:0.06320  GradNorm:0.00380  GradNormST:0.02598  AvgTextLen:4.7  AvgSpecLen:29.7  StepTime:0.32  LR:0.000100\n",
      "   | > Step:13/68  GlobalStep:87230  TotalLoss:0.00174  PostnetLoss:0.00084  DecoderLoss:0.00089  StopLoss:0.06929  GradNorm:0.00318  GradNormST:0.02989  AvgTextLen:11.5  AvgSpecLen:55.2  StepTime:0.42  LR:0.000100\n",
      "   | > Step:23/68  GlobalStep:87240  TotalLoss:0.00216  PostnetLoss:0.00105  DecoderLoss:0.00111  StopLoss:0.04996  GradNorm:0.00332  GradNormST:0.01493  AvgTextLen:18.3  AvgSpecLen:72.2  StepTime:0.43  LR:0.000100\n",
      "   | > Step:33/68  GlobalStep:87250  TotalLoss:0.00233  PostnetLoss:0.00113  DecoderLoss:0.00120  StopLoss:0.05394  GradNorm:0.00360  GradNormST:0.01138  AvgTextLen:25.9  AvgSpecLen:88.9  StepTime:0.56  LR:0.000100\n",
      "   | > Step:43/68  GlobalStep:87260  TotalLoss:0.00247  PostnetLoss:0.00119  DecoderLoss:0.00128  StopLoss:0.04514  GradNorm:0.00297  GradNormST:0.00925  AvgTextLen:33.4  AvgSpecLen:115.7  StepTime:0.67  LR:0.000100\n",
      "   | > Step:53/68  GlobalStep:87270  TotalLoss:0.00266  PostnetLoss:0.00128  DecoderLoss:0.00138  StopLoss:0.04325  GradNorm:0.00311  GradNormST:0.00869  AvgTextLen:42.4  AvgSpecLen:137.6  StepTime:0.77  LR:0.000100\n",
      "   | > Step:63/68  GlobalStep:87280  TotalLoss:0.00301  PostnetLoss:0.00145  DecoderLoss:0.00156  StopLoss:0.02916  GradNorm:0.00311  GradNormST:0.00771  AvgTextLen:58.6  AvgSpecLen:185.1  StepTime:0.92  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:87285  AvgTotalLoss:0.05048  AvgPostnetLoss:0.00111  AvgDecoderLoss:0.00119  AvgStopLoss:0.04818  EpochTime:43.36  AvgStepTime:0.63\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.09561   PostnetLoss: 0.00471   DecoderLoss:0.00513  StopLoss: 0.08576  \n",
      "   | > TotalLoss: 0.06732   PostnetLoss: 0.00674   DecoderLoss:0.00735  StopLoss: 0.05322  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00111   Validation Loss: 0.00631\n",
      "\n",
      " > Epoch 265/1000\n",
      "   | > Step:4/68  GlobalStep:87290  TotalLoss:0.00155  PostnetLoss:0.00075  DecoderLoss:0.00080  StopLoss:0.09432  GradNorm:0.00390  GradNormST:0.02727  AvgTextLen:4.8  AvgSpecLen:31.1  StepTime:0.28  LR:0.000100\n",
      "   | > Step:14/68  GlobalStep:87300  TotalLoss:0.00191  PostnetLoss:0.00092  DecoderLoss:0.00098  StopLoss:0.04368  GradNorm:0.00348  GradNormST:0.01134  AvgTextLen:12.2  AvgSpecLen:52.0  StepTime:0.34  LR:0.000100\n",
      "   | > Step:24/68  GlobalStep:87310  TotalLoss:0.00221  PostnetLoss:0.00107  DecoderLoss:0.00114  StopLoss:0.05666  GradNorm:0.00331  GradNormST:0.01056  AvgTextLen:19.2  AvgSpecLen:79.0  StepTime:0.40  LR:0.000100\n",
      "   | > Step:34/68  GlobalStep:87320  TotalLoss:0.00235  PostnetLoss:0.00114  DecoderLoss:0.00121  StopLoss:0.03578  GradNorm:0.00311  GradNormST:0.00756  AvgTextLen:26.2  AvgSpecLen:95.9  StepTime:0.57  LR:0.000100\n",
      "   | > Step:44/68  GlobalStep:87330  TotalLoss:0.00254  PostnetLoss:0.00123  DecoderLoss:0.00131  StopLoss:0.04530  GradNorm:0.00334  GradNormST:0.01256  AvgTextLen:33.8  AvgSpecLen:116.3  StepTime:0.56  LR:0.000100\n",
      "   | > Step:54/68  GlobalStep:87340  TotalLoss:0.00273  PostnetLoss:0.00131  DecoderLoss:0.00142  StopLoss:0.03987  GradNorm:0.00288  GradNormST:0.00730  AvgTextLen:42.2  AvgSpecLen:144.9  StepTime:0.77  LR:0.000100\n",
      "   | > Step:64/68  GlobalStep:87350  TotalLoss:0.00286  PostnetLoss:0.00137  DecoderLoss:0.00149  StopLoss:0.03567  GradNorm:0.00297  GradNormST:0.00937  AvgTextLen:60.6  AvgSpecLen:177.6  StepTime:0.85  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:87354  AvgTotalLoss:0.05023  AvgPostnetLoss:0.00112  AvgDecoderLoss:0.00119  AvgStopLoss:0.04792  EpochTime:42.91  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.09901   PostnetLoss: 0.00466   DecoderLoss:0.00509  StopLoss: 0.08927  \n",
      "   | > TotalLoss: 0.07157   PostnetLoss: 0.00701   DecoderLoss:0.00764  StopLoss: 0.05692  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00112   Validation Loss: 0.00642\n",
      "\n",
      " > Epoch 266/1000\n",
      "   | > Step:5/68  GlobalStep:87360  TotalLoss:0.00165  PostnetLoss:0.00080  DecoderLoss:0.00085  StopLoss:0.07438  GradNorm:0.00437  GradNormST:0.03190  AvgTextLen:5.9  AvgSpecLen:33.2  StepTime:0.32  LR:0.000100\n",
      "   | > Step:15/68  GlobalStep:87370  TotalLoss:0.00196  PostnetLoss:0.00095  DecoderLoss:0.00101  StopLoss:0.04582  GradNorm:0.00360  GradNormST:0.01444  AvgTextLen:12.8  AvgSpecLen:59.1  StepTime:0.56  LR:0.000100\n",
      "   | > Step:25/68  GlobalStep:87380  TotalLoss:0.00211  PostnetLoss:0.00102  DecoderLoss:0.00109  StopLoss:0.05592  GradNorm:0.00324  GradNormST:0.01190  AvgTextLen:19.8  AvgSpecLen:73.3  StepTime:0.40  LR:0.000100\n",
      "   | > Step:35/68  GlobalStep:87390  TotalLoss:0.00237  PostnetLoss:0.00114  DecoderLoss:0.00122  StopLoss:0.05414  GradNorm:0.00315  GradNormST:0.01996  AvgTextLen:27.1  AvgSpecLen:104.7  StepTime:0.75  LR:0.000100\n",
      "   | > Step:45/68  GlobalStep:87400  TotalLoss:0.00248  PostnetLoss:0.00120  DecoderLoss:0.00128  StopLoss:0.05514  GradNorm:0.00297  GradNormST:0.01756  AvgTextLen:34.4  AvgSpecLen:118.6  StepTime:0.64  LR:0.000100\n",
      "   | > Step:55/68  GlobalStep:87410  TotalLoss:0.00276  PostnetLoss:0.00133  DecoderLoss:0.00143  StopLoss:0.03558  GradNorm:0.00283  GradNormST:0.00605  AvgTextLen:43.9  AvgSpecLen:145.2  StepTime:0.80  LR:0.000100\n",
      "   | > Step:65/68  GlobalStep:87420  TotalLoss:0.00303  PostnetLoss:0.00145  DecoderLoss:0.00158  StopLoss:0.03919  GradNorm:0.00272  GradNormST:0.01596  AvgTextLen:64.2  AvgSpecLen:207.4  StepTime:1.20  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:87423  AvgTotalLoss:0.05040  AvgPostnetLoss:0.00111  AvgDecoderLoss:0.00119  AvgStopLoss:0.04809  EpochTime:43.34  AvgStepTime:0.63\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.09175   PostnetLoss: 0.00464   DecoderLoss:0.00507  StopLoss: 0.08204  \n",
      "   | > TotalLoss: 0.07055   PostnetLoss: 0.00688   DecoderLoss:0.00748  StopLoss: 0.05619  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00111   Validation Loss: 0.00636\n",
      "\n",
      " > Epoch 267/1000\n",
      "   | > Step:6/68  GlobalStep:87430  TotalLoss:0.00164  PostnetLoss:0.00080  DecoderLoss:0.00084  StopLoss:0.04133  GradNorm:0.00417  GradNormST:0.01538  AvgTextLen:6.0  AvgSpecLen:34.8  StepTime:0.39  LR:0.000100\n",
      "   | > Step:16/68  GlobalStep:87440  TotalLoss:0.00195  PostnetLoss:0.00095  DecoderLoss:0.00100  StopLoss:0.04496  GradNorm:0.00456  GradNormST:0.01572  AvgTextLen:13.8  AvgSpecLen:60.6  StepTime:0.38  LR:0.000100\n",
      "   | > Step:26/68  GlobalStep:87450  TotalLoss:0.00216  PostnetLoss:0.00105  DecoderLoss:0.00111  StopLoss:0.03960  GradNorm:0.00372  GradNormST:0.01161  AvgTextLen:20.3  AvgSpecLen:77.5  StepTime:0.52  LR:0.000100\n",
      "   | > Step:36/68  GlobalStep:87460  TotalLoss:0.00236  PostnetLoss:0.00114  DecoderLoss:0.00122  StopLoss:0.06331  GradNorm:0.00322  GradNormST:0.01319  AvgTextLen:28.5  AvgSpecLen:100.2  StepTime:0.62  LR:0.000100\n",
      "   | > Step:46/68  GlobalStep:87470  TotalLoss:0.00250  PostnetLoss:0.00121  DecoderLoss:0.00129  StopLoss:0.04096  GradNorm:0.00292  GradNormST:0.01027  AvgTextLen:35.4  AvgSpecLen:122.6  StepTime:0.78  LR:0.000100\n",
      "   | > Step:56/68  GlobalStep:87480  TotalLoss:0.00273  PostnetLoss:0.00132  DecoderLoss:0.00142  StopLoss:0.03763  GradNorm:0.00297  GradNormST:0.01794  AvgTextLen:45.8  AvgSpecLen:145.4  StepTime:1.06  LR:0.000100\n",
      "   | > Step:66/68  GlobalStep:87490  TotalLoss:0.00321  PostnetLoss:0.00154  DecoderLoss:0.00167  StopLoss:0.03466  GradNorm:0.00291  GradNormST:0.01219  AvgTextLen:71.2  AvgSpecLen:220.8  StepTime:1.07  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:87492  AvgTotalLoss:0.05008  AvgPostnetLoss:0.00111  AvgDecoderLoss:0.00119  AvgStopLoss:0.04778  EpochTime:43.59  AvgStepTime:0.63\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.09392   PostnetLoss: 0.00471   DecoderLoss:0.00514  StopLoss: 0.08407  \n",
      "   | > TotalLoss: 0.07071   PostnetLoss: 0.00691   DecoderLoss:0.00751  StopLoss: 0.05630  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00111   Validation Loss: 0.00640\n",
      "\n",
      " > Epoch 268/1000\n",
      "   | > Step:7/68  GlobalStep:87500  TotalLoss:0.00173  PostnetLoss:0.00084  DecoderLoss:0.00089  StopLoss:0.03789  GradNorm:0.00361  GradNormST:0.01710  AvgTextLen:7.4  AvgSpecLen:44.6  StepTime:0.38  LR:0.000100\n",
      "   | > Step:17/68  GlobalStep:87510  TotalLoss:0.00193  PostnetLoss:0.00094  DecoderLoss:0.00099  StopLoss:0.06671  GradNorm:0.00374  GradNormST:0.01708  AvgTextLen:14.7  AvgSpecLen:66.0  StepTime:0.45  LR:0.000100\n",
      "   | > Step:27/68  GlobalStep:87520  TotalLoss:0.00226  PostnetLoss:0.00109  DecoderLoss:0.00116  StopLoss:0.05016  GradNorm:0.00440  GradNormST:0.01615  AvgTextLen:21.0  AvgSpecLen:80.2  StepTime:0.67  LR:0.000100\n",
      "   | > Step:37/68  GlobalStep:87530  TotalLoss:0.00240  PostnetLoss:0.00116  DecoderLoss:0.00124  StopLoss:0.04358  GradNorm:0.00343  GradNormST:0.00906  AvgTextLen:29.3  AvgSpecLen:102.4  StepTime:0.63  LR:0.000100\n",
      "   | > Step:47/68  GlobalStep:87540  TotalLoss:0.00258  PostnetLoss:0.00125  DecoderLoss:0.00134  StopLoss:0.03899  GradNorm:0.00297  GradNormST:0.00934  AvgTextLen:36.7  AvgSpecLen:126.9  StepTime:0.59  LR:0.000100\n",
      "   | > Step:57/68  GlobalStep:87550  TotalLoss:0.00280  PostnetLoss:0.00135  DecoderLoss:0.00146  StopLoss:0.04038  GradNorm:0.00296  GradNormST:0.00900  AvgTextLen:47.1  AvgSpecLen:161.0  StepTime:0.84  LR:0.000100\n",
      "   | > Step:67/68  GlobalStep:87560  TotalLoss:0.00320  PostnetLoss:0.00153  DecoderLoss:0.00167  StopLoss:0.03590  GradNorm:0.00278  GradNormST:0.01469  AvgTextLen:80.6  AvgSpecLen:246.5  StepTime:1.24  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:87561  AvgTotalLoss:0.04990  AvgPostnetLoss:0.00111  AvgDecoderLoss:0.00119  AvgStopLoss:0.04760  EpochTime:42.40  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.09535   PostnetLoss: 0.00472   DecoderLoss:0.00515  StopLoss: 0.08548  \n",
      "   | > TotalLoss: 0.07402   PostnetLoss: 0.00702   DecoderLoss:0.00764  StopLoss: 0.05936  \n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00111   Validation Loss: 0.00659\n",
      "\n",
      " > Epoch 269/1000\n",
      "   | > Step:8/68  GlobalStep:87570  TotalLoss:0.00172  PostnetLoss:0.00084  DecoderLoss:0.00088  StopLoss:0.06897  GradNorm:0.00370  GradNormST:0.02083  AvgTextLen:8.3  AvgSpecLen:44.0  StepTime:0.33  LR:0.000100\n",
      "   | > Step:18/68  GlobalStep:87580  TotalLoss:0.00193  PostnetLoss:0.00094  DecoderLoss:0.00099  StopLoss:0.05319  GradNorm:0.00376  GradNormST:0.00949  AvgTextLen:15.2  AvgSpecLen:64.5  StepTime:0.41  LR:0.000100\n",
      "   | > Step:28/68  GlobalStep:87590  TotalLoss:0.00221  PostnetLoss:0.00107  DecoderLoss:0.00114  StopLoss:0.05283  GradNorm:0.00376  GradNormST:0.01023  AvgTextLen:22.1  AvgSpecLen:85.4  StepTime:0.54  LR:0.000100\n",
      "   | > Step:38/68  GlobalStep:87600  TotalLoss:0.00235  PostnetLoss:0.00114  DecoderLoss:0.00122  StopLoss:0.03661  GradNorm:0.00320  GradNormST:0.00755  AvgTextLen:29.4  AvgSpecLen:104.1  StepTime:0.65  LR:0.000100\n",
      "   | > Step:48/68  GlobalStep:87610  TotalLoss:0.00257  PostnetLoss:0.00124  DecoderLoss:0.00133  StopLoss:0.04036  GradNorm:0.00303  GradNormST:0.00912  AvgTextLen:37.5  AvgSpecLen:123.8  StepTime:0.58  LR:0.000100\n",
      "   | > Step:58/68  GlobalStep:87620  TotalLoss:0.00279  PostnetLoss:0.00135  DecoderLoss:0.00145  StopLoss:0.04426  GradNorm:0.00360  GradNormST:0.00983  AvgTextLen:47.8  AvgSpecLen:158.0  StepTime:0.89  LR:0.000100\n",
      "   | > Step:68/68  GlobalStep:87630  TotalLoss:0.00315  PostnetLoss:0.00151  DecoderLoss:0.00164  StopLoss:0.04286  GradNorm:0.00286  GradNormST:0.01752  AvgTextLen:103.0  AvgSpecLen:308.9  StepTime:1.90  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:87630  AvgTotalLoss:0.05034  AvgPostnetLoss:0.00111  AvgDecoderLoss:0.00119  AvgStopLoss:0.04804  EpochTime:43.71  AvgStepTime:0.63\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.09625   PostnetLoss: 0.00474   DecoderLoss:0.00516  StopLoss: 0.08635  \n",
      "   | > TotalLoss: 0.07221   PostnetLoss: 0.00690   DecoderLoss:0.00749  StopLoss: 0.05783  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00111   Validation Loss: 0.00648\n",
      "\n",
      " > Epoch 270/1000\n",
      "   | > Step:9/68  GlobalStep:87640  TotalLoss:0.00169  PostnetLoss:0.00082  DecoderLoss:0.00087  StopLoss:0.04793  GradNorm:0.00364  GradNormST:0.01975  AvgTextLen:9.0  AvgSpecLen:42.1  StepTime:0.40  LR:0.000100\n",
      "   | > Step:19/68  GlobalStep:87650  TotalLoss:0.00196  PostnetLoss:0.00095  DecoderLoss:0.00101  StopLoss:0.06221  GradNorm:0.00359  GradNormST:0.01575  AvgTextLen:15.5  AvgSpecLen:63.2  StepTime:0.33  LR:0.000100\n",
      "   | > Step:29/68  GlobalStep:87660  TotalLoss:0.00220  PostnetLoss:0.00107  DecoderLoss:0.00113  StopLoss:0.06927  GradNorm:0.00424  GradNormST:0.01373  AvgTextLen:21.8  AvgSpecLen:83.4  StepTime:0.57  LR:0.000100\n",
      "   | > Step:39/68  GlobalStep:87670  TotalLoss:0.00247  PostnetLoss:0.00120  DecoderLoss:0.00127  StopLoss:0.04092  GradNorm:0.00353  GradNormST:0.01404  AvgTextLen:30.2  AvgSpecLen:110.4  StepTime:0.58  LR:0.000100\n",
      "   | > Step:49/68  GlobalStep:87680  TotalLoss:0.00260  PostnetLoss:0.00126  DecoderLoss:0.00134  StopLoss:0.03621  GradNorm:0.00312  GradNormST:0.00786  AvgTextLen:38.2  AvgSpecLen:127.7  StepTime:0.75  LR:0.000100\n",
      "   | > Step:59/68  GlobalStep:87690  TotalLoss:0.00284  PostnetLoss:0.00137  DecoderLoss:0.00148  StopLoss:0.04257  GradNorm:0.00339  GradNormST:0.01071  AvgTextLen:50.5  AvgSpecLen:155.9  StepTime:0.80  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:87699  AvgTotalLoss:0.05102  AvgPostnetLoss:0.00111  AvgDecoderLoss:0.00119  AvgStopLoss:0.04871  EpochTime:43.48  AvgStepTime:0.63\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.09316   PostnetLoss: 0.00474   DecoderLoss:0.00516  StopLoss: 0.08326  \n",
      "   | > TotalLoss: 0.07213   PostnetLoss: 0.00672   DecoderLoss:0.00731  StopLoss: 0.05811  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00111   Validation Loss: 0.00627\n",
      "\n",
      " > Epoch 271/1000\n",
      "   | > Step:0/68  GlobalStep:87700  TotalLoss:0.00157  PostnetLoss:0.00075  DecoderLoss:0.00082  StopLoss:0.07353  GradNorm:0.00496  GradNormST:0.02622  AvgTextLen:3.0  AvgSpecLen:25.3  StepTime:0.60  LR:0.000100\n",
      "   | > Step:10/68  GlobalStep:87710  TotalLoss:0.00179  PostnetLoss:0.00087  DecoderLoss:0.00092  StopLoss:0.07290  GradNorm:0.00365  GradNormST:0.03140  AvgTextLen:9.8  AvgSpecLen:48.6  StepTime:0.51  LR:0.000100\n",
      "   | > Step:20/68  GlobalStep:87720  TotalLoss:0.00204  PostnetLoss:0.00099  DecoderLoss:0.00105  StopLoss:0.03794  GradNorm:0.00332  GradNormST:0.01166  AvgTextLen:16.4  AvgSpecLen:64.8  StepTime:0.49  LR:0.000100\n",
      "   | > Step:30/68  GlobalStep:87730  TotalLoss:0.00222  PostnetLoss:0.00108  DecoderLoss:0.00114  StopLoss:0.05134  GradNorm:0.00398  GradNormST:0.00974  AvgTextLen:22.9  AvgSpecLen:85.8  StepTime:0.58  LR:0.000100\n",
      "   | > Step:40/68  GlobalStep:87740  TotalLoss:0.00240  PostnetLoss:0.00116  DecoderLoss:0.00124  StopLoss:0.04245  GradNorm:0.00335  GradNormST:0.01205  AvgTextLen:30.9  AvgSpecLen:105.3  StepTime:0.63  LR:0.000100\n",
      "   | > Step:50/68  GlobalStep:87750  TotalLoss:0.00264  PostnetLoss:0.00127  DecoderLoss:0.00137  StopLoss:0.04109  GradNorm:0.00309  GradNormST:0.00837  AvgTextLen:38.8  AvgSpecLen:132.0  StepTime:0.70  LR:0.000100\n",
      "   | > Step:60/68  GlobalStep:87760  TotalLoss:0.00357  PostnetLoss:0.00171  DecoderLoss:0.00185  StopLoss:0.03005  GradNorm:0.00668  GradNormST:0.00649  AvgTextLen:52.0  AvgSpecLen:172.7  StepTime:0.83  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:87768  AvgTotalLoss:0.05481  AvgPostnetLoss:0.00117  AvgDecoderLoss:0.00125  AvgStopLoss:0.05240  EpochTime:42.68  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.11142   PostnetLoss: 0.00467   DecoderLoss:0.00515  StopLoss: 0.10160  \n",
      "   | > TotalLoss: 0.08538   PostnetLoss: 0.00742   DecoderLoss:0.00812  StopLoss: 0.06984  \n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00117   Validation Loss: 0.00652\n",
      "\n",
      " > Epoch 272/1000\n",
      "   | > Step:1/68  GlobalStep:87770  TotalLoss:0.00177  PostnetLoss:0.00085  DecoderLoss:0.00091  StopLoss:0.06274  GradNorm:0.00623  GradNormST:0.01667  AvgTextLen:3.7  AvgSpecLen:28.6  StepTime:0.42  LR:0.000100\n",
      "   | > Step:11/68  GlobalStep:87780  TotalLoss:0.00197  PostnetLoss:0.00096  DecoderLoss:0.00101  StopLoss:0.05011  GradNorm:0.00402  GradNormST:0.01257  AvgTextLen:10.3  AvgSpecLen:49.4  StepTime:0.42  LR:0.000100\n",
      "   | > Step:21/68  GlobalStep:87790  TotalLoss:0.00222  PostnetLoss:0.00108  DecoderLoss:0.00114  StopLoss:0.06405  GradNorm:0.00442  GradNormST:0.02081  AvgTextLen:16.6  AvgSpecLen:66.3  StepTime:0.40  LR:0.000100\n",
      "   | > Step:31/68  GlobalStep:87800  TotalLoss:0.00243  PostnetLoss:0.00118  DecoderLoss:0.00126  StopLoss:0.04742  GradNorm:0.00450  GradNormST:0.01731  AvgTextLen:24.1  AvgSpecLen:82.7  StepTime:0.54  LR:0.000100\n",
      "   | > Step:41/68  GlobalStep:87810  TotalLoss:0.00272  PostnetLoss:0.00131  DecoderLoss:0.00141  StopLoss:0.03467  GradNorm:0.00489  GradNormST:0.00928  AvgTextLen:32.3  AvgSpecLen:109.4  StepTime:0.67  LR:0.000100\n",
      "   | > Step:51/68  GlobalStep:87820  TotalLoss:0.00278  PostnetLoss:0.00134  DecoderLoss:0.00144  StopLoss:0.04407  GradNorm:0.00371  GradNormST:0.01004  AvgTextLen:39.2  AvgSpecLen:126.9  StepTime:0.69  LR:0.000100\n",
      "   | > Step:61/68  GlobalStep:87830  TotalLoss:0.00301  PostnetLoss:0.00145  DecoderLoss:0.00156  StopLoss:0.03525  GradNorm:0.00496  GradNormST:0.00812  AvgTextLen:54.6  AvgSpecLen:165.5  StepTime:0.95  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:87837  AvgTotalLoss:0.05427  AvgPostnetLoss:0.00121  AvgDecoderLoss:0.00129  AvgStopLoss:0.05177  EpochTime:42.97  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.09563   PostnetLoss: 0.00462   DecoderLoss:0.00502  StopLoss: 0.08599  \n",
      "   | > TotalLoss: 0.07554   PostnetLoss: 0.00706   DecoderLoss:0.00767  StopLoss: 0.06081  \n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00121   Validation Loss: 0.00638\n",
      "\n",
      " > Epoch 273/1000\n",
      "   | > Step:2/68  GlobalStep:87840  TotalLoss:0.00155  PostnetLoss:0.00075  DecoderLoss:0.00080  StopLoss:0.05540  GradNorm:0.00451  GradNormST:0.01847  AvgTextLen:4.0  AvgSpecLen:31.3  StepTime:0.41  LR:0.000100\n",
      "   | > Step:12/68  GlobalStep:87850  TotalLoss:0.00185  PostnetLoss:0.00090  DecoderLoss:0.00095  StopLoss:0.04839  GradNorm:0.00427  GradNormST:0.01253  AvgTextLen:11.0  AvgSpecLen:49.3  StepTime:0.38  LR:0.000100\n",
      "   | > Step:22/68  GlobalStep:87860  TotalLoss:0.00222  PostnetLoss:0.00107  DecoderLoss:0.00114  StopLoss:0.06085  GradNorm:0.00368  GradNormST:0.01166  AvgTextLen:18.0  AvgSpecLen:72.0  StepTime:0.48  LR:0.000100\n",
      "   | > Step:32/68  GlobalStep:87870  TotalLoss:0.00233  PostnetLoss:0.00113  DecoderLoss:0.00120  StopLoss:0.06250  GradNorm:0.00411  GradNormST:0.01162  AvgTextLen:24.4  AvgSpecLen:91.5  StepTime:0.58  LR:0.000100\n",
      "   | > Step:42/68  GlobalStep:87880  TotalLoss:0.00259  PostnetLoss:0.00125  DecoderLoss:0.00134  StopLoss:0.04395  GradNorm:0.00484  GradNormST:0.01502  AvgTextLen:33.1  AvgSpecLen:109.7  StepTime:0.59  LR:0.000100\n",
      "   | > Step:52/68  GlobalStep:87890  TotalLoss:0.00281  PostnetLoss:0.00136  DecoderLoss:0.00145  StopLoss:0.04544  GradNorm:0.00351  GradNormST:0.01153  AvgTextLen:42.1  AvgSpecLen:137.5  StepTime:0.71  LR:0.000100\n",
      "   | > Step:62/68  GlobalStep:87900  TotalLoss:0.00304  PostnetLoss:0.00146  DecoderLoss:0.00158  StopLoss:0.03745  GradNorm:0.00350  GradNormST:0.01700  AvgTextLen:54.0  AvgSpecLen:174.7  StepTime:0.76  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:87906  AvgTotalLoss:0.05198  AvgPostnetLoss:0.00116  AvgDecoderLoss:0.00124  AvgStopLoss:0.04959  EpochTime:42.97  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.08904   PostnetLoss: 0.00465   DecoderLoss:0.00507  StopLoss: 0.07931  \n",
      "   | > TotalLoss: 0.07229   PostnetLoss: 0.00666   DecoderLoss:0.00726  StopLoss: 0.05837  \n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00116   Validation Loss: 0.00608\n",
      "\n",
      " > Epoch 274/1000\n",
      "   | > Step:3/68  GlobalStep:87910  TotalLoss:0.00156  PostnetLoss:0.00076  DecoderLoss:0.00080  StopLoss:0.05929  GradNorm:0.00485  GradNormST:0.01633  AvgTextLen:4.7  AvgSpecLen:29.7  StepTime:0.32  LR:0.000100\n",
      "   | > Step:13/68  GlobalStep:87920  TotalLoss:0.00184  PostnetLoss:0.00089  DecoderLoss:0.00095  StopLoss:0.06317  GradNorm:0.00400  GradNormST:0.01993  AvgTextLen:11.5  AvgSpecLen:55.2  StepTime:0.44  LR:0.000100\n",
      "   | > Step:23/68  GlobalStep:87930  TotalLoss:0.00214  PostnetLoss:0.00104  DecoderLoss:0.00110  StopLoss:0.04780  GradNorm:0.00345  GradNormST:0.00919  AvgTextLen:18.3  AvgSpecLen:72.2  StepTime:0.46  LR:0.000100\n",
      "   | > Step:33/68  GlobalStep:87940  TotalLoss:0.00234  PostnetLoss:0.00114  DecoderLoss:0.00121  StopLoss:0.04745  GradNorm:0.00333  GradNormST:0.00945  AvgTextLen:25.9  AvgSpecLen:88.9  StepTime:0.55  LR:0.000100\n",
      "   | > Step:43/68  GlobalStep:87950  TotalLoss:0.00257  PostnetLoss:0.00124  DecoderLoss:0.00133  StopLoss:0.04849  GradNorm:0.00413  GradNormST:0.01374  AvgTextLen:33.4  AvgSpecLen:115.7  StepTime:0.61  LR:0.000100\n",
      "   | > Step:53/68  GlobalStep:87960  TotalLoss:0.00271  PostnetLoss:0.00130  DecoderLoss:0.00141  StopLoss:0.04158  GradNorm:0.00323  GradNormST:0.00959  AvgTextLen:42.4  AvgSpecLen:137.6  StepTime:0.81  LR:0.000100\n",
      "   | > Step:63/68  GlobalStep:87970  TotalLoss:0.00304  PostnetLoss:0.00146  DecoderLoss:0.00158  StopLoss:0.03370  GradNorm:0.00320  GradNormST:0.01320  AvgTextLen:58.6  AvgSpecLen:185.1  StepTime:0.94  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:87975  AvgTotalLoss:0.05038  AvgPostnetLoss:0.00114  AvgDecoderLoss:0.00122  AvgStopLoss:0.04802  EpochTime:43.19  AvgStepTime:0.63\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.08655   PostnetLoss: 0.00463   DecoderLoss:0.00505  StopLoss: 0.07686  \n",
      "   | > TotalLoss: 0.07051   PostnetLoss: 0.00649   DecoderLoss:0.00705  StopLoss: 0.05697  \n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00114   Validation Loss: 0.00608\n",
      "\n",
      " > Epoch 275/1000\n",
      "   | > Step:4/68  GlobalStep:87980  TotalLoss:0.00161  PostnetLoss:0.00078  DecoderLoss:0.00083  StopLoss:0.09699  GradNorm:0.00419  GradNormST:0.04654  AvgTextLen:4.8  AvgSpecLen:31.1  StepTime:0.29  LR:0.000100\n",
      "   | > Step:14/68  GlobalStep:87990  TotalLoss:0.00189  PostnetLoss:0.00092  DecoderLoss:0.00097  StopLoss:0.05170  GradNorm:0.00471  GradNormST:0.01143  AvgTextLen:12.2  AvgSpecLen:52.0  StepTime:0.37  LR:0.000100\n",
      "   | > Step:24/68  GlobalStep:88000  TotalLoss:0.00222  PostnetLoss:0.00107  DecoderLoss:0.00114  StopLoss:0.04689  GradNorm:0.00405  GradNormST:0.01215  AvgTextLen:19.2  AvgSpecLen:79.0  StepTime:0.45  LR:0.000100\n",
      " | | > Checkpoint saving : /vm/TTS/models/cartman_models/mozilla-no-loc-fattn-stopnet-sigmoid-loss_masking-August-15-2019_11+26AM-de6aeb9/checkpoint_88000.pth.tar\n",
      "warning: audio amplitude out of range, auto clipped.\n",
      "   | > Step:34/68  GlobalStep:88010  TotalLoss:0.00241  PostnetLoss:0.00117  DecoderLoss:0.00124  StopLoss:0.03491  GradNorm:0.00367  GradNormST:0.00694  AvgTextLen:26.2  AvgSpecLen:95.9  StepTime:0.51  LR:0.000100\n",
      "   | > Step:44/68  GlobalStep:88020  TotalLoss:0.00257  PostnetLoss:0.00124  DecoderLoss:0.00133  StopLoss:0.03831  GradNorm:0.00361  GradNormST:0.01089  AvgTextLen:33.8  AvgSpecLen:116.3  StepTime:0.62  LR:0.000100\n",
      "   | > Step:54/68  GlobalStep:88030  TotalLoss:0.00273  PostnetLoss:0.00131  DecoderLoss:0.00142  StopLoss:0.04249  GradNorm:0.00325  GradNormST:0.01076  AvgTextLen:42.2  AvgSpecLen:144.9  StepTime:0.88  LR:0.000100\n",
      "   | > Step:64/68  GlobalStep:88040  TotalLoss:0.00286  PostnetLoss:0.00137  DecoderLoss:0.00149  StopLoss:0.03741  GradNorm:0.00342  GradNormST:0.00790  AvgTextLen:60.6  AvgSpecLen:177.6  StepTime:0.84  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:88044  AvgTotalLoss:0.04993  AvgPostnetLoss:0.00113  AvgDecoderLoss:0.00121  AvgStopLoss:0.04758  EpochTime:41.99  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.08747   PostnetLoss: 0.00471   DecoderLoss:0.00510  StopLoss: 0.07766  \n",
      "   | > TotalLoss: 0.07367   PostnetLoss: 0.00677   DecoderLoss:0.00736  StopLoss: 0.05955  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00113   Validation Loss: 0.00623\n",
      "\n",
      " > Epoch 276/1000\n",
      "   | > Step:5/68  GlobalStep:88050  TotalLoss:0.00161  PostnetLoss:0.00078  DecoderLoss:0.00083  StopLoss:0.05676  GradNorm:0.00438  GradNormST:0.01564  AvgTextLen:5.9  AvgSpecLen:33.2  StepTime:0.36  LR:0.000100\n",
      "   | > Step:15/68  GlobalStep:88060  TotalLoss:0.00198  PostnetLoss:0.00096  DecoderLoss:0.00102  StopLoss:0.03980  GradNorm:0.00464  GradNormST:0.01279  AvgTextLen:12.8  AvgSpecLen:59.1  StepTime:0.52  LR:0.000100\n",
      "   | > Step:25/68  GlobalStep:88070  TotalLoss:0.00215  PostnetLoss:0.00104  DecoderLoss:0.00111  StopLoss:0.04910  GradNorm:0.00392  GradNormST:0.01352  AvgTextLen:19.8  AvgSpecLen:73.3  StepTime:0.44  LR:0.000100\n",
      "   | > Step:35/68  GlobalStep:88080  TotalLoss:0.00237  PostnetLoss:0.00115  DecoderLoss:0.00122  StopLoss:0.04306  GradNorm:0.00369  GradNormST:0.01981  AvgTextLen:27.1  AvgSpecLen:104.7  StepTime:0.64  LR:0.000100\n",
      "   | > Step:45/68  GlobalStep:88090  TotalLoss:0.00251  PostnetLoss:0.00121  DecoderLoss:0.00130  StopLoss:0.04889  GradNorm:0.00384  GradNormST:0.02220  AvgTextLen:34.4  AvgSpecLen:118.6  StepTime:0.54  LR:0.000100\n",
      "   | > Step:55/68  GlobalStep:88100  TotalLoss:0.00276  PostnetLoss:0.00133  DecoderLoss:0.00143  StopLoss:0.04026  GradNorm:0.00288  GradNormST:0.00729  AvgTextLen:43.9  AvgSpecLen:145.2  StepTime:0.67  LR:0.000100\n",
      "   | > Step:65/68  GlobalStep:88110  TotalLoss:0.00306  PostnetLoss:0.00147  DecoderLoss:0.00159  StopLoss:0.04743  GradNorm:0.00339  GradNormST:0.02292  AvgTextLen:64.2  AvgSpecLen:207.4  StepTime:1.19  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:88113  AvgTotalLoss:0.04864  AvgPostnetLoss:0.00112  AvgDecoderLoss:0.00120  AvgStopLoss:0.04632  EpochTime:41.90  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.08548   PostnetLoss: 0.00472   DecoderLoss:0.00512  StopLoss: 0.07564  \n",
      "   | > TotalLoss: 0.07118   PostnetLoss: 0.00656   DecoderLoss:0.00711  StopLoss: 0.05751  \n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00112   Validation Loss: 0.00617\n",
      "\n",
      " > Epoch 277/1000\n",
      "   | > Step:6/68  GlobalStep:88120  TotalLoss:0.00161  PostnetLoss:0.00078  DecoderLoss:0.00082  StopLoss:0.04227  GradNorm:0.00416  GradNormST:0.01627  AvgTextLen:6.0  AvgSpecLen:34.8  StepTime:0.39  LR:0.000100\n",
      "   | > Step:16/68  GlobalStep:88130  TotalLoss:0.00194  PostnetLoss:0.00094  DecoderLoss:0.00100  StopLoss:0.04946  GradNorm:0.00338  GradNormST:0.01554  AvgTextLen:13.8  AvgSpecLen:60.6  StepTime:0.29  LR:0.000100\n",
      "   | > Step:26/68  GlobalStep:88140  TotalLoss:0.00216  PostnetLoss:0.00105  DecoderLoss:0.00111  StopLoss:0.05031  GradNorm:0.00377  GradNormST:0.01235  AvgTextLen:20.3  AvgSpecLen:77.5  StepTime:0.40  LR:0.000100\n",
      "   | > Step:36/68  GlobalStep:88150  TotalLoss:0.00236  PostnetLoss:0.00114  DecoderLoss:0.00122  StopLoss:0.05082  GradNorm:0.00402  GradNormST:0.00865  AvgTextLen:28.5  AvgSpecLen:100.2  StepTime:0.60  LR:0.000100\n",
      "   | > Step:46/68  GlobalStep:88160  TotalLoss:0.00258  PostnetLoss:0.00124  DecoderLoss:0.00133  StopLoss:0.04691  GradNorm:0.00361  GradNormST:0.01123  AvgTextLen:35.4  AvgSpecLen:122.6  StepTime:0.77  LR:0.000100\n",
      "   | > Step:56/68  GlobalStep:88170  TotalLoss:0.00279  PostnetLoss:0.00134  DecoderLoss:0.00145  StopLoss:0.03555  GradNorm:0.00290  GradNormST:0.01474  AvgTextLen:45.8  AvgSpecLen:145.4  StepTime:0.99  LR:0.000100\n",
      "   | > Step:66/68  GlobalStep:88180  TotalLoss:0.00321  PostnetLoss:0.00154  DecoderLoss:0.00167  StopLoss:0.03974  GradNorm:0.00368  GradNormST:0.00712  AvgTextLen:71.2  AvgSpecLen:220.8  StepTime:1.08  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:88182  AvgTotalLoss:0.05066  AvgPostnetLoss:0.00112  AvgDecoderLoss:0.00120  AvgStopLoss:0.04834  EpochTime:42.21  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.08694   PostnetLoss: 0.00461   DecoderLoss:0.00500  StopLoss: 0.07733  \n",
      "   | > TotalLoss: 0.07192   PostnetLoss: 0.00670   DecoderLoss:0.00726  StopLoss: 0.05796  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00112   Validation Loss: 0.00621\n",
      "\n",
      " > Epoch 278/1000\n",
      "   | > Step:7/68  GlobalStep:88190  TotalLoss:0.00168  PostnetLoss:0.00082  DecoderLoss:0.00086  StopLoss:0.04297  GradNorm:0.00406  GradNormST:0.01609  AvgTextLen:7.4  AvgSpecLen:44.6  StepTime:0.38  LR:0.000100\n",
      "   | > Step:17/68  GlobalStep:88200  TotalLoss:0.00197  PostnetLoss:0.00096  DecoderLoss:0.00102  StopLoss:0.07246  GradNorm:0.00344  GradNormST:0.01813  AvgTextLen:14.7  AvgSpecLen:66.0  StepTime:0.43  LR:0.000100\n",
      "   | > Step:27/68  GlobalStep:88210  TotalLoss:0.00220  PostnetLoss:0.00106  DecoderLoss:0.00114  StopLoss:0.06478  GradNorm:0.00349  GradNormST:0.03162  AvgTextLen:21.0  AvgSpecLen:80.2  StepTime:0.68  LR:0.000100\n",
      "   | > Step:37/68  GlobalStep:88220  TotalLoss:0.00242  PostnetLoss:0.00117  DecoderLoss:0.00125  StopLoss:0.04705  GradNorm:0.00376  GradNormST:0.00798  AvgTextLen:29.3  AvgSpecLen:102.4  StepTime:0.49  LR:0.000100\n",
      "   | > Step:47/68  GlobalStep:88230  TotalLoss:0.00259  PostnetLoss:0.00125  DecoderLoss:0.00134  StopLoss:0.04758  GradNorm:0.00295  GradNormST:0.01108  AvgTextLen:36.7  AvgSpecLen:126.9  StepTime:0.59  LR:0.000100\n",
      "   | > Step:57/68  GlobalStep:88240  TotalLoss:0.00281  PostnetLoss:0.00135  DecoderLoss:0.00146  StopLoss:0.03411  GradNorm:0.00280  GradNormST:0.00755  AvgTextLen:47.1  AvgSpecLen:161.0  StepTime:0.83  LR:0.000100\n",
      "   | > Step:67/68  GlobalStep:88250  TotalLoss:0.00322  PostnetLoss:0.00154  DecoderLoss:0.00168  StopLoss:0.03411  GradNorm:0.00359  GradNormST:0.01271  AvgTextLen:80.6  AvgSpecLen:246.5  StepTime:1.28  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:88251  AvgTotalLoss:0.05168  AvgPostnetLoss:0.00111  AvgDecoderLoss:0.00119  AvgStopLoss:0.04938  EpochTime:43.30  AvgStepTime:0.63\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.08831   PostnetLoss: 0.00469   DecoderLoss:0.00509  StopLoss: 0.07853  \n",
      "   | > TotalLoss: 0.07262   PostnetLoss: 0.00675   DecoderLoss:0.00733  StopLoss: 0.05854  \n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00111   Validation Loss: 0.00628\n",
      "\n",
      " > Epoch 279/1000\n",
      "   | > Step:8/68  GlobalStep:88260  TotalLoss:0.00175  PostnetLoss:0.00085  DecoderLoss:0.00090  StopLoss:0.06096  GradNorm:0.00443  GradNormST:0.01200  AvgTextLen:8.3  AvgSpecLen:44.0  StepTime:0.33  LR:0.000100\n",
      "   | > Step:18/68  GlobalStep:88270  TotalLoss:0.00192  PostnetLoss:0.00093  DecoderLoss:0.00099  StopLoss:0.05760  GradNorm:0.00341  GradNormST:0.00955  AvgTextLen:15.2  AvgSpecLen:64.5  StepTime:0.34  LR:0.000100\n",
      "   | > Step:28/68  GlobalStep:88280  TotalLoss:0.00220  PostnetLoss:0.00106  DecoderLoss:0.00113  StopLoss:0.06104  GradNorm:0.00312  GradNormST:0.01183  AvgTextLen:22.1  AvgSpecLen:85.4  StepTime:0.52  LR:0.000100\n",
      "   | > Step:38/68  GlobalStep:88290  TotalLoss:0.00238  PostnetLoss:0.00115  DecoderLoss:0.00123  StopLoss:0.03579  GradNorm:0.00368  GradNormST:0.00743  AvgTextLen:29.4  AvgSpecLen:104.1  StepTime:0.65  LR:0.000100\n",
      "   | > Step:48/68  GlobalStep:88300  TotalLoss:0.00260  PostnetLoss:0.00126  DecoderLoss:0.00134  StopLoss:0.05693  GradNorm:0.00367  GradNormST:0.01423  AvgTextLen:37.5  AvgSpecLen:123.8  StepTime:0.56  LR:0.000100\n",
      "   | > Step:58/68  GlobalStep:88310  TotalLoss:0.00282  PostnetLoss:0.00136  DecoderLoss:0.00146  StopLoss:0.03771  GradNorm:0.00290  GradNormST:0.00893  AvgTextLen:47.8  AvgSpecLen:158.0  StepTime:0.86  LR:0.000100\n",
      "   | > Step:68/68  GlobalStep:88320  TotalLoss:0.00313  PostnetLoss:0.00150  DecoderLoss:0.00163  StopLoss:0.03852  GradNorm:0.00263  GradNormST:0.01328  AvgTextLen:103.0  AvgSpecLen:308.9  StepTime:1.90  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:88320  AvgTotalLoss:0.05102  AvgPostnetLoss:0.00111  AvgDecoderLoss:0.00119  AvgStopLoss:0.04872  EpochTime:42.31  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.08977   PostnetLoss: 0.00467   DecoderLoss:0.00507  StopLoss: 0.08004  \n",
      "   | > TotalLoss: 0.07159   PostnetLoss: 0.00672   DecoderLoss:0.00731  StopLoss: 0.05756  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00111   Validation Loss: 0.00625\n",
      "\n",
      " > Epoch 280/1000\n",
      "   | > Step:9/68  GlobalStep:88330  TotalLoss:0.00169  PostnetLoss:0.00082  DecoderLoss:0.00087  StopLoss:0.04093  GradNorm:0.00380  GradNormST:0.01974  AvgTextLen:9.0  AvgSpecLen:42.1  StepTime:0.46  LR:0.000100\n",
      "   | > Step:19/68  GlobalStep:88340  TotalLoss:0.00193  PostnetLoss:0.00094  DecoderLoss:0.00099  StopLoss:0.08773  GradNorm:0.00323  GradNormST:0.02129  AvgTextLen:15.5  AvgSpecLen:63.2  StepTime:0.44  LR:0.000100\n",
      "   | > Step:29/68  GlobalStep:88350  TotalLoss:0.00214  PostnetLoss:0.00104  DecoderLoss:0.00110  StopLoss:0.05713  GradNorm:0.00303  GradNormST:0.01060  AvgTextLen:21.8  AvgSpecLen:83.4  StepTime:0.61  LR:0.000100\n",
      "   | > Step:39/68  GlobalStep:88360  TotalLoss:0.00243  PostnetLoss:0.00117  DecoderLoss:0.00125  StopLoss:0.03687  GradNorm:0.00316  GradNormST:0.00944  AvgTextLen:30.2  AvgSpecLen:110.4  StepTime:0.67  LR:0.000100\n",
      "   | > Step:49/68  GlobalStep:88370  TotalLoss:0.00259  PostnetLoss:0.00125  DecoderLoss:0.00134  StopLoss:0.03706  GradNorm:0.00301  GradNormST:0.00793  AvgTextLen:38.2  AvgSpecLen:127.7  StepTime:0.70  LR:0.000100\n",
      "   | > Step:59/68  GlobalStep:88380  TotalLoss:0.00280  PostnetLoss:0.00135  DecoderLoss:0.00145  StopLoss:0.04016  GradNorm:0.00295  GradNormST:0.01120  AvgTextLen:50.5  AvgSpecLen:155.9  StepTime:0.73  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:88389  AvgTotalLoss:0.05117  AvgPostnetLoss:0.00111  AvgDecoderLoss:0.00118  AvgStopLoss:0.04888  EpochTime:43.71  AvgStepTime:0.63\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.08934   PostnetLoss: 0.00467   DecoderLoss:0.00508  StopLoss: 0.07959  \n",
      "   | > TotalLoss: 0.07259   PostnetLoss: 0.00692   DecoderLoss:0.00753  StopLoss: 0.05814  \n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00111   Validation Loss: 0.00637\n",
      "\n",
      " > Epoch 281/1000\n",
      "   | > Step:0/68  GlobalStep:88390  TotalLoss:0.00165  PostnetLoss:0.00079  DecoderLoss:0.00086  StopLoss:0.07579  GradNorm:0.00603  GradNormST:0.02714  AvgTextLen:3.0  AvgSpecLen:25.3  StepTime:0.56  LR:0.000100\n",
      "   | > Step:10/68  GlobalStep:88400  TotalLoss:0.00178  PostnetLoss:0.00086  DecoderLoss:0.00091  StopLoss:0.04463  GradNorm:0.00356  GradNormST:0.01709  AvgTextLen:9.8  AvgSpecLen:48.6  StepTime:0.51  LR:0.000100\n",
      "   | > Step:20/68  GlobalStep:88410  TotalLoss:0.00205  PostnetLoss:0.00100  DecoderLoss:0.00106  StopLoss:0.04315  GradNorm:0.00386  GradNormST:0.00977  AvgTextLen:16.4  AvgSpecLen:64.8  StepTime:0.53  LR:0.000100\n",
      "   | > Step:30/68  GlobalStep:88420  TotalLoss:0.00223  PostnetLoss:0.00108  DecoderLoss:0.00115  StopLoss:0.05437  GradNorm:0.00320  GradNormST:0.01150  AvgTextLen:22.9  AvgSpecLen:85.8  StepTime:0.57  LR:0.000100\n",
      "   | > Step:40/68  GlobalStep:88430  TotalLoss:0.00239  PostnetLoss:0.00116  DecoderLoss:0.00123  StopLoss:0.03556  GradNorm:0.00286  GradNormST:0.00937  AvgTextLen:30.9  AvgSpecLen:105.3  StepTime:0.61  LR:0.000100\n",
      "   | > Step:50/68  GlobalStep:88440  TotalLoss:0.00261  PostnetLoss:0.00126  DecoderLoss:0.00135  StopLoss:0.03970  GradNorm:0.00315  GradNormST:0.01173  AvgTextLen:38.8  AvgSpecLen:132.0  StepTime:0.71  LR:0.000100\n",
      "   | > Step:60/68  GlobalStep:88450  TotalLoss:0.00285  PostnetLoss:0.00137  DecoderLoss:0.00148  StopLoss:0.03479  GradNorm:0.00263  GradNormST:0.00855  AvgTextLen:52.0  AvgSpecLen:172.7  StepTime:1.03  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:88458  AvgTotalLoss:0.05007  AvgPostnetLoss:0.00111  AvgDecoderLoss:0.00118  AvgStopLoss:0.04778  EpochTime:42.47  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.08978   PostnetLoss: 0.00468   DecoderLoss:0.00511  StopLoss: 0.07999  \n",
      "   | > TotalLoss: 0.07539   PostnetLoss: 0.00711   DecoderLoss:0.00775  StopLoss: 0.06053  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00111   Validation Loss: 0.00645\n",
      "\n",
      " > Epoch 282/1000\n",
      "   | > Step:1/68  GlobalStep:88460  TotalLoss:0.00156  PostnetLoss:0.00076  DecoderLoss:0.00081  StopLoss:0.06047  GradNorm:0.00446  GradNormST:0.01953  AvgTextLen:3.7  AvgSpecLen:28.6  StepTime:0.33  LR:0.000100\n",
      "   | > Step:11/68  GlobalStep:88470  TotalLoss:0.00176  PostnetLoss:0.00085  DecoderLoss:0.00091  StopLoss:0.04625  GradNorm:0.00373  GradNormST:0.01676  AvgTextLen:10.3  AvgSpecLen:49.4  StepTime:0.37  LR:0.000100\n",
      "   | > Step:21/68  GlobalStep:88480  TotalLoss:0.00202  PostnetLoss:0.00098  DecoderLoss:0.00104  StopLoss:0.05198  GradNorm:0.00329  GradNormST:0.02346  AvgTextLen:16.6  AvgSpecLen:66.3  StepTime:0.38  LR:0.000100\n",
      "   | > Step:31/68  GlobalStep:88490  TotalLoss:0.00222  PostnetLoss:0.00107  DecoderLoss:0.00114  StopLoss:0.03844  GradNorm:0.00312  GradNormST:0.01123  AvgTextLen:24.1  AvgSpecLen:82.7  StepTime:0.52  LR:0.000100\n",
      "   | > Step:41/68  GlobalStep:88500  TotalLoss:0.00251  PostnetLoss:0.00121  DecoderLoss:0.00130  StopLoss:0.02453  GradNorm:0.00303  GradNormST:0.00821  AvgTextLen:32.3  AvgSpecLen:109.4  StepTime:0.65  LR:0.000100\n",
      "   | > Step:51/68  GlobalStep:88510  TotalLoss:0.00259  PostnetLoss:0.00125  DecoderLoss:0.00134  StopLoss:0.04856  GradNorm:0.00280  GradNormST:0.01566  AvgTextLen:39.2  AvgSpecLen:126.9  StepTime:0.68  LR:0.000100\n",
      "   | > Step:61/68  GlobalStep:88520  TotalLoss:0.00280  PostnetLoss:0.00135  DecoderLoss:0.00145  StopLoss:0.03338  GradNorm:0.00287  GradNormST:0.00676  AvgTextLen:54.6  AvgSpecLen:165.5  StepTime:0.91  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:88527  AvgTotalLoss:0.05154  AvgPostnetLoss:0.00111  AvgDecoderLoss:0.00118  AvgStopLoss:0.04925  EpochTime:42.80  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.09126   PostnetLoss: 0.00472   DecoderLoss:0.00515  StopLoss: 0.08139  \n",
      "   | > TotalLoss: 0.07581   PostnetLoss: 0.00721   DecoderLoss:0.00784  StopLoss: 0.06076  \n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00111   Validation Loss: 0.00652\n",
      "\n",
      " > Epoch 283/1000\n",
      "   | > Step:2/68  GlobalStep:88530  TotalLoss:0.00147  PostnetLoss:0.00071  DecoderLoss:0.00076  StopLoss:0.06277  GradNorm:0.00381  GradNormST:0.02809  AvgTextLen:4.0  AvgSpecLen:31.3  StepTime:0.48  LR:0.000100\n",
      "   | > Step:12/68  GlobalStep:88540  TotalLoss:0.00178  PostnetLoss:0.00086  DecoderLoss:0.00092  StopLoss:0.04890  GradNorm:0.00425  GradNormST:0.01175  AvgTextLen:11.0  AvgSpecLen:49.3  StepTime:0.44  LR:0.000100\n",
      "   | > Step:22/68  GlobalStep:88550  TotalLoss:0.00206  PostnetLoss:0.00100  DecoderLoss:0.00106  StopLoss:0.06059  GradNorm:0.00348  GradNormST:0.01292  AvgTextLen:18.0  AvgSpecLen:72.0  StepTime:0.46  LR:0.000100\n",
      "   | > Step:32/68  GlobalStep:88560  TotalLoss:0.00222  PostnetLoss:0.00108  DecoderLoss:0.00115  StopLoss:0.06178  GradNorm:0.00296  GradNormST:0.01156  AvgTextLen:24.4  AvgSpecLen:91.5  StepTime:0.56  LR:0.000100\n",
      "   | > Step:42/68  GlobalStep:88570  TotalLoss:0.00242  PostnetLoss:0.00117  DecoderLoss:0.00125  StopLoss:0.04321  GradNorm:0.00285  GradNormST:0.01253  AvgTextLen:33.1  AvgSpecLen:109.7  StepTime:0.62  LR:0.000100\n",
      "   | > Step:52/68  GlobalStep:88580  TotalLoss:0.00267  PostnetLoss:0.00129  DecoderLoss:0.00138  StopLoss:0.05071  GradNorm:0.00281  GradNormST:0.01404  AvgTextLen:42.1  AvgSpecLen:137.5  StepTime:0.78  LR:0.000100\n",
      "   | > Step:62/68  GlobalStep:88590  TotalLoss:0.00289  PostnetLoss:0.00139  DecoderLoss:0.00150  StopLoss:0.04179  GradNorm:0.00280  GradNormST:0.01436  AvgTextLen:54.0  AvgSpecLen:174.7  StepTime:0.77  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:88596  AvgTotalLoss:0.05053  AvgPostnetLoss:0.00110  AvgDecoderLoss:0.00118  AvgStopLoss:0.04825  EpochTime:44.03  AvgStepTime:0.64\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.09061   PostnetLoss: 0.00471   DecoderLoss:0.00514  StopLoss: 0.08076  \n",
      "   | > TotalLoss: 0.07624   PostnetLoss: 0.00692   DecoderLoss:0.00754  StopLoss: 0.06178  \n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00110   Validation Loss: 0.00632\n",
      "\n",
      " > Epoch 284/1000\n",
      "   | > Step:3/68  GlobalStep:88600  TotalLoss:0.00154  PostnetLoss:0.00075  DecoderLoss:0.00079  StopLoss:0.05974  GradNorm:0.00422  GradNormST:0.02743  AvgTextLen:4.7  AvgSpecLen:29.7  StepTime:0.38  LR:0.000100\n",
      "   | > Step:13/68  GlobalStep:88610  TotalLoss:0.00179  PostnetLoss:0.00087  DecoderLoss:0.00092  StopLoss:0.06911  GradNorm:0.00442  GradNormST:0.01884  AvgTextLen:11.5  AvgSpecLen:55.2  StepTime:0.43  LR:0.000100\n",
      "   | > Step:23/68  GlobalStep:88620  TotalLoss:0.00212  PostnetLoss:0.00103  DecoderLoss:0.00109  StopLoss:0.03930  GradNorm:0.00347  GradNormST:0.00897  AvgTextLen:18.3  AvgSpecLen:72.2  StepTime:0.42  LR:0.000100\n",
      "   | > Step:33/68  GlobalStep:88630  TotalLoss:0.00229  PostnetLoss:0.00111  DecoderLoss:0.00118  StopLoss:0.04844  GradNorm:0.00318  GradNormST:0.00932  AvgTextLen:25.9  AvgSpecLen:88.9  StepTime:0.50  LR:0.000100\n",
      "   | > Step:43/68  GlobalStep:88640  TotalLoss:0.00247  PostnetLoss:0.00119  DecoderLoss:0.00128  StopLoss:0.04991  GradNorm:0.00291  GradNormST:0.01062  AvgTextLen:33.4  AvgSpecLen:115.7  StepTime:0.65  LR:0.000100\n",
      "   | > Step:53/68  GlobalStep:88650  TotalLoss:0.00262  PostnetLoss:0.00126  DecoderLoss:0.00136  StopLoss:0.04165  GradNorm:0.00274  GradNormST:0.00743  AvgTextLen:42.4  AvgSpecLen:137.6  StepTime:0.80  LR:0.000100\n",
      "   | > Step:63/68  GlobalStep:88660  TotalLoss:0.00297  PostnetLoss:0.00143  DecoderLoss:0.00154  StopLoss:0.03199  GradNorm:0.00303  GradNormST:0.00896  AvgTextLen:58.6  AvgSpecLen:185.1  StepTime:0.92  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:88665  AvgTotalLoss:0.05063  AvgPostnetLoss:0.00110  AvgDecoderLoss:0.00118  AvgStopLoss:0.04834  EpochTime:43.35  AvgStepTime:0.63\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.08687   PostnetLoss: 0.00467   DecoderLoss:0.00509  StopLoss: 0.07710  \n",
      "   | > TotalLoss: 0.07371   PostnetLoss: 0.00698   DecoderLoss:0.00760  StopLoss: 0.05912  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00110   Validation Loss: 0.00633\n",
      "\n",
      " > Epoch 285/1000\n",
      "   | > Step:4/68  GlobalStep:88670  TotalLoss:0.00153  PostnetLoss:0.00074  DecoderLoss:0.00079  StopLoss:0.08722  GradNorm:0.00388  GradNormST:0.03366  AvgTextLen:4.8  AvgSpecLen:31.1  StepTime:0.28  LR:0.000100\n",
      "   | > Step:14/68  GlobalStep:88680  TotalLoss:0.00184  PostnetLoss:0.00090  DecoderLoss:0.00095  StopLoss:0.04834  GradNorm:0.00434  GradNormST:0.01633  AvgTextLen:12.2  AvgSpecLen:52.0  StepTime:0.33  LR:0.000100\n",
      "   | > Step:24/68  GlobalStep:88690  TotalLoss:0.00216  PostnetLoss:0.00105  DecoderLoss:0.00111  StopLoss:0.04857  GradNorm:0.00391  GradNormST:0.01037  AvgTextLen:19.2  AvgSpecLen:79.0  StepTime:0.51  LR:0.000100\n",
      "   | > Step:34/68  GlobalStep:88700  TotalLoss:0.00233  PostnetLoss:0.00113  DecoderLoss:0.00120  StopLoss:0.03766  GradNorm:0.00334  GradNormST:0.01020  AvgTextLen:26.2  AvgSpecLen:95.9  StepTime:0.48  LR:0.000100\n",
      "   | > Step:44/68  GlobalStep:88710  TotalLoss:0.00251  PostnetLoss:0.00121  DecoderLoss:0.00130  StopLoss:0.03763  GradNorm:0.00317  GradNormST:0.01157  AvgTextLen:33.8  AvgSpecLen:116.3  StepTime:0.73  LR:0.000100\n",
      "   | > Step:54/68  GlobalStep:88720  TotalLoss:0.00267  PostnetLoss:0.00129  DecoderLoss:0.00139  StopLoss:0.04559  GradNorm:0.00295  GradNormST:0.00903  AvgTextLen:42.2  AvgSpecLen:144.9  StepTime:0.77  LR:0.000100\n",
      "   | > Step:64/68  GlobalStep:88730  TotalLoss:0.00278  PostnetLoss:0.00133  DecoderLoss:0.00145  StopLoss:0.04053  GradNorm:0.00254  GradNormST:0.00754  AvgTextLen:60.6  AvgSpecLen:177.6  StepTime:0.81  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:88734  AvgTotalLoss:0.05205  AvgPostnetLoss:0.00110  AvgDecoderLoss:0.00118  AvgStopLoss:0.04977  EpochTime:43.62  AvgStepTime:0.63\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.09481   PostnetLoss: 0.00464   DecoderLoss:0.00506  StopLoss: 0.08511  \n",
      "   | > TotalLoss: 0.07756   PostnetLoss: 0.00717   DecoderLoss:0.00777  StopLoss: 0.06262  \n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00110   Validation Loss: 0.00644\n",
      "\n",
      " > Epoch 286/1000\n",
      "   | > Step:5/68  GlobalStep:88740  TotalLoss:0.00162  PostnetLoss:0.00078  DecoderLoss:0.00084  StopLoss:0.05524  GradNorm:0.00416  GradNormST:0.01426  AvgTextLen:5.9  AvgSpecLen:33.2  StepTime:0.32  LR:0.000100\n",
      "   | > Step:15/68  GlobalStep:88750  TotalLoss:0.00193  PostnetLoss:0.00094  DecoderLoss:0.00099  StopLoss:0.04488  GradNorm:0.00347  GradNormST:0.01454  AvgTextLen:12.8  AvgSpecLen:59.1  StepTime:0.55  LR:0.000100\n",
      "   | > Step:25/68  GlobalStep:88760  TotalLoss:0.00206  PostnetLoss:0.00100  DecoderLoss:0.00106  StopLoss:0.04929  GradNorm:0.00359  GradNormST:0.01259  AvgTextLen:19.8  AvgSpecLen:73.3  StepTime:0.46  LR:0.000100\n",
      "   | > Step:35/68  GlobalStep:88770  TotalLoss:0.00238  PostnetLoss:0.00115  DecoderLoss:0.00123  StopLoss:0.05458  GradNorm:0.00348  GradNormST:0.02161  AvgTextLen:27.1  AvgSpecLen:104.7  StepTime:0.76  LR:0.000100\n",
      "   | > Step:45/68  GlobalStep:88780  TotalLoss:0.00246  PostnetLoss:0.00119  DecoderLoss:0.00127  StopLoss:0.05101  GradNorm:0.00301  GradNormST:0.01936  AvgTextLen:34.4  AvgSpecLen:118.6  StepTime:0.59  LR:0.000100\n",
      "   | > Step:55/68  GlobalStep:88790  TotalLoss:0.00275  PostnetLoss:0.00133  DecoderLoss:0.00142  StopLoss:0.03890  GradNorm:0.00283  GradNormST:0.00630  AvgTextLen:43.9  AvgSpecLen:145.2  StepTime:0.79  LR:0.000100\n",
      "   | > Step:65/68  GlobalStep:88800  TotalLoss:0.00299  PostnetLoss:0.00144  DecoderLoss:0.00156  StopLoss:0.04080  GradNorm:0.00280  GradNormST:0.01633  AvgTextLen:64.2  AvgSpecLen:207.4  StepTime:1.21  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:88803  AvgTotalLoss:0.05047  AvgPostnetLoss:0.00110  AvgDecoderLoss:0.00118  AvgStopLoss:0.04819  EpochTime:43.04  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.09265   PostnetLoss: 0.00473   DecoderLoss:0.00514  StopLoss: 0.08278  \n",
      "   | > TotalLoss: 0.07748   PostnetLoss: 0.00712   DecoderLoss:0.00771  StopLoss: 0.06264  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00110   Validation Loss: 0.00645\n",
      "\n",
      " > Epoch 287/1000\n",
      "   | > Step:6/68  GlobalStep:88810  TotalLoss:0.00155  PostnetLoss:0.00076  DecoderLoss:0.00079  StopLoss:0.04721  GradNorm:0.00346  GradNormST:0.01422  AvgTextLen:6.0  AvgSpecLen:34.8  StepTime:0.37  LR:0.000100\n",
      "   | > Step:16/68  GlobalStep:88820  TotalLoss:0.00191  PostnetLoss:0.00093  DecoderLoss:0.00098  StopLoss:0.04601  GradNorm:0.00347  GradNormST:0.01162  AvgTextLen:13.8  AvgSpecLen:60.6  StepTime:0.38  LR:0.000100\n",
      "   | > Step:26/68  GlobalStep:88830  TotalLoss:0.00216  PostnetLoss:0.00105  DecoderLoss:0.00111  StopLoss:0.04550  GradNorm:0.00333  GradNormST:0.01415  AvgTextLen:20.3  AvgSpecLen:77.5  StepTime:0.50  LR:0.000100\n",
      "   | > Step:36/68  GlobalStep:88840  TotalLoss:0.00232  PostnetLoss:0.00112  DecoderLoss:0.00120  StopLoss:0.04859  GradNorm:0.00369  GradNormST:0.00883  AvgTextLen:28.5  AvgSpecLen:100.2  StepTime:0.50  LR:0.000100\n",
      "   | > Step:46/68  GlobalStep:88850  TotalLoss:0.00249  PostnetLoss:0.00120  DecoderLoss:0.00129  StopLoss:0.03704  GradNorm:0.00340  GradNormST:0.00703  AvgTextLen:35.4  AvgSpecLen:122.6  StepTime:0.81  LR:0.000100\n",
      "   | > Step:56/68  GlobalStep:88860  TotalLoss:0.00268  PostnetLoss:0.00129  DecoderLoss:0.00139  StopLoss:0.02934  GradNorm:0.00285  GradNormST:0.00730  AvgTextLen:45.8  AvgSpecLen:145.4  StepTime:1.03  LR:0.000100\n",
      "   | > Step:66/68  GlobalStep:88870  TotalLoss:0.00314  PostnetLoss:0.00151  DecoderLoss:0.00164  StopLoss:0.03359  GradNorm:0.00283  GradNormST:0.00844  AvgTextLen:71.2  AvgSpecLen:220.8  StepTime:1.04  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:88872  AvgTotalLoss:0.05185  AvgPostnetLoss:0.00110  AvgDecoderLoss:0.00117  AvgStopLoss:0.04958  EpochTime:42.92  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.08825   PostnetLoss: 0.00475   DecoderLoss:0.00517  StopLoss: 0.07832  \n",
      "   | > TotalLoss: 0.07366   PostnetLoss: 0.00708   DecoderLoss:0.00768  StopLoss: 0.05890  \n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00110   Validation Loss: 0.00637\n",
      "\n",
      " > Epoch 288/1000\n",
      "   | > Step:7/68  GlobalStep:88880  TotalLoss:0.00171  PostnetLoss:0.00083  DecoderLoss:0.00088  StopLoss:0.04213  GradNorm:0.00438  GradNormST:0.01345  AvgTextLen:7.4  AvgSpecLen:44.6  StepTime:0.42  LR:0.000100\n",
      "   | > Step:17/68  GlobalStep:88890  TotalLoss:0.00191  PostnetLoss:0.00093  DecoderLoss:0.00098  StopLoss:0.07061  GradNorm:0.00358  GradNormST:0.01387  AvgTextLen:14.7  AvgSpecLen:66.0  StepTime:0.41  LR:0.000100\n",
      "   | > Step:27/68  GlobalStep:88900  TotalLoss:0.00222  PostnetLoss:0.00108  DecoderLoss:0.00114  StopLoss:0.04827  GradNorm:0.00304  GradNormST:0.01456  AvgTextLen:21.0  AvgSpecLen:80.2  StepTime:0.56  LR:0.000100\n",
      "   | > Step:37/68  GlobalStep:88910  TotalLoss:0.00240  PostnetLoss:0.00116  DecoderLoss:0.00124  StopLoss:0.05339  GradNorm:0.00340  GradNormST:0.01214  AvgTextLen:29.3  AvgSpecLen:102.4  StepTime:0.63  LR:0.000100\n",
      "   | > Step:47/68  GlobalStep:88920  TotalLoss:0.00258  PostnetLoss:0.00124  DecoderLoss:0.00133  StopLoss:0.04814  GradNorm:0.00334  GradNormST:0.01131  AvgTextLen:36.7  AvgSpecLen:126.9  StepTime:0.62  LR:0.000100\n",
      "   | > Step:57/68  GlobalStep:88930  TotalLoss:0.00280  PostnetLoss:0.00135  DecoderLoss:0.00145  StopLoss:0.04123  GradNorm:0.00281  GradNormST:0.01174  AvgTextLen:47.1  AvgSpecLen:161.0  StepTime:0.85  LR:0.000100\n",
      "   | > Step:67/68  GlobalStep:88940  TotalLoss:0.00315  PostnetLoss:0.00151  DecoderLoss:0.00164  StopLoss:0.03639  GradNorm:0.00289  GradNormST:0.01092  AvgTextLen:80.6  AvgSpecLen:246.5  StepTime:1.22  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:88941  AvgTotalLoss:0.05128  AvgPostnetLoss:0.00110  AvgDecoderLoss:0.00117  AvgStopLoss:0.04900  EpochTime:43.46  AvgStepTime:0.63\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.09580   PostnetLoss: 0.00477   DecoderLoss:0.00518  StopLoss: 0.08586  \n",
      "   | > TotalLoss: 0.07614   PostnetLoss: 0.00728   DecoderLoss:0.00789  StopLoss: 0.06096  \n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00110   Validation Loss: 0.00648\n",
      "\n",
      " > Epoch 289/1000\n",
      "   | > Step:8/68  GlobalStep:88950  TotalLoss:0.00174  PostnetLoss:0.00085  DecoderLoss:0.00089  StopLoss:0.06583  GradNorm:0.00491  GradNormST:0.01787  AvgTextLen:8.3  AvgSpecLen:44.0  StepTime:0.25  LR:0.000100\n",
      "   | > Step:18/68  GlobalStep:88960  TotalLoss:0.00196  PostnetLoss:0.00095  DecoderLoss:0.00101  StopLoss:0.05703  GradNorm:0.00335  GradNormST:0.01202  AvgTextLen:15.2  AvgSpecLen:64.5  StepTime:0.44  LR:0.000100\n",
      "   | > Step:28/68  GlobalStep:88970  TotalLoss:0.00221  PostnetLoss:0.00107  DecoderLoss:0.00114  StopLoss:0.07077  GradNorm:0.00327  GradNormST:0.02011  AvgTextLen:22.1  AvgSpecLen:85.4  StepTime:0.58  LR:0.000100\n",
      "   | > Step:38/68  GlobalStep:88980  TotalLoss:0.00236  PostnetLoss:0.00114  DecoderLoss:0.00122  StopLoss:0.03940  GradNorm:0.00307  GradNormST:0.00802  AvgTextLen:29.4  AvgSpecLen:104.1  StepTime:0.63  LR:0.000100\n",
      "   | > Step:48/68  GlobalStep:88990  TotalLoss:0.00255  PostnetLoss:0.00123  DecoderLoss:0.00132  StopLoss:0.04680  GradNorm:0.00289  GradNormST:0.01113  AvgTextLen:37.5  AvgSpecLen:123.8  StepTime:0.70  LR:0.000100\n",
      "   | > Step:58/68  GlobalStep:89000  TotalLoss:0.00278  PostnetLoss:0.00134  DecoderLoss:0.00144  StopLoss:0.05327  GradNorm:0.00334  GradNormST:0.01845  AvgTextLen:47.8  AvgSpecLen:158.0  StepTime:0.87  LR:0.000100\n",
      " | | > Checkpoint saving : /vm/TTS/models/cartman_models/mozilla-no-loc-fattn-stopnet-sigmoid-loss_masking-August-15-2019_11+26AM-de6aeb9/checkpoint_89000.pth.tar\n",
      "warning: audio amplitude out of range, auto clipped.\n",
      "   | > Step:68/68  GlobalStep:89010  TotalLoss:0.00314  PostnetLoss:0.00151  DecoderLoss:0.00164  StopLoss:0.02954  GradNorm:0.00277  GradNormST:0.01506  AvgTextLen:103.0  AvgSpecLen:308.9  StepTime:1.93  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:89010  AvgTotalLoss:0.05493  AvgPostnetLoss:0.00110  AvgDecoderLoss:0.00118  AvgStopLoss:0.05265  EpochTime:43.47  AvgStepTime:0.63\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.09701   PostnetLoss: 0.00473   DecoderLoss:0.00512  StopLoss: 0.08715  \n",
      "   | > TotalLoss: 0.07359   PostnetLoss: 0.00710   DecoderLoss:0.00769  StopLoss: 0.05880  \n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00110   Validation Loss: 0.00642\n",
      "\n",
      " > Epoch 290/1000\n",
      "   | > Step:9/68  GlobalStep:89020  TotalLoss:0.00165  PostnetLoss:0.00080  DecoderLoss:0.00085  StopLoss:0.03884  GradNorm:0.00360  GradNormST:0.01186  AvgTextLen:9.0  AvgSpecLen:42.1  StepTime:0.43  LR:0.000100\n",
      "   | > Step:19/68  GlobalStep:89030  TotalLoss:0.00193  PostnetLoss:0.00094  DecoderLoss:0.00099  StopLoss:0.08802  GradNorm:0.00406  GradNormST:0.02102  AvgTextLen:15.5  AvgSpecLen:63.2  StepTime:0.43  LR:0.000100\n",
      "   | > Step:29/68  GlobalStep:89040  TotalLoss:0.00214  PostnetLoss:0.00104  DecoderLoss:0.00110  StopLoss:0.06462  GradNorm:0.00313  GradNormST:0.01448  AvgTextLen:21.8  AvgSpecLen:83.4  StepTime:0.47  LR:0.000100\n",
      "   | > Step:39/68  GlobalStep:89050  TotalLoss:0.00241  PostnetLoss:0.00117  DecoderLoss:0.00124  StopLoss:0.03964  GradNorm:0.00291  GradNormST:0.00825  AvgTextLen:30.2  AvgSpecLen:110.4  StepTime:0.63  LR:0.000100\n",
      "   | > Step:49/68  GlobalStep:89060  TotalLoss:0.00255  PostnetLoss:0.00123  DecoderLoss:0.00132  StopLoss:0.04208  GradNorm:0.00277  GradNormST:0.00961  AvgTextLen:38.2  AvgSpecLen:127.7  StepTime:0.73  LR:0.000100\n",
      "   | > Step:59/68  GlobalStep:89070  TotalLoss:0.00273  PostnetLoss:0.00131  DecoderLoss:0.00142  StopLoss:0.04034  GradNorm:0.00284  GradNormST:0.01048  AvgTextLen:50.5  AvgSpecLen:155.9  StepTime:0.91  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:89079  AvgTotalLoss:0.05098  AvgPostnetLoss:0.00110  AvgDecoderLoss:0.00117  AvgStopLoss:0.04871  EpochTime:43.17  AvgStepTime:0.63\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.09864   PostnetLoss: 0.00460   DecoderLoss:0.00501  StopLoss: 0.08903  \n",
      "   | > TotalLoss: 0.07306   PostnetLoss: 0.00696   DecoderLoss:0.00754  StopLoss: 0.05856  \n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00110   Validation Loss: 0.00639\n",
      "\n",
      " > Epoch 291/1000\n",
      "   | > Step:0/68  GlobalStep:89080  TotalLoss:0.00158  PostnetLoss:0.00076  DecoderLoss:0.00082  StopLoss:0.08736  GradNorm:0.00525  GradNormST:0.03387  AvgTextLen:3.0  AvgSpecLen:25.3  StepTime:0.57  LR:0.000100\n",
      "   | > Step:10/68  GlobalStep:89090  TotalLoss:0.00170  PostnetLoss:0.00083  DecoderLoss:0.00088  StopLoss:0.05504  GradNorm:0.00338  GradNormST:0.01525  AvgTextLen:9.8  AvgSpecLen:48.6  StepTime:0.55  LR:0.000100\n",
      "   | > Step:20/68  GlobalStep:89100  TotalLoss:0.00205  PostnetLoss:0.00099  DecoderLoss:0.00106  StopLoss:0.03823  GradNorm:0.00542  GradNormST:0.00782  AvgTextLen:16.4  AvgSpecLen:64.8  StepTime:0.48  LR:0.000100\n",
      "   | > Step:30/68  GlobalStep:89110  TotalLoss:0.00220  PostnetLoss:0.00107  DecoderLoss:0.00114  StopLoss:0.05729  GradNorm:0.00356  GradNormST:0.01555  AvgTextLen:22.9  AvgSpecLen:85.8  StepTime:0.56  LR:0.000100\n",
      "   | > Step:40/68  GlobalStep:89120  TotalLoss:0.00236  PostnetLoss:0.00114  DecoderLoss:0.00122  StopLoss:0.04915  GradNorm:0.00299  GradNormST:0.01288  AvgTextLen:30.9  AvgSpecLen:105.3  StepTime:0.64  LR:0.000100\n",
      "   | > Step:50/68  GlobalStep:89130  TotalLoss:0.00259  PostnetLoss:0.00125  DecoderLoss:0.00134  StopLoss:0.04207  GradNorm:0.00291  GradNormST:0.00910  AvgTextLen:38.8  AvgSpecLen:132.0  StepTime:0.92  LR:0.000100\n",
      "   | > Step:60/68  GlobalStep:89140  TotalLoss:0.00280  PostnetLoss:0.00135  DecoderLoss:0.00145  StopLoss:0.03680  GradNorm:0.00267  GradNormST:0.00892  AvgTextLen:52.0  AvgSpecLen:172.7  StepTime:0.79  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:89148  AvgTotalLoss:0.05469  AvgPostnetLoss:0.00110  AvgDecoderLoss:0.00117  AvgStopLoss:0.05242  EpochTime:42.53  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.10056   PostnetLoss: 0.00461   DecoderLoss:0.00503  StopLoss: 0.09092  \n",
      "   | > TotalLoss: 0.07781   PostnetLoss: 0.00721   DecoderLoss:0.00780  StopLoss: 0.06280  \n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00110   Validation Loss: 0.00651\n",
      "\n",
      " > Epoch 292/1000\n",
      "   | > Step:1/68  GlobalStep:89150  TotalLoss:0.00160  PostnetLoss:0.00078  DecoderLoss:0.00082  StopLoss:0.05660  GradNorm:0.00553  GradNormST:0.02157  AvgTextLen:3.7  AvgSpecLen:28.6  StepTime:0.25  LR:0.000100\n",
      "   | > Step:11/68  GlobalStep:89160  TotalLoss:0.00175  PostnetLoss:0.00085  DecoderLoss:0.00090  StopLoss:0.05166  GradNorm:0.00351  GradNormST:0.01287  AvgTextLen:10.3  AvgSpecLen:49.4  StepTime:0.32  LR:0.000100\n",
      "   | > Step:21/68  GlobalStep:89170  TotalLoss:0.00201  PostnetLoss:0.00098  DecoderLoss:0.00103  StopLoss:0.06156  GradNorm:0.00346  GradNormST:0.02337  AvgTextLen:16.6  AvgSpecLen:66.3  StepTime:0.44  LR:0.000100\n",
      "   | > Step:31/68  GlobalStep:89180  TotalLoss:0.00217  PostnetLoss:0.00105  DecoderLoss:0.00112  StopLoss:0.04063  GradNorm:0.00318  GradNormST:0.01071  AvgTextLen:24.1  AvgSpecLen:82.7  StepTime:0.56  LR:0.000100\n",
      "   | > Step:41/68  GlobalStep:89190  TotalLoss:0.00247  PostnetLoss:0.00120  DecoderLoss:0.00128  StopLoss:0.04137  GradNorm:0.00302  GradNormST:0.01408  AvgTextLen:32.3  AvgSpecLen:109.4  StepTime:0.58  LR:0.000100\n",
      "   | > Step:51/68  GlobalStep:89200  TotalLoss:0.00258  PostnetLoss:0.00125  DecoderLoss:0.00133  StopLoss:0.04022  GradNorm:0.00276  GradNormST:0.00851  AvgTextLen:39.2  AvgSpecLen:126.9  StepTime:0.81  LR:0.000100\n",
      "   | > Step:61/68  GlobalStep:89210  TotalLoss:0.00281  PostnetLoss:0.00135  DecoderLoss:0.00146  StopLoss:0.03837  GradNorm:0.00295  GradNormST:0.01129  AvgTextLen:54.6  AvgSpecLen:165.5  StepTime:0.92  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:89217  AvgTotalLoss:0.05373  AvgPostnetLoss:0.00110  AvgDecoderLoss:0.00117  AvgStopLoss:0.05146  EpochTime:43.57  AvgStepTime:0.63\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.09727   PostnetLoss: 0.00469   DecoderLoss:0.00509  StopLoss: 0.08750  \n",
      "   | > TotalLoss: 0.07770   PostnetLoss: 0.00716   DecoderLoss:0.00777  StopLoss: 0.06276  \n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00110   Validation Loss: 0.00647\n",
      "\n",
      " > Epoch 293/1000\n",
      "   | > Step:2/68  GlobalStep:89220  TotalLoss:0.00148  PostnetLoss:0.00072  DecoderLoss:0.00076  StopLoss:0.06176  GradNorm:0.00394  GradNormST:0.02428  AvgTextLen:4.0  AvgSpecLen:31.3  StepTime:0.61  LR:0.000100\n",
      "   | > Step:12/68  GlobalStep:89230  TotalLoss:0.00171  PostnetLoss:0.00083  DecoderLoss:0.00088  StopLoss:0.05249  GradNorm:0.00337  GradNormST:0.01226  AvgTextLen:11.0  AvgSpecLen:49.3  StepTime:0.43  LR:0.000100\n",
      "   | > Step:22/68  GlobalStep:89240  TotalLoss:0.00205  PostnetLoss:0.00099  DecoderLoss:0.00105  StopLoss:0.06622  GradNorm:0.00356  GradNormST:0.01532  AvgTextLen:18.0  AvgSpecLen:72.0  StepTime:0.43  LR:0.000100\n",
      "   | > Step:32/68  GlobalStep:89250  TotalLoss:0.00226  PostnetLoss:0.00109  DecoderLoss:0.00116  StopLoss:0.05725  GradNorm:0.00351  GradNormST:0.01246  AvgTextLen:24.4  AvgSpecLen:91.5  StepTime:0.57  LR:0.000100\n",
      "   | > Step:42/68  GlobalStep:89260  TotalLoss:0.00243  PostnetLoss:0.00117  DecoderLoss:0.00126  StopLoss:0.05201  GradNorm:0.00324  GradNormST:0.01179  AvgTextLen:33.1  AvgSpecLen:109.7  StepTime:0.62  LR:0.000100\n",
      "   | > Step:52/68  GlobalStep:89270  TotalLoss:0.00264  PostnetLoss:0.00128  DecoderLoss:0.00137  StopLoss:0.04737  GradNorm:0.00285  GradNormST:0.01349  AvgTextLen:42.1  AvgSpecLen:137.5  StepTime:0.87  LR:0.000100\n",
      "   | > Step:62/68  GlobalStep:89280  TotalLoss:0.00287  PostnetLoss:0.00138  DecoderLoss:0.00149  StopLoss:0.03591  GradNorm:0.00320  GradNormST:0.01278  AvgTextLen:54.0  AvgSpecLen:174.7  StepTime:0.79  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:89286  AvgTotalLoss:0.05334  AvgPostnetLoss:0.00110  AvgDecoderLoss:0.00117  AvgStopLoss:0.05107  EpochTime:44.37  AvgStepTime:0.64\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.09934   PostnetLoss: 0.00483   DecoderLoss:0.00524  StopLoss: 0.08927  \n",
      "   | > TotalLoss: 0.07630   PostnetLoss: 0.00725   DecoderLoss:0.00786  StopLoss: 0.06119  \n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00110   Validation Loss: 0.00661\n",
      "\n",
      " > Epoch 294/1000\n",
      "   | > Step:3/68  GlobalStep:89290  TotalLoss:0.00153  PostnetLoss:0.00074  DecoderLoss:0.00079  StopLoss:0.08036  GradNorm:0.00425  GradNormST:0.02799  AvgTextLen:4.7  AvgSpecLen:29.7  StepTime:0.31  LR:0.000100\n",
      "   | > Step:13/68  GlobalStep:89300  TotalLoss:0.00173  PostnetLoss:0.00084  DecoderLoss:0.00089  StopLoss:0.06226  GradNorm:0.00331  GradNormST:0.01895  AvgTextLen:11.5  AvgSpecLen:55.2  StepTime:0.34  LR:0.000100\n",
      "   | > Step:23/68  GlobalStep:89310  TotalLoss:0.00210  PostnetLoss:0.00102  DecoderLoss:0.00108  StopLoss:0.05433  GradNorm:0.00330  GradNormST:0.01218  AvgTextLen:18.3  AvgSpecLen:72.2  StepTime:0.39  LR:0.000100\n",
      "   | > Step:33/68  GlobalStep:89320  TotalLoss:0.00229  PostnetLoss:0.00111  DecoderLoss:0.00118  StopLoss:0.06432  GradNorm:0.00333  GradNormST:0.01911  AvgTextLen:25.9  AvgSpecLen:88.9  StepTime:0.53  LR:0.000100\n",
      "   | > Step:43/68  GlobalStep:89330  TotalLoss:0.00241  PostnetLoss:0.00117  DecoderLoss:0.00125  StopLoss:0.05124  GradNorm:0.00288  GradNormST:0.01359  AvgTextLen:33.4  AvgSpecLen:115.7  StepTime:0.64  LR:0.000100\n",
      "   | > Step:53/68  GlobalStep:89340  TotalLoss:0.00260  PostnetLoss:0.00125  DecoderLoss:0.00135  StopLoss:0.03691  GradNorm:0.00289  GradNormST:0.00651  AvgTextLen:42.4  AvgSpecLen:137.6  StepTime:0.83  LR:0.000100\n",
      "   | > Step:63/68  GlobalStep:89350  TotalLoss:0.00293  PostnetLoss:0.00141  DecoderLoss:0.00152  StopLoss:0.02608  GradNorm:0.00286  GradNormST:0.00604  AvgTextLen:58.6  AvgSpecLen:185.1  StepTime:0.95  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:89355  AvgTotalLoss:0.05163  AvgPostnetLoss:0.00109  AvgDecoderLoss:0.00117  AvgStopLoss:0.04937  EpochTime:43.58  AvgStepTime:0.63\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.09554   PostnetLoss: 0.00476   DecoderLoss:0.00517  StopLoss: 0.08561  \n",
      "   | > TotalLoss: 0.07686   PostnetLoss: 0.00734   DecoderLoss:0.00798  StopLoss: 0.06154  \n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00109   Validation Loss: 0.00667\n",
      "\n",
      " > Epoch 295/1000\n",
      "   | > Step:4/68  GlobalStep:89360  TotalLoss:0.00150  PostnetLoss:0.00073  DecoderLoss:0.00077  StopLoss:0.08406  GradNorm:0.00413  GradNormST:0.03821  AvgTextLen:4.8  AvgSpecLen:31.1  StepTime:0.27  LR:0.000100\n",
      "   | > Step:14/68  GlobalStep:89370  TotalLoss:0.00181  PostnetLoss:0.00088  DecoderLoss:0.00093  StopLoss:0.07643  GradNorm:0.00351  GradNormST:0.02027  AvgTextLen:12.2  AvgSpecLen:52.0  StepTime:0.32  LR:0.000100\n",
      "   | > Step:24/68  GlobalStep:89380  TotalLoss:0.00214  PostnetLoss:0.00104  DecoderLoss:0.00110  StopLoss:0.06956  GradNorm:0.00315  GradNormST:0.01907  AvgTextLen:19.2  AvgSpecLen:79.0  StepTime:0.53  LR:0.000100\n",
      "   | > Step:34/68  GlobalStep:89390  TotalLoss:0.00230  PostnetLoss:0.00111  DecoderLoss:0.00119  StopLoss:0.03781  GradNorm:0.00327  GradNormST:0.00717  AvgTextLen:26.2  AvgSpecLen:95.9  StepTime:0.65  LR:0.000100\n",
      "   | > Step:44/68  GlobalStep:89400  TotalLoss:0.00249  PostnetLoss:0.00120  DecoderLoss:0.00128  StopLoss:0.03689  GradNorm:0.00285  GradNormST:0.00951  AvgTextLen:33.8  AvgSpecLen:116.3  StepTime:0.64  LR:0.000100\n",
      "   | > Step:54/68  GlobalStep:89410  TotalLoss:0.00267  PostnetLoss:0.00129  DecoderLoss:0.00138  StopLoss:0.03661  GradNorm:0.00282  GradNormST:0.00724  AvgTextLen:42.2  AvgSpecLen:144.9  StepTime:0.87  LR:0.000100\n",
      "   | > Step:64/68  GlobalStep:89420  TotalLoss:0.00279  PostnetLoss:0.00134  DecoderLoss:0.00145  StopLoss:0.03553  GradNorm:0.00303  GradNormST:0.00891  AvgTextLen:60.6  AvgSpecLen:177.6  StepTime:0.83  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:89424  AvgTotalLoss:0.05381  AvgPostnetLoss:0.00110  AvgDecoderLoss:0.00117  AvgStopLoss:0.05154  EpochTime:42.77  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.09286   PostnetLoss: 0.00478   DecoderLoss:0.00520  StopLoss: 0.08288  \n",
      "   | > TotalLoss: 0.07097   PostnetLoss: 0.00711   DecoderLoss:0.00772  StopLoss: 0.05614  \n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00110   Validation Loss: 0.00656\n",
      "\n",
      " > Epoch 296/1000\n",
      "   | > Step:5/68  GlobalStep:89430  TotalLoss:0.00161  PostnetLoss:0.00078  DecoderLoss:0.00083  StopLoss:0.05040  GradNorm:0.00392  GradNormST:0.01362  AvgTextLen:5.9  AvgSpecLen:33.2  StepTime:0.35  LR:0.000100\n",
      "   | > Step:15/68  GlobalStep:89440  TotalLoss:0.00196  PostnetLoss:0.00095  DecoderLoss:0.00101  StopLoss:0.06135  GradNorm:0.00355  GradNormST:0.03146  AvgTextLen:12.8  AvgSpecLen:59.1  StepTime:0.55  LR:0.000100\n",
      "   | > Step:25/68  GlobalStep:89450  TotalLoss:0.00214  PostnetLoss:0.00104  DecoderLoss:0.00110  StopLoss:0.06260  GradNorm:0.00336  GradNormST:0.01790  AvgTextLen:19.8  AvgSpecLen:73.3  StepTime:0.49  LR:0.000100\n",
      "   | > Step:35/68  GlobalStep:89460  TotalLoss:0.00242  PostnetLoss:0.00117  DecoderLoss:0.00125  StopLoss:0.05451  GradNorm:0.00330  GradNormST:0.03020  AvgTextLen:27.1  AvgSpecLen:104.7  StepTime:0.70  LR:0.000100\n",
      "   | > Step:45/68  GlobalStep:89470  TotalLoss:0.00246  PostnetLoss:0.00119  DecoderLoss:0.00127  StopLoss:0.06592  GradNorm:0.00301  GradNormST:0.02311  AvgTextLen:34.4  AvgSpecLen:118.6  StepTime:0.64  LR:0.000100\n",
      "   | > Step:55/68  GlobalStep:89480  TotalLoss:0.00277  PostnetLoss:0.00133  DecoderLoss:0.00143  StopLoss:0.05496  GradNorm:0.00285  GradNormST:0.02368  AvgTextLen:43.9  AvgSpecLen:145.2  StepTime:0.81  LR:0.000100\n",
      "   | > Step:65/68  GlobalStep:89490  TotalLoss:0.00304  PostnetLoss:0.00146  DecoderLoss:0.00158  StopLoss:0.05673  GradNorm:0.00307  GradNormST:0.03943  AvgTextLen:64.2  AvgSpecLen:207.4  StepTime:1.23  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:89493  AvgTotalLoss:0.05946  AvgPostnetLoss:0.00112  AvgDecoderLoss:0.00119  AvgStopLoss:0.05715  EpochTime:43.86  AvgStepTime:0.64\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.09192   PostnetLoss: 0.00473   DecoderLoss:0.00515  StopLoss: 0.08205  \n",
      "   | > TotalLoss: 0.06669   PostnetLoss: 0.00738   DecoderLoss:0.00799  StopLoss: 0.05132  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00112   Validation Loss: 0.00656\n",
      "\n",
      " > Epoch 297/1000\n",
      "   | > Step:6/68  GlobalStep:89500  TotalLoss:0.00155  PostnetLoss:0.00076  DecoderLoss:0.00079  StopLoss:0.04926  GradNorm:0.00399  GradNormST:0.01454  AvgTextLen:6.0  AvgSpecLen:34.8  StepTime:0.40  LR:0.000100\n",
      "   | > Step:16/68  GlobalStep:89510  TotalLoss:0.00194  PostnetLoss:0.00094  DecoderLoss:0.00100  StopLoss:0.05136  GradNorm:0.00339  GradNormST:0.01366  AvgTextLen:13.8  AvgSpecLen:60.6  StepTime:0.33  LR:0.000100\n",
      "   | > Step:26/68  GlobalStep:89520  TotalLoss:0.00214  PostnetLoss:0.00104  DecoderLoss:0.00110  StopLoss:0.04928  GradNorm:0.00324  GradNormST:0.01280  AvgTextLen:20.3  AvgSpecLen:77.5  StepTime:0.51  LR:0.000100\n",
      "   | > Step:36/68  GlobalStep:89530  TotalLoss:0.00233  PostnetLoss:0.00113  DecoderLoss:0.00120  StopLoss:0.06047  GradNorm:0.00303  GradNormST:0.01219  AvgTextLen:28.5  AvgSpecLen:100.2  StepTime:0.59  LR:0.000100\n",
      "   | > Step:46/68  GlobalStep:89540  TotalLoss:0.00253  PostnetLoss:0.00122  DecoderLoss:0.00131  StopLoss:0.04554  GradNorm:0.00290  GradNormST:0.01596  AvgTextLen:35.4  AvgSpecLen:122.6  StepTime:0.87  LR:0.000100\n",
      "   | > Step:56/68  GlobalStep:89550  TotalLoss:0.00273  PostnetLoss:0.00131  DecoderLoss:0.00141  StopLoss:0.04380  GradNorm:0.00328  GradNormST:0.02948  AvgTextLen:45.8  AvgSpecLen:145.4  StepTime:1.03  LR:0.000100\n",
      "   | > Step:66/68  GlobalStep:89560  TotalLoss:0.00319  PostnetLoss:0.00153  DecoderLoss:0.00166  StopLoss:0.04910  GradNorm:0.00287  GradNormST:0.02866  AvgTextLen:71.2  AvgSpecLen:220.8  StepTime:1.07  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:89562  AvgTotalLoss:0.05548  AvgPostnetLoss:0.00110  AvgDecoderLoss:0.00118  AvgStopLoss:0.05321  EpochTime:44.18  AvgStepTime:0.64\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.09391   PostnetLoss: 0.00465   DecoderLoss:0.00506  StopLoss: 0.08419  \n",
      "   | > TotalLoss: 0.07011   PostnetLoss: 0.00742   DecoderLoss:0.00802  StopLoss: 0.05467  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00110   Validation Loss: 0.00671\n",
      "\n",
      " > Epoch 298/1000\n",
      "   | > Step:7/68  GlobalStep:89570  TotalLoss:0.00167  PostnetLoss:0.00081  DecoderLoss:0.00086  StopLoss:0.05871  GradNorm:0.00353  GradNormST:0.02710  AvgTextLen:7.4  AvgSpecLen:44.6  StepTime:0.39  LR:0.000100\n",
      "   | > Step:17/68  GlobalStep:89580  TotalLoss:0.00191  PostnetLoss:0.00093  DecoderLoss:0.00098  StopLoss:0.06420  GradNorm:0.00324  GradNormST:0.01453  AvgTextLen:14.7  AvgSpecLen:66.0  StepTime:0.43  LR:0.000100\n",
      "   | > Step:27/68  GlobalStep:89590  TotalLoss:0.00215  PostnetLoss:0.00104  DecoderLoss:0.00111  StopLoss:0.04465  GradNorm:0.00312  GradNormST:0.00887  AvgTextLen:21.0  AvgSpecLen:80.2  StepTime:0.63  LR:0.000100\n",
      "   | > Step:37/68  GlobalStep:89600  TotalLoss:0.00235  PostnetLoss:0.00114  DecoderLoss:0.00121  StopLoss:0.05062  GradNorm:0.00294  GradNormST:0.01421  AvgTextLen:29.3  AvgSpecLen:102.4  StepTime:0.49  LR:0.000100\n",
      "   | > Step:47/68  GlobalStep:89610  TotalLoss:0.00256  PostnetLoss:0.00124  DecoderLoss:0.00132  StopLoss:0.05044  GradNorm:0.00289  GradNormST:0.01229  AvgTextLen:36.7  AvgSpecLen:126.9  StepTime:0.70  LR:0.000100\n",
      "   | > Step:57/68  GlobalStep:89620  TotalLoss:0.00279  PostnetLoss:0.00135  DecoderLoss:0.00144  StopLoss:0.04138  GradNorm:0.00316  GradNormST:0.01407  AvgTextLen:47.1  AvgSpecLen:161.0  StepTime:0.92  LR:0.000100\n",
      "   | > Step:67/68  GlobalStep:89630  TotalLoss:0.00318  PostnetLoss:0.00153  DecoderLoss:0.00165  StopLoss:0.06035  GradNorm:0.00297  GradNormST:0.03771  AvgTextLen:80.6  AvgSpecLen:246.5  StepTime:1.23  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:89631  AvgTotalLoss:0.05334  AvgPostnetLoss:0.00110  AvgDecoderLoss:0.00117  AvgStopLoss:0.05107  EpochTime:43.84  AvgStepTime:0.64\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.09361   PostnetLoss: 0.00470   DecoderLoss:0.00512  StopLoss: 0.08379  \n",
      "   | > TotalLoss: 0.07016   PostnetLoss: 0.00730   DecoderLoss:0.00787  StopLoss: 0.05499  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00110   Validation Loss: 0.00658\n",
      "\n",
      " > Epoch 299/1000\n",
      "   | > Step:8/68  GlobalStep:89640  TotalLoss:0.00172  PostnetLoss:0.00084  DecoderLoss:0.00089  StopLoss:0.06247  GradNorm:0.00392  GradNormST:0.01884  AvgTextLen:8.3  AvgSpecLen:44.0  StepTime:0.33  LR:0.000100\n",
      "   | > Step:18/68  GlobalStep:89650  TotalLoss:0.00189  PostnetLoss:0.00092  DecoderLoss:0.00097  StopLoss:0.06561  GradNorm:0.00330  GradNormST:0.01468  AvgTextLen:15.2  AvgSpecLen:64.5  StepTime:0.46  LR:0.000100\n",
      "   | > Step:28/68  GlobalStep:89660  TotalLoss:0.00216  PostnetLoss:0.00105  DecoderLoss:0.00111  StopLoss:0.05245  GradNorm:0.00307  GradNormST:0.01077  AvgTextLen:22.1  AvgSpecLen:85.4  StepTime:0.50  LR:0.000100\n",
      "   | > Step:38/68  GlobalStep:89670  TotalLoss:0.00232  PostnetLoss:0.00112  DecoderLoss:0.00120  StopLoss:0.04054  GradNorm:0.00308  GradNormST:0.00795  AvgTextLen:29.4  AvgSpecLen:104.1  StepTime:0.59  LR:0.000100\n",
      "   | > Step:48/68  GlobalStep:89680  TotalLoss:0.00253  PostnetLoss:0.00122  DecoderLoss:0.00130  StopLoss:0.04110  GradNorm:0.00272  GradNormST:0.01028  AvgTextLen:37.5  AvgSpecLen:123.8  StepTime:0.60  LR:0.000100\n",
      "   | > Step:58/68  GlobalStep:89690  TotalLoss:0.00282  PostnetLoss:0.00136  DecoderLoss:0.00146  StopLoss:0.04563  GradNorm:0.00398  GradNormST:0.01031  AvgTextLen:47.8  AvgSpecLen:158.0  StepTime:0.93  LR:0.000100\n",
      "   | > Step:68/68  GlobalStep:89700  TotalLoss:0.00312  PostnetLoss:0.00149  DecoderLoss:0.00162  StopLoss:0.04504  GradNorm:0.00309  GradNormST:0.02909  AvgTextLen:103.0  AvgSpecLen:308.9  StepTime:1.89  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:89700  AvgTotalLoss:0.05181  AvgPostnetLoss:0.00110  AvgDecoderLoss:0.00117  AvgStopLoss:0.04955  EpochTime:43.92  AvgStepTime:0.64\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.09038   PostnetLoss: 0.00478   DecoderLoss:0.00518  StopLoss: 0.08042  \n",
      "   | > TotalLoss: 0.06878   PostnetLoss: 0.00737   DecoderLoss:0.00795  StopLoss: 0.05346  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00110   Validation Loss: 0.00659\n",
      "\n",
      " > Epoch 300/1000\n",
      "   | > Step:9/68  GlobalStep:89710  TotalLoss:0.00171  PostnetLoss:0.00083  DecoderLoss:0.00088  StopLoss:0.03861  GradNorm:0.00456  GradNormST:0.01021  AvgTextLen:9.0  AvgSpecLen:42.1  StepTime:0.45  LR:0.000100\n",
      "   | > Step:19/68  GlobalStep:89720  TotalLoss:0.00191  PostnetLoss:0.00093  DecoderLoss:0.00098  StopLoss:0.05791  GradNorm:0.00327  GradNormST:0.01799  AvgTextLen:15.5  AvgSpecLen:63.2  StepTime:0.45  LR:0.000100\n",
      "   | > Step:29/68  GlobalStep:89730  TotalLoss:0.00214  PostnetLoss:0.00104  DecoderLoss:0.00110  StopLoss:0.05350  GradNorm:0.00339  GradNormST:0.00910  AvgTextLen:21.8  AvgSpecLen:83.4  StepTime:0.59  LR:0.000100\n",
      "   | > Step:39/68  GlobalStep:89740  TotalLoss:0.00241  PostnetLoss:0.00117  DecoderLoss:0.00125  StopLoss:0.04188  GradNorm:0.00295  GradNormST:0.00988  AvgTextLen:30.2  AvgSpecLen:110.4  StepTime:0.73  LR:0.000100\n",
      "   | > Step:49/68  GlobalStep:89750  TotalLoss:0.00254  PostnetLoss:0.00123  DecoderLoss:0.00131  StopLoss:0.04090  GradNorm:0.00293  GradNormST:0.00818  AvgTextLen:38.2  AvgSpecLen:127.7  StepTime:0.77  LR:0.000100\n",
      "   | > Step:59/68  GlobalStep:89760  TotalLoss:0.00275  PostnetLoss:0.00132  DecoderLoss:0.00142  StopLoss:0.03264  GradNorm:0.00404  GradNormST:0.00717  AvgTextLen:50.5  AvgSpecLen:155.9  StepTime:0.88  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:89769  AvgTotalLoss:0.04902  AvgPostnetLoss:0.00110  AvgDecoderLoss:0.00117  AvgStopLoss:0.04675  EpochTime:43.75  AvgStepTime:0.63\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.09686   PostnetLoss: 0.00477   DecoderLoss:0.00518  StopLoss: 0.08691  \n",
      "   | > TotalLoss: 0.07328   PostnetLoss: 0.00779   DecoderLoss:0.00838  StopLoss: 0.05712  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00110   Validation Loss: 0.00689\n",
      "\n",
      " > Epoch 301/1000\n",
      "   | > Step:0/68  GlobalStep:89770  TotalLoss:0.00157  PostnetLoss:0.00076  DecoderLoss:0.00082  StopLoss:0.09161  GradNorm:0.00632  GradNormST:0.03047  AvgTextLen:3.0  AvgSpecLen:25.3  StepTime:0.57  LR:0.000100\n",
      "   | > Step:10/68  GlobalStep:89780  TotalLoss:0.00177  PostnetLoss:0.00086  DecoderLoss:0.00091  StopLoss:0.04718  GradNorm:0.00405  GradNormST:0.01109  AvgTextLen:9.8  AvgSpecLen:48.6  StepTime:0.61  LR:0.000100\n",
      "   | > Step:20/68  GlobalStep:89790  TotalLoss:0.00201  PostnetLoss:0.00098  DecoderLoss:0.00103  StopLoss:0.05226  GradNorm:0.00340  GradNormST:0.01532  AvgTextLen:16.4  AvgSpecLen:64.8  StepTime:0.52  LR:0.000100\n",
      "   | > Step:30/68  GlobalStep:89800  TotalLoss:0.00221  PostnetLoss:0.00107  DecoderLoss:0.00114  StopLoss:0.04457  GradNorm:0.00330  GradNormST:0.00801  AvgTextLen:22.9  AvgSpecLen:85.8  StepTime:0.62  LR:0.000100\n",
      "   | > Step:40/68  GlobalStep:89810  TotalLoss:0.00238  PostnetLoss:0.00115  DecoderLoss:0.00123  StopLoss:0.04759  GradNorm:0.00298  GradNormST:0.00969  AvgTextLen:30.9  AvgSpecLen:105.3  StepTime:0.49  LR:0.000100\n",
      "   | > Step:50/68  GlobalStep:89820  TotalLoss:0.00257  PostnetLoss:0.00124  DecoderLoss:0.00133  StopLoss:0.03531  GradNorm:0.00289  GradNormST:0.00859  AvgTextLen:38.8  AvgSpecLen:132.0  StepTime:0.82  LR:0.000100\n",
      "   | > Step:60/68  GlobalStep:89830  TotalLoss:0.00280  PostnetLoss:0.00135  DecoderLoss:0.00145  StopLoss:0.03564  GradNorm:0.00336  GradNormST:0.01084  AvgTextLen:52.0  AvgSpecLen:172.7  StepTime:1.08  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:89838  AvgTotalLoss:0.05094  AvgPostnetLoss:0.00109  AvgDecoderLoss:0.00117  AvgStopLoss:0.04868  EpochTime:43.87  AvgStepTime:0.64\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.09121   PostnetLoss: 0.00480   DecoderLoss:0.00521  StopLoss: 0.08120  \n",
      "   | > TotalLoss: 0.07161   PostnetLoss: 0.00749   DecoderLoss:0.00808  StopLoss: 0.05604  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00109   Validation Loss: 0.00672\n",
      "\n",
      " > Epoch 302/1000\n",
      "   | > Step:1/68  GlobalStep:89840  TotalLoss:0.00157  PostnetLoss:0.00077  DecoderLoss:0.00081  StopLoss:0.06057  GradNorm:0.00530  GradNormST:0.01983  AvgTextLen:3.7  AvgSpecLen:28.6  StepTime:0.35  LR:0.000100\n",
      "   | > Step:11/68  GlobalStep:89850  TotalLoss:0.00175  PostnetLoss:0.00085  DecoderLoss:0.00090  StopLoss:0.04495  GradNorm:0.00360  GradNormST:0.01319  AvgTextLen:10.3  AvgSpecLen:49.4  StepTime:0.33  LR:0.000100\n",
      "   | > Step:21/68  GlobalStep:89860  TotalLoss:0.00199  PostnetLoss:0.00097  DecoderLoss:0.00102  StopLoss:0.06103  GradNorm:0.00316  GradNormST:0.02158  AvgTextLen:16.6  AvgSpecLen:66.3  StepTime:0.43  LR:0.000100\n",
      "   | > Step:31/68  GlobalStep:89870  TotalLoss:0.00222  PostnetLoss:0.00108  DecoderLoss:0.00114  StopLoss:0.04286  GradNorm:0.00315  GradNormST:0.01162  AvgTextLen:24.1  AvgSpecLen:82.7  StepTime:0.56  LR:0.000100\n",
      "   | > Step:41/68  GlobalStep:89880  TotalLoss:0.00244  PostnetLoss:0.00118  DecoderLoss:0.00126  StopLoss:0.03406  GradNorm:0.00295  GradNormST:0.01043  AvgTextLen:32.3  AvgSpecLen:109.4  StepTime:0.56  LR:0.000100\n",
      "   | > Step:51/68  GlobalStep:89890  TotalLoss:0.00253  PostnetLoss:0.00122  DecoderLoss:0.00131  StopLoss:0.04002  GradNorm:0.00300  GradNormST:0.00891  AvgTextLen:39.2  AvgSpecLen:126.9  StepTime:0.76  LR:0.000100\n",
      "   | > Step:61/68  GlobalStep:89900  TotalLoss:0.00280  PostnetLoss:0.00135  DecoderLoss:0.00145  StopLoss:0.03921  GradNorm:0.00380  GradNormST:0.01943  AvgTextLen:54.6  AvgSpecLen:165.5  StepTime:0.95  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:89907  AvgTotalLoss:0.05140  AvgPostnetLoss:0.00109  AvgDecoderLoss:0.00116  AvgStopLoss:0.04914  EpochTime:44.40  AvgStepTime:0.64\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.09167   PostnetLoss: 0.00486   DecoderLoss:0.00527  StopLoss: 0.08154  \n",
      "   | > TotalLoss: 0.07359   PostnetLoss: 0.00772   DecoderLoss:0.00831  StopLoss: 0.05757  \n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00109   Validation Loss: 0.00688\n",
      "\n",
      " > Epoch 303/1000\n",
      "   | > Step:2/68  GlobalStep:89910  TotalLoss:0.00147  PostnetLoss:0.00072  DecoderLoss:0.00076  StopLoss:0.05391  GradNorm:0.00383  GradNormST:0.02276  AvgTextLen:4.0  AvgSpecLen:31.3  StepTime:0.45  LR:0.000100\n",
      "   | > Step:12/68  GlobalStep:89920  TotalLoss:0.00176  PostnetLoss:0.00085  DecoderLoss:0.00090  StopLoss:0.04909  GradNorm:0.00362  GradNormST:0.01342  AvgTextLen:11.0  AvgSpecLen:49.3  StepTime:0.45  LR:0.000100\n",
      "   | > Step:22/68  GlobalStep:89930  TotalLoss:0.00203  PostnetLoss:0.00099  DecoderLoss:0.00105  StopLoss:0.07299  GradNorm:0.00348  GradNormST:0.01482  AvgTextLen:18.0  AvgSpecLen:72.0  StepTime:0.45  LR:0.000100\n",
      "   | > Step:32/68  GlobalStep:89940  TotalLoss:0.00222  PostnetLoss:0.00107  DecoderLoss:0.00114  StopLoss:0.06520  GradNorm:0.00298  GradNormST:0.01187  AvgTextLen:24.4  AvgSpecLen:91.5  StepTime:0.47  LR:0.000100\n",
      "   | > Step:42/68  GlobalStep:89950  TotalLoss:0.00240  PostnetLoss:0.00116  DecoderLoss:0.00124  StopLoss:0.04709  GradNorm:0.00321  GradNormST:0.01350  AvgTextLen:33.1  AvgSpecLen:109.7  StepTime:0.62  LR:0.000100\n",
      "   | > Step:52/68  GlobalStep:89960  TotalLoss:0.00265  PostnetLoss:0.00128  DecoderLoss:0.00137  StopLoss:0.04481  GradNorm:0.00353  GradNormST:0.01436  AvgTextLen:42.1  AvgSpecLen:137.5  StepTime:0.80  LR:0.000100\n",
      "   | > Step:62/68  GlobalStep:89970  TotalLoss:0.00285  PostnetLoss:0.00137  DecoderLoss:0.00147  StopLoss:0.04434  GradNorm:0.00341  GradNormST:0.01226  AvgTextLen:54.0  AvgSpecLen:174.7  StepTime:0.76  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:89976  AvgTotalLoss:0.05066  AvgPostnetLoss:0.00109  AvgDecoderLoss:0.00116  AvgStopLoss:0.04840  EpochTime:44.01  AvgStepTime:0.64\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.08986   PostnetLoss: 0.00482   DecoderLoss:0.00524  StopLoss: 0.07980  \n",
      "   | > TotalLoss: 0.07459   PostnetLoss: 0.00744   DecoderLoss:0.00803  StopLoss: 0.05912  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00109   Validation Loss: 0.00666\n",
      "\n",
      " > Epoch 304/1000\n",
      "   | > Step:3/68  GlobalStep:89980  TotalLoss:0.00151  PostnetLoss:0.00073  DecoderLoss:0.00077  StopLoss:0.05986  GradNorm:0.00382  GradNormST:0.02394  AvgTextLen:4.7  AvgSpecLen:29.7  StepTime:0.33  LR:0.000100\n",
      "   | > Step:13/68  GlobalStep:89990  TotalLoss:0.00173  PostnetLoss:0.00084  DecoderLoss:0.00089  StopLoss:0.07488  GradNorm:0.00325  GradNormST:0.01982  AvgTextLen:11.5  AvgSpecLen:55.2  StepTime:0.37  LR:0.000100\n",
      "   | > Step:23/68  GlobalStep:90000  TotalLoss:0.00206  PostnetLoss:0.00100  DecoderLoss:0.00106  StopLoss:0.04913  GradNorm:0.00322  GradNormST:0.01106  AvgTextLen:18.3  AvgSpecLen:72.2  StepTime:0.39  LR:0.000100\n",
      " | | > Checkpoint saving : /vm/TTS/models/cartman_models/mozilla-no-loc-fattn-stopnet-sigmoid-loss_masking-August-15-2019_11+26AM-de6aeb9/checkpoint_90000.pth.tar\n",
      "warning: audio amplitude out of range, auto clipped.\n",
      "   | > Step:33/68  GlobalStep:90010  TotalLoss:0.00225  PostnetLoss:0.00109  DecoderLoss:0.00116  StopLoss:0.04787  GradNorm:0.00304  GradNormST:0.01098  AvgTextLen:25.9  AvgSpecLen:88.9  StepTime:0.52  LR:0.000100\n",
      "   | > Step:43/68  GlobalStep:90020  TotalLoss:0.00244  PostnetLoss:0.00118  DecoderLoss:0.00126  StopLoss:0.05660  GradNorm:0.00298  GradNormST:0.01044  AvgTextLen:33.4  AvgSpecLen:115.7  StepTime:0.69  LR:0.000100\n",
      "   | > Step:53/68  GlobalStep:90030  TotalLoss:0.00258  PostnetLoss:0.00124  DecoderLoss:0.00133  StopLoss:0.04366  GradNorm:0.00281  GradNormST:0.01116  AvgTextLen:42.4  AvgSpecLen:137.6  StepTime:0.77  LR:0.000100\n",
      "   | > Step:63/68  GlobalStep:90040  TotalLoss:0.00291  PostnetLoss:0.00140  DecoderLoss:0.00151  StopLoss:0.03555  GradNorm:0.00323  GradNormST:0.01665  AvgTextLen:58.6  AvgSpecLen:185.1  StepTime:0.92  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:90045  AvgTotalLoss:0.05116  AvgPostnetLoss:0.00109  AvgDecoderLoss:0.00116  AvgStopLoss:0.04890  EpochTime:44.18  AvgStepTime:0.64\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.09411   PostnetLoss: 0.00471   DecoderLoss:0.00512  StopLoss: 0.08428  \n",
      "   | > TotalLoss: 0.07449   PostnetLoss: 0.00745   DecoderLoss:0.00803  StopLoss: 0.05902  \n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00109   Validation Loss: 0.00669\n",
      "\n",
      " > Epoch 305/1000\n",
      "   | > Step:4/68  GlobalStep:90050  TotalLoss:0.00154  PostnetLoss:0.00075  DecoderLoss:0.00079  StopLoss:0.08437  GradNorm:0.00389  GradNormST:0.03892  AvgTextLen:4.8  AvgSpecLen:31.1  StepTime:0.27  LR:0.000100\n",
      "   | > Step:14/68  GlobalStep:90060  TotalLoss:0.00180  PostnetLoss:0.00088  DecoderLoss:0.00093  StopLoss:0.05392  GradNorm:0.00369  GradNormST:0.01733  AvgTextLen:12.2  AvgSpecLen:52.0  StepTime:0.29  LR:0.000100\n",
      "   | > Step:24/68  GlobalStep:90070  TotalLoss:0.00213  PostnetLoss:0.00103  DecoderLoss:0.00109  StopLoss:0.05330  GradNorm:0.00310  GradNormST:0.01184  AvgTextLen:19.2  AvgSpecLen:79.0  StepTime:0.40  LR:0.000100\n",
      "   | > Step:34/68  GlobalStep:90080  TotalLoss:0.00229  PostnetLoss:0.00111  DecoderLoss:0.00118  StopLoss:0.03450  GradNorm:0.00287  GradNormST:0.00636  AvgTextLen:26.2  AvgSpecLen:95.9  StepTime:0.72  LR:0.000100\n",
      "   | > Step:44/68  GlobalStep:90090  TotalLoss:0.00249  PostnetLoss:0.00121  DecoderLoss:0.00129  StopLoss:0.03524  GradNorm:0.00290  GradNormST:0.00861  AvgTextLen:33.8  AvgSpecLen:116.3  StepTime:0.67  LR:0.000100\n",
      "   | > Step:54/68  GlobalStep:90100  TotalLoss:0.00268  PostnetLoss:0.00129  DecoderLoss:0.00138  StopLoss:0.04032  GradNorm:0.00324  GradNormST:0.00906  AvgTextLen:42.2  AvgSpecLen:144.9  StepTime:0.82  LR:0.000100\n",
      "   | > Step:64/68  GlobalStep:90110  TotalLoss:0.00281  PostnetLoss:0.00135  DecoderLoss:0.00146  StopLoss:0.04109  GradNorm:0.00353  GradNormST:0.00926  AvgTextLen:60.6  AvgSpecLen:177.6  StepTime:0.82  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:90114  AvgTotalLoss:0.05087  AvgPostnetLoss:0.00109  AvgDecoderLoss:0.00117  AvgStopLoss:0.04862  EpochTime:44.51  AvgStepTime:0.65\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.08760   PostnetLoss: 0.00478   DecoderLoss:0.00519  StopLoss: 0.07763  \n",
      "   | > TotalLoss: 0.07406   PostnetLoss: 0.00772   DecoderLoss:0.00831  StopLoss: 0.05803  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00109   Validation Loss: 0.00678\n",
      "\n",
      " > Epoch 306/1000\n",
      "   | > Step:5/68  GlobalStep:90120  TotalLoss:0.00162  PostnetLoss:0.00078  DecoderLoss:0.00084  StopLoss:0.06972  GradNorm:0.00424  GradNormST:0.02660  AvgTextLen:5.9  AvgSpecLen:33.2  StepTime:0.37  LR:0.000100\n",
      "   | > Step:15/68  GlobalStep:90130  TotalLoss:0.00189  PostnetLoss:0.00092  DecoderLoss:0.00097  StopLoss:0.05053  GradNorm:0.00356  GradNormST:0.02011  AvgTextLen:12.8  AvgSpecLen:59.1  StepTime:0.54  LR:0.000100\n",
      "   | > Step:25/68  GlobalStep:90140  TotalLoss:0.00205  PostnetLoss:0.00099  DecoderLoss:0.00105  StopLoss:0.04460  GradNorm:0.00321  GradNormST:0.01231  AvgTextLen:19.8  AvgSpecLen:73.3  StepTime:0.45  LR:0.000100\n",
      "   | > Step:35/68  GlobalStep:90150  TotalLoss:0.00228  PostnetLoss:0.00110  DecoderLoss:0.00117  StopLoss:0.04773  GradNorm:0.00297  GradNormST:0.01503  AvgTextLen:27.1  AvgSpecLen:104.7  StepTime:0.70  LR:0.000100\n",
      "   | > Step:45/68  GlobalStep:90160  TotalLoss:0.00242  PostnetLoss:0.00117  DecoderLoss:0.00125  StopLoss:0.04973  GradNorm:0.00292  GradNormST:0.02212  AvgTextLen:34.4  AvgSpecLen:118.6  StepTime:0.65  LR:0.000100\n",
      "   | > Step:55/68  GlobalStep:90170  TotalLoss:0.00271  PostnetLoss:0.00131  DecoderLoss:0.00140  StopLoss:0.04133  GradNorm:0.00309  GradNormST:0.00800  AvgTextLen:43.9  AvgSpecLen:145.2  StepTime:0.75  LR:0.000100\n",
      "   | > Step:65/68  GlobalStep:90180  TotalLoss:0.00295  PostnetLoss:0.00142  DecoderLoss:0.00153  StopLoss:0.04640  GradNorm:0.00290  GradNormST:0.03087  AvgTextLen:64.2  AvgSpecLen:207.4  StepTime:1.21  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:90183  AvgTotalLoss:0.05141  AvgPostnetLoss:0.00109  AvgDecoderLoss:0.00116  AvgStopLoss:0.04915  EpochTime:44.27  AvgStepTime:0.64\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.08993   PostnetLoss: 0.00478   DecoderLoss:0.00519  StopLoss: 0.07996  \n",
      "   | > TotalLoss: 0.07443   PostnetLoss: 0.00743   DecoderLoss:0.00803  StopLoss: 0.05897  \n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00109   Validation Loss: 0.00667\n",
      "\n",
      " > Epoch 307/1000\n",
      "   | > Step:6/68  GlobalStep:90190  TotalLoss:0.00157  PostnetLoss:0.00076  DecoderLoss:0.00081  StopLoss:0.04070  GradNorm:0.00364  GradNormST:0.01344  AvgTextLen:6.0  AvgSpecLen:34.8  StepTime:0.36  LR:0.000100\n",
      "   | > Step:16/68  GlobalStep:90200  TotalLoss:0.00189  PostnetLoss:0.00092  DecoderLoss:0.00097  StopLoss:0.04014  GradNorm:0.00375  GradNormST:0.01411  AvgTextLen:13.8  AvgSpecLen:60.6  StepTime:0.37  LR:0.000100\n",
      "   | > Step:26/68  GlobalStep:90210  TotalLoss:0.00213  PostnetLoss:0.00104  DecoderLoss:0.00110  StopLoss:0.04348  GradNorm:0.00334  GradNormST:0.01151  AvgTextLen:20.3  AvgSpecLen:77.5  StepTime:0.39  LR:0.000100\n",
      "   | > Step:36/68  GlobalStep:90220  TotalLoss:0.00231  PostnetLoss:0.00112  DecoderLoss:0.00119  StopLoss:0.05114  GradNorm:0.00298  GradNormST:0.00892  AvgTextLen:28.5  AvgSpecLen:100.2  StepTime:0.69  LR:0.000100\n",
      "   | > Step:46/68  GlobalStep:90230  TotalLoss:0.00245  PostnetLoss:0.00118  DecoderLoss:0.00127  StopLoss:0.04052  GradNorm:0.00290  GradNormST:0.00796  AvgTextLen:35.4  AvgSpecLen:122.6  StepTime:0.80  LR:0.000100\n",
      "   | > Step:56/68  GlobalStep:90240  TotalLoss:0.00269  PostnetLoss:0.00130  DecoderLoss:0.00139  StopLoss:0.03431  GradNorm:0.00355  GradNormST:0.01178  AvgTextLen:45.8  AvgSpecLen:145.4  StepTime:0.84  LR:0.000100\n",
      "   | > Step:66/68  GlobalStep:90250  TotalLoss:0.00312  PostnetLoss:0.00150  DecoderLoss:0.00162  StopLoss:0.03473  GradNorm:0.00303  GradNormST:0.01088  AvgTextLen:71.2  AvgSpecLen:220.8  StepTime:1.12  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:90252  AvgTotalLoss:0.04859  AvgPostnetLoss:0.00109  AvgDecoderLoss:0.00116  AvgStopLoss:0.04634  EpochTime:43.16  AvgStepTime:0.63\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.08774   PostnetLoss: 0.00481   DecoderLoss:0.00523  StopLoss: 0.07770  \n",
      "   | > TotalLoss: 0.07119   PostnetLoss: 0.00743   DecoderLoss:0.00802  StopLoss: 0.05575  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00109   Validation Loss: 0.00671\n",
      "\n",
      " > Epoch 308/1000\n",
      "   | > Step:7/68  GlobalStep:90260  TotalLoss:0.00170  PostnetLoss:0.00083  DecoderLoss:0.00087  StopLoss:0.04127  GradNorm:0.00447  GradNormST:0.01739  AvgTextLen:7.4  AvgSpecLen:44.6  StepTime:0.40  LR:0.000100\n",
      "   | > Step:17/68  GlobalStep:90270  TotalLoss:0.00188  PostnetLoss:0.00091  DecoderLoss:0.00097  StopLoss:0.07206  GradNorm:0.00336  GradNormST:0.01279  AvgTextLen:14.7  AvgSpecLen:66.0  StepTime:0.33  LR:0.000100\n",
      "   | > Step:27/68  GlobalStep:90280  TotalLoss:0.00213  PostnetLoss:0.00103  DecoderLoss:0.00110  StopLoss:0.04519  GradNorm:0.00309  GradNormST:0.01102  AvgTextLen:21.0  AvgSpecLen:80.2  StepTime:0.56  LR:0.000100\n",
      "   | > Step:37/68  GlobalStep:90290  TotalLoss:0.00237  PostnetLoss:0.00115  DecoderLoss:0.00122  StopLoss:0.04082  GradNorm:0.00298  GradNormST:0.00611  AvgTextLen:29.3  AvgSpecLen:102.4  StepTime:0.64  LR:0.000100\n",
      "   | > Step:47/68  GlobalStep:90300  TotalLoss:0.00255  PostnetLoss:0.00123  DecoderLoss:0.00132  StopLoss:0.04031  GradNorm:0.00311  GradNormST:0.01038  AvgTextLen:36.7  AvgSpecLen:126.9  StepTime:0.82  LR:0.000100\n",
      "   | > Step:57/68  GlobalStep:90310  TotalLoss:0.00276  PostnetLoss:0.00133  DecoderLoss:0.00143  StopLoss:0.04587  GradNorm:0.00328  GradNormST:0.00986  AvgTextLen:47.1  AvgSpecLen:161.0  StepTime:0.94  LR:0.000100\n",
      "   | > Step:67/68  GlobalStep:90320  TotalLoss:0.00327  PostnetLoss:0.00157  DecoderLoss:0.00170  StopLoss:0.03831  GradNorm:0.00492  GradNormST:0.01240  AvgTextLen:80.6  AvgSpecLen:246.5  StepTime:1.23  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:90321  AvgTotalLoss:0.04931  AvgPostnetLoss:0.00109  AvgDecoderLoss:0.00116  AvgStopLoss:0.04706  EpochTime:45.16  AvgStepTime:0.65\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.08329   PostnetLoss: 0.00484   DecoderLoss:0.00527  StopLoss: 0.07318  \n",
      "   | > TotalLoss: 0.07031   PostnetLoss: 0.00725   DecoderLoss:0.00785  StopLoss: 0.05521  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00109   Validation Loss: 0.00666\n",
      "\n",
      " > Epoch 309/1000\n",
      "   | > Step:8/68  GlobalStep:90330  TotalLoss:0.00187  PostnetLoss:0.00091  DecoderLoss:0.00096  StopLoss:0.06604  GradNorm:0.00525  GradNormST:0.01428  AvgTextLen:8.3  AvgSpecLen:44.0  StepTime:0.33  LR:0.000100\n",
      "   | > Step:18/68  GlobalStep:90340  TotalLoss:0.00213  PostnetLoss:0.00103  DecoderLoss:0.00110  StopLoss:0.07645  GradNorm:0.00454  GradNormST:0.02515  AvgTextLen:15.2  AvgSpecLen:64.5  StepTime:0.44  LR:0.000100\n",
      "   | > Step:28/68  GlobalStep:90350  TotalLoss:0.00243  PostnetLoss:0.00118  DecoderLoss:0.00125  StopLoss:0.08022  GradNorm:0.00417  GradNormST:0.02329  AvgTextLen:22.1  AvgSpecLen:85.4  StepTime:0.47  LR:0.000100\n",
      "   | > Step:38/68  GlobalStep:90360  TotalLoss:0.00247  PostnetLoss:0.00120  DecoderLoss:0.00128  StopLoss:0.06688  GradNorm:0.00346  GradNormST:0.03452  AvgTextLen:29.4  AvgSpecLen:104.1  StepTime:0.50  LR:0.000100\n",
      "   | > Step:48/68  GlobalStep:90370  TotalLoss:0.00273  PostnetLoss:0.00132  DecoderLoss:0.00141  StopLoss:0.07634  GradNorm:0.00339  GradNormST:0.04069  AvgTextLen:37.5  AvgSpecLen:123.8  StepTime:0.71  LR:0.000100\n",
      "   | > Step:58/68  GlobalStep:90380  TotalLoss:0.00296  PostnetLoss:0.00143  DecoderLoss:0.00153  StopLoss:0.06182  GradNorm:0.00347  GradNormST:0.02885  AvgTextLen:47.8  AvgSpecLen:158.0  StepTime:0.83  LR:0.000100\n",
      "   | > Step:68/68  GlobalStep:90390  TotalLoss:0.00322  PostnetLoss:0.00154  DecoderLoss:0.00168  StopLoss:0.04930  GradNorm:0.00863  GradNormST:0.02423  AvgTextLen:103.0  AvgSpecLen:308.9  StepTime:1.98  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:90390  AvgTotalLoss:0.07230  AvgPostnetLoss:0.00118  AvgDecoderLoss:0.00127  AvgStopLoss:0.06985  EpochTime:43.43  AvgStepTime:0.63\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.08834   PostnetLoss: 0.00464   DecoderLoss:0.00506  StopLoss: 0.07863  \n",
      "   | > TotalLoss: 0.06996   PostnetLoss: 0.00676   DecoderLoss:0.00737  StopLoss: 0.05583  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00118   Validation Loss: 0.00612\n",
      "\n",
      " > Epoch 310/1000\n",
      "   | > Step:9/68  GlobalStep:90400  TotalLoss:0.00177  PostnetLoss:0.00086  DecoderLoss:0.00091  StopLoss:0.06235  GradNorm:0.00408  GradNormST:0.03462  AvgTextLen:9.0  AvgSpecLen:42.1  StepTime:0.45  LR:0.000100\n",
      "   | > Step:19/68  GlobalStep:90410  TotalLoss:0.00201  PostnetLoss:0.00097  DecoderLoss:0.00103  StopLoss:0.08866  GradNorm:0.00384  GradNormST:0.01987  AvgTextLen:15.5  AvgSpecLen:63.2  StepTime:0.44  LR:0.000100\n",
      "   | > Step:29/68  GlobalStep:90420  TotalLoss:0.00220  PostnetLoss:0.00107  DecoderLoss:0.00113  StopLoss:0.07152  GradNorm:0.00351  GradNormST:0.02125  AvgTextLen:21.8  AvgSpecLen:83.4  StepTime:0.59  LR:0.000100\n",
      "   | > Step:39/68  GlobalStep:90430  TotalLoss:0.00247  PostnetLoss:0.00119  DecoderLoss:0.00128  StopLoss:0.04088  GradNorm:0.00309  GradNormST:0.01490  AvgTextLen:30.2  AvgSpecLen:110.4  StepTime:0.76  LR:0.000100\n",
      "   | > Step:49/68  GlobalStep:90440  TotalLoss:0.00262  PostnetLoss:0.00127  DecoderLoss:0.00136  StopLoss:0.04801  GradNorm:0.00289  GradNormST:0.01351  AvgTextLen:38.2  AvgSpecLen:127.7  StepTime:0.72  LR:0.000100\n",
      "   | > Step:59/68  GlobalStep:90450  TotalLoss:0.00282  PostnetLoss:0.00136  DecoderLoss:0.00146  StopLoss:0.05102  GradNorm:0.00306  GradNormST:0.02949  AvgTextLen:50.5  AvgSpecLen:155.9  StepTime:0.82  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:90459  AvgTotalLoss:0.06183  AvgPostnetLoss:0.00113  AvgDecoderLoss:0.00121  AvgStopLoss:0.05949  EpochTime:43.26  AvgStepTime:0.63\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.09251   PostnetLoss: 0.00470   DecoderLoss:0.00510  StopLoss: 0.08271  \n",
      "   | > TotalLoss: 0.07500   PostnetLoss: 0.00687   DecoderLoss:0.00745  StopLoss: 0.06069  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00113   Validation Loss: 0.00635\n",
      "\n",
      " > Epoch 311/1000\n",
      "   | > Step:0/68  GlobalStep:90460  TotalLoss:0.00165  PostnetLoss:0.00079  DecoderLoss:0.00086  StopLoss:0.07564  GradNorm:0.00619  GradNormST:0.02405  AvgTextLen:3.0  AvgSpecLen:25.3  StepTime:0.39  LR:0.000100\n",
      "   | > Step:10/68  GlobalStep:90470  TotalLoss:0.00179  PostnetLoss:0.00087  DecoderLoss:0.00092  StopLoss:0.05437  GradNorm:0.00381  GradNormST:0.01418  AvgTextLen:9.8  AvgSpecLen:48.6  StepTime:0.57  LR:0.000100\n",
      "   | > Step:20/68  GlobalStep:90480  TotalLoss:0.00210  PostnetLoss:0.00102  DecoderLoss:0.00108  StopLoss:0.05166  GradNorm:0.00387  GradNormST:0.01311  AvgTextLen:16.4  AvgSpecLen:64.8  StepTime:0.51  LR:0.000100\n",
      "   | > Step:30/68  GlobalStep:90490  TotalLoss:0.00224  PostnetLoss:0.00109  DecoderLoss:0.00116  StopLoss:0.05681  GradNorm:0.00369  GradNormST:0.01252  AvgTextLen:22.9  AvgSpecLen:85.8  StepTime:0.57  LR:0.000100\n",
      "   | > Step:40/68  GlobalStep:90500  TotalLoss:0.00242  PostnetLoss:0.00117  DecoderLoss:0.00125  StopLoss:0.04822  GradNorm:0.00338  GradNormST:0.01078  AvgTextLen:30.9  AvgSpecLen:105.3  StepTime:0.52  LR:0.000100\n",
      "   | > Step:50/68  GlobalStep:90510  TotalLoss:0.00262  PostnetLoss:0.00126  DecoderLoss:0.00136  StopLoss:0.03983  GradNorm:0.00297  GradNormST:0.00817  AvgTextLen:38.8  AvgSpecLen:132.0  StepTime:0.80  LR:0.000100\n",
      "   | > Step:60/68  GlobalStep:90520  TotalLoss:0.00285  PostnetLoss:0.00137  DecoderLoss:0.00147  StopLoss:0.03669  GradNorm:0.00290  GradNormST:0.01266  AvgTextLen:52.0  AvgSpecLen:172.7  StepTime:1.01  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:90528  AvgTotalLoss:0.05518  AvgPostnetLoss:0.00111  AvgDecoderLoss:0.00119  AvgStopLoss:0.05288  EpochTime:44.30  AvgStepTime:0.64\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.09809   PostnetLoss: 0.00471   DecoderLoss:0.00511  StopLoss: 0.08826  \n",
      "   | > TotalLoss: 0.07738   PostnetLoss: 0.00714   DecoderLoss:0.00775  StopLoss: 0.06249  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00111   Validation Loss: 0.00643\n",
      "\n",
      " > Epoch 312/1000\n",
      "   | > Step:1/68  GlobalStep:90530  TotalLoss:0.00161  PostnetLoss:0.00078  DecoderLoss:0.00083  StopLoss:0.06051  GradNorm:0.00619  GradNormST:0.02783  AvgTextLen:3.7  AvgSpecLen:28.6  StepTime:0.37  LR:0.000100\n",
      "   | > Step:11/68  GlobalStep:90540  TotalLoss:0.00174  PostnetLoss:0.00085  DecoderLoss:0.00089  StopLoss:0.06036  GradNorm:0.00354  GradNormST:0.01889  AvgTextLen:10.3  AvgSpecLen:49.4  StepTime:0.39  LR:0.000100\n",
      "   | > Step:21/68  GlobalStep:90550  TotalLoss:0.00204  PostnetLoss:0.00099  DecoderLoss:0.00105  StopLoss:0.06935  GradNorm:0.00333  GradNormST:0.02286  AvgTextLen:16.6  AvgSpecLen:66.3  StepTime:0.36  LR:0.000100\n",
      "   | > Step:31/68  GlobalStep:90560  TotalLoss:0.00222  PostnetLoss:0.00108  DecoderLoss:0.00114  StopLoss:0.04241  GradNorm:0.00352  GradNormST:0.01599  AvgTextLen:24.1  AvgSpecLen:82.7  StepTime:0.51  LR:0.000100\n",
      "   | > Step:41/68  GlobalStep:90570  TotalLoss:0.00245  PostnetLoss:0.00119  DecoderLoss:0.00126  StopLoss:0.04712  GradNorm:0.00340  GradNormST:0.02104  AvgTextLen:32.3  AvgSpecLen:109.4  StepTime:0.67  LR:0.000100\n",
      "   | > Step:51/68  GlobalStep:90580  TotalLoss:0.00259  PostnetLoss:0.00125  DecoderLoss:0.00134  StopLoss:0.04092  GradNorm:0.00327  GradNormST:0.01021  AvgTextLen:39.2  AvgSpecLen:126.9  StepTime:0.65  LR:0.000100\n",
      "   | > Step:61/68  GlobalStep:90590  TotalLoss:0.00279  PostnetLoss:0.00135  DecoderLoss:0.00145  StopLoss:0.04353  GradNorm:0.00275  GradNormST:0.01713  AvgTextLen:54.6  AvgSpecLen:165.5  StepTime:0.96  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:90597  AvgTotalLoss:0.05488  AvgPostnetLoss:0.00110  AvgDecoderLoss:0.00118  AvgStopLoss:0.05260  EpochTime:42.95  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.09621   PostnetLoss: 0.00478   DecoderLoss:0.00519  StopLoss: 0.08625  \n",
      "   | > TotalLoss: 0.07689   PostnetLoss: 0.00717   DecoderLoss:0.00778  StopLoss: 0.06194  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00110   Validation Loss: 0.00651\n",
      "\n",
      " > Epoch 313/1000\n",
      "   | > Step:2/68  GlobalStep:90600  TotalLoss:0.00147  PostnetLoss:0.00071  DecoderLoss:0.00076  StopLoss:0.08903  GradNorm:0.00397  GradNormST:0.02387  AvgTextLen:4.0  AvgSpecLen:31.3  StepTime:0.44  LR:0.000100\n",
      "   | > Step:12/68  GlobalStep:90610  TotalLoss:0.00171  PostnetLoss:0.00083  DecoderLoss:0.00088  StopLoss:0.05360  GradNorm:0.00349  GradNormST:0.01558  AvgTextLen:11.0  AvgSpecLen:49.3  StepTime:0.47  LR:0.000100\n",
      "   | > Step:22/68  GlobalStep:90620  TotalLoss:0.00201  PostnetLoss:0.00097  DecoderLoss:0.00103  StopLoss:0.06206  GradNorm:0.00338  GradNormST:0.01580  AvgTextLen:18.0  AvgSpecLen:72.0  StepTime:0.40  LR:0.000100\n",
      "   | > Step:32/68  GlobalStep:90630  TotalLoss:0.00224  PostnetLoss:0.00109  DecoderLoss:0.00115  StopLoss:0.05353  GradNorm:0.00319  GradNormST:0.01195  AvgTextLen:24.4  AvgSpecLen:91.5  StepTime:0.61  LR:0.000100\n",
      "   | > Step:42/68  GlobalStep:90640  TotalLoss:0.00242  PostnetLoss:0.00117  DecoderLoss:0.00125  StopLoss:0.05482  GradNorm:0.00313  GradNormST:0.01965  AvgTextLen:33.1  AvgSpecLen:109.7  StepTime:0.63  LR:0.000100\n",
      "   | > Step:52/68  GlobalStep:90650  TotalLoss:0.00263  PostnetLoss:0.00127  DecoderLoss:0.00136  StopLoss:0.05262  GradNorm:0.00292  GradNormST:0.01465  AvgTextLen:42.1  AvgSpecLen:137.5  StepTime:0.84  LR:0.000100\n",
      "   | > Step:62/68  GlobalStep:90660  TotalLoss:0.00286  PostnetLoss:0.00138  DecoderLoss:0.00148  StopLoss:0.03975  GradNorm:0.00275  GradNormST:0.01146  AvgTextLen:54.0  AvgSpecLen:174.7  StepTime:0.75  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:90666  AvgTotalLoss:0.05444  AvgPostnetLoss:0.00110  AvgDecoderLoss:0.00117  AvgStopLoss:0.05217  EpochTime:42.48  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.09209   PostnetLoss: 0.00475   DecoderLoss:0.00518  StopLoss: 0.08216  \n",
      "   | > TotalLoss: 0.07320   PostnetLoss: 0.00694   DecoderLoss:0.00753  StopLoss: 0.05873  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00110   Validation Loss: 0.00637\n",
      "\n",
      " > Epoch 314/1000\n",
      "   | > Step:3/68  GlobalStep:90670  TotalLoss:0.00152  PostnetLoss:0.00074  DecoderLoss:0.00078  StopLoss:0.06061  GradNorm:0.00375  GradNormST:0.02426  AvgTextLen:4.7  AvgSpecLen:29.7  StepTime:0.28  LR:0.000100\n",
      "   | > Step:13/68  GlobalStep:90680  TotalLoss:0.00175  PostnetLoss:0.00085  DecoderLoss:0.00090  StopLoss:0.05826  GradNorm:0.00394  GradNormST:0.01636  AvgTextLen:11.5  AvgSpecLen:55.2  StepTime:0.42  LR:0.000100\n",
      "   | > Step:23/68  GlobalStep:90690  TotalLoss:0.00209  PostnetLoss:0.00101  DecoderLoss:0.00108  StopLoss:0.04368  GradNorm:0.00337  GradNormST:0.01151  AvgTextLen:18.3  AvgSpecLen:72.2  StepTime:0.45  LR:0.000100\n",
      "   | > Step:33/68  GlobalStep:90700  TotalLoss:0.00226  PostnetLoss:0.00110  DecoderLoss:0.00116  StopLoss:0.05534  GradNorm:0.00323  GradNormST:0.01256  AvgTextLen:25.9  AvgSpecLen:88.9  StepTime:0.45  LR:0.000100\n",
      "   | > Step:43/68  GlobalStep:90710  TotalLoss:0.00246  PostnetLoss:0.00119  DecoderLoss:0.00127  StopLoss:0.04901  GradNorm:0.00299  GradNormST:0.01221  AvgTextLen:33.4  AvgSpecLen:115.7  StepTime:0.66  LR:0.000100\n",
      "   | > Step:53/68  GlobalStep:90720  TotalLoss:0.00261  PostnetLoss:0.00126  DecoderLoss:0.00135  StopLoss:0.04339  GradNorm:0.00288  GradNormST:0.00808  AvgTextLen:42.4  AvgSpecLen:137.6  StepTime:0.82  LR:0.000100\n",
      "   | > Step:63/68  GlobalStep:90730  TotalLoss:0.00294  PostnetLoss:0.00141  DecoderLoss:0.00153  StopLoss:0.03573  GradNorm:0.00292  GradNormST:0.01384  AvgTextLen:58.6  AvgSpecLen:185.1  StepTime:0.93  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:90735  AvgTotalLoss:0.05174  AvgPostnetLoss:0.00109  AvgDecoderLoss:0.00117  AvgStopLoss:0.04948  EpochTime:44.07  AvgStepTime:0.64\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.09511   PostnetLoss: 0.00478   DecoderLoss:0.00519  StopLoss: 0.08515  \n",
      "   | > TotalLoss: 0.07305   PostnetLoss: 0.00699   DecoderLoss:0.00758  StopLoss: 0.05848  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00109   Validation Loss: 0.00642\n",
      "\n",
      " > Epoch 315/1000\n",
      "   | > Step:4/68  GlobalStep:90740  TotalLoss:0.00153  PostnetLoss:0.00074  DecoderLoss:0.00079  StopLoss:0.10080  GradNorm:0.00425  GradNormST:0.04568  AvgTextLen:4.8  AvgSpecLen:31.1  StepTime:0.21  LR:0.000100\n",
      "   | > Step:14/68  GlobalStep:90750  TotalLoss:0.00181  PostnetLoss:0.00088  DecoderLoss:0.00093  StopLoss:0.06076  GradNorm:0.00349  GradNormST:0.01548  AvgTextLen:12.2  AvgSpecLen:52.0  StepTime:0.32  LR:0.000100\n",
      "   | > Step:24/68  GlobalStep:90760  TotalLoss:0.00214  PostnetLoss:0.00104  DecoderLoss:0.00110  StopLoss:0.05210  GradNorm:0.00334  GradNormST:0.01305  AvgTextLen:19.2  AvgSpecLen:79.0  StepTime:0.41  LR:0.000100\n",
      "   | > Step:34/68  GlobalStep:90770  TotalLoss:0.00229  PostnetLoss:0.00110  DecoderLoss:0.00118  StopLoss:0.03827  GradNorm:0.00331  GradNormST:0.00741  AvgTextLen:26.2  AvgSpecLen:95.9  StepTime:0.61  LR:0.000100\n",
      "   | > Step:44/68  GlobalStep:90780  TotalLoss:0.00250  PostnetLoss:0.00121  DecoderLoss:0.00129  StopLoss:0.03381  GradNorm:0.00307  GradNormST:0.00893  AvgTextLen:33.8  AvgSpecLen:116.3  StepTime:0.72  LR:0.000100\n",
      "   | > Step:54/68  GlobalStep:90790  TotalLoss:0.00264  PostnetLoss:0.00127  DecoderLoss:0.00137  StopLoss:0.04170  GradNorm:0.00301  GradNormST:0.01012  AvgTextLen:42.2  AvgSpecLen:144.9  StepTime:0.87  LR:0.000100\n",
      "   | > Step:64/68  GlobalStep:90800  TotalLoss:0.00279  PostnetLoss:0.00134  DecoderLoss:0.00145  StopLoss:0.03965  GradNorm:0.00300  GradNormST:0.01243  AvgTextLen:60.6  AvgSpecLen:177.6  StepTime:0.84  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:90804  AvgTotalLoss:0.05219  AvgPostnetLoss:0.00109  AvgDecoderLoss:0.00116  AvgStopLoss:0.04994  EpochTime:44.32  AvgStepTime:0.64\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.09455   PostnetLoss: 0.00481   DecoderLoss:0.00522  StopLoss: 0.08452  \n",
      "   | > TotalLoss: 0.07439   PostnetLoss: 0.00717   DecoderLoss:0.00777  StopLoss: 0.05945  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00109   Validation Loss: 0.00650\n",
      "\n",
      " > Epoch 316/1000\n",
      "   | > Step:5/68  GlobalStep:90810  TotalLoss:0.00157  PostnetLoss:0.00076  DecoderLoss:0.00081  StopLoss:0.07562  GradNorm:0.00397  GradNormST:0.02461  AvgTextLen:5.9  AvgSpecLen:33.2  StepTime:0.38  LR:0.000100\n",
      "   | > Step:15/68  GlobalStep:90820  TotalLoss:0.00189  PostnetLoss:0.00092  DecoderLoss:0.00097  StopLoss:0.04870  GradNorm:0.00340  GradNormST:0.01240  AvgTextLen:12.8  AvgSpecLen:59.1  StepTime:0.52  LR:0.000100\n",
      "   | > Step:25/68  GlobalStep:90830  TotalLoss:0.00205  PostnetLoss:0.00099  DecoderLoss:0.00105  StopLoss:0.05727  GradNorm:0.00305  GradNormST:0.01187  AvgTextLen:19.8  AvgSpecLen:73.3  StepTime:0.49  LR:0.000100\n",
      "   | > Step:35/68  GlobalStep:90840  TotalLoss:0.00230  PostnetLoss:0.00111  DecoderLoss:0.00119  StopLoss:0.05208  GradNorm:0.00308  GradNormST:0.02026  AvgTextLen:27.1  AvgSpecLen:104.7  StepTime:0.83  LR:0.000100\n",
      "   | > Step:45/68  GlobalStep:90850  TotalLoss:0.00241  PostnetLoss:0.00116  DecoderLoss:0.00125  StopLoss:0.04850  GradNorm:0.00306  GradNormST:0.01854  AvgTextLen:34.4  AvgSpecLen:118.6  StepTime:0.64  LR:0.000100\n",
      "   | > Step:55/68  GlobalStep:90860  TotalLoss:0.00271  PostnetLoss:0.00131  DecoderLoss:0.00140  StopLoss:0.04679  GradNorm:0.00275  GradNormST:0.00859  AvgTextLen:43.9  AvgSpecLen:145.2  StepTime:0.87  LR:0.000100\n",
      "   | > Step:65/68  GlobalStep:90870  TotalLoss:0.00299  PostnetLoss:0.00144  DecoderLoss:0.00155  StopLoss:0.06016  GradNorm:0.00277  GradNormST:0.03587  AvgTextLen:64.2  AvgSpecLen:207.4  StepTime:1.20  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:90873  AvgTotalLoss:0.05252  AvgPostnetLoss:0.00109  AvgDecoderLoss:0.00116  AvgStopLoss:0.05027  EpochTime:44.53  AvgStepTime:0.65\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.09315   PostnetLoss: 0.00477   DecoderLoss:0.00519  StopLoss: 0.08319  \n",
      "   | > TotalLoss: 0.07166   PostnetLoss: 0.00698   DecoderLoss:0.00758  StopLoss: 0.05711  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00109   Validation Loss: 0.00642\n",
      "\n",
      " > Epoch 317/1000\n",
      "   | > Step:6/68  GlobalStep:90880  TotalLoss:0.00154  PostnetLoss:0.00075  DecoderLoss:0.00079  StopLoss:0.04699  GradNorm:0.00361  GradNormST:0.02394  AvgTextLen:6.0  AvgSpecLen:34.8  StepTime:0.35  LR:0.000100\n",
      "   | > Step:16/68  GlobalStep:90890  TotalLoss:0.00186  PostnetLoss:0.00090  DecoderLoss:0.00096  StopLoss:0.04727  GradNorm:0.00347  GradNormST:0.01197  AvgTextLen:13.8  AvgSpecLen:60.6  StepTime:0.38  LR:0.000100\n",
      "   | > Step:26/68  GlobalStep:90900  TotalLoss:0.00216  PostnetLoss:0.00105  DecoderLoss:0.00111  StopLoss:0.04876  GradNorm:0.00332  GradNormST:0.01264  AvgTextLen:20.3  AvgSpecLen:77.5  StepTime:0.50  LR:0.000100\n",
      "   | > Step:36/68  GlobalStep:90910  TotalLoss:0.00230  PostnetLoss:0.00111  DecoderLoss:0.00119  StopLoss:0.06940  GradNorm:0.00302  GradNormST:0.01248  AvgTextLen:28.5  AvgSpecLen:100.2  StepTime:0.68  LR:0.000100\n",
      "   | > Step:46/68  GlobalStep:90920  TotalLoss:0.00245  PostnetLoss:0.00118  DecoderLoss:0.00126  StopLoss:0.04289  GradNorm:0.00299  GradNormST:0.00848  AvgTextLen:35.4  AvgSpecLen:122.6  StepTime:0.85  LR:0.000100\n",
      "   | > Step:56/68  GlobalStep:90930  TotalLoss:0.00268  PostnetLoss:0.00129  DecoderLoss:0.00139  StopLoss:0.04151  GradNorm:0.00281  GradNormST:0.01675  AvgTextLen:45.8  AvgSpecLen:145.4  StepTime:0.98  LR:0.000100\n",
      "   | > Step:66/68  GlobalStep:90940  TotalLoss:0.00313  PostnetLoss:0.00150  DecoderLoss:0.00163  StopLoss:0.04154  GradNorm:0.00287  GradNormST:0.01234  AvgTextLen:71.2  AvgSpecLen:220.8  StepTime:1.06  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:90942  AvgTotalLoss:0.05327  AvgPostnetLoss:0.00109  AvgDecoderLoss:0.00116  AvgStopLoss:0.05102  EpochTime:43.32  AvgStepTime:0.63\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.09171   PostnetLoss: 0.00465   DecoderLoss:0.00506  StopLoss: 0.08200  \n",
      "   | > TotalLoss: 0.07098   PostnetLoss: 0.00704   DecoderLoss:0.00763  StopLoss: 0.05631  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00109   Validation Loss: 0.00644\n",
      "\n",
      " > Epoch 318/1000\n",
      "   | > Step:7/68  GlobalStep:90950  TotalLoss:0.00161  PostnetLoss:0.00078  DecoderLoss:0.00083  StopLoss:0.03828  GradNorm:0.00357  GradNormST:0.01252  AvgTextLen:7.4  AvgSpecLen:44.6  StepTime:0.33  LR:0.000100\n",
      "   | > Step:17/68  GlobalStep:90960  TotalLoss:0.00190  PostnetLoss:0.00092  DecoderLoss:0.00098  StopLoss:0.06600  GradNorm:0.00315  GradNormST:0.01635  AvgTextLen:14.7  AvgSpecLen:66.0  StepTime:0.46  LR:0.000100\n",
      "   | > Step:27/68  GlobalStep:90970  TotalLoss:0.00217  PostnetLoss:0.00105  DecoderLoss:0.00112  StopLoss:0.05622  GradNorm:0.00309  GradNormST:0.01535  AvgTextLen:21.0  AvgSpecLen:80.2  StepTime:0.65  LR:0.000100\n",
      "   | > Step:37/68  GlobalStep:90980  TotalLoss:0.00233  PostnetLoss:0.00113  DecoderLoss:0.00120  StopLoss:0.04598  GradNorm:0.00315  GradNormST:0.00813  AvgTextLen:29.3  AvgSpecLen:102.4  StepTime:0.60  LR:0.000100\n",
      "   | > Step:47/68  GlobalStep:90990  TotalLoss:0.00251  PostnetLoss:0.00121  DecoderLoss:0.00130  StopLoss:0.03707  GradNorm:0.00286  GradNormST:0.00971  AvgTextLen:36.7  AvgSpecLen:126.9  StepTime:0.72  LR:0.000100\n",
      "   | > Step:57/68  GlobalStep:91000  TotalLoss:0.00274  PostnetLoss:0.00132  DecoderLoss:0.00142  StopLoss:0.03713  GradNorm:0.00290  GradNormST:0.00666  AvgTextLen:47.1  AvgSpecLen:161.0  StepTime:0.79  LR:0.000100\n",
      " | | > Checkpoint saving : /vm/TTS/models/cartman_models/mozilla-no-loc-fattn-stopnet-sigmoid-loss_masking-August-15-2019_11+26AM-de6aeb9/checkpoint_91000.pth.tar\n",
      "   | > Step:67/68  GlobalStep:91010  TotalLoss:0.00315  PostnetLoss:0.00151  DecoderLoss:0.00163  StopLoss:0.04298  GradNorm:0.00264  GradNormST:0.01271  AvgTextLen:80.6  AvgSpecLen:246.5  StepTime:1.24  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:91011  AvgTotalLoss:0.05270  AvgPostnetLoss:0.00108  AvgDecoderLoss:0.00116  AvgStopLoss:0.05046  EpochTime:43.78  AvgStepTime:0.63\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.09038   PostnetLoss: 0.00481   DecoderLoss:0.00522  StopLoss: 0.08036  \n",
      "   | > TotalLoss: 0.07140   PostnetLoss: 0.00726   DecoderLoss:0.00787  StopLoss: 0.05626  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00108   Validation Loss: 0.00658\n",
      "\n",
      " > Epoch 319/1000\n",
      "   | > Step:8/68  GlobalStep:91020  TotalLoss:0.00166  PostnetLoss:0.00081  DecoderLoss:0.00085  StopLoss:0.07070  GradNorm:0.00395  GradNormST:0.02247  AvgTextLen:8.3  AvgSpecLen:44.0  StepTime:0.25  LR:0.000100\n",
      "   | > Step:18/68  GlobalStep:91030  TotalLoss:0.00189  PostnetLoss:0.00092  DecoderLoss:0.00097  StopLoss:0.06758  GradNorm:0.00355  GradNormST:0.01643  AvgTextLen:15.2  AvgSpecLen:64.5  StepTime:0.41  LR:0.000100\n",
      "   | > Step:28/68  GlobalStep:91040  TotalLoss:0.00217  PostnetLoss:0.00105  DecoderLoss:0.00112  StopLoss:0.04883  GradNorm:0.00342  GradNormST:0.00959  AvgTextLen:22.1  AvgSpecLen:85.4  StepTime:0.57  LR:0.000100\n",
      "   | > Step:38/68  GlobalStep:91050  TotalLoss:0.00228  PostnetLoss:0.00111  DecoderLoss:0.00118  StopLoss:0.06008  GradNorm:0.00300  GradNormST:0.02415  AvgTextLen:29.4  AvgSpecLen:104.1  StepTime:0.51  LR:0.000100\n",
      "   | > Step:48/68  GlobalStep:91060  TotalLoss:0.00254  PostnetLoss:0.00123  DecoderLoss:0.00131  StopLoss:0.03664  GradNorm:0.00298  GradNormST:0.00813  AvgTextLen:37.5  AvgSpecLen:123.8  StepTime:0.72  LR:0.000100\n",
      "   | > Step:58/68  GlobalStep:91070  TotalLoss:0.00276  PostnetLoss:0.00133  DecoderLoss:0.00143  StopLoss:0.04852  GradNorm:0.00289  GradNormST:0.01492  AvgTextLen:47.8  AvgSpecLen:158.0  StepTime:0.83  LR:0.000100\n",
      "   | > Step:68/68  GlobalStep:91080  TotalLoss:0.00307  PostnetLoss:0.00147  DecoderLoss:0.00160  StopLoss:0.03989  GradNorm:0.00280  GradNormST:0.01946  AvgTextLen:103.0  AvgSpecLen:308.9  StepTime:1.98  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:91080  AvgTotalLoss:0.05130  AvgPostnetLoss:0.00109  AvgDecoderLoss:0.00116  AvgStopLoss:0.04906  EpochTime:43.78  AvgStepTime:0.63\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.09125   PostnetLoss: 0.00490   DecoderLoss:0.00532  StopLoss: 0.08103  \n",
      "   | > TotalLoss: 0.07132   PostnetLoss: 0.00721   DecoderLoss:0.00782  StopLoss: 0.05629  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00109   Validation Loss: 0.00652\n",
      "\n",
      " > Epoch 320/1000\n",
      "   | > Step:9/68  GlobalStep:91090  TotalLoss:0.00166  PostnetLoss:0.00081  DecoderLoss:0.00085  StopLoss:0.03543  GradNorm:0.00346  GradNormST:0.00947  AvgTextLen:9.0  AvgSpecLen:42.1  StepTime:0.45  LR:0.000100\n",
      "   | > Step:19/68  GlobalStep:91100  TotalLoss:0.00188  PostnetLoss:0.00091  DecoderLoss:0.00097  StopLoss:0.06175  GradNorm:0.00367  GradNormST:0.02167  AvgTextLen:15.5  AvgSpecLen:63.2  StepTime:0.42  LR:0.000100\n",
      "   | > Step:29/68  GlobalStep:91110  TotalLoss:0.00209  PostnetLoss:0.00101  DecoderLoss:0.00107  StopLoss:0.05797  GradNorm:0.00323  GradNormST:0.01159  AvgTextLen:21.8  AvgSpecLen:83.4  StepTime:0.49  LR:0.000100\n",
      "   | > Step:39/68  GlobalStep:91120  TotalLoss:0.00235  PostnetLoss:0.00114  DecoderLoss:0.00121  StopLoss:0.03784  GradNorm:0.00288  GradNormST:0.01112  AvgTextLen:30.2  AvgSpecLen:110.4  StepTime:0.54  LR:0.000100\n",
      "   | > Step:49/68  GlobalStep:91130  TotalLoss:0.00252  PostnetLoss:0.00122  DecoderLoss:0.00130  StopLoss:0.03710  GradNorm:0.00280  GradNormST:0.01079  AvgTextLen:38.2  AvgSpecLen:127.7  StepTime:0.73  LR:0.000100\n",
      "   | > Step:59/68  GlobalStep:91140  TotalLoss:0.00272  PostnetLoss:0.00131  DecoderLoss:0.00141  StopLoss:0.03901  GradNorm:0.00294  GradNormST:0.00855  AvgTextLen:50.5  AvgSpecLen:155.9  StepTime:0.94  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:91149  AvgTotalLoss:0.05143  AvgPostnetLoss:0.00108  AvgDecoderLoss:0.00116  AvgStopLoss:0.04919  EpochTime:43.37  AvgStepTime:0.63\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.09041   PostnetLoss: 0.00474   DecoderLoss:0.00516  StopLoss: 0.08052  \n",
      "   | > TotalLoss: 0.06952   PostnetLoss: 0.00702   DecoderLoss:0.00763  StopLoss: 0.05488  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00108   Validation Loss: 0.00654\n",
      "\n",
      " > Epoch 321/1000\n",
      "   | > Step:0/68  GlobalStep:91150  TotalLoss:0.00156  PostnetLoss:0.00075  DecoderLoss:0.00081  StopLoss:0.06987  GradNorm:0.00503  GradNormST:0.02230  AvgTextLen:3.0  AvgSpecLen:25.3  StepTime:0.56  LR:0.000100\n",
      "   | > Step:10/68  GlobalStep:91160  TotalLoss:0.00171  PostnetLoss:0.00083  DecoderLoss:0.00088  StopLoss:0.05686  GradNorm:0.00351  GradNormST:0.01429  AvgTextLen:9.8  AvgSpecLen:48.6  StepTime:0.53  LR:0.000100\n",
      "   | > Step:20/68  GlobalStep:91170  TotalLoss:0.00201  PostnetLoss:0.00098  DecoderLoss:0.00103  StopLoss:0.04184  GradNorm:0.00347  GradNormST:0.01122  AvgTextLen:16.4  AvgSpecLen:64.8  StepTime:0.52  LR:0.000100\n",
      "   | > Step:30/68  GlobalStep:91180  TotalLoss:0.00215  PostnetLoss:0.00104  DecoderLoss:0.00111  StopLoss:0.05132  GradNorm:0.00324  GradNormST:0.00849  AvgTextLen:22.9  AvgSpecLen:85.8  StepTime:0.52  LR:0.000100\n",
      "   | > Step:40/68  GlobalStep:91190  TotalLoss:0.00235  PostnetLoss:0.00114  DecoderLoss:0.00121  StopLoss:0.03858  GradNorm:0.00334  GradNormST:0.00872  AvgTextLen:30.9  AvgSpecLen:105.3  StepTime:0.49  LR:0.000100\n",
      "   | > Step:50/68  GlobalStep:91200  TotalLoss:0.00258  PostnetLoss:0.00125  DecoderLoss:0.00134  StopLoss:0.04030  GradNorm:0.00313  GradNormST:0.00889  AvgTextLen:38.8  AvgSpecLen:132.0  StepTime:0.90  LR:0.000100\n",
      "   | > Step:60/68  GlobalStep:91210  TotalLoss:0.00281  PostnetLoss:0.00135  DecoderLoss:0.00145  StopLoss:0.03549  GradNorm:0.00286  GradNormST:0.00703  AvgTextLen:52.0  AvgSpecLen:172.7  StepTime:0.92  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:91218  AvgTotalLoss:0.05101  AvgPostnetLoss:0.00108  AvgDecoderLoss:0.00116  AvgStopLoss:0.04877  EpochTime:43.42  AvgStepTime:0.63\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.09121   PostnetLoss: 0.00485   DecoderLoss:0.00527  StopLoss: 0.08108  \n",
      "   | > TotalLoss: 0.07015   PostnetLoss: 0.00740   DecoderLoss:0.00803  StopLoss: 0.05472  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00108   Validation Loss: 0.00666\n",
      "\n",
      " > Epoch 322/1000\n",
      "   | > Step:1/68  GlobalStep:91220  TotalLoss:0.00153  PostnetLoss:0.00074  DecoderLoss:0.00079  StopLoss:0.05764  GradNorm:0.00538  GradNormST:0.02043  AvgTextLen:3.7  AvgSpecLen:28.6  StepTime:0.34  LR:0.000100\n",
      "   | > Step:11/68  GlobalStep:91230  TotalLoss:0.00175  PostnetLoss:0.00085  DecoderLoss:0.00090  StopLoss:0.04686  GradNorm:0.00354  GradNormST:0.01384  AvgTextLen:10.3  AvgSpecLen:49.4  StepTime:0.41  LR:0.000100\n",
      "   | > Step:21/68  GlobalStep:91240  TotalLoss:0.00198  PostnetLoss:0.00096  DecoderLoss:0.00102  StopLoss:0.05800  GradNorm:0.00348  GradNormST:0.02232  AvgTextLen:16.6  AvgSpecLen:66.3  StepTime:0.34  LR:0.000100\n",
      "   | > Step:31/68  GlobalStep:91250  TotalLoss:0.00214  PostnetLoss:0.00104  DecoderLoss:0.00110  StopLoss:0.03797  GradNorm:0.00318  GradNormST:0.01314  AvgTextLen:24.1  AvgSpecLen:82.7  StepTime:0.54  LR:0.000100\n",
      "   | > Step:41/68  GlobalStep:91260  TotalLoss:0.00246  PostnetLoss:0.00119  DecoderLoss:0.00127  StopLoss:0.03870  GradNorm:0.00341  GradNormST:0.00964  AvgTextLen:32.3  AvgSpecLen:109.4  StepTime:0.55  LR:0.000100\n",
      "   | > Step:51/68  GlobalStep:91270  TotalLoss:0.00254  PostnetLoss:0.00123  DecoderLoss:0.00131  StopLoss:0.04173  GradNorm:0.00301  GradNormST:0.00959  AvgTextLen:39.2  AvgSpecLen:126.9  StepTime:0.80  LR:0.000100\n",
      "   | > Step:61/68  GlobalStep:91280  TotalLoss:0.00276  PostnetLoss:0.00133  DecoderLoss:0.00143  StopLoss:0.04868  GradNorm:0.00292  GradNormST:0.01629  AvgTextLen:54.6  AvgSpecLen:165.5  StepTime:1.02  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:91287  AvgTotalLoss:0.05103  AvgPostnetLoss:0.00108  AvgDecoderLoss:0.00116  AvgStopLoss:0.04879  EpochTime:44.66  AvgStepTime:0.65\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.09050   PostnetLoss: 0.00476   DecoderLoss:0.00517  StopLoss: 0.08058  \n",
      "   | > TotalLoss: 0.07012   PostnetLoss: 0.00730   DecoderLoss:0.00792  StopLoss: 0.05490  \n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00108   Validation Loss: 0.00655\n",
      "\n",
      " > Epoch 323/1000\n",
      "   | > Step:2/68  GlobalStep:91290  TotalLoss:0.00146  PostnetLoss:0.00071  DecoderLoss:0.00075  StopLoss:0.06343  GradNorm:0.00413  GradNormST:0.02327  AvgTextLen:4.0  AvgSpecLen:31.3  StepTime:0.53  LR:0.000100\n",
      "   | > Step:12/68  GlobalStep:91300  TotalLoss:0.00172  PostnetLoss:0.00083  DecoderLoss:0.00088  StopLoss:0.04774  GradNorm:0.00314  GradNormST:0.01321  AvgTextLen:11.0  AvgSpecLen:49.3  StepTime:0.46  LR:0.000100\n",
      "   | > Step:22/68  GlobalStep:91310  TotalLoss:0.00200  PostnetLoss:0.00097  DecoderLoss:0.00103  StopLoss:0.05448  GradNorm:0.00317  GradNormST:0.01042  AvgTextLen:18.0  AvgSpecLen:72.0  StepTime:0.50  LR:0.000100\n",
      "   | > Step:32/68  GlobalStep:91320  TotalLoss:0.00220  PostnetLoss:0.00107  DecoderLoss:0.00113  StopLoss:0.06165  GradNorm:0.00362  GradNormST:0.01314  AvgTextLen:24.4  AvgSpecLen:91.5  StepTime:0.51  LR:0.000100\n",
      "   | > Step:42/68  GlobalStep:91330  TotalLoss:0.00239  PostnetLoss:0.00116  DecoderLoss:0.00123  StopLoss:0.05123  GradNorm:0.00320  GradNormST:0.01167  AvgTextLen:33.1  AvgSpecLen:109.7  StepTime:0.66  LR:0.000100\n",
      "   | > Step:52/68  GlobalStep:91340  TotalLoss:0.00264  PostnetLoss:0.00127  DecoderLoss:0.00136  StopLoss:0.04721  GradNorm:0.00339  GradNormST:0.01243  AvgTextLen:42.1  AvgSpecLen:137.5  StepTime:0.84  LR:0.000100\n",
      "   | > Step:62/68  GlobalStep:91350  TotalLoss:0.00286  PostnetLoss:0.00138  DecoderLoss:0.00148  StopLoss:0.04636  GradNorm:0.00310  GradNormST:0.01467  AvgTextLen:54.0  AvgSpecLen:174.7  StepTime:0.77  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:91356  AvgTotalLoss:0.05201  AvgPostnetLoss:0.00108  AvgDecoderLoss:0.00115  AvgStopLoss:0.04978  EpochTime:44.21  AvgStepTime:0.64\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.08498   PostnetLoss: 0.00476   DecoderLoss:0.00516  StopLoss: 0.07506  \n",
      "   | > TotalLoss: 0.06750   PostnetLoss: 0.00708   DecoderLoss:0.00767  StopLoss: 0.05275  \n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00108   Validation Loss: 0.00650\n",
      "\n",
      " > Epoch 324/1000\n",
      "   | > Step:3/68  GlobalStep:91360  TotalLoss:0.00151  PostnetLoss:0.00074  DecoderLoss:0.00078  StopLoss:0.05827  GradNorm:0.00388  GradNormST:0.01873  AvgTextLen:4.7  AvgSpecLen:29.7  StepTime:0.43  LR:0.000100\n",
      "   | > Step:13/68  GlobalStep:91370  TotalLoss:0.00171  PostnetLoss:0.00083  DecoderLoss:0.00088  StopLoss:0.07177  GradNorm:0.00329  GradNormST:0.02536  AvgTextLen:11.5  AvgSpecLen:55.2  StepTime:0.44  LR:0.000100\n",
      "   | > Step:23/68  GlobalStep:91380  TotalLoss:0.00208  PostnetLoss:0.00101  DecoderLoss:0.00107  StopLoss:0.04948  GradNorm:0.00349  GradNormST:0.01303  AvgTextLen:18.3  AvgSpecLen:72.2  StepTime:0.40  LR:0.000100\n",
      "   | > Step:33/68  GlobalStep:91390  TotalLoss:0.00227  PostnetLoss:0.00110  DecoderLoss:0.00117  StopLoss:0.04583  GradNorm:0.00406  GradNormST:0.01050  AvgTextLen:25.9  AvgSpecLen:88.9  StepTime:0.55  LR:0.000100\n",
      "   | > Step:43/68  GlobalStep:91400  TotalLoss:0.00237  PostnetLoss:0.00115  DecoderLoss:0.00123  StopLoss:0.04412  GradNorm:0.00308  GradNormST:0.00999  AvgTextLen:33.4  AvgSpecLen:115.7  StepTime:0.56  LR:0.000100\n",
      "   | > Step:53/68  GlobalStep:91410  TotalLoss:0.00260  PostnetLoss:0.00125  DecoderLoss:0.00134  StopLoss:0.03778  GradNorm:0.00335  GradNormST:0.00857  AvgTextLen:42.4  AvgSpecLen:137.6  StepTime:0.69  LR:0.000100\n",
      "   | > Step:63/68  GlobalStep:91420  TotalLoss:0.00290  PostnetLoss:0.00140  DecoderLoss:0.00150  StopLoss:0.02688  GradNorm:0.00274  GradNormST:0.00919  AvgTextLen:58.6  AvgSpecLen:185.1  StepTime:0.92  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:91425  AvgTotalLoss:0.05002  AvgPostnetLoss:0.00108  AvgDecoderLoss:0.00115  AvgStopLoss:0.04779  EpochTime:44.38  AvgStepTime:0.64\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.08951   PostnetLoss: 0.00479   DecoderLoss:0.00520  StopLoss: 0.07952  \n",
      "   | > TotalLoss: 0.07050   PostnetLoss: 0.00721   DecoderLoss:0.00782  StopLoss: 0.05546  \n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00108   Validation Loss: 0.00661\n",
      "\n",
      " > Epoch 325/1000\n",
      "   | > Step:4/68  GlobalStep:91430  TotalLoss:0.00156  PostnetLoss:0.00076  DecoderLoss:0.00080  StopLoss:0.09401  GradNorm:0.00518  GradNormST:0.03665  AvgTextLen:4.8  AvgSpecLen:31.1  StepTime:0.29  LR:0.000100\n",
      "   | > Step:14/68  GlobalStep:91440  TotalLoss:0.00181  PostnetLoss:0.00088  DecoderLoss:0.00093  StopLoss:0.07216  GradNorm:0.00351  GradNormST:0.01742  AvgTextLen:12.2  AvgSpecLen:52.0  StepTime:0.34  LR:0.000100\n",
      "   | > Step:24/68  GlobalStep:91450  TotalLoss:0.00212  PostnetLoss:0.00103  DecoderLoss:0.00109  StopLoss:0.04829  GradNorm:0.00341  GradNormST:0.00958  AvgTextLen:19.2  AvgSpecLen:79.0  StepTime:0.52  LR:0.000100\n",
      "   | > Step:34/68  GlobalStep:91460  TotalLoss:0.00229  PostnetLoss:0.00111  DecoderLoss:0.00118  StopLoss:0.03781  GradNorm:0.00404  GradNormST:0.00859  AvgTextLen:26.2  AvgSpecLen:95.9  StepTime:0.58  LR:0.000100\n",
      "   | > Step:44/68  GlobalStep:91470  TotalLoss:0.00245  PostnetLoss:0.00118  DecoderLoss:0.00126  StopLoss:0.03038  GradNorm:0.00381  GradNormST:0.00823  AvgTextLen:33.8  AvgSpecLen:116.3  StepTime:0.56  LR:0.000100\n",
      "   | > Step:54/68  GlobalStep:91480  TotalLoss:0.00264  PostnetLoss:0.00127  DecoderLoss:0.00137  StopLoss:0.04137  GradNorm:0.00337  GradNormST:0.00815  AvgTextLen:42.2  AvgSpecLen:144.9  StepTime:0.75  LR:0.000100\n",
      "   | > Step:64/68  GlobalStep:91490  TotalLoss:0.00275  PostnetLoss:0.00132  DecoderLoss:0.00143  StopLoss:0.04688  GradNorm:0.00291  GradNormST:0.01382  AvgTextLen:60.6  AvgSpecLen:177.6  StepTime:0.84  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:91494  AvgTotalLoss:0.04954  AvgPostnetLoss:0.00108  AvgDecoderLoss:0.00115  AvgStopLoss:0.04730  EpochTime:43.65  AvgStepTime:0.63\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.08865   PostnetLoss: 0.00472   DecoderLoss:0.00513  StopLoss: 0.07880  \n",
      "   | > TotalLoss: 0.06919   PostnetLoss: 0.00723   DecoderLoss:0.00784  StopLoss: 0.05412  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00108   Validation Loss: 0.00656\n",
      "\n",
      " > Epoch 326/1000\n",
      "   | > Step:5/68  GlobalStep:91500  TotalLoss:0.00157  PostnetLoss:0.00076  DecoderLoss:0.00081  StopLoss:0.06087  GradNorm:0.00371  GradNormST:0.01499  AvgTextLen:5.9  AvgSpecLen:33.2  StepTime:0.37  LR:0.000100\n",
      "   | > Step:15/68  GlobalStep:91510  TotalLoss:0.00189  PostnetLoss:0.00092  DecoderLoss:0.00097  StopLoss:0.05418  GradNorm:0.00348  GradNormST:0.02288  AvgTextLen:12.8  AvgSpecLen:59.1  StepTime:0.54  LR:0.000100\n",
      "   | > Step:25/68  GlobalStep:91520  TotalLoss:0.00206  PostnetLoss:0.00100  DecoderLoss:0.00106  StopLoss:0.05264  GradNorm:0.00356  GradNormST:0.01173  AvgTextLen:19.8  AvgSpecLen:73.3  StepTime:0.38  LR:0.000100\n",
      "   | > Step:35/68  GlobalStep:91530  TotalLoss:0.00231  PostnetLoss:0.00112  DecoderLoss:0.00119  StopLoss:0.04734  GradNorm:0.00405  GradNormST:0.01819  AvgTextLen:27.1  AvgSpecLen:104.7  StepTime:0.75  LR:0.000100\n",
      "   | > Step:45/68  GlobalStep:91540  TotalLoss:0.00240  PostnetLoss:0.00116  DecoderLoss:0.00124  StopLoss:0.05091  GradNorm:0.00392  GradNormST:0.01710  AvgTextLen:34.4  AvgSpecLen:118.6  StepTime:0.60  LR:0.000100\n",
      "   | > Step:55/68  GlobalStep:91550  TotalLoss:0.00269  PostnetLoss:0.00130  DecoderLoss:0.00139  StopLoss:0.03914  GradNorm:0.00367  GradNormST:0.00677  AvgTextLen:43.9  AvgSpecLen:145.2  StepTime:0.66  LR:0.000100\n",
      "   | > Step:65/68  GlobalStep:91560  TotalLoss:0.00296  PostnetLoss:0.00142  DecoderLoss:0.00153  StopLoss:0.04690  GradNorm:0.00275  GradNormST:0.02380  AvgTextLen:64.2  AvgSpecLen:207.4  StepTime:1.22  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:91563  AvgTotalLoss:0.05077  AvgPostnetLoss:0.00108  AvgDecoderLoss:0.00115  AvgStopLoss:0.04853  EpochTime:43.52  AvgStepTime:0.63\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.08749   PostnetLoss: 0.00487   DecoderLoss:0.00529  StopLoss: 0.07733  \n",
      "   | > TotalLoss: 0.06687   PostnetLoss: 0.00709   DecoderLoss:0.00766  StopLoss: 0.05212  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00108   Validation Loss: 0.00650\n",
      "\n",
      " > Epoch 327/1000\n",
      "   | > Step:6/68  GlobalStep:91570  TotalLoss:0.00156  PostnetLoss:0.00076  DecoderLoss:0.00080  StopLoss:0.04100  GradNorm:0.00426  GradNormST:0.01056  AvgTextLen:6.0  AvgSpecLen:34.8  StepTime:0.36  LR:0.000100\n",
      "   | > Step:16/68  GlobalStep:91580  TotalLoss:0.00190  PostnetLoss:0.00093  DecoderLoss:0.00098  StopLoss:0.04486  GradNorm:0.00346  GradNormST:0.01163  AvgTextLen:13.8  AvgSpecLen:60.6  StepTime:0.38  LR:0.000100\n",
      "   | > Step:26/68  GlobalStep:91590  TotalLoss:0.00208  PostnetLoss:0.00101  DecoderLoss:0.00107  StopLoss:0.04108  GradNorm:0.00322  GradNormST:0.01202  AvgTextLen:20.3  AvgSpecLen:77.5  StepTime:0.52  LR:0.000100\n",
      "   | > Step:36/68  GlobalStep:91600  TotalLoss:0.00229  PostnetLoss:0.00111  DecoderLoss:0.00118  StopLoss:0.05830  GradNorm:0.00406  GradNormST:0.01171  AvgTextLen:28.5  AvgSpecLen:100.2  StepTime:0.70  LR:0.000100\n",
      "   | > Step:46/68  GlobalStep:91610  TotalLoss:0.00247  PostnetLoss:0.00119  DecoderLoss:0.00127  StopLoss:0.03251  GradNorm:0.00372  GradNormST:0.00696  AvgTextLen:35.4  AvgSpecLen:122.6  StepTime:0.77  LR:0.000100\n",
      "   | > Step:56/68  GlobalStep:91620  TotalLoss:0.00271  PostnetLoss:0.00131  DecoderLoss:0.00140  StopLoss:0.05090  GradNorm:0.00344  GradNormST:0.02903  AvgTextLen:45.8  AvgSpecLen:145.4  StepTime:0.96  LR:0.000100\n",
      "   | > Step:66/68  GlobalStep:91630  TotalLoss:0.00308  PostnetLoss:0.00148  DecoderLoss:0.00160  StopLoss:0.03789  GradNorm:0.00266  GradNormST:0.01170  AvgTextLen:71.2  AvgSpecLen:220.8  StepTime:1.06  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:91632  AvgTotalLoss:0.05042  AvgPostnetLoss:0.00108  AvgDecoderLoss:0.00115  AvgStopLoss:0.04818  EpochTime:43.19  AvgStepTime:0.63\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.09367   PostnetLoss: 0.00483   DecoderLoss:0.00524  StopLoss: 0.08360  \n",
      "   | > TotalLoss: 0.07058   PostnetLoss: 0.00730   DecoderLoss:0.00790  StopLoss: 0.05537  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00108   Validation Loss: 0.00664\n",
      "\n",
      " > Epoch 328/1000\n",
      "   | > Step:7/68  GlobalStep:91640  TotalLoss:0.00164  PostnetLoss:0.00080  DecoderLoss:0.00084  StopLoss:0.03405  GradNorm:0.00368  GradNormST:0.01210  AvgTextLen:7.4  AvgSpecLen:44.6  StepTime:0.40  LR:0.000100\n",
      "   | > Step:17/68  GlobalStep:91650  TotalLoss:0.00189  PostnetLoss:0.00092  DecoderLoss:0.00097  StopLoss:0.06951  GradNorm:0.00349  GradNormST:0.01164  AvgTextLen:14.7  AvgSpecLen:66.0  StepTime:0.42  LR:0.000100\n",
      "   | > Step:27/68  GlobalStep:91660  TotalLoss:0.00212  PostnetLoss:0.00103  DecoderLoss:0.00109  StopLoss:0.04664  GradNorm:0.00330  GradNormST:0.01228  AvgTextLen:21.0  AvgSpecLen:80.2  StepTime:0.68  LR:0.000100\n",
      "   | > Step:37/68  GlobalStep:91670  TotalLoss:0.00235  PostnetLoss:0.00114  DecoderLoss:0.00121  StopLoss:0.04465  GradNorm:0.00361  GradNormST:0.00958  AvgTextLen:29.3  AvgSpecLen:102.4  StepTime:0.64  LR:0.000100\n",
      "   | > Step:47/68  GlobalStep:91680  TotalLoss:0.00253  PostnetLoss:0.00122  DecoderLoss:0.00130  StopLoss:0.03982  GradNorm:0.00404  GradNormST:0.01059  AvgTextLen:36.7  AvgSpecLen:126.9  StepTime:0.75  LR:0.000100\n",
      "   | > Step:57/68  GlobalStep:91690  TotalLoss:0.00276  PostnetLoss:0.00133  DecoderLoss:0.00143  StopLoss:0.03362  GradNorm:0.00385  GradNormST:0.00572  AvgTextLen:47.1  AvgSpecLen:161.0  StepTime:0.70  LR:0.000100\n",
      "   | > Step:67/68  GlobalStep:91700  TotalLoss:0.00309  PostnetLoss:0.00148  DecoderLoss:0.00160  StopLoss:0.04113  GradNorm:0.00280  GradNormST:0.01332  AvgTextLen:80.6  AvgSpecLen:246.5  StepTime:1.27  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:91701  AvgTotalLoss:0.05060  AvgPostnetLoss:0.00108  AvgDecoderLoss:0.00115  AvgStopLoss:0.04837  EpochTime:43.50  AvgStepTime:0.63\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.09404   PostnetLoss: 0.00483   DecoderLoss:0.00522  StopLoss: 0.08400  \n",
      "   | > TotalLoss: 0.07225   PostnetLoss: 0.00742   DecoderLoss:0.00802  StopLoss: 0.05681  \n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00108   Validation Loss: 0.00671\n",
      "\n",
      " > Epoch 329/1000\n",
      "   | > Step:8/68  GlobalStep:91710  TotalLoss:0.00170  PostnetLoss:0.00083  DecoderLoss:0.00087  StopLoss:0.07075  GradNorm:0.00434  GradNormST:0.02069  AvgTextLen:8.3  AvgSpecLen:44.0  StepTime:0.33  LR:0.000100\n",
      "   | > Step:18/68  GlobalStep:91720  TotalLoss:0.00184  PostnetLoss:0.00090  DecoderLoss:0.00095  StopLoss:0.05139  GradNorm:0.00382  GradNormST:0.01420  AvgTextLen:15.2  AvgSpecLen:64.5  StepTime:0.41  LR:0.000100\n",
      "   | > Step:28/68  GlobalStep:91730  TotalLoss:0.00212  PostnetLoss:0.00103  DecoderLoss:0.00109  StopLoss:0.05622  GradNorm:0.00351  GradNormST:0.01422  AvgTextLen:22.1  AvgSpecLen:85.4  StepTime:0.44  LR:0.000100\n",
      "   | > Step:38/68  GlobalStep:91740  TotalLoss:0.00233  PostnetLoss:0.00113  DecoderLoss:0.00120  StopLoss:0.04838  GradNorm:0.00373  GradNormST:0.01594  AvgTextLen:29.4  AvgSpecLen:104.1  StepTime:0.61  LR:0.000100\n",
      "   | > Step:48/68  GlobalStep:91750  TotalLoss:0.00252  PostnetLoss:0.00122  DecoderLoss:0.00130  StopLoss:0.04583  GradNorm:0.00390  GradNormST:0.00972  AvgTextLen:37.5  AvgSpecLen:123.8  StepTime:0.61  LR:0.000100\n",
      "   | > Step:58/68  GlobalStep:91760  TotalLoss:0.00272  PostnetLoss:0.00131  DecoderLoss:0.00140  StopLoss:0.03923  GradNorm:0.00312  GradNormST:0.01301  AvgTextLen:47.8  AvgSpecLen:158.0  StepTime:0.92  LR:0.000100\n",
      "   | > Step:68/68  GlobalStep:91770  TotalLoss:0.00305  PostnetLoss:0.00146  DecoderLoss:0.00159  StopLoss:0.03160  GradNorm:0.00271  GradNormST:0.01292  AvgTextLen:103.0  AvgSpecLen:308.9  StepTime:1.89  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:91770  AvgTotalLoss:0.05142  AvgPostnetLoss:0.00108  AvgDecoderLoss:0.00115  AvgStopLoss:0.04919  EpochTime:42.43  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.09031   PostnetLoss: 0.00478   DecoderLoss:0.00519  StopLoss: 0.08034  \n",
      "   | > TotalLoss: 0.07013   PostnetLoss: 0.00739   DecoderLoss:0.00798  StopLoss: 0.05476  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00108   Validation Loss: 0.00674\n",
      "\n",
      " > Epoch 330/1000\n",
      "   | > Step:9/68  GlobalStep:91780  TotalLoss:0.00163  PostnetLoss:0.00079  DecoderLoss:0.00083  StopLoss:0.04179  GradNorm:0.00391  GradNormST:0.01711  AvgTextLen:9.0  AvgSpecLen:42.1  StepTime:0.47  LR:0.000100\n",
      "   | > Step:19/68  GlobalStep:91790  TotalLoss:0.00188  PostnetLoss:0.00091  DecoderLoss:0.00096  StopLoss:0.07017  GradNorm:0.00339  GradNormST:0.02034  AvgTextLen:15.5  AvgSpecLen:63.2  StepTime:0.42  LR:0.000100\n",
      "   | > Step:29/68  GlobalStep:91800  TotalLoss:0.00209  PostnetLoss:0.00101  DecoderLoss:0.00107  StopLoss:0.05843  GradNorm:0.00357  GradNormST:0.01252  AvgTextLen:21.8  AvgSpecLen:83.4  StepTime:0.48  LR:0.000100\n",
      "   | > Step:39/68  GlobalStep:91810  TotalLoss:0.00235  PostnetLoss:0.00114  DecoderLoss:0.00121  StopLoss:0.03811  GradNorm:0.00411  GradNormST:0.01236  AvgTextLen:30.2  AvgSpecLen:110.4  StepTime:0.64  LR:0.000100\n",
      "   | > Step:49/68  GlobalStep:91820  TotalLoss:0.00254  PostnetLoss:0.00123  DecoderLoss:0.00131  StopLoss:0.03045  GradNorm:0.00409  GradNormST:0.00729  AvgTextLen:38.2  AvgSpecLen:127.7  StepTime:0.72  LR:0.000100\n",
      "   | > Step:59/68  GlobalStep:91830  TotalLoss:0.00268  PostnetLoss:0.00129  DecoderLoss:0.00139  StopLoss:0.03109  GradNorm:0.00316  GradNormST:0.00595  AvgTextLen:50.5  AvgSpecLen:155.9  StepTime:0.93  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:91839  AvgTotalLoss:0.04905  AvgPostnetLoss:0.00108  AvgDecoderLoss:0.00115  AvgStopLoss:0.04682  EpochTime:42.63  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.09245   PostnetLoss: 0.00486   DecoderLoss:0.00526  StopLoss: 0.08233  \n",
      "   | > TotalLoss: 0.07031   PostnetLoss: 0.00742   DecoderLoss:0.00802  StopLoss: 0.05487  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00108   Validation Loss: 0.00680\n",
      "\n",
      " > Epoch 331/1000\n",
      "   | > Step:0/68  GlobalStep:91840  TotalLoss:0.00156  PostnetLoss:0.00074  DecoderLoss:0.00081  StopLoss:0.08443  GradNorm:0.00474  GradNormST:0.02401  AvgTextLen:3.0  AvgSpecLen:25.3  StepTime:0.59  LR:0.000100\n",
      "   | > Step:10/68  GlobalStep:91850  TotalLoss:0.00173  PostnetLoss:0.00084  DecoderLoss:0.00089  StopLoss:0.04922  GradNorm:0.00363  GradNormST:0.01413  AvgTextLen:9.8  AvgSpecLen:48.6  StepTime:0.44  LR:0.000100\n",
      "   | > Step:20/68  GlobalStep:91860  TotalLoss:0.00197  PostnetLoss:0.00096  DecoderLoss:0.00101  StopLoss:0.04765  GradNorm:0.00329  GradNormST:0.01771  AvgTextLen:16.4  AvgSpecLen:64.8  StepTime:0.52  LR:0.000100\n",
      "   | > Step:30/68  GlobalStep:91870  TotalLoss:0.00215  PostnetLoss:0.00104  DecoderLoss:0.00111  StopLoss:0.04495  GradNorm:0.00337  GradNormST:0.00937  AvgTextLen:22.9  AvgSpecLen:85.8  StepTime:0.57  LR:0.000100\n",
      "   | > Step:40/68  GlobalStep:91880  TotalLoss:0.00236  PostnetLoss:0.00114  DecoderLoss:0.00122  StopLoss:0.04025  GradNorm:0.00381  GradNormST:0.00809  AvgTextLen:30.9  AvgSpecLen:105.3  StepTime:0.62  LR:0.000100\n",
      "   | > Step:50/68  GlobalStep:91890  TotalLoss:0.00254  PostnetLoss:0.00123  DecoderLoss:0.00131  StopLoss:0.03361  GradNorm:0.00444  GradNormST:0.00709  AvgTextLen:38.8  AvgSpecLen:132.0  StepTime:0.79  LR:0.000100\n",
      "   | > Step:60/68  GlobalStep:91900  TotalLoss:0.00279  PostnetLoss:0.00135  DecoderLoss:0.00145  StopLoss:0.02597  GradNorm:0.00336  GradNormST:0.00611  AvgTextLen:52.0  AvgSpecLen:172.7  StepTime:0.90  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:91908  AvgTotalLoss:0.04867  AvgPostnetLoss:0.00108  AvgDecoderLoss:0.00115  AvgStopLoss:0.04644  EpochTime:42.50  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.09127   PostnetLoss: 0.00486   DecoderLoss:0.00525  StopLoss: 0.08115  \n",
      "   | > TotalLoss: 0.07259   PostnetLoss: 0.00735   DecoderLoss:0.00796  StopLoss: 0.05729  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00108   Validation Loss: 0.00679\n",
      "\n",
      " > Epoch 332/1000\n",
      "   | > Step:1/68  GlobalStep:91910  TotalLoss:0.00153  PostnetLoss:0.00074  DecoderLoss:0.00079  StopLoss:0.04649  GradNorm:0.00432  GradNormST:0.01656  AvgTextLen:3.7  AvgSpecLen:28.6  StepTime:0.38  LR:0.000100\n",
      "   | > Step:11/68  GlobalStep:91920  TotalLoss:0.00167  PostnetLoss:0.00081  DecoderLoss:0.00086  StopLoss:0.03967  GradNorm:0.00337  GradNormST:0.01639  AvgTextLen:10.3  AvgSpecLen:49.4  StepTime:0.40  LR:0.000100\n",
      "   | > Step:21/68  GlobalStep:91930  TotalLoss:0.00196  PostnetLoss:0.00095  DecoderLoss:0.00101  StopLoss:0.06142  GradNorm:0.00344  GradNormST:0.01476  AvgTextLen:16.6  AvgSpecLen:66.3  StepTime:0.44  LR:0.000100\n",
      "   | > Step:31/68  GlobalStep:91940  TotalLoss:0.00218  PostnetLoss:0.00106  DecoderLoss:0.00112  StopLoss:0.04423  GradNorm:0.00336  GradNormST:0.01230  AvgTextLen:24.1  AvgSpecLen:82.7  StepTime:0.57  LR:0.000100\n",
      "   | > Step:41/68  GlobalStep:91950  TotalLoss:0.00242  PostnetLoss:0.00117  DecoderLoss:0.00125  StopLoss:0.03663  GradNorm:0.00355  GradNormST:0.01473  AvgTextLen:32.3  AvgSpecLen:109.4  StepTime:0.67  LR:0.000100\n",
      "   | > Step:51/68  GlobalStep:91960  TotalLoss:0.00252  PostnetLoss:0.00122  DecoderLoss:0.00130  StopLoss:0.03596  GradNorm:0.00322  GradNormST:0.00588  AvgTextLen:39.2  AvgSpecLen:126.9  StepTime:0.82  LR:0.000100\n",
      "   | > Step:61/68  GlobalStep:91970  TotalLoss:0.00276  PostnetLoss:0.00133  DecoderLoss:0.00143  StopLoss:0.03747  GradNorm:0.00330  GradNormST:0.00962  AvgTextLen:54.6  AvgSpecLen:165.5  StepTime:0.97  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:91977  AvgTotalLoss:0.04995  AvgPostnetLoss:0.00108  AvgDecoderLoss:0.00115  AvgStopLoss:0.04772  EpochTime:44.41  AvgStepTime:0.64\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.09200   PostnetLoss: 0.00487   DecoderLoss:0.00530  StopLoss: 0.08184  \n",
      "   | > TotalLoss: 0.07343   PostnetLoss: 0.00764   DecoderLoss:0.00827  StopLoss: 0.05751  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00108   Validation Loss: 0.00694\n",
      "\n",
      " > Epoch 333/1000\n",
      "   | > Step:2/68  GlobalStep:91980  TotalLoss:0.00143  PostnetLoss:0.00070  DecoderLoss:0.00074  StopLoss:0.03919  GradNorm:0.00374  GradNormST:0.01697  AvgTextLen:4.0  AvgSpecLen:31.3  StepTime:0.49  LR:0.000100\n",
      "   | > Step:12/68  GlobalStep:91990  TotalLoss:0.00169  PostnetLoss:0.00082  DecoderLoss:0.00087  StopLoss:0.04587  GradNorm:0.00336  GradNormST:0.01090  AvgTextLen:11.0  AvgSpecLen:49.3  StepTime:0.47  LR:0.000100\n",
      "   | > Step:22/68  GlobalStep:92000  TotalLoss:0.00198  PostnetLoss:0.00096  DecoderLoss:0.00102  StopLoss:0.05964  GradNorm:0.00327  GradNormST:0.01177  AvgTextLen:18.0  AvgSpecLen:72.0  StepTime:0.48  LR:0.000100\n",
      " | | > Checkpoint saving : /vm/TTS/models/cartman_models/mozilla-no-loc-fattn-stopnet-sigmoid-loss_masking-August-15-2019_11+26AM-de6aeb9/checkpoint_92000.pth.tar\n",
      "warning: audio amplitude out of range, auto clipped.\n",
      "   | > Step:32/68  GlobalStep:92010  TotalLoss:0.00217  PostnetLoss:0.00105  DecoderLoss:0.00112  StopLoss:0.04981  GradNorm:0.00325  GradNormST:0.01403  AvgTextLen:24.4  AvgSpecLen:91.5  StepTime:0.48  LR:0.000100\n",
      "   | > Step:42/68  GlobalStep:92020  TotalLoss:0.00239  PostnetLoss:0.00116  DecoderLoss:0.00124  StopLoss:0.05508  GradNorm:0.00310  GradNormST:0.01511  AvgTextLen:33.1  AvgSpecLen:109.7  StepTime:0.62  LR:0.000100\n",
      "   | > Step:52/68  GlobalStep:92030  TotalLoss:0.00260  PostnetLoss:0.00126  DecoderLoss:0.00134  StopLoss:0.04372  GradNorm:0.00340  GradNormST:0.01300  AvgTextLen:42.1  AvgSpecLen:137.5  StepTime:0.92  LR:0.000100\n",
      "   | > Step:62/68  GlobalStep:92040  TotalLoss:0.00282  PostnetLoss:0.00136  DecoderLoss:0.00146  StopLoss:0.05062  GradNorm:0.00333  GradNormST:0.01835  AvgTextLen:54.0  AvgSpecLen:174.7  StepTime:0.85  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:92046  AvgTotalLoss:0.04937  AvgPostnetLoss:0.00108  AvgDecoderLoss:0.00115  AvgStopLoss:0.04714  EpochTime:43.42  AvgStepTime:0.63\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.09021   PostnetLoss: 0.00491   DecoderLoss:0.00532  StopLoss: 0.07999  \n",
      "   | > TotalLoss: 0.07000   PostnetLoss: 0.00740   DecoderLoss:0.00800  StopLoss: 0.05460  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00108   Validation Loss: 0.00686\n",
      "\n",
      " > Epoch 334/1000\n",
      "   | > Step:3/68  GlobalStep:92050  TotalLoss:0.00148  PostnetLoss:0.00072  DecoderLoss:0.00076  StopLoss:0.07375  GradNorm:0.00416  GradNormST:0.02160  AvgTextLen:4.7  AvgSpecLen:29.7  StepTime:0.24  LR:0.000100\n",
      "   | > Step:13/68  GlobalStep:92060  TotalLoss:0.00172  PostnetLoss:0.00083  DecoderLoss:0.00088  StopLoss:0.06908  GradNorm:0.00329  GradNormST:0.02800  AvgTextLen:11.5  AvgSpecLen:55.2  StepTime:0.42  LR:0.000100\n",
      "   | > Step:23/68  GlobalStep:92070  TotalLoss:0.00203  PostnetLoss:0.00099  DecoderLoss:0.00104  StopLoss:0.03909  GradNorm:0.00343  GradNormST:0.00989  AvgTextLen:18.3  AvgSpecLen:72.2  StepTime:0.34  LR:0.000100\n",
      "   | > Step:33/68  GlobalStep:92080  TotalLoss:0.00223  PostnetLoss:0.00108  DecoderLoss:0.00115  StopLoss:0.04282  GradNorm:0.00310  GradNormST:0.00885  AvgTextLen:25.9  AvgSpecLen:88.9  StepTime:0.57  LR:0.000100\n",
      "   | > Step:43/68  GlobalStep:92090  TotalLoss:0.00240  PostnetLoss:0.00116  DecoderLoss:0.00124  StopLoss:0.05036  GradNorm:0.00358  GradNormST:0.01001  AvgTextLen:33.4  AvgSpecLen:115.7  StepTime:0.51  LR:0.000100\n",
      "   | > Step:53/68  GlobalStep:92100  TotalLoss:0.00256  PostnetLoss:0.00124  DecoderLoss:0.00132  StopLoss:0.03824  GradNorm:0.00332  GradNormST:0.00801  AvgTextLen:42.4  AvgSpecLen:137.6  StepTime:0.63  LR:0.000100\n",
      "   | > Step:63/68  GlobalStep:92110  TotalLoss:0.00294  PostnetLoss:0.00142  DecoderLoss:0.00153  StopLoss:0.03080  GradNorm:0.00309  GradNormST:0.01068  AvgTextLen:58.6  AvgSpecLen:185.1  StepTime:0.91  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:92115  AvgTotalLoss:0.04941  AvgPostnetLoss:0.00108  AvgDecoderLoss:0.00115  AvgStopLoss:0.04718  EpochTime:42.53  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.09453   PostnetLoss: 0.00494   DecoderLoss:0.00536  StopLoss: 0.08423  \n",
      "   | > TotalLoss: 0.07654   PostnetLoss: 0.00820   DecoderLoss:0.00884  StopLoss: 0.05949  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00108   Validation Loss: 0.00725\n",
      "\n",
      " > Epoch 335/1000\n",
      "   | > Step:4/68  GlobalStep:92120  TotalLoss:0.00151  PostnetLoss:0.00073  DecoderLoss:0.00078  StopLoss:0.08615  GradNorm:0.00508  GradNormST:0.03355  AvgTextLen:4.8  AvgSpecLen:31.1  StepTime:0.20  LR:0.000100\n",
      "   | > Step:14/68  GlobalStep:92130  TotalLoss:0.00183  PostnetLoss:0.00089  DecoderLoss:0.00094  StopLoss:0.05520  GradNorm:0.00347  GradNormST:0.01527  AvgTextLen:12.2  AvgSpecLen:52.0  StepTime:0.34  LR:0.000100\n",
      "   | > Step:24/68  GlobalStep:92140  TotalLoss:0.00214  PostnetLoss:0.00104  DecoderLoss:0.00110  StopLoss:0.06456  GradNorm:0.00334  GradNormST:0.01647  AvgTextLen:19.2  AvgSpecLen:79.0  StepTime:0.49  LR:0.000100\n",
      "   | > Step:34/68  GlobalStep:92150  TotalLoss:0.00226  PostnetLoss:0.00110  DecoderLoss:0.00116  StopLoss:0.04311  GradNorm:0.00319  GradNormST:0.01243  AvgTextLen:26.2  AvgSpecLen:95.9  StepTime:0.62  LR:0.000100\n",
      "   | > Step:44/68  GlobalStep:92160  TotalLoss:0.00244  PostnetLoss:0.00118  DecoderLoss:0.00126  StopLoss:0.03193  GradNorm:0.00301  GradNormST:0.00719  AvgTextLen:33.8  AvgSpecLen:116.3  StepTime:0.62  LR:0.000100\n",
      "   | > Step:54/68  GlobalStep:92170  TotalLoss:0.00265  PostnetLoss:0.00128  DecoderLoss:0.00137  StopLoss:0.04173  GradNorm:0.00328  GradNormST:0.00849  AvgTextLen:42.2  AvgSpecLen:144.9  StepTime:0.83  LR:0.000100\n",
      "   | > Step:64/68  GlobalStep:92180  TotalLoss:0.00271  PostnetLoss:0.00130  DecoderLoss:0.00141  StopLoss:0.03919  GradNorm:0.00353  GradNormST:0.01321  AvgTextLen:60.6  AvgSpecLen:177.6  StepTime:0.80  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:92184  AvgTotalLoss:0.05082  AvgPostnetLoss:0.00108  AvgDecoderLoss:0.00115  AvgStopLoss:0.04859  EpochTime:42.54  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.09189   PostnetLoss: 0.00495   DecoderLoss:0.00537  StopLoss: 0.08158  \n",
      "   | > TotalLoss: 0.07583   PostnetLoss: 0.00784   DecoderLoss:0.00847  StopLoss: 0.05952  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00108   Validation Loss: 0.00711\n",
      "\n",
      " > Epoch 336/1000\n",
      "   | > Step:5/68  GlobalStep:92190  TotalLoss:0.00152  PostnetLoss:0.00074  DecoderLoss:0.00078  StopLoss:0.06172  GradNorm:0.00362  GradNormST:0.01817  AvgTextLen:5.9  AvgSpecLen:33.2  StepTime:0.33  LR:0.000100\n",
      "   | > Step:15/68  GlobalStep:92200  TotalLoss:0.00186  PostnetLoss:0.00090  DecoderLoss:0.00095  StopLoss:0.04598  GradNorm:0.00351  GradNormST:0.01177  AvgTextLen:12.8  AvgSpecLen:59.1  StepTime:0.51  LR:0.000100\n",
      "   | > Step:25/68  GlobalStep:92210  TotalLoss:0.00205  PostnetLoss:0.00099  DecoderLoss:0.00105  StopLoss:0.05466  GradNorm:0.00303  GradNormST:0.01237  AvgTextLen:19.8  AvgSpecLen:73.3  StepTime:0.46  LR:0.000100\n",
      "   | > Step:35/68  GlobalStep:92220  TotalLoss:0.00226  PostnetLoss:0.00109  DecoderLoss:0.00116  StopLoss:0.06067  GradNorm:0.00311  GradNormST:0.03039  AvgTextLen:27.1  AvgSpecLen:104.7  StepTime:0.63  LR:0.000100\n",
      "   | > Step:45/68  GlobalStep:92230  TotalLoss:0.00238  PostnetLoss:0.00115  DecoderLoss:0.00123  StopLoss:0.06997  GradNorm:0.00346  GradNormST:0.03128  AvgTextLen:34.4  AvgSpecLen:118.6  StepTime:0.65  LR:0.000100\n",
      "   | > Step:55/68  GlobalStep:92240  TotalLoss:0.00270  PostnetLoss:0.00131  DecoderLoss:0.00140  StopLoss:0.04380  GradNorm:0.00315  GradNormST:0.01436  AvgTextLen:43.9  AvgSpecLen:145.2  StepTime:0.78  LR:0.000100\n",
      "   | > Step:65/68  GlobalStep:92250  TotalLoss:0.00299  PostnetLoss:0.00144  DecoderLoss:0.00155  StopLoss:0.04888  GradNorm:0.00358  GradNormST:0.02903  AvgTextLen:64.2  AvgSpecLen:207.4  StepTime:1.19  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:92253  AvgTotalLoss:0.05647  AvgPostnetLoss:0.00108  AvgDecoderLoss:0.00115  AvgStopLoss:0.05424  EpochTime:42.17  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.09657   PostnetLoss: 0.00497   DecoderLoss:0.00538  StopLoss: 0.08622  \n",
      "   | > TotalLoss: 0.08121   PostnetLoss: 0.00844   DecoderLoss:0.00906  StopLoss: 0.06371  \n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00108   Validation Loss: 0.00756\n",
      "\n",
      " > Epoch 337/1000\n",
      "   | > Step:6/68  GlobalStep:92260  TotalLoss:0.00152  PostnetLoss:0.00074  DecoderLoss:0.00078  StopLoss:0.03922  GradNorm:0.00373  GradNormST:0.01731  AvgTextLen:6.0  AvgSpecLen:34.8  StepTime:0.38  LR:0.000100\n",
      "   | > Step:16/68  GlobalStep:92270  TotalLoss:0.00185  PostnetLoss:0.00090  DecoderLoss:0.00095  StopLoss:0.03150  GradNorm:0.00334  GradNormST:0.00791  AvgTextLen:13.8  AvgSpecLen:60.6  StepTime:0.36  LR:0.000100\n",
      "   | > Step:26/68  GlobalStep:92280  TotalLoss:0.00211  PostnetLoss:0.00103  DecoderLoss:0.00108  StopLoss:0.04447  GradNorm:0.00317  GradNormST:0.01222  AvgTextLen:20.3  AvgSpecLen:77.5  StepTime:0.48  LR:0.000100\n",
      "   | > Step:36/68  GlobalStep:92290  TotalLoss:0.00222  PostnetLoss:0.00107  DecoderLoss:0.00115  StopLoss:0.06730  GradNorm:0.00293  GradNormST:0.01317  AvgTextLen:28.5  AvgSpecLen:100.2  StepTime:0.50  LR:0.000100\n",
      "   | > Step:46/68  GlobalStep:92300  TotalLoss:0.00244  PostnetLoss:0.00118  DecoderLoss:0.00126  StopLoss:0.04449  GradNorm:0.00313  GradNormST:0.01178  AvgTextLen:35.4  AvgSpecLen:122.6  StepTime:0.67  LR:0.000100\n",
      "   | > Step:56/68  GlobalStep:92310  TotalLoss:0.00265  PostnetLoss:0.00128  DecoderLoss:0.00137  StopLoss:0.03605  GradNorm:0.00317  GradNormST:0.01751  AvgTextLen:45.8  AvgSpecLen:145.4  StepTime:0.95  LR:0.000100\n",
      "   | > Step:66/68  GlobalStep:92320  TotalLoss:0.00312  PostnetLoss:0.00150  DecoderLoss:0.00162  StopLoss:0.03267  GradNorm:0.00489  GradNormST:0.01159  AvgTextLen:71.2  AvgSpecLen:220.8  StepTime:1.03  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:92322  AvgTotalLoss:0.05248  AvgPostnetLoss:0.00108  AvgDecoderLoss:0.00115  AvgStopLoss:0.05025  EpochTime:42.31  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.09143   PostnetLoss: 0.00487   DecoderLoss:0.00527  StopLoss: 0.08130  \n",
      "   | > TotalLoss: 0.07312   PostnetLoss: 0.00797   DecoderLoss:0.00856  StopLoss: 0.05659  \n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00108   Validation Loss: 0.00713\n",
      "\n",
      " > Epoch 338/1000\n",
      "   | > Step:7/68  GlobalStep:92330  TotalLoss:0.00164  PostnetLoss:0.00080  DecoderLoss:0.00084  StopLoss:0.04458  GradNorm:0.00358  GradNormST:0.01734  AvgTextLen:7.4  AvgSpecLen:44.6  StepTime:0.38  LR:0.000100\n",
      "   | > Step:17/68  GlobalStep:92340  TotalLoss:0.00187  PostnetLoss:0.00091  DecoderLoss:0.00096  StopLoss:0.06317  GradNorm:0.00359  GradNormST:0.01427  AvgTextLen:14.7  AvgSpecLen:66.0  StepTime:0.34  LR:0.000100\n",
      "   | > Step:27/68  GlobalStep:92350  TotalLoss:0.00213  PostnetLoss:0.00103  DecoderLoss:0.00109  StopLoss:0.04618  GradNorm:0.00307  GradNormST:0.01002  AvgTextLen:21.0  AvgSpecLen:80.2  StepTime:0.64  LR:0.000100\n",
      "   | > Step:37/68  GlobalStep:92360  TotalLoss:0.00230  PostnetLoss:0.00111  DecoderLoss:0.00118  StopLoss:0.04025  GradNorm:0.00300  GradNormST:0.01074  AvgTextLen:29.3  AvgSpecLen:102.4  StepTime:0.59  LR:0.000100\n",
      "   | > Step:47/68  GlobalStep:92370  TotalLoss:0.00255  PostnetLoss:0.00123  DecoderLoss:0.00132  StopLoss:0.04008  GradNorm:0.00292  GradNormST:0.00900  AvgTextLen:36.7  AvgSpecLen:126.9  StepTime:0.68  LR:0.000100\n",
      "   | > Step:57/68  GlobalStep:92380  TotalLoss:0.00278  PostnetLoss:0.00134  DecoderLoss:0.00144  StopLoss:0.03605  GradNorm:0.00521  GradNormST:0.00598  AvgTextLen:47.1  AvgSpecLen:161.0  StepTime:0.81  LR:0.000100\n",
      "   | > Step:67/68  GlobalStep:92390  TotalLoss:0.00311  PostnetLoss:0.00149  DecoderLoss:0.00162  StopLoss:0.04064  GradNorm:0.00327  GradNormST:0.01333  AvgTextLen:80.6  AvgSpecLen:246.5  StepTime:1.21  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:92391  AvgTotalLoss:0.04956  AvgPostnetLoss:0.00108  AvgDecoderLoss:0.00115  AvgStopLoss:0.04733  EpochTime:42.22  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.09754   PostnetLoss: 0.00492   DecoderLoss:0.00532  StopLoss: 0.08730  \n",
      "   | > TotalLoss: 0.08020   PostnetLoss: 0.00856   DecoderLoss:0.00918  StopLoss: 0.06246  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00108   Validation Loss: 0.00760\n",
      "\n",
      " > Epoch 339/1000\n",
      "   | > Step:8/68  GlobalStep:92400  TotalLoss:0.00166  PostnetLoss:0.00081  DecoderLoss:0.00085  StopLoss:0.06403  GradNorm:0.00403  GradNormST:0.01849  AvgTextLen:8.3  AvgSpecLen:44.0  StepTime:0.24  LR:0.000100\n",
      "   | > Step:18/68  GlobalStep:92410  TotalLoss:0.00187  PostnetLoss:0.00091  DecoderLoss:0.00096  StopLoss:0.04961  GradNorm:0.00333  GradNormST:0.01141  AvgTextLen:15.2  AvgSpecLen:64.5  StepTime:0.42  LR:0.000100\n",
      "   | > Step:28/68  GlobalStep:92420  TotalLoss:0.00211  PostnetLoss:0.00102  DecoderLoss:0.00109  StopLoss:0.06001  GradNorm:0.00318  GradNormST:0.01384  AvgTextLen:22.1  AvgSpecLen:85.4  StepTime:0.55  LR:0.000100\n",
      "   | > Step:38/68  GlobalStep:92430  TotalLoss:0.00227  PostnetLoss:0.00110  DecoderLoss:0.00117  StopLoss:0.03467  GradNorm:0.00308  GradNormST:0.00926  AvgTextLen:29.4  AvgSpecLen:104.1  StepTime:0.53  LR:0.000100\n",
      "   | > Step:48/68  GlobalStep:92440  TotalLoss:0.00250  PostnetLoss:0.00121  DecoderLoss:0.00129  StopLoss:0.04274  GradNorm:0.00284  GradNormST:0.01310  AvgTextLen:37.5  AvgSpecLen:123.8  StepTime:0.73  LR:0.000100\n",
      "   | > Step:58/68  GlobalStep:92450  TotalLoss:0.00272  PostnetLoss:0.00132  DecoderLoss:0.00141  StopLoss:0.04683  GradNorm:0.00392  GradNormST:0.01521  AvgTextLen:47.8  AvgSpecLen:158.0  StepTime:0.91  LR:0.000100\n",
      "   | > Step:68/68  GlobalStep:92460  TotalLoss:0.00306  PostnetLoss:0.00147  DecoderLoss:0.00159  StopLoss:0.03746  GradNorm:0.00330  GradNormST:0.01527  AvgTextLen:103.0  AvgSpecLen:308.9  StepTime:1.91  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:92460  AvgTotalLoss:0.05070  AvgPostnetLoss:0.00108  AvgDecoderLoss:0.00115  AvgStopLoss:0.04847  EpochTime:42.82  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.09410   PostnetLoss: 0.00494   DecoderLoss:0.00534  StopLoss: 0.08383  \n",
      "   | > TotalLoss: 0.07939   PostnetLoss: 0.00846   DecoderLoss:0.00907  StopLoss: 0.06186  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00108   Validation Loss: 0.00750\n",
      "\n",
      " > Epoch 340/1000\n",
      "   | > Step:9/68  GlobalStep:92470  TotalLoss:0.00164  PostnetLoss:0.00080  DecoderLoss:0.00084  StopLoss:0.04943  GradNorm:0.00367  GradNormST:0.01401  AvgTextLen:9.0  AvgSpecLen:42.1  StepTime:0.42  LR:0.000100\n",
      "   | > Step:19/68  GlobalStep:92480  TotalLoss:0.00187  PostnetLoss:0.00091  DecoderLoss:0.00096  StopLoss:0.06546  GradNorm:0.00346  GradNormST:0.01600  AvgTextLen:15.5  AvgSpecLen:63.2  StepTime:0.37  LR:0.000100\n",
      "   | > Step:29/68  GlobalStep:92490  TotalLoss:0.00207  PostnetLoss:0.00101  DecoderLoss:0.00107  StopLoss:0.05516  GradNorm:0.00305  GradNormST:0.00929  AvgTextLen:21.8  AvgSpecLen:83.4  StepTime:0.55  LR:0.000100\n",
      "   | > Step:39/68  GlobalStep:92500  TotalLoss:0.00237  PostnetLoss:0.00115  DecoderLoss:0.00122  StopLoss:0.04110  GradNorm:0.00309  GradNormST:0.01394  AvgTextLen:30.2  AvgSpecLen:110.4  StepTime:0.68  LR:0.000100\n",
      "   | > Step:49/68  GlobalStep:92510  TotalLoss:0.00251  PostnetLoss:0.00121  DecoderLoss:0.00130  StopLoss:0.04627  GradNorm:0.00292  GradNormST:0.00986  AvgTextLen:38.2  AvgSpecLen:127.7  StepTime:0.77  LR:0.000100\n",
      "   | > Step:59/68  GlobalStep:92520  TotalLoss:0.00273  PostnetLoss:0.00132  DecoderLoss:0.00141  StopLoss:0.03588  GradNorm:0.00425  GradNormST:0.00654  AvgTextLen:50.5  AvgSpecLen:155.9  StepTime:0.97  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:92529  AvgTotalLoss:0.05043  AvgPostnetLoss:0.00108  AvgDecoderLoss:0.00115  AvgStopLoss:0.04821  EpochTime:42.67  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.09183   PostnetLoss: 0.00499   DecoderLoss:0.00538  StopLoss: 0.08146  \n",
      "   | > TotalLoss: 0.07529   PostnetLoss: 0.00838   DecoderLoss:0.00893  StopLoss: 0.05798  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00108   Validation Loss: 0.00752\n",
      "\n",
      " > Epoch 341/1000\n",
      "   | > Step:0/68  GlobalStep:92530  TotalLoss:0.00159  PostnetLoss:0.00076  DecoderLoss:0.00082  StopLoss:0.06433  GradNorm:0.00633  GradNormST:0.02571  AvgTextLen:3.0  AvgSpecLen:25.3  StepTime:0.54  LR:0.000100\n",
      "   | > Step:10/68  GlobalStep:92540  TotalLoss:0.00173  PostnetLoss:0.00084  DecoderLoss:0.00089  StopLoss:0.05821  GradNorm:0.00383  GradNormST:0.02469  AvgTextLen:9.8  AvgSpecLen:48.6  StepTime:0.51  LR:0.000100\n",
      "   | > Step:20/68  GlobalStep:92550  TotalLoss:0.00201  PostnetLoss:0.00098  DecoderLoss:0.00103  StopLoss:0.08024  GradNorm:0.00353  GradNormST:0.05956  AvgTextLen:16.4  AvgSpecLen:64.8  StepTime:0.49  LR:0.000100\n",
      "   | > Step:30/68  GlobalStep:92560  TotalLoss:0.00214  PostnetLoss:0.00104  DecoderLoss:0.00110  StopLoss:0.08481  GradNorm:0.00359  GradNormST:0.07670  AvgTextLen:22.9  AvgSpecLen:85.8  StepTime:0.57  LR:0.000100\n",
      "   | > Step:40/68  GlobalStep:92570  TotalLoss:0.00235  PostnetLoss:0.00114  DecoderLoss:0.00121  StopLoss:0.06243  GradNorm:0.00332  GradNormST:0.03694  AvgTextLen:30.9  AvgSpecLen:105.3  StepTime:0.64  LR:0.000100\n",
      "   | > Step:50/68  GlobalStep:92580  TotalLoss:0.00252  PostnetLoss:0.00122  DecoderLoss:0.00131  StopLoss:0.07658  GradNorm:0.00341  GradNormST:0.05810  AvgTextLen:38.8  AvgSpecLen:132.0  StepTime:0.79  LR:0.000100\n",
      "   | > Step:60/68  GlobalStep:92590  TotalLoss:0.00281  PostnetLoss:0.00136  DecoderLoss:0.00145  StopLoss:0.06077  GradNorm:0.00384  GradNormST:0.04579  AvgTextLen:52.0  AvgSpecLen:172.7  StepTime:0.78  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:92598  AvgTotalLoss:0.07530  AvgPostnetLoss:0.00108  AvgDecoderLoss:0.00115  AvgStopLoss:0.07307  EpochTime:42.42  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.09475   PostnetLoss: 0.00498   DecoderLoss:0.00536  StopLoss: 0.08441  \n",
      "   | > TotalLoss: 0.07514   PostnetLoss: 0.00798   DecoderLoss:0.00851  StopLoss: 0.05865  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00108   Validation Loss: 0.00721\n",
      "\n",
      " > Epoch 342/1000\n",
      "   | > Step:1/68  GlobalStep:92600  TotalLoss:0.00156  PostnetLoss:0.00076  DecoderLoss:0.00080  StopLoss:0.05534  GradNorm:0.00641  GradNormST:0.01563  AvgTextLen:3.7  AvgSpecLen:28.6  StepTime:0.36  LR:0.000100\n",
      "   | > Step:11/68  GlobalStep:92610  TotalLoss:0.00172  PostnetLoss:0.00084  DecoderLoss:0.00088  StopLoss:0.10147  GradNorm:0.00357  GradNormST:0.08518  AvgTextLen:10.3  AvgSpecLen:49.4  StepTime:0.39  LR:0.000100\n",
      "   | > Step:21/68  GlobalStep:92620  TotalLoss:0.00206  PostnetLoss:0.00100  DecoderLoss:0.00106  StopLoss:0.09413  GradNorm:0.00357  GradNormST:0.05297  AvgTextLen:16.6  AvgSpecLen:66.3  StepTime:0.43  LR:0.000100\n",
      "   | > Step:31/68  GlobalStep:92630  TotalLoss:0.00217  PostnetLoss:0.00105  DecoderLoss:0.00111  StopLoss:0.08981  GradNorm:0.00320  GradNormST:0.05874  AvgTextLen:24.1  AvgSpecLen:82.7  StepTime:0.54  LR:0.000100\n",
      "   | > Step:41/68  GlobalStep:92640  TotalLoss:0.00240  PostnetLoss:0.00116  DecoderLoss:0.00123  StopLoss:0.06449  GradNorm:0.00334  GradNormST:0.05248  AvgTextLen:32.3  AvgSpecLen:109.4  StepTime:0.63  LR:0.000100\n",
      "   | > Step:51/68  GlobalStep:92650  TotalLoss:0.00250  PostnetLoss:0.00121  DecoderLoss:0.00129  StopLoss:0.07313  GradNorm:0.00343  GradNormST:0.05204  AvgTextLen:39.2  AvgSpecLen:126.9  StepTime:0.79  LR:0.000100\n",
      "   | > Step:61/68  GlobalStep:92660  TotalLoss:0.00282  PostnetLoss:0.00136  DecoderLoss:0.00146  StopLoss:0.09340  GradNorm:0.00428  GradNormST:0.08917  AvgTextLen:54.6  AvgSpecLen:165.5  StepTime:0.89  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:92667  AvgTotalLoss:0.08323  AvgPostnetLoss:0.00109  AvgDecoderLoss:0.00116  AvgStopLoss:0.08099  EpochTime:43.15  AvgStepTime:0.63\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.09641   PostnetLoss: 0.00489   DecoderLoss:0.00525  StopLoss: 0.08627  \n",
      "   | > TotalLoss: 0.07835   PostnetLoss: 0.00803   DecoderLoss:0.00853  StopLoss: 0.06180  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00109   Validation Loss: 0.00730\n",
      "\n",
      " > Epoch 343/1000\n",
      "   | > Step:2/68  GlobalStep:92670  TotalLoss:0.00143  PostnetLoss:0.00070  DecoderLoss:0.00074  StopLoss:0.06178  GradNorm:0.00443  GradNormST:0.02801  AvgTextLen:4.0  AvgSpecLen:31.3  StepTime:0.55  LR:0.000100\n",
      "   | > Step:12/68  GlobalStep:92680  TotalLoss:0.00171  PostnetLoss:0.00083  DecoderLoss:0.00088  StopLoss:0.05801  GradNorm:0.00408  GradNormST:0.01967  AvgTextLen:11.0  AvgSpecLen:49.3  StepTime:0.40  LR:0.000100\n",
      "   | > Step:22/68  GlobalStep:92690  TotalLoss:0.00198  PostnetLoss:0.00096  DecoderLoss:0.00102  StopLoss:0.08618  GradNorm:0.00323  GradNormST:0.02452  AvgTextLen:18.0  AvgSpecLen:72.0  StepTime:0.48  LR:0.000100\n",
      "   | > Step:32/68  GlobalStep:92700  TotalLoss:0.00219  PostnetLoss:0.00106  DecoderLoss:0.00113  StopLoss:0.06313  GradNorm:0.00292  GradNormST:0.01762  AvgTextLen:24.4  AvgSpecLen:91.5  StepTime:0.56  LR:0.000100\n",
      "   | > Step:42/68  GlobalStep:92710  TotalLoss:0.00238  PostnetLoss:0.00115  DecoderLoss:0.00123  StopLoss:0.06430  GradNorm:0.00314  GradNormST:0.02468  AvgTextLen:33.1  AvgSpecLen:109.7  StepTime:0.49  LR:0.000100\n",
      "   | > Step:52/68  GlobalStep:92720  TotalLoss:0.00263  PostnetLoss:0.00127  DecoderLoss:0.00136  StopLoss:0.05192  GradNorm:0.00433  GradNormST:0.01616  AvgTextLen:42.1  AvgSpecLen:137.5  StepTime:0.67  LR:0.000100\n",
      "   | > Step:62/68  GlobalStep:92730  TotalLoss:0.00288  PostnetLoss:0.00139  DecoderLoss:0.00149  StopLoss:0.05177  GradNorm:0.00592  GradNormST:0.01866  AvgTextLen:54.0  AvgSpecLen:174.7  StepTime:0.76  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:92736  AvgTotalLoss:0.06153  AvgPostnetLoss:0.00108  AvgDecoderLoss:0.00115  AvgStopLoss:0.05930  EpochTime:42.51  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.09590   PostnetLoss: 0.00484   DecoderLoss:0.00522  StopLoss: 0.08584  \n",
      "   | > TotalLoss: 0.07521   PostnetLoss: 0.00787   DecoderLoss:0.00840  StopLoss: 0.05894  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00108   Validation Loss: 0.00714\n",
      "\n",
      " > Epoch 344/1000\n",
      "   | > Step:3/68  GlobalStep:92740  TotalLoss:0.00150  PostnetLoss:0.00073  DecoderLoss:0.00077  StopLoss:0.08275  GradNorm:0.00432  GradNormST:0.02887  AvgTextLen:4.7  AvgSpecLen:29.7  StepTime:0.31  LR:0.000100\n",
      "   | > Step:13/68  GlobalStep:92750  TotalLoss:0.00175  PostnetLoss:0.00085  DecoderLoss:0.00090  StopLoss:0.06931  GradNorm:0.00419  GradNormST:0.02823  AvgTextLen:11.5  AvgSpecLen:55.2  StepTime:0.42  LR:0.000100\n",
      "   | > Step:23/68  GlobalStep:92760  TotalLoss:0.00207  PostnetLoss:0.00101  DecoderLoss:0.00107  StopLoss:0.05487  GradNorm:0.00381  GradNormST:0.01211  AvgTextLen:18.3  AvgSpecLen:72.2  StepTime:0.45  LR:0.000100\n",
      "   | > Step:33/68  GlobalStep:92770  TotalLoss:0.00226  PostnetLoss:0.00110  DecoderLoss:0.00116  StopLoss:0.04746  GradNorm:0.00370  GradNormST:0.01173  AvgTextLen:25.9  AvgSpecLen:88.9  StepTime:0.52  LR:0.000100\n",
      "   | > Step:43/68  GlobalStep:92780  TotalLoss:0.00238  PostnetLoss:0.00115  DecoderLoss:0.00123  StopLoss:0.06193  GradNorm:0.00290  GradNormST:0.01543  AvgTextLen:33.4  AvgSpecLen:115.7  StepTime:0.61  LR:0.000100\n",
      "   | > Step:53/68  GlobalStep:92790  TotalLoss:0.00256  PostnetLoss:0.00124  DecoderLoss:0.00132  StopLoss:0.03584  GradNorm:0.00368  GradNormST:0.00760  AvgTextLen:42.4  AvgSpecLen:137.6  StepTime:0.76  LR:0.000100\n",
      "   | > Step:63/68  GlobalStep:92800  TotalLoss:0.00291  PostnetLoss:0.00140  DecoderLoss:0.00151  StopLoss:0.03899  GradNorm:0.00453  GradNormST:0.02080  AvgTextLen:58.6  AvgSpecLen:185.1  StepTime:0.91  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:92805  AvgTotalLoss:0.05417  AvgPostnetLoss:0.00108  AvgDecoderLoss:0.00115  AvgStopLoss:0.05194  EpochTime:42.94  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.09027   PostnetLoss: 0.00491   DecoderLoss:0.00530  StopLoss: 0.08006  \n",
      "   | > TotalLoss: 0.07825   PostnetLoss: 0.00795   DecoderLoss:0.00850  StopLoss: 0.06181  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00108   Validation Loss: 0.00718\n",
      "\n",
      " > Epoch 345/1000\n",
      "   | > Step:4/68  GlobalStep:92810  TotalLoss:0.00151  PostnetLoss:0.00073  DecoderLoss:0.00078  StopLoss:0.09572  GradNorm:0.00452  GradNormST:0.04040  AvgTextLen:4.8  AvgSpecLen:31.1  StepTime:0.25  LR:0.000100\n",
      "   | > Step:14/68  GlobalStep:92820  TotalLoss:0.00181  PostnetLoss:0.00088  DecoderLoss:0.00093  StopLoss:0.05949  GradNorm:0.00394  GradNormST:0.01932  AvgTextLen:12.2  AvgSpecLen:52.0  StepTime:0.26  LR:0.000100\n",
      "   | > Step:24/68  GlobalStep:92830  TotalLoss:0.00212  PostnetLoss:0.00103  DecoderLoss:0.00109  StopLoss:0.05329  GradNorm:0.00382  GradNormST:0.01572  AvgTextLen:19.2  AvgSpecLen:79.0  StepTime:0.48  LR:0.000100\n",
      "   | > Step:34/68  GlobalStep:92840  TotalLoss:0.00224  PostnetLoss:0.00109  DecoderLoss:0.00116  StopLoss:0.04350  GradNorm:0.00319  GradNormST:0.01014  AvgTextLen:26.2  AvgSpecLen:95.9  StepTime:0.59  LR:0.000100\n",
      "   | > Step:44/68  GlobalStep:92850  TotalLoss:0.00245  PostnetLoss:0.00118  DecoderLoss:0.00126  StopLoss:0.04294  GradNorm:0.00382  GradNormST:0.01270  AvgTextLen:33.8  AvgSpecLen:116.3  StepTime:0.69  LR:0.000100\n",
      "   | > Step:54/68  GlobalStep:92860  TotalLoss:0.00262  PostnetLoss:0.00126  DecoderLoss:0.00136  StopLoss:0.04885  GradNorm:0.00377  GradNormST:0.01269  AvgTextLen:42.2  AvgSpecLen:144.9  StepTime:0.86  LR:0.000100\n",
      "   | > Step:64/68  GlobalStep:92870  TotalLoss:0.00271  PostnetLoss:0.00131  DecoderLoss:0.00140  StopLoss:0.03882  GradNorm:0.00357  GradNormST:0.01294  AvgTextLen:60.6  AvgSpecLen:177.6  StepTime:0.81  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:92874  AvgTotalLoss:0.05356  AvgPostnetLoss:0.00108  AvgDecoderLoss:0.00115  AvgStopLoss:0.05133  EpochTime:43.05  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.09181   PostnetLoss: 0.00494   DecoderLoss:0.00533  StopLoss: 0.08154  \n",
      "   | > TotalLoss: 0.07742   PostnetLoss: 0.00775   DecoderLoss:0.00830  StopLoss: 0.06136  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00108   Validation Loss: 0.00707\n",
      "\n",
      " > Epoch 346/1000\n",
      "   | > Step:5/68  GlobalStep:92880  TotalLoss:0.00154  PostnetLoss:0.00075  DecoderLoss:0.00080  StopLoss:0.07897  GradNorm:0.00360  GradNormST:0.03555  AvgTextLen:5.9  AvgSpecLen:33.2  StepTime:0.40  LR:0.000100\n",
      "   | > Step:15/68  GlobalStep:92890  TotalLoss:0.00188  PostnetLoss:0.00092  DecoderLoss:0.00097  StopLoss:0.05376  GradNorm:0.00393  GradNormST:0.01233  AvgTextLen:12.8  AvgSpecLen:59.1  StepTime:0.50  LR:0.000100\n",
      "   | > Step:25/68  GlobalStep:92900  TotalLoss:0.00204  PostnetLoss:0.00099  DecoderLoss:0.00105  StopLoss:0.05751  GradNorm:0.00388  GradNormST:0.01494  AvgTextLen:19.8  AvgSpecLen:73.3  StepTime:0.48  LR:0.000100\n",
      "   | > Step:35/68  GlobalStep:92910  TotalLoss:0.00228  PostnetLoss:0.00111  DecoderLoss:0.00117  StopLoss:0.05709  GradNorm:0.00364  GradNormST:0.01714  AvgTextLen:27.1  AvgSpecLen:104.7  StepTime:0.63  LR:0.000100\n",
      "   | > Step:45/68  GlobalStep:92920  TotalLoss:0.00236  PostnetLoss:0.00114  DecoderLoss:0.00122  StopLoss:0.05605  GradNorm:0.00288  GradNormST:0.01935  AvgTextLen:34.4  AvgSpecLen:118.6  StepTime:0.64  LR:0.000100\n",
      "   | > Step:55/68  GlobalStep:92930  TotalLoss:0.00268  PostnetLoss:0.00129  DecoderLoss:0.00138  StopLoss:0.03755  GradNorm:0.00382  GradNormST:0.00945  AvgTextLen:43.9  AvgSpecLen:145.2  StepTime:0.80  LR:0.000100\n",
      "   | > Step:65/68  GlobalStep:92940  TotalLoss:0.00293  PostnetLoss:0.00141  DecoderLoss:0.00152  StopLoss:0.04227  GradNorm:0.00355  GradNormST:0.01819  AvgTextLen:64.2  AvgSpecLen:207.4  StepTime:1.20  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:92943  AvgTotalLoss:0.05293  AvgPostnetLoss:0.00108  AvgDecoderLoss:0.00115  AvgStopLoss:0.05071  EpochTime:42.20  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.09438   PostnetLoss: 0.00498   DecoderLoss:0.00537  StopLoss: 0.08404  \n",
      "   | > TotalLoss: 0.07600   PostnetLoss: 0.00782   DecoderLoss:0.00836  StopLoss: 0.05982  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00108   Validation Loss: 0.00710\n",
      "\n",
      " > Epoch 347/1000\n",
      "   | > Step:6/68  GlobalStep:92950  TotalLoss:0.00157  PostnetLoss:0.00076  DecoderLoss:0.00081  StopLoss:0.04458  GradNorm:0.00390  GradNormST:0.01889  AvgTextLen:6.0  AvgSpecLen:34.8  StepTime:0.36  LR:0.000100\n",
      "   | > Step:16/68  GlobalStep:92960  TotalLoss:0.00191  PostnetLoss:0.00093  DecoderLoss:0.00098  StopLoss:0.04748  GradNorm:0.00464  GradNormST:0.01917  AvgTextLen:13.8  AvgSpecLen:60.6  StepTime:0.36  LR:0.000100\n",
      "   | > Step:26/68  GlobalStep:92970  TotalLoss:0.00207  PostnetLoss:0.00101  DecoderLoss:0.00106  StopLoss:0.05598  GradNorm:0.00400  GradNormST:0.01427  AvgTextLen:20.3  AvgSpecLen:77.5  StepTime:0.42  LR:0.000100\n",
      "   | > Step:36/68  GlobalStep:92980  TotalLoss:0.00227  PostnetLoss:0.00110  DecoderLoss:0.00117  StopLoss:0.07213  GradNorm:0.00358  GradNormST:0.01474  AvgTextLen:28.5  AvgSpecLen:100.2  StepTime:0.57  LR:0.000100\n",
      "   | > Step:46/68  GlobalStep:92990  TotalLoss:0.00243  PostnetLoss:0.00118  DecoderLoss:0.00126  StopLoss:0.04461  GradNorm:0.00671  GradNormST:0.01587  AvgTextLen:35.4  AvgSpecLen:122.6  StepTime:0.77  LR:0.000100\n",
      "   | > Step:56/68  GlobalStep:93000  TotalLoss:0.00264  PostnetLoss:0.00127  DecoderLoss:0.00137  StopLoss:0.04144  GradNorm:0.00339  GradNormST:0.01775  AvgTextLen:45.8  AvgSpecLen:145.4  StepTime:1.05  LR:0.000100\n",
      " | | > Checkpoint saving : /vm/TTS/models/cartman_models/mozilla-no-loc-fattn-stopnet-sigmoid-loss_masking-August-15-2019_11+26AM-de6aeb9/checkpoint_93000.pth.tar\n",
      "warning: audio amplitude out of range, auto clipped.\n",
      "   | > Step:66/68  GlobalStep:93010  TotalLoss:0.00309  PostnetLoss:0.00148  DecoderLoss:0.00160  StopLoss:0.04080  GradNorm:0.00306  GradNormST:0.01544  AvgTextLen:71.2  AvgSpecLen:220.8  StepTime:1.04  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:93012  AvgTotalLoss:0.05314  AvgPostnetLoss:0.00107  AvgDecoderLoss:0.00114  AvgStopLoss:0.05093  EpochTime:42.99  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.09665   PostnetLoss: 0.00493   DecoderLoss:0.00531  StopLoss: 0.08642  \n",
      "   | > TotalLoss: 0.07689   PostnetLoss: 0.00773   DecoderLoss:0.00827  StopLoss: 0.06089  \n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00107   Validation Loss: 0.00702\n",
      "\n",
      " > Epoch 348/1000\n",
      "   | > Step:7/68  GlobalStep:93020  TotalLoss:0.00162  PostnetLoss:0.00079  DecoderLoss:0.00083  StopLoss:0.05255  GradNorm:0.00385  GradNormST:0.01976  AvgTextLen:7.4  AvgSpecLen:44.6  StepTime:0.30  LR:0.000100\n",
      "   | > Step:17/68  GlobalStep:93030  TotalLoss:0.00186  PostnetLoss:0.00090  DecoderLoss:0.00096  StopLoss:0.06988  GradNorm:0.00363  GradNormST:0.01403  AvgTextLen:14.7  AvgSpecLen:66.0  StepTime:0.43  LR:0.000100\n",
      "   | > Step:27/68  GlobalStep:93040  TotalLoss:0.00216  PostnetLoss:0.00105  DecoderLoss:0.00111  StopLoss:0.05219  GradNorm:0.00373  GradNormST:0.01122  AvgTextLen:21.0  AvgSpecLen:80.2  StepTime:0.65  LR:0.000100\n",
      "   | > Step:37/68  GlobalStep:93050  TotalLoss:0.00232  PostnetLoss:0.00112  DecoderLoss:0.00119  StopLoss:0.06010  GradNorm:0.00346  GradNormST:0.01527  AvgTextLen:29.3  AvgSpecLen:102.4  StepTime:0.57  LR:0.000100\n",
      "   | > Step:47/68  GlobalStep:93060  TotalLoss:0.00251  PostnetLoss:0.00121  DecoderLoss:0.00130  StopLoss:0.04004  GradNorm:0.00312  GradNormST:0.01608  AvgTextLen:36.7  AvgSpecLen:126.9  StepTime:0.58  LR:0.000100\n",
      "   | > Step:57/68  GlobalStep:93070  TotalLoss:0.00271  PostnetLoss:0.00131  DecoderLoss:0.00140  StopLoss:0.03453  GradNorm:0.00356  GradNormST:0.00751  AvgTextLen:47.1  AvgSpecLen:161.0  StepTime:0.90  LR:0.000100\n",
      "   | > Step:67/68  GlobalStep:93080  TotalLoss:0.00307  PostnetLoss:0.00147  DecoderLoss:0.00159  StopLoss:0.03862  GradNorm:0.00284  GradNormST:0.01294  AvgTextLen:80.6  AvgSpecLen:246.5  StepTime:1.21  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:93081  AvgTotalLoss:0.05469  AvgPostnetLoss:0.00108  AvgDecoderLoss:0.00114  AvgStopLoss:0.05247  EpochTime:41.64  AvgStepTime:0.60\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.08663   PostnetLoss: 0.00494   DecoderLoss:0.00532  StopLoss: 0.07636  \n",
      "   | > TotalLoss: 0.07623   PostnetLoss: 0.00766   DecoderLoss:0.00819  StopLoss: 0.06039  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00108   Validation Loss: 0.00699\n",
      "\n",
      " > Epoch 349/1000\n",
      "   | > Step:8/68  GlobalStep:93090  TotalLoss:0.00168  PostnetLoss:0.00082  DecoderLoss:0.00086  StopLoss:0.05928  GradNorm:0.00441  GradNormST:0.01535  AvgTextLen:8.3  AvgSpecLen:44.0  StepTime:0.31  LR:0.000100\n",
      "   | > Step:18/68  GlobalStep:93100  TotalLoss:0.00187  PostnetLoss:0.00091  DecoderLoss:0.00096  StopLoss:0.06211  GradNorm:0.00406  GradNormST:0.01449  AvgTextLen:15.2  AvgSpecLen:64.5  StepTime:0.41  LR:0.000100\n",
      "   | > Step:28/68  GlobalStep:93110  TotalLoss:0.00214  PostnetLoss:0.00104  DecoderLoss:0.00110  StopLoss:0.05452  GradNorm:0.00382  GradNormST:0.01295  AvgTextLen:22.1  AvgSpecLen:85.4  StepTime:0.52  LR:0.000100\n",
      "   | > Step:38/68  GlobalStep:93120  TotalLoss:0.00233  PostnetLoss:0.00113  DecoderLoss:0.00120  StopLoss:0.04161  GradNorm:0.00392  GradNormST:0.00959  AvgTextLen:29.4  AvgSpecLen:104.1  StepTime:0.64  LR:0.000100\n",
      "   | > Step:48/68  GlobalStep:93130  TotalLoss:0.00252  PostnetLoss:0.00122  DecoderLoss:0.00130  StopLoss:0.04951  GradNorm:0.00324  GradNormST:0.01777  AvgTextLen:37.5  AvgSpecLen:123.8  StepTime:0.77  LR:0.000100\n",
      "   | > Step:58/68  GlobalStep:93140  TotalLoss:0.00270  PostnetLoss:0.00130  DecoderLoss:0.00139  StopLoss:0.04169  GradNorm:0.00296  GradNormST:0.01198  AvgTextLen:47.8  AvgSpecLen:158.0  StepTime:0.84  LR:0.000100\n",
      "   | > Step:68/68  GlobalStep:93150  TotalLoss:0.00301  PostnetLoss:0.00144  DecoderLoss:0.00156  StopLoss:0.03202  GradNorm:0.00277  GradNormST:0.01182  AvgTextLen:103.0  AvgSpecLen:308.9  StepTime:1.86  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:93150  AvgTotalLoss:0.05241  AvgPostnetLoss:0.00108  AvgDecoderLoss:0.00115  AvgStopLoss:0.05019  EpochTime:41.88  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.09090   PostnetLoss: 0.00487   DecoderLoss:0.00525  StopLoss: 0.08078  \n",
      "   | > TotalLoss: 0.07723   PostnetLoss: 0.00759   DecoderLoss:0.00814  StopLoss: 0.06151  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00108   Validation Loss: 0.00694\n",
      "\n",
      " > Epoch 350/1000\n",
      "   | > Step:9/68  GlobalStep:93160  TotalLoss:0.00162  PostnetLoss:0.00079  DecoderLoss:0.00083  StopLoss:0.04283  GradNorm:0.00368  GradNormST:0.01400  AvgTextLen:9.0  AvgSpecLen:42.1  StepTime:0.49  LR:0.000100\n",
      "   | > Step:19/68  GlobalStep:93170  TotalLoss:0.00188  PostnetLoss:0.00092  DecoderLoss:0.00096  StopLoss:0.07492  GradNorm:0.00422  GradNormST:0.01870  AvgTextLen:15.5  AvgSpecLen:63.2  StepTime:0.33  LR:0.000100\n",
      "   | > Step:29/68  GlobalStep:93180  TotalLoss:0.00209  PostnetLoss:0.00101  DecoderLoss:0.00107  StopLoss:0.06473  GradNorm:0.00364  GradNormST:0.01420  AvgTextLen:21.8  AvgSpecLen:83.4  StepTime:0.59  LR:0.000100\n",
      "   | > Step:39/68  GlobalStep:93190  TotalLoss:0.00235  PostnetLoss:0.00114  DecoderLoss:0.00121  StopLoss:0.05501  GradNorm:0.00374  GradNormST:0.02504  AvgTextLen:30.2  AvgSpecLen:110.4  StepTime:0.56  LR:0.000100\n",
      "   | > Step:49/68  GlobalStep:93200  TotalLoss:0.00249  PostnetLoss:0.00120  DecoderLoss:0.00128  StopLoss:0.05106  GradNorm:0.00326  GradNormST:0.01447  AvgTextLen:38.2  AvgSpecLen:127.7  StepTime:0.60  LR:0.000100\n",
      "   | > Step:59/68  GlobalStep:93210  TotalLoss:0.00268  PostnetLoss:0.00129  DecoderLoss:0.00138  StopLoss:0.04366  GradNorm:0.00276  GradNormST:0.01080  AvgTextLen:50.5  AvgSpecLen:155.9  StepTime:0.76  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:93219  AvgTotalLoss:0.05302  AvgPostnetLoss:0.00107  AvgDecoderLoss:0.00114  AvgStopLoss:0.05080  EpochTime:41.52  AvgStepTime:0.60\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.09302   PostnetLoss: 0.00492   DecoderLoss:0.00530  StopLoss: 0.08280  \n",
      "   | > TotalLoss: 0.07963   PostnetLoss: 0.00826   DecoderLoss:0.00881  StopLoss: 0.06256  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00107   Validation Loss: 0.00727\n",
      "\n",
      " > Epoch 351/1000\n",
      "   | > Step:0/68  GlobalStep:93220  TotalLoss:0.00164  PostnetLoss:0.00079  DecoderLoss:0.00085  StopLoss:0.08184  GradNorm:0.00853  GradNormST:0.03132  AvgTextLen:3.0  AvgSpecLen:25.3  StepTime:0.36  LR:0.000100\n",
      "   | > Step:10/68  GlobalStep:93230  TotalLoss:0.00170  PostnetLoss:0.00083  DecoderLoss:0.00087  StopLoss:0.05426  GradNorm:0.00376  GradNormST:0.01329  AvgTextLen:9.8  AvgSpecLen:48.6  StepTime:0.55  LR:0.000100\n",
      "   | > Step:20/68  GlobalStep:93240  TotalLoss:0.00198  PostnetLoss:0.00096  DecoderLoss:0.00102  StopLoss:0.03998  GradNorm:0.00397  GradNormST:0.01605  AvgTextLen:16.4  AvgSpecLen:64.8  StepTime:0.43  LR:0.000100\n",
      "   | > Step:30/68  GlobalStep:93250  TotalLoss:0.00214  PostnetLoss:0.00104  DecoderLoss:0.00110  StopLoss:0.04631  GradNorm:0.00359  GradNormST:0.00953  AvgTextLen:22.9  AvgSpecLen:85.8  StepTime:0.48  LR:0.000100\n",
      "   | > Step:40/68  GlobalStep:93260  TotalLoss:0.00232  PostnetLoss:0.00113  DecoderLoss:0.00120  StopLoss:0.03847  GradNorm:0.00323  GradNormST:0.01128  AvgTextLen:30.9  AvgSpecLen:105.3  StepTime:0.49  LR:0.000100\n",
      "   | > Step:50/68  GlobalStep:93270  TotalLoss:0.00250  PostnetLoss:0.00121  DecoderLoss:0.00129  StopLoss:0.03839  GradNorm:0.00325  GradNormST:0.00968  AvgTextLen:38.8  AvgSpecLen:132.0  StepTime:0.76  LR:0.000100\n",
      "   | > Step:60/68  GlobalStep:93280  TotalLoss:0.00278  PostnetLoss:0.00134  DecoderLoss:0.00144  StopLoss:0.03568  GradNorm:0.00283  GradNormST:0.00909  AvgTextLen:52.0  AvgSpecLen:172.7  StepTime:1.00  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:93288  AvgTotalLoss:0.05265  AvgPostnetLoss:0.00108  AvgDecoderLoss:0.00115  AvgStopLoss:0.05043  EpochTime:41.55  AvgStepTime:0.60\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.08533   PostnetLoss: 0.00503   DecoderLoss:0.00541  StopLoss: 0.07489  \n",
      "   | > TotalLoss: 0.07082   PostnetLoss: 0.00761   DecoderLoss:0.00814  StopLoss: 0.05508  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00108   Validation Loss: 0.00691\n",
      "\n",
      " > Epoch 352/1000\n",
      "   | > Step:1/68  GlobalStep:93290  TotalLoss:0.00155  PostnetLoss:0.00075  DecoderLoss:0.00080  StopLoss:0.03844  GradNorm:0.00590  GradNormST:0.01546  AvgTextLen:3.7  AvgSpecLen:28.6  StepTime:0.31  LR:0.000100\n",
      "   | > Step:11/68  GlobalStep:93300  TotalLoss:0.00177  PostnetLoss:0.00086  DecoderLoss:0.00090  StopLoss:0.06158  GradNorm:0.00509  GradNormST:0.02251  AvgTextLen:10.3  AvgSpecLen:49.4  StepTime:0.39  LR:0.000100\n",
      "   | > Step:21/68  GlobalStep:93310  TotalLoss:0.00199  PostnetLoss:0.00097  DecoderLoss:0.00102  StopLoss:0.05575  GradNorm:0.00422  GradNormST:0.01565  AvgTextLen:16.6  AvgSpecLen:66.3  StepTime:0.44  LR:0.000100\n",
      "   | > Step:31/68  GlobalStep:93320  TotalLoss:0.00218  PostnetLoss:0.00106  DecoderLoss:0.00112  StopLoss:0.03958  GradNorm:0.00374  GradNormST:0.01268  AvgTextLen:24.1  AvgSpecLen:82.7  StepTime:0.52  LR:0.000100\n",
      "   | > Step:41/68  GlobalStep:93330  TotalLoss:0.00244  PostnetLoss:0.00118  DecoderLoss:0.00126  StopLoss:0.03820  GradNorm:0.00358  GradNormST:0.01198  AvgTextLen:32.3  AvgSpecLen:109.4  StepTime:0.64  LR:0.000100\n",
      "   | > Step:51/68  GlobalStep:93340  TotalLoss:0.00250  PostnetLoss:0.00121  DecoderLoss:0.00129  StopLoss:0.03879  GradNorm:0.00314  GradNormST:0.00824  AvgTextLen:39.2  AvgSpecLen:126.9  StepTime:0.74  LR:0.000100\n",
      "   | > Step:61/68  GlobalStep:93350  TotalLoss:0.00274  PostnetLoss:0.00132  DecoderLoss:0.00142  StopLoss:0.06604  GradNorm:0.00372  GradNormST:0.02749  AvgTextLen:54.6  AvgSpecLen:165.5  StepTime:0.90  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:93357  AvgTotalLoss:0.05421  AvgPostnetLoss:0.00108  AvgDecoderLoss:0.00115  AvgStopLoss:0.05197  EpochTime:42.05  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.09108   PostnetLoss: 0.00494   DecoderLoss:0.00531  StopLoss: 0.08083  \n",
      "   | > TotalLoss: 0.07400   PostnetLoss: 0.00765   DecoderLoss:0.00819  StopLoss: 0.05815  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00108   Validation Loss: 0.00697\n",
      "\n",
      " > Epoch 353/1000\n",
      "   | > Step:2/68  GlobalStep:93360  TotalLoss:0.00144  PostnetLoss:0.00070  DecoderLoss:0.00074  StopLoss:0.06379  GradNorm:0.00436  GradNormST:0.02018  AvgTextLen:4.0  AvgSpecLen:31.3  StepTime:0.43  LR:0.000100\n",
      "   | > Step:12/68  GlobalStep:93370  TotalLoss:0.00167  PostnetLoss:0.00081  DecoderLoss:0.00086  StopLoss:0.06644  GradNorm:0.00343  GradNormST:0.02253  AvgTextLen:11.0  AvgSpecLen:49.3  StepTime:0.43  LR:0.000100\n",
      "   | > Step:22/68  GlobalStep:93380  TotalLoss:0.00202  PostnetLoss:0.00098  DecoderLoss:0.00104  StopLoss:0.05982  GradNorm:0.00468  GradNormST:0.01073  AvgTextLen:18.0  AvgSpecLen:72.0  StepTime:0.47  LR:0.000100\n",
      "   | > Step:32/68  GlobalStep:93390  TotalLoss:0.00220  PostnetLoss:0.00107  DecoderLoss:0.00114  StopLoss:0.06166  GradNorm:0.00416  GradNormST:0.01644  AvgTextLen:24.4  AvgSpecLen:91.5  StepTime:0.59  LR:0.000100\n",
      "   | > Step:42/68  GlobalStep:93400  TotalLoss:0.00234  PostnetLoss:0.00113  DecoderLoss:0.00121  StopLoss:0.05437  GradNorm:0.00291  GradNormST:0.01804  AvgTextLen:33.1  AvgSpecLen:109.7  StepTime:0.61  LR:0.000100\n",
      "   | > Step:52/68  GlobalStep:93410  TotalLoss:0.00261  PostnetLoss:0.00126  DecoderLoss:0.00135  StopLoss:0.04578  GradNorm:0.00360  GradNormST:0.01785  AvgTextLen:42.1  AvgSpecLen:137.5  StepTime:0.78  LR:0.000100\n",
      "   | > Step:62/68  GlobalStep:93420  TotalLoss:0.00282  PostnetLoss:0.00136  DecoderLoss:0.00146  StopLoss:0.04396  GradNorm:0.00339  GradNormST:0.01480  AvgTextLen:54.0  AvgSpecLen:174.7  StepTime:0.76  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:93426  AvgTotalLoss:0.05357  AvgPostnetLoss:0.00108  AvgDecoderLoss:0.00115  AvgStopLoss:0.05135  EpochTime:42.22  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.09141   PostnetLoss: 0.00493   DecoderLoss:0.00530  StopLoss: 0.08118  \n",
      "   | > TotalLoss: 0.07447   PostnetLoss: 0.00752   DecoderLoss:0.00806  StopLoss: 0.05890  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00108   Validation Loss: 0.00686\n",
      "\n",
      " > Epoch 354/1000\n",
      "   | > Step:3/68  GlobalStep:93430  TotalLoss:0.00147  PostnetLoss:0.00071  DecoderLoss:0.00075  StopLoss:0.07895  GradNorm:0.00385  GradNormST:0.02195  AvgTextLen:4.7  AvgSpecLen:29.7  StepTime:0.26  LR:0.000100\n",
      "   | > Step:13/68  GlobalStep:93440  TotalLoss:0.00169  PostnetLoss:0.00082  DecoderLoss:0.00086  StopLoss:0.07807  GradNorm:0.00371  GradNormST:0.02385  AvgTextLen:11.5  AvgSpecLen:55.2  StepTime:0.41  LR:0.000100\n",
      "   | > Step:23/68  GlobalStep:93450  TotalLoss:0.00207  PostnetLoss:0.00101  DecoderLoss:0.00107  StopLoss:0.05790  GradNorm:0.00367  GradNormST:0.01732  AvgTextLen:18.3  AvgSpecLen:72.2  StepTime:0.33  LR:0.000100\n",
      "   | > Step:33/68  GlobalStep:93460  TotalLoss:0.00222  PostnetLoss:0.00108  DecoderLoss:0.00114  StopLoss:0.04786  GradNorm:0.00381  GradNormST:0.01085  AvgTextLen:25.9  AvgSpecLen:88.9  StepTime:0.51  LR:0.000100\n",
      "   | > Step:43/68  GlobalStep:93470  TotalLoss:0.00240  PostnetLoss:0.00116  DecoderLoss:0.00124  StopLoss:0.05520  GradNorm:0.00344  GradNormST:0.01340  AvgTextLen:33.4  AvgSpecLen:115.7  StepTime:0.51  LR:0.000100\n",
      "   | > Step:53/68  GlobalStep:93480  TotalLoss:0.00253  PostnetLoss:0.00123  DecoderLoss:0.00131  StopLoss:0.03882  GradNorm:0.00301  GradNormST:0.00902  AvgTextLen:42.4  AvgSpecLen:137.6  StepTime:0.64  LR:0.000100\n",
      "   | > Step:63/68  GlobalStep:93490  TotalLoss:0.00288  PostnetLoss:0.00139  DecoderLoss:0.00149  StopLoss:0.03496  GradNorm:0.00392  GradNormST:0.01371  AvgTextLen:58.6  AvgSpecLen:185.1  StepTime:0.91  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:93495  AvgTotalLoss:0.05353  AvgPostnetLoss:0.00108  AvgDecoderLoss:0.00115  AvgStopLoss:0.05130  EpochTime:41.61  AvgStepTime:0.60\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.08945   PostnetLoss: 0.00494   DecoderLoss:0.00533  StopLoss: 0.07919  \n",
      "   | > TotalLoss: 0.07157   PostnetLoss: 0.00738   DecoderLoss:0.00792  StopLoss: 0.05627  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00108   Validation Loss: 0.00685\n",
      "\n",
      " > Epoch 355/1000\n",
      "   | > Step:4/68  GlobalStep:93500  TotalLoss:0.00148  PostnetLoss:0.00071  DecoderLoss:0.00076  StopLoss:0.10632  GradNorm:0.00393  GradNormST:0.04184  AvgTextLen:4.8  AvgSpecLen:31.1  StepTime:0.27  LR:0.000100\n",
      "   | > Step:14/68  GlobalStep:93510  TotalLoss:0.00181  PostnetLoss:0.00088  DecoderLoss:0.00093  StopLoss:0.05578  GradNorm:0.00437  GradNormST:0.01800  AvgTextLen:12.2  AvgSpecLen:52.0  StepTime:0.36  LR:0.000100\n",
      "   | > Step:24/68  GlobalStep:93520  TotalLoss:0.00211  PostnetLoss:0.00103  DecoderLoss:0.00109  StopLoss:0.05584  GradNorm:0.00411  GradNormST:0.01263  AvgTextLen:19.2  AvgSpecLen:79.0  StepTime:0.51  LR:0.000100\n",
      "   | > Step:34/68  GlobalStep:93530  TotalLoss:0.00225  PostnetLoss:0.00109  DecoderLoss:0.00116  StopLoss:0.03487  GradNorm:0.00370  GradNormST:0.00862  AvgTextLen:26.2  AvgSpecLen:95.9  StepTime:0.59  LR:0.000100\n",
      "   | > Step:44/68  GlobalStep:93540  TotalLoss:0.00246  PostnetLoss:0.00119  DecoderLoss:0.00127  StopLoss:0.04357  GradNorm:0.00338  GradNormST:0.01692  AvgTextLen:33.8  AvgSpecLen:116.3  StepTime:0.70  LR:0.000100\n",
      "   | > Step:54/68  GlobalStep:93550  TotalLoss:0.00259  PostnetLoss:0.00125  DecoderLoss:0.00134  StopLoss:0.04429  GradNorm:0.00333  GradNormST:0.01236  AvgTextLen:42.2  AvgSpecLen:144.9  StepTime:0.81  LR:0.000100\n",
      "   | > Step:64/68  GlobalStep:93560  TotalLoss:0.00280  PostnetLoss:0.00135  DecoderLoss:0.00145  StopLoss:0.03868  GradNorm:0.00607  GradNormST:0.00760  AvgTextLen:60.6  AvgSpecLen:177.6  StepTime:0.81  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:93564  AvgTotalLoss:0.05317  AvgPostnetLoss:0.00108  AvgDecoderLoss:0.00115  AvgStopLoss:0.05094  EpochTime:43.26  AvgStepTime:0.63\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.09262   PostnetLoss: 0.00489   DecoderLoss:0.00527  StopLoss: 0.08246  \n",
      "   | > TotalLoss: 0.07563   PostnetLoss: 0.00756   DecoderLoss:0.00810  StopLoss: 0.05997  \n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00108   Validation Loss: 0.00691\n",
      "\n",
      " > Epoch 356/1000\n",
      "   | > Step:5/68  GlobalStep:93570  TotalLoss:0.00159  PostnetLoss:0.00078  DecoderLoss:0.00082  StopLoss:0.05878  GradNorm:0.00375  GradNormST:0.01440  AvgTextLen:5.9  AvgSpecLen:33.2  StepTime:0.34  LR:0.000100\n",
      "   | > Step:15/68  GlobalStep:93580  TotalLoss:0.00185  PostnetLoss:0.00090  DecoderLoss:0.00095  StopLoss:0.04872  GradNorm:0.00358  GradNormST:0.01706  AvgTextLen:12.8  AvgSpecLen:59.1  StepTime:0.54  LR:0.000100\n",
      "   | > Step:25/68  GlobalStep:93590  TotalLoss:0.00205  PostnetLoss:0.00100  DecoderLoss:0.00105  StopLoss:0.05079  GradNorm:0.00412  GradNormST:0.01280  AvgTextLen:19.8  AvgSpecLen:73.3  StepTime:0.48  LR:0.000100\n",
      "   | > Step:35/68  GlobalStep:93600  TotalLoss:0.00229  PostnetLoss:0.00111  DecoderLoss:0.00118  StopLoss:0.05370  GradNorm:0.00373  GradNormST:0.01789  AvgTextLen:27.1  AvgSpecLen:104.7  StepTime:0.64  LR:0.000100\n",
      "   | > Step:45/68  GlobalStep:93610  TotalLoss:0.00241  PostnetLoss:0.00117  DecoderLoss:0.00124  StopLoss:0.05597  GradNorm:0.00399  GradNormST:0.01788  AvgTextLen:34.4  AvgSpecLen:118.6  StepTime:0.58  LR:0.000100\n",
      "   | > Step:55/68  GlobalStep:93620  TotalLoss:0.00265  PostnetLoss:0.00128  DecoderLoss:0.00137  StopLoss:0.04177  GradNorm:0.00347  GradNormST:0.01387  AvgTextLen:43.9  AvgSpecLen:145.2  StepTime:0.71  LR:0.000100\n",
      "   | > Step:65/68  GlobalStep:93630  TotalLoss:0.00299  PostnetLoss:0.00144  DecoderLoss:0.00155  StopLoss:0.03978  GradNorm:0.00482  GradNormST:0.01323  AvgTextLen:64.2  AvgSpecLen:207.4  StepTime:1.18  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:93633  AvgTotalLoss:0.05302  AvgPostnetLoss:0.00108  AvgDecoderLoss:0.00115  AvgStopLoss:0.05079  EpochTime:42.29  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.08241   PostnetLoss: 0.00495   DecoderLoss:0.00533  StopLoss: 0.07212  \n",
      "   | > TotalLoss: 0.06845   PostnetLoss: 0.00736   DecoderLoss:0.00792  StopLoss: 0.05317  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00108   Validation Loss: 0.00682\n",
      "\n",
      " > Epoch 357/1000\n",
      "   | > Step:6/68  GlobalStep:93640  TotalLoss:0.00158  PostnetLoss:0.00077  DecoderLoss:0.00081  StopLoss:0.04346  GradNorm:0.00420  GradNormST:0.01731  AvgTextLen:6.0  AvgSpecLen:34.8  StepTime:0.39  LR:0.000100\n",
      "   | > Step:16/68  GlobalStep:93650  TotalLoss:0.00188  PostnetLoss:0.00091  DecoderLoss:0.00096  StopLoss:0.04350  GradNorm:0.00359  GradNormST:0.01286  AvgTextLen:13.8  AvgSpecLen:60.6  StepTime:0.39  LR:0.000100\n",
      "   | > Step:26/68  GlobalStep:93660  TotalLoss:0.00210  PostnetLoss:0.00102  DecoderLoss:0.00108  StopLoss:0.04980  GradNorm:0.00404  GradNormST:0.01301  AvgTextLen:20.3  AvgSpecLen:77.5  StepTime:0.48  LR:0.000100\n",
      "   | > Step:36/68  GlobalStep:93670  TotalLoss:0.00225  PostnetLoss:0.00109  DecoderLoss:0.00116  StopLoss:0.06163  GradNorm:0.00372  GradNormST:0.01136  AvgTextLen:28.5  AvgSpecLen:100.2  StepTime:0.47  LR:0.000100\n",
      "   | > Step:46/68  GlobalStep:93680  TotalLoss:0.00244  PostnetLoss:0.00118  DecoderLoss:0.00125  StopLoss:0.04990  GradNorm:0.00344  GradNormST:0.01069  AvgTextLen:35.4  AvgSpecLen:122.6  StepTime:0.84  LR:0.000100\n",
      "   | > Step:56/68  GlobalStep:93690  TotalLoss:0.00265  PostnetLoss:0.00128  DecoderLoss:0.00137  StopLoss:0.03723  GradNorm:0.00343  GradNormST:0.01001  AvgTextLen:45.8  AvgSpecLen:145.4  StepTime:0.96  LR:0.000100\n",
      "   | > Step:66/68  GlobalStep:93700  TotalLoss:0.00325  PostnetLoss:0.00157  DecoderLoss:0.00168  StopLoss:0.03381  GradNorm:0.00757  GradNormST:0.01066  AvgTextLen:71.2  AvgSpecLen:220.8  StepTime:1.04  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:93702  AvgTotalLoss:0.05284  AvgPostnetLoss:0.00109  AvgDecoderLoss:0.00115  AvgStopLoss:0.05060  EpochTime:43.34  AvgStepTime:0.63\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.09109   PostnetLoss: 0.00483   DecoderLoss:0.00520  StopLoss: 0.08106  \n",
      "   | > TotalLoss: 0.07067   PostnetLoss: 0.00748   DecoderLoss:0.00804  StopLoss: 0.05515  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00109   Validation Loss: 0.00688\n",
      "\n",
      " > Epoch 358/1000\n",
      "   | > Step:7/68  GlobalStep:93710  TotalLoss:0.00162  PostnetLoss:0.00079  DecoderLoss:0.00083  StopLoss:0.03604  GradNorm:0.00373  GradNormST:0.01389  AvgTextLen:7.4  AvgSpecLen:44.6  StepTime:0.38  LR:0.000100\n",
      "   | > Step:17/68  GlobalStep:93720  TotalLoss:0.00188  PostnetLoss:0.00091  DecoderLoss:0.00096  StopLoss:0.06271  GradNorm:0.00397  GradNormST:0.01584  AvgTextLen:14.7  AvgSpecLen:66.0  StepTime:0.42  LR:0.000100\n",
      "   | > Step:27/68  GlobalStep:93730  TotalLoss:0.00213  PostnetLoss:0.00103  DecoderLoss:0.00110  StopLoss:0.04625  GradNorm:0.00402  GradNormST:0.00949  AvgTextLen:21.0  AvgSpecLen:80.2  StepTime:0.63  LR:0.000100\n",
      "   | > Step:37/68  GlobalStep:93740  TotalLoss:0.00229  PostnetLoss:0.00111  DecoderLoss:0.00118  StopLoss:0.04376  GradNorm:0.00386  GradNormST:0.01315  AvgTextLen:29.3  AvgSpecLen:102.4  StepTime:0.63  LR:0.000100\n",
      "   | > Step:47/68  GlobalStep:93750  TotalLoss:0.00252  PostnetLoss:0.00122  DecoderLoss:0.00130  StopLoss:0.04578  GradNorm:0.00330  GradNormST:0.01624  AvgTextLen:36.7  AvgSpecLen:126.9  StepTime:0.70  LR:0.000100\n",
      "   | > Step:57/68  GlobalStep:93760  TotalLoss:0.00276  PostnetLoss:0.00133  DecoderLoss:0.00142  StopLoss:0.04125  GradNorm:0.00428  GradNormST:0.01066  AvgTextLen:47.1  AvgSpecLen:161.0  StepTime:0.90  LR:0.000100\n",
      "   | > Step:67/68  GlobalStep:93770  TotalLoss:0.00317  PostnetLoss:0.00153  DecoderLoss:0.00164  StopLoss:0.03449  GradNorm:0.00641  GradNormST:0.01121  AvgTextLen:80.6  AvgSpecLen:246.5  StepTime:1.22  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:93771  AvgTotalLoss:0.04990  AvgPostnetLoss:0.00109  AvgDecoderLoss:0.00115  AvgStopLoss:0.04766  EpochTime:42.81  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.08288   PostnetLoss: 0.00494   DecoderLoss:0.00531  StopLoss: 0.07263  \n",
      "   | > TotalLoss: 0.06945   PostnetLoss: 0.00723   DecoderLoss:0.00776  StopLoss: 0.05446  \n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00109   Validation Loss: 0.00681\n",
      "\n",
      " > Epoch 359/1000\n",
      "   | > Step:8/68  GlobalStep:93780  TotalLoss:0.00170  PostnetLoss:0.00083  DecoderLoss:0.00087  StopLoss:0.06433  GradNorm:0.00416  GradNormST:0.01805  AvgTextLen:8.3  AvgSpecLen:44.0  StepTime:0.31  LR:0.000100\n",
      "   | > Step:18/68  GlobalStep:93790  TotalLoss:0.00184  PostnetLoss:0.00090  DecoderLoss:0.00095  StopLoss:0.05174  GradNorm:0.00317  GradNormST:0.01616  AvgTextLen:15.2  AvgSpecLen:64.5  StepTime:0.32  LR:0.000100\n",
      "   | > Step:28/68  GlobalStep:93800  TotalLoss:0.00213  PostnetLoss:0.00104  DecoderLoss:0.00110  StopLoss:0.05356  GradNorm:0.00321  GradNormST:0.01075  AvgTextLen:22.1  AvgSpecLen:85.4  StepTime:0.53  LR:0.000100\n",
      "   | > Step:38/68  GlobalStep:93810  TotalLoss:0.00230  PostnetLoss:0.00111  DecoderLoss:0.00119  StopLoss:0.04803  GradNorm:0.00349  GradNormST:0.01471  AvgTextLen:29.4  AvgSpecLen:104.1  StepTime:0.61  LR:0.000100\n",
      "   | > Step:48/68  GlobalStep:93820  TotalLoss:0.00250  PostnetLoss:0.00121  DecoderLoss:0.00129  StopLoss:0.03785  GradNorm:0.00335  GradNormST:0.01081  AvgTextLen:37.5  AvgSpecLen:123.8  StepTime:0.71  LR:0.000100\n",
      "   | > Step:58/68  GlobalStep:93830  TotalLoss:0.00272  PostnetLoss:0.00131  DecoderLoss:0.00140  StopLoss:0.04145  GradNorm:0.00377  GradNormST:0.01487  AvgTextLen:47.8  AvgSpecLen:158.0  StepTime:0.91  LR:0.000100\n",
      "   | > Step:68/68  GlobalStep:93840  TotalLoss:0.00308  PostnetLoss:0.00148  DecoderLoss:0.00160  StopLoss:0.03739  GradNorm:0.00449  GradNormST:0.01496  AvgTextLen:103.0  AvgSpecLen:308.9  StepTime:1.86  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:93840  AvgTotalLoss:0.04944  AvgPostnetLoss:0.00108  AvgDecoderLoss:0.00115  AvgStopLoss:0.04721  EpochTime:42.44  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.08373   PostnetLoss: 0.00487   DecoderLoss:0.00524  StopLoss: 0.07362  \n",
      "   | > TotalLoss: 0.06610   PostnetLoss: 0.00722   DecoderLoss:0.00776  StopLoss: 0.05112  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00108   Validation Loss: 0.00677\n",
      "\n",
      " > Epoch 360/1000\n",
      "   | > Step:9/68  GlobalStep:93850  TotalLoss:0.00161  PostnetLoss:0.00078  DecoderLoss:0.00083  StopLoss:0.03993  GradNorm:0.00397  GradNormST:0.01208  AvgTextLen:9.0  AvgSpecLen:42.1  StepTime:0.41  LR:0.000100\n",
      "   | > Step:19/68  GlobalStep:93860  TotalLoss:0.00188  PostnetLoss:0.00091  DecoderLoss:0.00097  StopLoss:0.06067  GradNorm:0.00352  GradNormST:0.01571  AvgTextLen:15.5  AvgSpecLen:63.2  StepTime:0.46  LR:0.000100\n",
      "   | > Step:29/68  GlobalStep:93870  TotalLoss:0.00208  PostnetLoss:0.00101  DecoderLoss:0.00107  StopLoss:0.06246  GradNorm:0.00309  GradNormST:0.00997  AvgTextLen:21.8  AvgSpecLen:83.4  StepTime:0.55  LR:0.000100\n",
      "   | > Step:39/68  GlobalStep:93880  TotalLoss:0.00231  PostnetLoss:0.00112  DecoderLoss:0.00119  StopLoss:0.03454  GradNorm:0.00306  GradNormST:0.01152  AvgTextLen:30.2  AvgSpecLen:110.4  StepTime:0.64  LR:0.000100\n",
      "   | > Step:49/68  GlobalStep:93890  TotalLoss:0.00248  PostnetLoss:0.00120  DecoderLoss:0.00128  StopLoss:0.03377  GradNorm:0.00303  GradNormST:0.00871  AvgTextLen:38.2  AvgSpecLen:127.7  StepTime:0.76  LR:0.000100\n",
      "   | > Step:59/68  GlobalStep:93900  TotalLoss:0.00269  PostnetLoss:0.00130  DecoderLoss:0.00139  StopLoss:0.04233  GradNorm:0.00428  GradNormST:0.01036  AvgTextLen:50.5  AvgSpecLen:155.9  StepTime:0.87  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:93909  AvgTotalLoss:0.05008  AvgPostnetLoss:0.00108  AvgDecoderLoss:0.00115  AvgStopLoss:0.04786  EpochTime:44.03  AvgStepTime:0.64\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.08581   PostnetLoss: 0.00477   DecoderLoss:0.00513  StopLoss: 0.07591  \n",
      "   | > TotalLoss: 0.06920   PostnetLoss: 0.00732   DecoderLoss:0.00787  StopLoss: 0.05401  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00108   Validation Loss: 0.00679\n",
      "\n",
      " > Epoch 361/1000\n",
      "   | > Step:0/68  GlobalStep:93910  TotalLoss:0.00163  PostnetLoss:0.00078  DecoderLoss:0.00084  StopLoss:0.07707  GradNorm:0.00727  GradNormST:0.02862  AvgTextLen:3.0  AvgSpecLen:25.3  StepTime:0.35  LR:0.000100\n",
      "   | > Step:10/68  GlobalStep:93920  TotalLoss:0.00175  PostnetLoss:0.00085  DecoderLoss:0.00090  StopLoss:0.04070  GradNorm:0.00506  GradNormST:0.01245  AvgTextLen:9.8  AvgSpecLen:48.6  StepTime:0.43  LR:0.000100\n",
      "   | > Step:20/68  GlobalStep:93930  TotalLoss:0.00200  PostnetLoss:0.00097  DecoderLoss:0.00103  StopLoss:0.04418  GradNorm:0.00383  GradNormST:0.01304  AvgTextLen:16.4  AvgSpecLen:64.8  StepTime:0.50  LR:0.000100\n",
      "   | > Step:30/68  GlobalStep:93940  TotalLoss:0.00214  PostnetLoss:0.00104  DecoderLoss:0.00110  StopLoss:0.04588  GradNorm:0.00339  GradNormST:0.01181  AvgTextLen:22.9  AvgSpecLen:85.8  StepTime:0.51  LR:0.000100\n",
      "   | > Step:40/68  GlobalStep:93950  TotalLoss:0.00232  PostnetLoss:0.00113  DecoderLoss:0.00119  StopLoss:0.03938  GradNorm:0.00341  GradNormST:0.01168  AvgTextLen:30.9  AvgSpecLen:105.3  StepTime:0.64  LR:0.000100\n",
      "   | > Step:50/68  GlobalStep:93960  TotalLoss:0.00250  PostnetLoss:0.00121  DecoderLoss:0.00129  StopLoss:0.03654  GradNorm:0.00297  GradNormST:0.01177  AvgTextLen:38.8  AvgSpecLen:132.0  StepTime:0.79  LR:0.000100\n",
      "   | > Step:60/68  GlobalStep:93970  TotalLoss:0.00276  PostnetLoss:0.00133  DecoderLoss:0.00143  StopLoss:0.03331  GradNorm:0.00364  GradNormST:0.00767  AvgTextLen:52.0  AvgSpecLen:172.7  StepTime:0.78  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:93978  AvgTotalLoss:0.04990  AvgPostnetLoss:0.00108  AvgDecoderLoss:0.00115  AvgStopLoss:0.04767  EpochTime:42.30  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.08652   PostnetLoss: 0.00481   DecoderLoss:0.00518  StopLoss: 0.07653  \n",
      "   | > TotalLoss: 0.07122   PostnetLoss: 0.00744   DecoderLoss:0.00801  StopLoss: 0.05577  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00108   Validation Loss: 0.00675\n",
      "\n",
      " > Epoch 362/1000\n",
      "   | > Step:1/68  GlobalStep:93980  TotalLoss:0.00162  PostnetLoss:0.00079  DecoderLoss:0.00083  StopLoss:0.04933  GradNorm:0.00750  GradNormST:0.01811  AvgTextLen:3.7  AvgSpecLen:28.6  StepTime:0.53  LR:0.000100\n",
      "   | > Step:11/68  GlobalStep:93990  TotalLoss:0.00183  PostnetLoss:0.00089  DecoderLoss:0.00094  StopLoss:0.05041  GradNorm:0.00408  GradNormST:0.00964  AvgTextLen:10.3  AvgSpecLen:49.4  StepTime:0.33  LR:0.000100\n",
      "   | > Step:21/68  GlobalStep:94000  TotalLoss:0.00214  PostnetLoss:0.00104  DecoderLoss:0.00110  StopLoss:0.04743  GradNorm:0.00505  GradNormST:0.01408  AvgTextLen:16.6  AvgSpecLen:66.3  StepTime:0.43  LR:0.000100\n",
      " | | > Checkpoint saving : /vm/TTS/models/cartman_models/mozilla-no-loc-fattn-stopnet-sigmoid-loss_masking-August-15-2019_11+26AM-de6aeb9/checkpoint_94000.pth.tar\n",
      "warning: audio amplitude out of range, auto clipped.\n",
      "   | > Step:31/68  GlobalStep:94010  TotalLoss:0.00240  PostnetLoss:0.00116  DecoderLoss:0.00124  StopLoss:0.04294  GradNorm:0.00454  GradNormST:0.01162  AvgTextLen:24.1  AvgSpecLen:82.7  StepTime:0.54  LR:0.000100\n",
      "   | > Step:41/68  GlobalStep:94020  TotalLoss:0.00265  PostnetLoss:0.00129  DecoderLoss:0.00136  StopLoss:0.02661  GradNorm:0.00555  GradNormST:0.00880  AvgTextLen:32.3  AvgSpecLen:109.4  StepTime:0.68  LR:0.000100\n",
      "   | > Step:51/68  GlobalStep:94030  TotalLoss:0.00258  PostnetLoss:0.00125  DecoderLoss:0.00133  StopLoss:0.03584  GradNorm:0.00323  GradNormST:0.00939  AvgTextLen:39.2  AvgSpecLen:126.9  StepTime:0.79  LR:0.000100\n",
      "   | > Step:61/68  GlobalStep:94040  TotalLoss:0.00288  PostnetLoss:0.00139  DecoderLoss:0.00149  StopLoss:0.02699  GradNorm:0.00502  GradNormST:0.01137  AvgTextLen:54.6  AvgSpecLen:165.5  StepTime:0.90  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:94047  AvgTotalLoss:0.04669  AvgPostnetLoss:0.00115  AvgDecoderLoss:0.00122  AvgStopLoss:0.04432  EpochTime:42.03  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.08292   PostnetLoss: 0.00497   DecoderLoss:0.00535  StopLoss: 0.07260  \n",
      "   | > TotalLoss: 0.06841   PostnetLoss: 0.00687   DecoderLoss:0.00737  StopLoss: 0.05417  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00115   Validation Loss: 0.00633\n",
      "\n",
      " > Epoch 363/1000\n",
      "   | > Step:2/68  GlobalStep:94050  TotalLoss:0.00152  PostnetLoss:0.00074  DecoderLoss:0.00078  StopLoss:0.04948  GradNorm:0.00492  GradNormST:0.01344  AvgTextLen:4.0  AvgSpecLen:31.3  StepTime:0.37  LR:0.000100\n",
      "   | > Step:12/68  GlobalStep:94060  TotalLoss:0.00182  PostnetLoss:0.00089  DecoderLoss:0.00093  StopLoss:0.04081  GradNorm:0.00503  GradNormST:0.01103  AvgTextLen:11.0  AvgSpecLen:49.3  StepTime:0.45  LR:0.000100\n",
      "   | > Step:22/68  GlobalStep:94070  TotalLoss:0.00203  PostnetLoss:0.00099  DecoderLoss:0.00104  StopLoss:0.06239  GradNorm:0.00344  GradNormST:0.01664  AvgTextLen:18.0  AvgSpecLen:72.0  StepTime:0.45  LR:0.000100\n",
      "   | > Step:32/68  GlobalStep:94080  TotalLoss:0.00221  PostnetLoss:0.00107  DecoderLoss:0.00114  StopLoss:0.05577  GradNorm:0.00352  GradNormST:0.01554  AvgTextLen:24.4  AvgSpecLen:91.5  StepTime:0.56  LR:0.000100\n",
      "   | > Step:42/68  GlobalStep:94090  TotalLoss:0.00239  PostnetLoss:0.00115  DecoderLoss:0.00123  StopLoss:0.04057  GradNorm:0.00372  GradNormST:0.00961  AvgTextLen:33.1  AvgSpecLen:109.7  StepTime:0.61  LR:0.000100\n",
      "   | > Step:52/68  GlobalStep:94100  TotalLoss:0.00259  PostnetLoss:0.00125  DecoderLoss:0.00134  StopLoss:0.04141  GradNorm:0.00299  GradNormST:0.01518  AvgTextLen:42.1  AvgSpecLen:137.5  StepTime:0.79  LR:0.000100\n",
      "   | > Step:62/68  GlobalStep:94110  TotalLoss:0.00282  PostnetLoss:0.00136  DecoderLoss:0.00146  StopLoss:0.04086  GradNorm:0.00385  GradNormST:0.01908  AvgTextLen:54.0  AvgSpecLen:174.7  StepTime:0.78  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:94116  AvgTotalLoss:0.04592  AvgPostnetLoss:0.00110  AvgDecoderLoss:0.00118  AvgStopLoss:0.04363  EpochTime:42.80  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.08433   PostnetLoss: 0.00503   DecoderLoss:0.00541  StopLoss: 0.07389  \n",
      "   | > TotalLoss: 0.07048   PostnetLoss: 0.00735   DecoderLoss:0.00788  StopLoss: 0.05526  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00110   Validation Loss: 0.00666\n",
      "\n",
      " > Epoch 364/1000\n",
      "   | > Step:3/68  GlobalStep:94120  TotalLoss:0.00154  PostnetLoss:0.00075  DecoderLoss:0.00079  StopLoss:0.09178  GradNorm:0.00389  GradNormST:0.02752  AvgTextLen:4.7  AvgSpecLen:29.7  StepTime:0.36  LR:0.000100\n",
      "   | > Step:13/68  GlobalStep:94130  TotalLoss:0.00178  PostnetLoss:0.00087  DecoderLoss:0.00091  StopLoss:0.06183  GradNorm:0.00457  GradNormST:0.02319  AvgTextLen:11.5  AvgSpecLen:55.2  StepTime:0.41  LR:0.000100\n",
      "   | > Step:23/68  GlobalStep:94140  TotalLoss:0.00205  PostnetLoss:0.00100  DecoderLoss:0.00105  StopLoss:0.05763  GradNorm:0.00372  GradNormST:0.01309  AvgTextLen:18.3  AvgSpecLen:72.2  StepTime:0.45  LR:0.000100\n",
      "   | > Step:33/68  GlobalStep:94150  TotalLoss:0.00225  PostnetLoss:0.00109  DecoderLoss:0.00116  StopLoss:0.03941  GradNorm:0.00337  GradNormST:0.00995  AvgTextLen:25.9  AvgSpecLen:88.9  StepTime:0.52  LR:0.000100\n",
      "   | > Step:43/68  GlobalStep:94160  TotalLoss:0.00236  PostnetLoss:0.00114  DecoderLoss:0.00122  StopLoss:0.04594  GradNorm:0.00290  GradNormST:0.01261  AvgTextLen:33.4  AvgSpecLen:115.7  StepTime:0.66  LR:0.000100\n",
      "   | > Step:53/68  GlobalStep:94170  TotalLoss:0.00257  PostnetLoss:0.00124  DecoderLoss:0.00133  StopLoss:0.03502  GradNorm:0.00334  GradNormST:0.00825  AvgTextLen:42.4  AvgSpecLen:137.6  StepTime:0.81  LR:0.000100\n",
      "   | > Step:63/68  GlobalStep:94180  TotalLoss:0.00286  PostnetLoss:0.00138  DecoderLoss:0.00148  StopLoss:0.02495  GradNorm:0.00317  GradNormST:0.00640  AvgTextLen:58.6  AvgSpecLen:185.1  StepTime:0.90  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:94185  AvgTotalLoss:0.04665  AvgPostnetLoss:0.00109  AvgDecoderLoss:0.00116  AvgStopLoss:0.04440  EpochTime:43.12  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.08689   PostnetLoss: 0.00504   DecoderLoss:0.00541  StopLoss: 0.07644  \n",
      "   | > TotalLoss: 0.07341   PostnetLoss: 0.00779   DecoderLoss:0.00833  StopLoss: 0.05729  \n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00109   Validation Loss: 0.00698\n",
      "\n",
      " > Epoch 365/1000\n",
      "   | > Step:4/68  GlobalStep:94190  TotalLoss:0.00151  PostnetLoss:0.00074  DecoderLoss:0.00078  StopLoss:0.08064  GradNorm:0.00409  GradNormST:0.02376  AvgTextLen:4.8  AvgSpecLen:31.1  StepTime:0.26  LR:0.000100\n",
      "   | > Step:14/68  GlobalStep:94200  TotalLoss:0.00179  PostnetLoss:0.00087  DecoderLoss:0.00092  StopLoss:0.04334  GradNorm:0.00374  GradNormST:0.01227  AvgTextLen:12.2  AvgSpecLen:52.0  StepTime:0.32  LR:0.000100\n",
      "   | > Step:24/68  GlobalStep:94210  TotalLoss:0.00213  PostnetLoss:0.00103  DecoderLoss:0.00110  StopLoss:0.05379  GradNorm:0.00373  GradNormST:0.01351  AvgTextLen:19.2  AvgSpecLen:79.0  StepTime:0.51  LR:0.000100\n",
      "   | > Step:34/68  GlobalStep:94220  TotalLoss:0.00228  PostnetLoss:0.00111  DecoderLoss:0.00118  StopLoss:0.03717  GradNorm:0.00312  GradNormST:0.00869  AvgTextLen:26.2  AvgSpecLen:95.9  StepTime:0.58  LR:0.000100\n",
      "   | > Step:44/68  GlobalStep:94230  TotalLoss:0.00246  PostnetLoss:0.00119  DecoderLoss:0.00127  StopLoss:0.04267  GradNorm:0.00321  GradNormST:0.01376  AvgTextLen:33.8  AvgSpecLen:116.3  StepTime:0.66  LR:0.000100\n",
      "   | > Step:54/68  GlobalStep:94240  TotalLoss:0.00263  PostnetLoss:0.00127  DecoderLoss:0.00136  StopLoss:0.03477  GradNorm:0.00328  GradNormST:0.01142  AvgTextLen:42.2  AvgSpecLen:144.9  StepTime:0.81  LR:0.000100\n",
      "   | > Step:64/68  GlobalStep:94250  TotalLoss:0.00273  PostnetLoss:0.00131  DecoderLoss:0.00142  StopLoss:0.02901  GradNorm:0.00348  GradNormST:0.01039  AvgTextLen:60.6  AvgSpecLen:177.6  StepTime:0.80  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:94254  AvgTotalLoss:0.04542  AvgPostnetLoss:0.00108  AvgDecoderLoss:0.00115  AvgStopLoss:0.04319  EpochTime:42.78  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.08244   PostnetLoss: 0.00502   DecoderLoss:0.00541  StopLoss: 0.07201  \n",
      "   | > TotalLoss: 0.06724   PostnetLoss: 0.00745   DecoderLoss:0.00796  StopLoss: 0.05182  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00108   Validation Loss: 0.00683\n",
      "\n",
      " > Epoch 366/1000\n",
      "   | > Step:5/68  GlobalStep:94260  TotalLoss:0.00156  PostnetLoss:0.00075  DecoderLoss:0.00080  StopLoss:0.05054  GradNorm:0.00521  GradNormST:0.01506  AvgTextLen:5.9  AvgSpecLen:33.2  StepTime:0.25  LR:0.000100\n",
      "   | > Step:15/68  GlobalStep:94270  TotalLoss:0.00185  PostnetLoss:0.00090  DecoderLoss:0.00095  StopLoss:0.04316  GradNorm:0.00382  GradNormST:0.00999  AvgTextLen:12.8  AvgSpecLen:59.1  StepTime:0.57  LR:0.000100\n",
      "   | > Step:25/68  GlobalStep:94280  TotalLoss:0.00206  PostnetLoss:0.00100  DecoderLoss:0.00106  StopLoss:0.04453  GradNorm:0.00356  GradNormST:0.01355  AvgTextLen:19.8  AvgSpecLen:73.3  StepTime:0.36  LR:0.000100\n",
      "   | > Step:35/68  GlobalStep:94290  TotalLoss:0.00227  PostnetLoss:0.00110  DecoderLoss:0.00117  StopLoss:0.03940  GradNorm:0.00352  GradNormST:0.00991  AvgTextLen:27.1  AvgSpecLen:104.7  StepTime:0.74  LR:0.000100\n",
      "   | > Step:45/68  GlobalStep:94300  TotalLoss:0.00238  PostnetLoss:0.00115  DecoderLoss:0.00123  StopLoss:0.04806  GradNorm:0.00317  GradNormST:0.01704  AvgTextLen:34.4  AvgSpecLen:118.6  StepTime:0.64  LR:0.000100\n",
      "   | > Step:55/68  GlobalStep:94310  TotalLoss:0.00261  PostnetLoss:0.00126  DecoderLoss:0.00135  StopLoss:0.03493  GradNorm:0.00271  GradNormST:0.00847  AvgTextLen:43.9  AvgSpecLen:145.2  StepTime:0.85  LR:0.000100\n",
      "   | > Step:65/68  GlobalStep:94320  TotalLoss:0.00290  PostnetLoss:0.00139  DecoderLoss:0.00150  StopLoss:0.02591  GradNorm:0.00294  GradNormST:0.00774  AvgTextLen:64.2  AvgSpecLen:207.4  StepTime:1.19  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:94323  AvgTotalLoss:0.04502  AvgPostnetLoss:0.00107  AvgDecoderLoss:0.00114  AvgStopLoss:0.04280  EpochTime:42.75  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.08587   PostnetLoss: 0.00511   DecoderLoss:0.00547  StopLoss: 0.07529  \n",
      "   | > TotalLoss: 0.07029   PostnetLoss: 0.00772   DecoderLoss:0.00823  StopLoss: 0.05435  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00107   Validation Loss: 0.00697\n",
      "\n",
      " > Epoch 367/1000\n",
      "   | > Step:6/68  GlobalStep:94330  TotalLoss:0.00153  PostnetLoss:0.00075  DecoderLoss:0.00079  StopLoss:0.03470  GradNorm:0.00400  GradNormST:0.01551  AvgTextLen:6.0  AvgSpecLen:34.8  StepTime:0.42  LR:0.000100\n",
      "   | > Step:16/68  GlobalStep:94340  TotalLoss:0.00188  PostnetLoss:0.00091  DecoderLoss:0.00097  StopLoss:0.03589  GradNorm:0.00391  GradNormST:0.00942  AvgTextLen:13.8  AvgSpecLen:60.6  StepTime:0.39  LR:0.000100\n",
      "   | > Step:26/68  GlobalStep:94350  TotalLoss:0.00209  PostnetLoss:0.00102  DecoderLoss:0.00108  StopLoss:0.03951  GradNorm:0.00387  GradNormST:0.01078  AvgTextLen:20.3  AvgSpecLen:77.5  StepTime:0.48  LR:0.000100\n",
      "   | > Step:36/68  GlobalStep:94360  TotalLoss:0.00226  PostnetLoss:0.00110  DecoderLoss:0.00117  StopLoss:0.05764  GradNorm:0.00344  GradNormST:0.01107  AvgTextLen:28.5  AvgSpecLen:100.2  StepTime:0.75  LR:0.000100\n",
      "   | > Step:46/68  GlobalStep:94370  TotalLoss:0.00240  PostnetLoss:0.00116  DecoderLoss:0.00124  StopLoss:0.04210  GradNorm:0.00300  GradNormST:0.01019  AvgTextLen:35.4  AvgSpecLen:122.6  StepTime:0.78  LR:0.000100\n",
      "   | > Step:56/68  GlobalStep:94380  TotalLoss:0.00260  PostnetLoss:0.00126  DecoderLoss:0.00134  StopLoss:0.02500  GradNorm:0.00291  GradNormST:0.00568  AvgTextLen:45.8  AvgSpecLen:145.4  StepTime:0.96  LR:0.000100\n",
      "   | > Step:66/68  GlobalStep:94390  TotalLoss:0.00304  PostnetLoss:0.00146  DecoderLoss:0.00158  StopLoss:0.02983  GradNorm:0.00311  GradNormST:0.01420  AvgTextLen:71.2  AvgSpecLen:220.8  StepTime:1.03  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:94392  AvgTotalLoss:0.04535  AvgPostnetLoss:0.00107  AvgDecoderLoss:0.00114  AvgStopLoss:0.04314  EpochTime:43.51  AvgStepTime:0.63\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.08670   PostnetLoss: 0.00512   DecoderLoss:0.00549  StopLoss: 0.07609  \n",
      "   | > TotalLoss: 0.07336   PostnetLoss: 0.00789   DecoderLoss:0.00843  StopLoss: 0.05704  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00107   Validation Loss: 0.00714\n",
      "\n",
      " > Epoch 368/1000\n",
      "   | > Step:7/68  GlobalStep:94400  TotalLoss:0.00165  PostnetLoss:0.00080  DecoderLoss:0.00085  StopLoss:0.03749  GradNorm:0.00407  GradNormST:0.01520  AvgTextLen:7.4  AvgSpecLen:44.6  StepTime:0.39  LR:0.000100\n",
      "   | > Step:17/68  GlobalStep:94410  TotalLoss:0.00187  PostnetLoss:0.00091  DecoderLoss:0.00096  StopLoss:0.05911  GradNorm:0.00367  GradNormST:0.01064  AvgTextLen:14.7  AvgSpecLen:66.0  StepTime:0.42  LR:0.000100\n",
      "   | > Step:27/68  GlobalStep:94420  TotalLoss:0.00211  PostnetLoss:0.00102  DecoderLoss:0.00108  StopLoss:0.04025  GradNorm:0.00376  GradNormST:0.00914  AvgTextLen:21.0  AvgSpecLen:80.2  StepTime:0.64  LR:0.000100\n",
      "   | > Step:37/68  GlobalStep:94430  TotalLoss:0.00231  PostnetLoss:0.00112  DecoderLoss:0.00119  StopLoss:0.03757  GradNorm:0.00363  GradNormST:0.00822  AvgTextLen:29.3  AvgSpecLen:102.4  StepTime:0.62  LR:0.000100\n",
      "   | > Step:47/68  GlobalStep:94440  TotalLoss:0.00249  PostnetLoss:0.00121  DecoderLoss:0.00129  StopLoss:0.03694  GradNorm:0.00369  GradNormST:0.00880  AvgTextLen:36.7  AvgSpecLen:126.9  StepTime:0.74  LR:0.000100\n",
      "   | > Step:57/68  GlobalStep:94450  TotalLoss:0.00272  PostnetLoss:0.00132  DecoderLoss:0.00141  StopLoss:0.03010  GradNorm:0.00358  GradNormST:0.00563  AvgTextLen:47.1  AvgSpecLen:161.0  StepTime:0.84  LR:0.000100\n",
      "   | > Step:67/68  GlobalStep:94460  TotalLoss:0.00306  PostnetLoss:0.00147  DecoderLoss:0.00159  StopLoss:0.02946  GradNorm:0.00277  GradNormST:0.01446  AvgTextLen:80.6  AvgSpecLen:246.5  StepTime:1.22  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:94461  AvgTotalLoss:0.04466  AvgPostnetLoss:0.00107  AvgDecoderLoss:0.00114  AvgStopLoss:0.04245  EpochTime:42.80  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.08587   PostnetLoss: 0.00509   DecoderLoss:0.00547  StopLoss: 0.07530  \n",
      "   | > TotalLoss: 0.06956   PostnetLoss: 0.00767   DecoderLoss:0.00819  StopLoss: 0.05370  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00107   Validation Loss: 0.00698\n",
      "\n",
      " > Epoch 369/1000\n",
      "   | > Step:8/68  GlobalStep:94470  TotalLoss:0.00165  PostnetLoss:0.00081  DecoderLoss:0.00084  StopLoss:0.07629  GradNorm:0.00454  GradNormST:0.02468  AvgTextLen:8.3  AvgSpecLen:44.0  StepTime:0.33  LR:0.000100\n",
      "   | > Step:18/68  GlobalStep:94480  TotalLoss:0.00183  PostnetLoss:0.00089  DecoderLoss:0.00094  StopLoss:0.05027  GradNorm:0.00411  GradNormST:0.01123  AvgTextLen:15.2  AvgSpecLen:64.5  StepTime:0.42  LR:0.000100\n",
      "   | > Step:28/68  GlobalStep:94490  TotalLoss:0.00211  PostnetLoss:0.00103  DecoderLoss:0.00109  StopLoss:0.04710  GradNorm:0.00361  GradNormST:0.01063  AvgTextLen:22.1  AvgSpecLen:85.4  StepTime:0.57  LR:0.000100\n",
      "   | > Step:38/68  GlobalStep:94500  TotalLoss:0.00225  PostnetLoss:0.00109  DecoderLoss:0.00116  StopLoss:0.03438  GradNorm:0.00344  GradNormST:0.00901  AvgTextLen:29.4  AvgSpecLen:104.1  StepTime:0.53  LR:0.000100\n",
      "   | > Step:48/68  GlobalStep:94510  TotalLoss:0.00246  PostnetLoss:0.00119  DecoderLoss:0.00127  StopLoss:0.02981  GradNorm:0.00356  GradNormST:0.00663  AvgTextLen:37.5  AvgSpecLen:123.8  StepTime:0.67  LR:0.000100\n",
      "   | > Step:58/68  GlobalStep:94520  TotalLoss:0.00269  PostnetLoss:0.00130  DecoderLoss:0.00139  StopLoss:0.03565  GradNorm:0.00315  GradNormST:0.01484  AvgTextLen:47.8  AvgSpecLen:158.0  StepTime:0.84  LR:0.000100\n",
      "   | > Step:68/68  GlobalStep:94530  TotalLoss:0.00305  PostnetLoss:0.00147  DecoderLoss:0.00158  StopLoss:0.02587  GradNorm:0.00287  GradNormST:0.01323  AvgTextLen:103.0  AvgSpecLen:308.9  StepTime:1.88  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:94530  AvgTotalLoss:0.04558  AvgPostnetLoss:0.00106  AvgDecoderLoss:0.00113  AvgStopLoss:0.04338  EpochTime:42.69  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.08842   PostnetLoss: 0.00508   DecoderLoss:0.00545  StopLoss: 0.07788  \n",
      "   | > TotalLoss: 0.07469   PostnetLoss: 0.00797   DecoderLoss:0.00847  StopLoss: 0.05825  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00106   Validation Loss: 0.00716\n",
      "\n",
      " > Epoch 370/1000\n",
      "   | > Step:9/68  GlobalStep:94540  TotalLoss:0.00159  PostnetLoss:0.00077  DecoderLoss:0.00081  StopLoss:0.03816  GradNorm:0.00384  GradNormST:0.01127  AvgTextLen:9.0  AvgSpecLen:42.1  StepTime:0.33  LR:0.000100\n",
      "   | > Step:19/68  GlobalStep:94550  TotalLoss:0.00189  PostnetLoss:0.00092  DecoderLoss:0.00097  StopLoss:0.05970  GradNorm:0.00393  GradNormST:0.01692  AvgTextLen:15.5  AvgSpecLen:63.2  StepTime:0.41  LR:0.000100\n",
      "   | > Step:29/68  GlobalStep:94560  TotalLoss:0.00208  PostnetLoss:0.00101  DecoderLoss:0.00107  StopLoss:0.05712  GradNorm:0.00377  GradNormST:0.01304  AvgTextLen:21.8  AvgSpecLen:83.4  StepTime:0.57  LR:0.000100\n",
      "   | > Step:39/68  GlobalStep:94570  TotalLoss:0.00232  PostnetLoss:0.00113  DecoderLoss:0.00119  StopLoss:0.03151  GradNorm:0.00409  GradNormST:0.00891  AvgTextLen:30.2  AvgSpecLen:110.4  StepTime:0.68  LR:0.000100\n",
      "   | > Step:49/68  GlobalStep:94580  TotalLoss:0.00252  PostnetLoss:0.00122  DecoderLoss:0.00130  StopLoss:0.03380  GradNorm:0.00406  GradNormST:0.01004  AvgTextLen:38.2  AvgSpecLen:127.7  StepTime:0.70  LR:0.000100\n",
      "   | > Step:59/68  GlobalStep:94590  TotalLoss:0.00268  PostnetLoss:0.00129  DecoderLoss:0.00139  StopLoss:0.03756  GradNorm:0.00351  GradNormST:0.00823  AvgTextLen:50.5  AvgSpecLen:155.9  StepTime:0.81  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:94599  AvgTotalLoss:0.04409  AvgPostnetLoss:0.00106  AvgDecoderLoss:0.00113  AvgStopLoss:0.04189  EpochTime:42.08  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.08553   PostnetLoss: 0.00500   DecoderLoss:0.00537  StopLoss: 0.07517  \n",
      "   | > TotalLoss: 0.07449   PostnetLoss: 0.00781   DecoderLoss:0.00832  StopLoss: 0.05837  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00106   Validation Loss: 0.00713\n",
      "\n",
      " > Epoch 371/1000\n",
      "   | > Step:0/68  GlobalStep:94600  TotalLoss:0.00155  PostnetLoss:0.00075  DecoderLoss:0.00080  StopLoss:0.07468  GradNorm:0.00502  GradNormST:0.02320  AvgTextLen:3.0  AvgSpecLen:25.3  StepTime:0.50  LR:0.000100\n",
      "   | > Step:10/68  GlobalStep:94610  TotalLoss:0.00168  PostnetLoss:0.00082  DecoderLoss:0.00086  StopLoss:0.04081  GradNorm:0.00345  GradNormST:0.00946  AvgTextLen:9.8  AvgSpecLen:48.6  StepTime:0.55  LR:0.000100\n",
      "   | > Step:20/68  GlobalStep:94620  TotalLoss:0.00196  PostnetLoss:0.00095  DecoderLoss:0.00100  StopLoss:0.03241  GradNorm:0.00394  GradNormST:0.00840  AvgTextLen:16.4  AvgSpecLen:64.8  StepTime:0.50  LR:0.000100\n",
      "   | > Step:30/68  GlobalStep:94630  TotalLoss:0.00214  PostnetLoss:0.00104  DecoderLoss:0.00110  StopLoss:0.03932  GradNorm:0.00422  GradNormST:0.00773  AvgTextLen:22.9  AvgSpecLen:85.8  StepTime:0.60  LR:0.000100\n",
      "   | > Step:40/68  GlobalStep:94640  TotalLoss:0.00230  PostnetLoss:0.00112  DecoderLoss:0.00118  StopLoss:0.04125  GradNorm:0.00452  GradNormST:0.00854  AvgTextLen:30.9  AvgSpecLen:105.3  StepTime:0.59  LR:0.000100\n",
      "   | > Step:50/68  GlobalStep:94650  TotalLoss:0.00252  PostnetLoss:0.00122  DecoderLoss:0.00130  StopLoss:0.03061  GradNorm:0.00428  GradNormST:0.00839  AvgTextLen:38.8  AvgSpecLen:132.0  StepTime:0.72  LR:0.000100\n",
      "   | > Step:60/68  GlobalStep:94660  TotalLoss:0.00273  PostnetLoss:0.00132  DecoderLoss:0.00141  StopLoss:0.02618  GradNorm:0.00364  GradNormST:0.00666  AvgTextLen:52.0  AvgSpecLen:172.7  StepTime:0.92  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:94668  AvgTotalLoss:0.04466  AvgPostnetLoss:0.00106  AvgDecoderLoss:0.00113  AvgStopLoss:0.04246  EpochTime:42.60  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.08620   PostnetLoss: 0.00509   DecoderLoss:0.00547  StopLoss: 0.07564  \n",
      "   | > TotalLoss: 0.07341   PostnetLoss: 0.00775   DecoderLoss:0.00826  StopLoss: 0.05740  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00106   Validation Loss: 0.00713\n",
      "\n",
      " > Epoch 372/1000\n",
      "   | > Step:1/68  GlobalStep:94670  TotalLoss:0.00149  PostnetLoss:0.00072  DecoderLoss:0.00077  StopLoss:0.04478  GradNorm:0.00491  GradNormST:0.01448  AvgTextLen:3.7  AvgSpecLen:28.6  StepTime:0.32  LR:0.000100\n",
      "   | > Step:11/68  GlobalStep:94680  TotalLoss:0.00170  PostnetLoss:0.00083  DecoderLoss:0.00087  StopLoss:0.05703  GradNorm:0.00364  GradNormST:0.01533  AvgTextLen:10.3  AvgSpecLen:49.4  StepTime:0.39  LR:0.000100\n",
      "   | > Step:21/68  GlobalStep:94690  TotalLoss:0.00195  PostnetLoss:0.00095  DecoderLoss:0.00100  StopLoss:0.05434  GradNorm:0.00425  GradNormST:0.01907  AvgTextLen:16.6  AvgSpecLen:66.3  StepTime:0.43  LR:0.000100\n",
      "   | > Step:31/68  GlobalStep:94700  TotalLoss:0.00214  PostnetLoss:0.00104  DecoderLoss:0.00110  StopLoss:0.03509  GradNorm:0.00381  GradNormST:0.01143  AvgTextLen:24.1  AvgSpecLen:82.7  StepTime:0.52  LR:0.000100\n",
      "   | > Step:41/68  GlobalStep:94710  TotalLoss:0.00237  PostnetLoss:0.00115  DecoderLoss:0.00122  StopLoss:0.02949  GradNorm:0.00421  GradNormST:0.00749  AvgTextLen:32.3  AvgSpecLen:109.4  StepTime:0.62  LR:0.000100\n",
      "   | > Step:51/68  GlobalStep:94720  TotalLoss:0.00249  PostnetLoss:0.00120  DecoderLoss:0.00129  StopLoss:0.02692  GradNorm:0.00414  GradNormST:0.00903  AvgTextLen:39.2  AvgSpecLen:126.9  StepTime:0.74  LR:0.000100\n",
      "   | > Step:61/68  GlobalStep:94730  TotalLoss:0.00270  PostnetLoss:0.00130  DecoderLoss:0.00139  StopLoss:0.02898  GradNorm:0.00367  GradNormST:0.00687  AvgTextLen:54.6  AvgSpecLen:165.5  StepTime:0.90  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:94737  AvgTotalLoss:0.04457  AvgPostnetLoss:0.00106  AvgDecoderLoss:0.00113  AvgStopLoss:0.04238  EpochTime:42.51  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.08793   PostnetLoss: 0.00504   DecoderLoss:0.00541  StopLoss: 0.07747  \n",
      "   | > TotalLoss: 0.07340   PostnetLoss: 0.00797   DecoderLoss:0.00849  StopLoss: 0.05694  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00106   Validation Loss: 0.00725\n",
      "\n",
      " > Epoch 373/1000\n",
      "   | > Step:2/68  GlobalStep:94740  TotalLoss:0.00144  PostnetLoss:0.00070  DecoderLoss:0.00074  StopLoss:0.05864  GradNorm:0.00394  GradNormST:0.02470  AvgTextLen:4.0  AvgSpecLen:31.3  StepTime:0.42  LR:0.000100\n",
      "   | > Step:12/68  GlobalStep:94750  TotalLoss:0.00167  PostnetLoss:0.00081  DecoderLoss:0.00086  StopLoss:0.04167  GradNorm:0.00368  GradNormST:0.01112  AvgTextLen:11.0  AvgSpecLen:49.3  StepTime:0.44  LR:0.000100\n",
      "   | > Step:22/68  GlobalStep:94760  TotalLoss:0.00196  PostnetLoss:0.00096  DecoderLoss:0.00101  StopLoss:0.05003  GradNorm:0.00409  GradNormST:0.01022  AvgTextLen:18.0  AvgSpecLen:72.0  StepTime:0.46  LR:0.000100\n",
      "   | > Step:32/68  GlobalStep:94770  TotalLoss:0.00216  PostnetLoss:0.00105  DecoderLoss:0.00111  StopLoss:0.05394  GradNorm:0.00401  GradNormST:0.01090  AvgTextLen:24.4  AvgSpecLen:91.5  StepTime:0.56  LR:0.000100\n",
      "   | > Step:42/68  GlobalStep:94780  TotalLoss:0.00238  PostnetLoss:0.00115  DecoderLoss:0.00123  StopLoss:0.04148  GradNorm:0.00434  GradNormST:0.01510  AvgTextLen:33.1  AvgSpecLen:109.7  StepTime:0.60  LR:0.000100\n",
      "   | > Step:52/68  GlobalStep:94790  TotalLoss:0.00255  PostnetLoss:0.00124  DecoderLoss:0.00132  StopLoss:0.04261  GradNorm:0.00428  GradNormST:0.01318  AvgTextLen:42.1  AvgSpecLen:137.5  StepTime:0.80  LR:0.000100\n",
      "   | > Step:62/68  GlobalStep:94800  TotalLoss:0.00278  PostnetLoss:0.00134  DecoderLoss:0.00144  StopLoss:0.03000  GradNorm:0.00395  GradNormST:0.00841  AvgTextLen:54.0  AvgSpecLen:174.7  StepTime:0.76  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:94806  AvgTotalLoss:0.04428  AvgPostnetLoss:0.00106  AvgDecoderLoss:0.00113  AvgStopLoss:0.04209  EpochTime:41.75  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.08268   PostnetLoss: 0.00503   DecoderLoss:0.00540  StopLoss: 0.07224  \n",
      "   | > TotalLoss: 0.07184   PostnetLoss: 0.00794   DecoderLoss:0.00847  StopLoss: 0.05543  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00106   Validation Loss: 0.00717\n",
      "\n",
      " > Epoch 374/1000\n",
      "   | > Step:3/68  GlobalStep:94810  TotalLoss:0.00147  PostnetLoss:0.00071  DecoderLoss:0.00076  StopLoss:0.07522  GradNorm:0.00406  GradNormST:0.02740  AvgTextLen:4.7  AvgSpecLen:29.7  StepTime:0.30  LR:0.000100\n",
      "   | > Step:13/68  GlobalStep:94820  TotalLoss:0.00168  PostnetLoss:0.00082  DecoderLoss:0.00086  StopLoss:0.06107  GradNorm:0.00325  GradNormST:0.01551  AvgTextLen:11.5  AvgSpecLen:55.2  StepTime:0.44  LR:0.000100\n",
      "   | > Step:23/68  GlobalStep:94830  TotalLoss:0.00199  PostnetLoss:0.00097  DecoderLoss:0.00102  StopLoss:0.04524  GradNorm:0.00342  GradNormST:0.00985  AvgTextLen:18.3  AvgSpecLen:72.2  StepTime:0.45  LR:0.000100\n",
      "   | > Step:33/68  GlobalStep:94840  TotalLoss:0.00225  PostnetLoss:0.00110  DecoderLoss:0.00116  StopLoss:0.04112  GradNorm:0.00485  GradNormST:0.01063  AvgTextLen:25.9  AvgSpecLen:88.9  StepTime:0.56  LR:0.000100\n",
      "   | > Step:43/68  GlobalStep:94850  TotalLoss:0.00236  PostnetLoss:0.00114  DecoderLoss:0.00121  StopLoss:0.04255  GradNorm:0.00392  GradNormST:0.01135  AvgTextLen:33.4  AvgSpecLen:115.7  StepTime:0.51  LR:0.000100\n",
      "   | > Step:53/68  GlobalStep:94860  TotalLoss:0.00254  PostnetLoss:0.00123  DecoderLoss:0.00131  StopLoss:0.03632  GradNorm:0.00470  GradNormST:0.00639  AvgTextLen:42.4  AvgSpecLen:137.6  StepTime:0.76  LR:0.000100\n",
      "   | > Step:63/68  GlobalStep:94870  TotalLoss:0.00282  PostnetLoss:0.00136  DecoderLoss:0.00146  StopLoss:0.02507  GradNorm:0.00364  GradNormST:0.00529  AvgTextLen:58.6  AvgSpecLen:185.1  StepTime:0.92  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:94875  AvgTotalLoss:0.04392  AvgPostnetLoss:0.00106  AvgDecoderLoss:0.00113  AvgStopLoss:0.04173  EpochTime:42.24  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.08500   PostnetLoss: 0.00503   DecoderLoss:0.00539  StopLoss: 0.07458  \n",
      "   | > TotalLoss: 0.07625   PostnetLoss: 0.00801   DecoderLoss:0.00853  StopLoss: 0.05971  \n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00106   Validation Loss: 0.00732\n",
      "\n",
      " > Epoch 375/1000\n",
      "   | > Step:4/68  GlobalStep:94880  TotalLoss:0.00151  PostnetLoss:0.00073  DecoderLoss:0.00077  StopLoss:0.07751  GradNorm:0.00367  GradNormST:0.02803  AvgTextLen:4.8  AvgSpecLen:31.1  StepTime:0.25  LR:0.000100\n",
      "   | > Step:14/68  GlobalStep:94890  TotalLoss:0.00176  PostnetLoss:0.00085  DecoderLoss:0.00091  StopLoss:0.05428  GradNorm:0.00354  GradNormST:0.01428  AvgTextLen:12.2  AvgSpecLen:52.0  StepTime:0.33  LR:0.000100\n",
      "   | > Step:24/68  GlobalStep:94900  TotalLoss:0.00208  PostnetLoss:0.00101  DecoderLoss:0.00107  StopLoss:0.04124  GradNorm:0.00373  GradNormST:0.01073  AvgTextLen:19.2  AvgSpecLen:79.0  StepTime:0.49  LR:0.000100\n",
      "   | > Step:34/68  GlobalStep:94910  TotalLoss:0.00225  PostnetLoss:0.00109  DecoderLoss:0.00116  StopLoss:0.02917  GradNorm:0.00411  GradNormST:0.00849  AvgTextLen:26.2  AvgSpecLen:95.9  StepTime:0.57  LR:0.000100\n",
      "   | > Step:44/68  GlobalStep:94920  TotalLoss:0.00241  PostnetLoss:0.00117  DecoderLoss:0.00124  StopLoss:0.04053  GradNorm:0.00386  GradNormST:0.01526  AvgTextLen:33.8  AvgSpecLen:116.3  StepTime:0.57  LR:0.000100\n",
      "   | > Step:54/68  GlobalStep:94930  TotalLoss:0.00260  PostnetLoss:0.00126  DecoderLoss:0.00134  StopLoss:0.03661  GradNorm:0.00460  GradNormST:0.00838  AvgTextLen:42.2  AvgSpecLen:144.9  StepTime:0.82  LR:0.000100\n",
      "   | > Step:64/68  GlobalStep:94940  TotalLoss:0.00270  PostnetLoss:0.00130  DecoderLoss:0.00140  StopLoss:0.03117  GradNorm:0.00326  GradNormST:0.00886  AvgTextLen:60.6  AvgSpecLen:177.6  StepTime:0.80  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:94944  AvgTotalLoss:0.04409  AvgPostnetLoss:0.00106  AvgDecoderLoss:0.00113  AvgStopLoss:0.04190  EpochTime:42.63  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.08537   PostnetLoss: 0.00502   DecoderLoss:0.00537  StopLoss: 0.07497  \n",
      "   | > TotalLoss: 0.07193   PostnetLoss: 0.00771   DecoderLoss:0.00821  StopLoss: 0.05601  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00106   Validation Loss: 0.00711\n",
      "\n",
      " > Epoch 376/1000\n",
      "   | > Step:5/68  GlobalStep:94950  TotalLoss:0.00152  PostnetLoss:0.00073  DecoderLoss:0.00078  StopLoss:0.04886  GradNorm:0.00341  GradNormST:0.01341  AvgTextLen:5.9  AvgSpecLen:33.2  StepTime:0.35  LR:0.000100\n",
      "   | > Step:15/68  GlobalStep:94960  TotalLoss:0.00182  PostnetLoss:0.00089  DecoderLoss:0.00093  StopLoss:0.04503  GradNorm:0.00336  GradNormST:0.01576  AvgTextLen:12.8  AvgSpecLen:59.1  StepTime:0.53  LR:0.000100\n",
      "   | > Step:25/68  GlobalStep:94970  TotalLoss:0.00198  PostnetLoss:0.00096  DecoderLoss:0.00102  StopLoss:0.05027  GradNorm:0.00325  GradNormST:0.01072  AvgTextLen:19.8  AvgSpecLen:73.3  StepTime:0.48  LR:0.000100\n",
      "   | > Step:35/68  GlobalStep:94980  TotalLoss:0.00223  PostnetLoss:0.00108  DecoderLoss:0.00115  StopLoss:0.03641  GradNorm:0.00357  GradNormST:0.00908  AvgTextLen:27.1  AvgSpecLen:104.7  StepTime:0.74  LR:0.000100\n",
      "   | > Step:45/68  GlobalStep:94990  TotalLoss:0.00238  PostnetLoss:0.00115  DecoderLoss:0.00123  StopLoss:0.04533  GradNorm:0.00341  GradNormST:0.01249  AvgTextLen:34.4  AvgSpecLen:118.6  StepTime:0.57  LR:0.000100\n",
      "   | > Step:55/68  GlobalStep:95000  TotalLoss:0.00264  PostnetLoss:0.00128  DecoderLoss:0.00136  StopLoss:0.03579  GradNorm:0.00454  GradNormST:0.00947  AvgTextLen:43.9  AvgSpecLen:145.2  StepTime:0.86  LR:0.000100\n",
      " | | > Checkpoint saving : /vm/TTS/models/cartman_models/mozilla-no-loc-fattn-stopnet-sigmoid-loss_masking-August-15-2019_11+26AM-de6aeb9/checkpoint_95000.pth.tar\n",
      "warning: audio amplitude out of range, auto clipped.\n",
      "   | > Step:65/68  GlobalStep:95010  TotalLoss:0.00288  PostnetLoss:0.00139  DecoderLoss:0.00149  StopLoss:0.02724  GradNorm:0.00317  GradNormST:0.00787  AvgTextLen:64.2  AvgSpecLen:207.4  StepTime:1.23  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:95013  AvgTotalLoss:0.04400  AvgPostnetLoss:0.00106  AvgDecoderLoss:0.00112  AvgStopLoss:0.04182  EpochTime:43.08  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.08347   PostnetLoss: 0.00497   DecoderLoss:0.00534  StopLoss: 0.07317  \n",
      "   | > TotalLoss: 0.07007   PostnetLoss: 0.00760   DecoderLoss:0.00812  StopLoss: 0.05435  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00106   Validation Loss: 0.00701\n",
      "\n",
      " > Epoch 377/1000\n",
      "   | > Step:6/68  GlobalStep:95020  TotalLoss:0.00149  PostnetLoss:0.00073  DecoderLoss:0.00076  StopLoss:0.04850  GradNorm:0.00380  GradNormST:0.01294  AvgTextLen:6.0  AvgSpecLen:34.8  StepTime:0.39  LR:0.000100\n",
      "   | > Step:16/68  GlobalStep:95030  TotalLoss:0.00187  PostnetLoss:0.00091  DecoderLoss:0.00096  StopLoss:0.04040  GradNorm:0.00378  GradNormST:0.01145  AvgTextLen:13.8  AvgSpecLen:60.6  StepTime:0.39  LR:0.000100\n",
      "   | > Step:26/68  GlobalStep:95040  TotalLoss:0.00207  PostnetLoss:0.00100  DecoderLoss:0.00106  StopLoss:0.03866  GradNorm:0.00334  GradNormST:0.01147  AvgTextLen:20.3  AvgSpecLen:77.5  StepTime:0.38  LR:0.000100\n",
      "   | > Step:36/68  GlobalStep:95050  TotalLoss:0.00222  PostnetLoss:0.00108  DecoderLoss:0.00114  StopLoss:0.05102  GradNorm:0.00323  GradNormST:0.01082  AvgTextLen:28.5  AvgSpecLen:100.2  StepTime:0.74  LR:0.000100\n",
      "   | > Step:46/68  GlobalStep:95060  TotalLoss:0.00236  PostnetLoss:0.00114  DecoderLoss:0.00122  StopLoss:0.03779  GradNorm:0.00358  GradNormST:0.01216  AvgTextLen:35.4  AvgSpecLen:122.6  StepTime:0.84  LR:0.000100\n",
      "   | > Step:56/68  GlobalStep:95070  TotalLoss:0.00263  PostnetLoss:0.00127  DecoderLoss:0.00136  StopLoss:0.02384  GradNorm:0.00416  GradNormST:0.00660  AvgTextLen:45.8  AvgSpecLen:145.4  StepTime:0.96  LR:0.000100\n",
      "   | > Step:66/68  GlobalStep:95080  TotalLoss:0.00304  PostnetLoss:0.00146  DecoderLoss:0.00158  StopLoss:0.02997  GradNorm:0.00297  GradNormST:0.01249  AvgTextLen:71.2  AvgSpecLen:220.8  StepTime:1.08  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:95082  AvgTotalLoss:0.04554  AvgPostnetLoss:0.00105  AvgDecoderLoss:0.00112  AvgStopLoss:0.04337  EpochTime:42.73  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.08158   PostnetLoss: 0.00496   DecoderLoss:0.00532  StopLoss: 0.07129  \n",
      "   | > TotalLoss: 0.07407   PostnetLoss: 0.00780   DecoderLoss:0.00832  StopLoss: 0.05796  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00105   Validation Loss: 0.00714\n",
      "\n",
      " > Epoch 378/1000\n",
      "   | > Step:7/68  GlobalStep:95090  TotalLoss:0.00159  PostnetLoss:0.00078  DecoderLoss:0.00081  StopLoss:0.04311  GradNorm:0.00343  GradNormST:0.01740  AvgTextLen:7.4  AvgSpecLen:44.6  StepTime:0.35  LR:0.000100\n",
      "   | > Step:17/68  GlobalStep:95100  TotalLoss:0.00185  PostnetLoss:0.00090  DecoderLoss:0.00095  StopLoss:0.05621  GradNorm:0.00340  GradNormST:0.01407  AvgTextLen:14.7  AvgSpecLen:66.0  StepTime:0.42  LR:0.000100\n",
      "   | > Step:27/68  GlobalStep:95110  TotalLoss:0.00210  PostnetLoss:0.00102  DecoderLoss:0.00108  StopLoss:0.03581  GradNorm:0.00310  GradNormST:0.00785  AvgTextLen:21.0  AvgSpecLen:80.2  StepTime:0.67  LR:0.000100\n",
      "   | > Step:37/68  GlobalStep:95120  TotalLoss:0.00228  PostnetLoss:0.00110  DecoderLoss:0.00117  StopLoss:0.03802  GradNorm:0.00334  GradNormST:0.01249  AvgTextLen:29.3  AvgSpecLen:102.4  StepTime:0.59  LR:0.000100\n",
      "   | > Step:47/68  GlobalStep:95130  TotalLoss:0.00245  PostnetLoss:0.00119  DecoderLoss:0.00127  StopLoss:0.03931  GradNorm:0.00375  GradNormST:0.00872  AvgTextLen:36.7  AvgSpecLen:126.9  StepTime:0.74  LR:0.000100\n",
      "   | > Step:57/68  GlobalStep:95140  TotalLoss:0.00268  PostnetLoss:0.00130  DecoderLoss:0.00139  StopLoss:0.03422  GradNorm:0.00440  GradNormST:0.00586  AvgTextLen:47.1  AvgSpecLen:161.0  StepTime:0.83  LR:0.000100\n",
      "   | > Step:67/68  GlobalStep:95150  TotalLoss:0.00304  PostnetLoss:0.00146  DecoderLoss:0.00158  StopLoss:0.03133  GradNorm:0.00272  GradNormST:0.01255  AvgTextLen:80.6  AvgSpecLen:246.5  StepTime:1.22  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:95151  AvgTotalLoss:0.04388  AvgPostnetLoss:0.00105  AvgDecoderLoss:0.00112  AvgStopLoss:0.04171  EpochTime:42.29  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.08089   PostnetLoss: 0.00504   DecoderLoss:0.00540  StopLoss: 0.07045  \n",
      "   | > TotalLoss: 0.07147   PostnetLoss: 0.00760   DecoderLoss:0.00813  StopLoss: 0.05574  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00105   Validation Loss: 0.00703\n",
      "\n",
      " > Epoch 379/1000\n",
      "   | > Step:8/68  GlobalStep:95160  TotalLoss:0.00162  PostnetLoss:0.00079  DecoderLoss:0.00083  StopLoss:0.06008  GradNorm:0.00395  GradNormST:0.01463  AvgTextLen:8.3  AvgSpecLen:44.0  StepTime:0.34  LR:0.000100\n",
      "   | > Step:18/68  GlobalStep:95170  TotalLoss:0.00183  PostnetLoss:0.00089  DecoderLoss:0.00094  StopLoss:0.05808  GradNorm:0.00343  GradNormST:0.01901  AvgTextLen:15.2  AvgSpecLen:64.5  StepTime:0.37  LR:0.000100\n",
      "   | > Step:28/68  GlobalStep:95180  TotalLoss:0.00210  PostnetLoss:0.00102  DecoderLoss:0.00108  StopLoss:0.04072  GradNorm:0.00325  GradNormST:0.01002  AvgTextLen:22.1  AvgSpecLen:85.4  StepTime:0.53  LR:0.000100\n",
      "   | > Step:38/68  GlobalStep:95190  TotalLoss:0.00221  PostnetLoss:0.00107  DecoderLoss:0.00114  StopLoss:0.03229  GradNorm:0.00303  GradNormST:0.00975  AvgTextLen:29.4  AvgSpecLen:104.1  StepTime:0.61  LR:0.000100\n",
      "   | > Step:48/68  GlobalStep:95200  TotalLoss:0.00244  PostnetLoss:0.00118  DecoderLoss:0.00126  StopLoss:0.03069  GradNorm:0.00297  GradNormST:0.00643  AvgTextLen:37.5  AvgSpecLen:123.8  StepTime:0.59  LR:0.000100\n",
      "   | > Step:58/68  GlobalStep:95210  TotalLoss:0.00269  PostnetLoss:0.00130  DecoderLoss:0.00139  StopLoss:0.03728  GradNorm:0.00371  GradNormST:0.01108  AvgTextLen:47.8  AvgSpecLen:158.0  StepTime:0.85  LR:0.000100\n",
      "   | > Step:68/68  GlobalStep:95220  TotalLoss:0.00298  PostnetLoss:0.00143  DecoderLoss:0.00155  StopLoss:0.02838  GradNorm:0.00295  GradNormST:0.01264  AvgTextLen:103.0  AvgSpecLen:308.9  StepTime:1.86  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:95220  AvgTotalLoss:0.04467  AvgPostnetLoss:0.00105  AvgDecoderLoss:0.00112  AvgStopLoss:0.04249  EpochTime:42.21  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.08017   PostnetLoss: 0.00500   DecoderLoss:0.00537  StopLoss: 0.06981  \n",
      "   | > TotalLoss: 0.07005   PostnetLoss: 0.00760   DecoderLoss:0.00813  StopLoss: 0.05432  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00105   Validation Loss: 0.00704\n",
      "\n",
      " > Epoch 380/1000\n",
      "   | > Step:9/68  GlobalStep:95230  TotalLoss:0.00161  PostnetLoss:0.00078  DecoderLoss:0.00083  StopLoss:0.03539  GradNorm:0.00356  GradNormST:0.01320  AvgTextLen:9.0  AvgSpecLen:42.1  StepTime:0.40  LR:0.000100\n",
      "   | > Step:19/68  GlobalStep:95240  TotalLoss:0.00186  PostnetLoss:0.00090  DecoderLoss:0.00095  StopLoss:0.06986  GradNorm:0.00388  GradNormST:0.01428  AvgTextLen:15.5  AvgSpecLen:63.2  StepTime:0.45  LR:0.000100\n",
      "   | > Step:29/68  GlobalStep:95250  TotalLoss:0.00205  PostnetLoss:0.00099  DecoderLoss:0.00106  StopLoss:0.05545  GradNorm:0.00324  GradNormST:0.01571  AvgTextLen:21.8  AvgSpecLen:83.4  StepTime:0.56  LR:0.000100\n",
      "   | > Step:39/68  GlobalStep:95260  TotalLoss:0.00235  PostnetLoss:0.00114  DecoderLoss:0.00121  StopLoss:0.03449  GradNorm:0.00343  GradNormST:0.01039  AvgTextLen:30.2  AvgSpecLen:110.4  StepTime:0.67  LR:0.000100\n",
      "   | > Step:49/68  GlobalStep:95270  TotalLoss:0.00247  PostnetLoss:0.00119  DecoderLoss:0.00127  StopLoss:0.03140  GradNorm:0.00301  GradNormST:0.00655  AvgTextLen:38.2  AvgSpecLen:127.7  StepTime:0.77  LR:0.000100\n",
      "   | > Step:59/68  GlobalStep:95280  TotalLoss:0.00263  PostnetLoss:0.00127  DecoderLoss:0.00136  StopLoss:0.03128  GradNorm:0.00309  GradNormST:0.00632  AvgTextLen:50.5  AvgSpecLen:155.9  StepTime:0.91  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:95289  AvgTotalLoss:0.04359  AvgPostnetLoss:0.00105  AvgDecoderLoss:0.00112  AvgStopLoss:0.04141  EpochTime:43.18  AvgStepTime:0.63\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.07925   PostnetLoss: 0.00511   DecoderLoss:0.00547  StopLoss: 0.06867  \n",
      "   | > TotalLoss: 0.06829   PostnetLoss: 0.00744   DecoderLoss:0.00796  StopLoss: 0.05289  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00105   Validation Loss: 0.00695\n",
      "\n",
      " > Epoch 381/1000\n",
      "   | > Step:0/68  GlobalStep:95290  TotalLoss:0.00156  PostnetLoss:0.00075  DecoderLoss:0.00081  StopLoss:0.05124  GradNorm:0.00635  GradNormST:0.01536  AvgTextLen:3.0  AvgSpecLen:25.3  StepTime:0.43  LR:0.000100\n",
      "   | > Step:10/68  GlobalStep:95300  TotalLoss:0.00170  PostnetLoss:0.00083  DecoderLoss:0.00088  StopLoss:0.03961  GradNorm:0.00442  GradNormST:0.01233  AvgTextLen:9.8  AvgSpecLen:48.6  StepTime:0.43  LR:0.000100\n",
      "   | > Step:20/68  GlobalStep:95310  TotalLoss:0.00198  PostnetLoss:0.00097  DecoderLoss:0.00102  StopLoss:0.04273  GradNorm:0.00354  GradNormST:0.01427  AvgTextLen:16.4  AvgSpecLen:64.8  StepTime:0.47  LR:0.000100\n",
      "   | > Step:30/68  GlobalStep:95320  TotalLoss:0.00213  PostnetLoss:0.00103  DecoderLoss:0.00110  StopLoss:0.04645  GradNorm:0.00355  GradNormST:0.00948  AvgTextLen:22.9  AvgSpecLen:85.8  StepTime:0.57  LR:0.000100\n",
      "   | > Step:40/68  GlobalStep:95330  TotalLoss:0.00225  PostnetLoss:0.00110  DecoderLoss:0.00116  StopLoss:0.03685  GradNorm:0.00338  GradNormST:0.00956  AvgTextLen:30.9  AvgSpecLen:105.3  StepTime:0.62  LR:0.000100\n",
      "   | > Step:50/68  GlobalStep:95340  TotalLoss:0.00247  PostnetLoss:0.00119  DecoderLoss:0.00128  StopLoss:0.03356  GradNorm:0.00314  GradNormST:0.00737  AvgTextLen:38.8  AvgSpecLen:132.0  StepTime:0.78  LR:0.000100\n",
      "   | > Step:60/68  GlobalStep:95350  TotalLoss:0.00273  PostnetLoss:0.00132  DecoderLoss:0.00141  StopLoss:0.02602  GradNorm:0.00395  GradNormST:0.00562  AvgTextLen:52.0  AvgSpecLen:172.7  StepTime:1.01  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:95358  AvgTotalLoss:0.04433  AvgPostnetLoss:0.00105  AvgDecoderLoss:0.00112  AvgStopLoss:0.04215  EpochTime:42.12  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.07940   PostnetLoss: 0.00503   DecoderLoss:0.00538  StopLoss: 0.06899  \n",
      "   | > TotalLoss: 0.06876   PostnetLoss: 0.00734   DecoderLoss:0.00785  StopLoss: 0.05358  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00105   Validation Loss: 0.00689\n",
      "\n",
      " > Epoch 382/1000\n",
      "   | > Step:1/68  GlobalStep:95360  TotalLoss:0.00151  PostnetLoss:0.00073  DecoderLoss:0.00078  StopLoss:0.04359  GradNorm:0.00507  GradNormST:0.02011  AvgTextLen:3.7  AvgSpecLen:28.6  StepTime:0.41  LR:0.000100\n",
      "   | > Step:11/68  GlobalStep:95370  TotalLoss:0.00169  PostnetLoss:0.00082  DecoderLoss:0.00087  StopLoss:0.05531  GradNorm:0.00439  GradNormST:0.02018  AvgTextLen:10.3  AvgSpecLen:49.4  StepTime:0.40  LR:0.000100\n",
      "   | > Step:21/68  GlobalStep:95380  TotalLoss:0.00192  PostnetLoss:0.00093  DecoderLoss:0.00099  StopLoss:0.04478  GradNorm:0.00382  GradNormST:0.01473  AvgTextLen:16.6  AvgSpecLen:66.3  StepTime:0.34  LR:0.000100\n",
      "   | > Step:31/68  GlobalStep:95390  TotalLoss:0.00212  PostnetLoss:0.00103  DecoderLoss:0.00109  StopLoss:0.04129  GradNorm:0.00367  GradNormST:0.01257  AvgTextLen:24.1  AvgSpecLen:82.7  StepTime:0.54  LR:0.000100\n",
      "   | > Step:41/68  GlobalStep:95400  TotalLoss:0.00234  PostnetLoss:0.00113  DecoderLoss:0.00120  StopLoss:0.02747  GradNorm:0.00301  GradNormST:0.00898  AvgTextLen:32.3  AvgSpecLen:109.4  StepTime:0.63  LR:0.000100\n",
      "   | > Step:51/68  GlobalStep:95410  TotalLoss:0.00245  PostnetLoss:0.00118  DecoderLoss:0.00127  StopLoss:0.02948  GradNorm:0.00273  GradNormST:0.00688  AvgTextLen:39.2  AvgSpecLen:126.9  StepTime:0.72  LR:0.000100\n",
      "   | > Step:61/68  GlobalStep:95420  TotalLoss:0.00268  PostnetLoss:0.00129  DecoderLoss:0.00138  StopLoss:0.02849  GradNorm:0.00293  GradNormST:0.00758  AvgTextLen:54.6  AvgSpecLen:165.5  StepTime:0.91  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:95427  AvgTotalLoss:0.04555  AvgPostnetLoss:0.00105  AvgDecoderLoss:0.00112  AvgStopLoss:0.04338  EpochTime:41.64  AvgStepTime:0.60\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.08081   PostnetLoss: 0.00500   DecoderLoss:0.00536  StopLoss: 0.07046  \n",
      "   | > TotalLoss: 0.06885   PostnetLoss: 0.00740   DecoderLoss:0.00792  StopLoss: 0.05353  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00105   Validation Loss: 0.00689\n",
      "\n",
      " > Epoch 383/1000\n",
      "   | > Step:2/68  GlobalStep:95430  TotalLoss:0.00140  PostnetLoss:0.00068  DecoderLoss:0.00072  StopLoss:0.04971  GradNorm:0.00386  GradNormST:0.01433  AvgTextLen:4.0  AvgSpecLen:31.3  StepTime:0.36  LR:0.000100\n",
      "   | > Step:12/68  GlobalStep:95440  TotalLoss:0.00165  PostnetLoss:0.00080  DecoderLoss:0.00085  StopLoss:0.04361  GradNorm:0.00386  GradNormST:0.01042  AvgTextLen:11.0  AvgSpecLen:49.3  StepTime:0.41  LR:0.000100\n",
      "   | > Step:22/68  GlobalStep:95450  TotalLoss:0.00199  PostnetLoss:0.00097  DecoderLoss:0.00102  StopLoss:0.04888  GradNorm:0.00415  GradNormST:0.01028  AvgTextLen:18.0  AvgSpecLen:72.0  StepTime:0.47  LR:0.000100\n",
      "   | > Step:32/68  GlobalStep:95460  TotalLoss:0.00213  PostnetLoss:0.00103  DecoderLoss:0.00109  StopLoss:0.05732  GradNorm:0.00391  GradNormST:0.01750  AvgTextLen:24.4  AvgSpecLen:91.5  StepTime:0.50  LR:0.000100\n",
      "   | > Step:42/68  GlobalStep:95470  TotalLoss:0.00232  PostnetLoss:0.00112  DecoderLoss:0.00120  StopLoss:0.04650  GradNorm:0.00289  GradNormST:0.01045  AvgTextLen:33.1  AvgSpecLen:109.7  StepTime:0.63  LR:0.000100\n",
      "   | > Step:52/68  GlobalStep:95480  TotalLoss:0.00251  PostnetLoss:0.00122  DecoderLoss:0.00130  StopLoss:0.04382  GradNorm:0.00287  GradNormST:0.01496  AvgTextLen:42.1  AvgSpecLen:137.5  StepTime:0.81  LR:0.000100\n",
      "   | > Step:62/68  GlobalStep:95490  TotalLoss:0.00274  PostnetLoss:0.00132  DecoderLoss:0.00142  StopLoss:0.03269  GradNorm:0.00284  GradNormST:0.01132  AvgTextLen:54.0  AvgSpecLen:174.7  StepTime:0.75  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:95496  AvgTotalLoss:0.04457  AvgPostnetLoss:0.00105  AvgDecoderLoss:0.00112  AvgStopLoss:0.04239  EpochTime:41.99  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.08092   PostnetLoss: 0.00512   DecoderLoss:0.00549  StopLoss: 0.07031  \n",
      "   | > TotalLoss: 0.06953   PostnetLoss: 0.00737   DecoderLoss:0.00790  StopLoss: 0.05426  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00105   Validation Loss: 0.00686\n",
      "\n",
      " > Epoch 384/1000\n",
      "   | > Step:3/68  GlobalStep:95500  TotalLoss:0.00149  PostnetLoss:0.00073  DecoderLoss:0.00076  StopLoss:0.07323  GradNorm:0.00394  GradNormST:0.01686  AvgTextLen:4.7  AvgSpecLen:29.7  StepTime:0.31  LR:0.000100\n",
      "   | > Step:13/68  GlobalStep:95510  TotalLoss:0.00167  PostnetLoss:0.00081  DecoderLoss:0.00086  StopLoss:0.06462  GradNorm:0.00361  GradNormST:0.01731  AvgTextLen:11.5  AvgSpecLen:55.2  StepTime:0.41  LR:0.000100\n",
      "   | > Step:23/68  GlobalStep:95520  TotalLoss:0.00198  PostnetLoss:0.00097  DecoderLoss:0.00102  StopLoss:0.04943  GradNorm:0.00346  GradNormST:0.00944  AvgTextLen:18.3  AvgSpecLen:72.2  StepTime:0.43  LR:0.000100\n",
      "   | > Step:33/68  GlobalStep:95530  TotalLoss:0.00219  PostnetLoss:0.00106  DecoderLoss:0.00112  StopLoss:0.04318  GradNorm:0.00360  GradNormST:0.01272  AvgTextLen:25.9  AvgSpecLen:88.9  StepTime:0.54  LR:0.000100\n",
      "   | > Step:43/68  GlobalStep:95540  TotalLoss:0.00232  PostnetLoss:0.00113  DecoderLoss:0.00119  StopLoss:0.04381  GradNorm:0.00296  GradNormST:0.00842  AvgTextLen:33.4  AvgSpecLen:115.7  StepTime:0.67  LR:0.000100\n",
      "   | > Step:53/68  GlobalStep:95550  TotalLoss:0.00248  PostnetLoss:0.00120  DecoderLoss:0.00128  StopLoss:0.03254  GradNorm:0.00277  GradNormST:0.00634  AvgTextLen:42.4  AvgSpecLen:137.6  StepTime:0.76  LR:0.000100\n",
      "   | > Step:63/68  GlobalStep:95560  TotalLoss:0.00281  PostnetLoss:0.00136  DecoderLoss:0.00145  StopLoss:0.02386  GradNorm:0.00274  GradNormST:0.00540  AvgTextLen:58.6  AvgSpecLen:185.1  StepTime:0.90  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:95565  AvgTotalLoss:0.04458  AvgPostnetLoss:0.00105  AvgDecoderLoss:0.00112  AvgStopLoss:0.04242  EpochTime:42.76  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.07824   PostnetLoss: 0.00500   DecoderLoss:0.00536  StopLoss: 0.06787  \n",
      "   | > TotalLoss: 0.06917   PostnetLoss: 0.00734   DecoderLoss:0.00786  StopLoss: 0.05397  \n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00105   Validation Loss: 0.00687\n",
      "\n",
      " > Epoch 385/1000\n",
      "   | > Step:4/68  GlobalStep:95570  TotalLoss:0.00143  PostnetLoss:0.00070  DecoderLoss:0.00074  StopLoss:0.07382  GradNorm:0.00407  GradNormST:0.03088  AvgTextLen:4.8  AvgSpecLen:31.1  StepTime:0.25  LR:0.000100\n",
      "   | > Step:14/68  GlobalStep:95580  TotalLoss:0.00173  PostnetLoss:0.00084  DecoderLoss:0.00089  StopLoss:0.05125  GradNorm:0.00373  GradNormST:0.01202  AvgTextLen:12.2  AvgSpecLen:52.0  StepTime:0.36  LR:0.000100\n",
      "   | > Step:24/68  GlobalStep:95590  TotalLoss:0.00206  PostnetLoss:0.00100  DecoderLoss:0.00106  StopLoss:0.04261  GradNorm:0.00367  GradNormST:0.00950  AvgTextLen:19.2  AvgSpecLen:79.0  StepTime:0.49  LR:0.000100\n",
      "   | > Step:34/68  GlobalStep:95600  TotalLoss:0.00222  PostnetLoss:0.00108  DecoderLoss:0.00114  StopLoss:0.03214  GradNorm:0.00335  GradNormST:0.00712  AvgTextLen:26.2  AvgSpecLen:95.9  StepTime:0.59  LR:0.000100\n",
      "   | > Step:44/68  GlobalStep:95610  TotalLoss:0.00237  PostnetLoss:0.00115  DecoderLoss:0.00122  StopLoss:0.03823  GradNorm:0.00318  GradNormST:0.01431  AvgTextLen:33.8  AvgSpecLen:116.3  StepTime:0.65  LR:0.000100\n",
      "   | > Step:54/68  GlobalStep:95620  TotalLoss:0.00255  PostnetLoss:0.00124  DecoderLoss:0.00132  StopLoss:0.03418  GradNorm:0.00296  GradNormST:0.00733  AvgTextLen:42.2  AvgSpecLen:144.9  StepTime:0.80  LR:0.000100\n",
      "   | > Step:64/68  GlobalStep:95630  TotalLoss:0.00267  PostnetLoss:0.00128  DecoderLoss:0.00138  StopLoss:0.03279  GradNorm:0.00314  GradNormST:0.00628  AvgTextLen:60.6  AvgSpecLen:177.6  StepTime:0.80  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:95634  AvgTotalLoss:0.04436  AvgPostnetLoss:0.00105  AvgDecoderLoss:0.00112  AvgStopLoss:0.04219  EpochTime:41.23  AvgStepTime:0.60\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.07650   PostnetLoss: 0.00501   DecoderLoss:0.00537  StopLoss: 0.06612  \n",
      "   | > TotalLoss: 0.06659   PostnetLoss: 0.00737   DecoderLoss:0.00789  StopLoss: 0.05133  \n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00105   Validation Loss: 0.00686\n",
      "\n",
      " > Epoch 386/1000\n",
      "   | > Step:5/68  GlobalStep:95640  TotalLoss:0.00156  PostnetLoss:0.00075  DecoderLoss:0.00080  StopLoss:0.03921  GradNorm:0.00354  GradNormST:0.01070  AvgTextLen:5.9  AvgSpecLen:33.2  StepTime:0.34  LR:0.000100\n",
      "   | > Step:15/68  GlobalStep:95650  TotalLoss:0.00184  PostnetLoss:0.00089  DecoderLoss:0.00094  StopLoss:0.04253  GradNorm:0.00357  GradNormST:0.01200  AvgTextLen:12.8  AvgSpecLen:59.1  StepTime:0.53  LR:0.000100\n",
      "   | > Step:25/68  GlobalStep:95660  TotalLoss:0.00199  PostnetLoss:0.00097  DecoderLoss:0.00103  StopLoss:0.04741  GradNorm:0.00385  GradNormST:0.01182  AvgTextLen:19.8  AvgSpecLen:73.3  StepTime:0.46  LR:0.000100\n",
      "   | > Step:35/68  GlobalStep:95670  TotalLoss:0.00219  PostnetLoss:0.00106  DecoderLoss:0.00113  StopLoss:0.03705  GradNorm:0.00315  GradNormST:0.01164  AvgTextLen:27.1  AvgSpecLen:104.7  StepTime:0.81  LR:0.000100\n",
      "   | > Step:45/68  GlobalStep:95680  TotalLoss:0.00233  PostnetLoss:0.00113  DecoderLoss:0.00120  StopLoss:0.04687  GradNorm:0.00308  GradNormST:0.01306  AvgTextLen:34.4  AvgSpecLen:118.6  StepTime:0.67  LR:0.000100\n",
      "   | > Step:55/68  GlobalStep:95690  TotalLoss:0.00262  PostnetLoss:0.00127  DecoderLoss:0.00135  StopLoss:0.03205  GradNorm:0.00282  GradNormST:0.00609  AvgTextLen:43.9  AvgSpecLen:145.2  StepTime:0.80  LR:0.000100\n",
      "   | > Step:65/68  GlobalStep:95700  TotalLoss:0.00289  PostnetLoss:0.00139  DecoderLoss:0.00150  StopLoss:0.03050  GradNorm:0.00344  GradNormST:0.01004  AvgTextLen:64.2  AvgSpecLen:207.4  StepTime:1.20  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:95703  AvgTotalLoss:0.04366  AvgPostnetLoss:0.00105  AvgDecoderLoss:0.00112  AvgStopLoss:0.04149  EpochTime:42.81  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.07883   PostnetLoss: 0.00500   DecoderLoss:0.00536  StopLoss: 0.06846  \n",
      "   | > TotalLoss: 0.06644   PostnetLoss: 0.00741   DecoderLoss:0.00794  StopLoss: 0.05109  \n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00105   Validation Loss: 0.00682\n",
      "\n",
      " > Epoch 387/1000\n",
      "   | > Step:6/68  GlobalStep:95710  TotalLoss:0.00152  PostnetLoss:0.00074  DecoderLoss:0.00078  StopLoss:0.03147  GradNorm:0.00381  GradNormST:0.01047  AvgTextLen:6.0  AvgSpecLen:34.8  StepTime:0.38  LR:0.000100\n",
      "   | > Step:16/68  GlobalStep:95720  TotalLoss:0.00183  PostnetLoss:0.00089  DecoderLoss:0.00094  StopLoss:0.04518  GradNorm:0.00374  GradNormST:0.01190  AvgTextLen:13.8  AvgSpecLen:60.6  StepTime:0.29  LR:0.000100\n",
      "   | > Step:26/68  GlobalStep:95730  TotalLoss:0.00204  PostnetLoss:0.00099  DecoderLoss:0.00105  StopLoss:0.03498  GradNorm:0.00395  GradNormST:0.01093  AvgTextLen:20.3  AvgSpecLen:77.5  StepTime:0.50  LR:0.000100\n",
      "   | > Step:36/68  GlobalStep:95740  TotalLoss:0.00219  PostnetLoss:0.00106  DecoderLoss:0.00113  StopLoss:0.05114  GradNorm:0.00332  GradNormST:0.01137  AvgTextLen:28.5  AvgSpecLen:100.2  StepTime:0.57  LR:0.000100\n",
      "   | > Step:46/68  GlobalStep:95750  TotalLoss:0.00235  PostnetLoss:0.00114  DecoderLoss:0.00121  StopLoss:0.03197  GradNorm:0.00320  GradNormST:0.00676  AvgTextLen:35.4  AvgSpecLen:122.6  StepTime:0.77  LR:0.000100\n",
      "   | > Step:56/68  GlobalStep:95760  TotalLoss:0.00258  PostnetLoss:0.00125  DecoderLoss:0.00133  StopLoss:0.03103  GradNorm:0.00290  GradNormST:0.01332  AvgTextLen:45.8  AvgSpecLen:145.4  StepTime:1.04  LR:0.000100\n",
      "   | > Step:66/68  GlobalStep:95770  TotalLoss:0.00302  PostnetLoss:0.00145  DecoderLoss:0.00157  StopLoss:0.03145  GradNorm:0.00331  GradNormST:0.01311  AvgTextLen:71.2  AvgSpecLen:220.8  StepTime:1.04  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:95772  AvgTotalLoss:0.04344  AvgPostnetLoss:0.00105  AvgDecoderLoss:0.00112  AvgStopLoss:0.04127  EpochTime:42.49  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.07637   PostnetLoss: 0.00500   DecoderLoss:0.00537  StopLoss: 0.06600  \n",
      "   | > TotalLoss: 0.06738   PostnetLoss: 0.00726   DecoderLoss:0.00777  StopLoss: 0.05235  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00105   Validation Loss: 0.00677\n",
      "\n",
      " > Epoch 388/1000\n",
      "   | > Step:7/68  GlobalStep:95780  TotalLoss:0.00157  PostnetLoss:0.00076  DecoderLoss:0.00080  StopLoss:0.03553  GradNorm:0.00403  GradNormST:0.01524  AvgTextLen:7.4  AvgSpecLen:44.6  StepTime:0.38  LR:0.000100\n",
      "   | > Step:17/68  GlobalStep:95790  TotalLoss:0.00182  PostnetLoss:0.00088  DecoderLoss:0.00093  StopLoss:0.06458  GradNorm:0.00353  GradNormST:0.01421  AvgTextLen:14.7  AvgSpecLen:66.0  StepTime:0.44  LR:0.000100\n",
      "   | > Step:27/68  GlobalStep:95800  TotalLoss:0.00206  PostnetLoss:0.00100  DecoderLoss:0.00106  StopLoss:0.04065  GradNorm:0.00321  GradNormST:0.00765  AvgTextLen:21.0  AvgSpecLen:80.2  StepTime:0.62  LR:0.000100\n",
      "   | > Step:37/68  GlobalStep:95810  TotalLoss:0.00228  PostnetLoss:0.00110  DecoderLoss:0.00117  StopLoss:0.03547  GradNorm:0.00390  GradNormST:0.00573  AvgTextLen:29.3  AvgSpecLen:102.4  StepTime:0.57  LR:0.000100\n",
      "   | > Step:47/68  GlobalStep:95820  TotalLoss:0.00245  PostnetLoss:0.00119  DecoderLoss:0.00126  StopLoss:0.03821  GradNorm:0.00297  GradNormST:0.00741  AvgTextLen:36.7  AvgSpecLen:126.9  StepTime:0.70  LR:0.000100\n",
      "   | > Step:57/68  GlobalStep:95830  TotalLoss:0.00269  PostnetLoss:0.00130  DecoderLoss:0.00139  StopLoss:0.03115  GradNorm:0.00327  GradNormST:0.00616  AvgTextLen:47.1  AvgSpecLen:161.0  StepTime:0.90  LR:0.000100\n",
      "   | > Step:67/68  GlobalStep:95840  TotalLoss:0.00303  PostnetLoss:0.00146  DecoderLoss:0.00157  StopLoss:0.02795  GradNorm:0.00287  GradNormST:0.01072  AvgTextLen:80.6  AvgSpecLen:246.5  StepTime:1.22  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:95841  AvgTotalLoss:0.04278  AvgPostnetLoss:0.00105  AvgDecoderLoss:0.00112  AvgStopLoss:0.04061  EpochTime:42.77  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.08024   PostnetLoss: 0.00491   DecoderLoss:0.00527  StopLoss: 0.07006  \n",
      "   | > TotalLoss: 0.06669   PostnetLoss: 0.00726   DecoderLoss:0.00779  StopLoss: 0.05163  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00105   Validation Loss: 0.00676\n",
      "\n",
      " > Epoch 389/1000\n",
      "   | > Step:8/68  GlobalStep:95850  TotalLoss:0.00169  PostnetLoss:0.00083  DecoderLoss:0.00087  StopLoss:0.04937  GradNorm:0.00456  GradNormST:0.01997  AvgTextLen:8.3  AvgSpecLen:44.0  StepTime:0.31  LR:0.000100\n",
      "   | > Step:18/68  GlobalStep:95860  TotalLoss:0.00185  PostnetLoss:0.00090  DecoderLoss:0.00095  StopLoss:0.06034  GradNorm:0.00398  GradNormST:0.01797  AvgTextLen:15.2  AvgSpecLen:64.5  StepTime:0.32  LR:0.000100\n",
      "   | > Step:28/68  GlobalStep:95870  TotalLoss:0.00207  PostnetLoss:0.00101  DecoderLoss:0.00107  StopLoss:0.04246  GradNorm:0.00309  GradNormST:0.01030  AvgTextLen:22.1  AvgSpecLen:85.4  StepTime:0.58  LR:0.000100\n",
      "   | > Step:38/68  GlobalStep:95880  TotalLoss:0.00222  PostnetLoss:0.00108  DecoderLoss:0.00114  StopLoss:0.06817  GradNorm:0.00308  GradNormST:0.03320  AvgTextLen:29.4  AvgSpecLen:104.1  StepTime:0.64  LR:0.000100\n",
      "   | > Step:48/68  GlobalStep:95890  TotalLoss:0.00245  PostnetLoss:0.00119  DecoderLoss:0.00126  StopLoss:0.03059  GradNorm:0.00304  GradNormST:0.00666  AvgTextLen:37.5  AvgSpecLen:123.8  StepTime:0.55  LR:0.000100\n",
      "   | > Step:58/68  GlobalStep:95900  TotalLoss:0.00267  PostnetLoss:0.00129  DecoderLoss:0.00138  StopLoss:0.03270  GradNorm:0.00311  GradNormST:0.01064  AvgTextLen:47.8  AvgSpecLen:158.0  StepTime:0.70  LR:0.000100\n",
      "   | > Step:68/68  GlobalStep:95910  TotalLoss:0.00298  PostnetLoss:0.00143  DecoderLoss:0.00154  StopLoss:0.02602  GradNorm:0.00299  GradNormST:0.01224  AvgTextLen:103.0  AvgSpecLen:308.9  StepTime:1.86  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:95910  AvgTotalLoss:0.04484  AvgPostnetLoss:0.00106  AvgDecoderLoss:0.00112  AvgStopLoss:0.04266  EpochTime:42.33  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.08470   PostnetLoss: 0.00491   DecoderLoss:0.00527  StopLoss: 0.07452  \n",
      "   | > TotalLoss: 0.06721   PostnetLoss: 0.00756   DecoderLoss:0.00809  StopLoss: 0.05155  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00106   Validation Loss: 0.00686\n",
      "\n",
      " > Epoch 390/1000\n",
      "   | > Step:9/68  GlobalStep:95920  TotalLoss:0.00164  PostnetLoss:0.00080  DecoderLoss:0.00084  StopLoss:0.04201  GradNorm:0.00470  GradNormST:0.01174  AvgTextLen:9.0  AvgSpecLen:42.1  StepTime:0.43  LR:0.000100\n",
      "   | > Step:19/68  GlobalStep:95930  TotalLoss:0.00192  PostnetLoss:0.00094  DecoderLoss:0.00099  StopLoss:0.05780  GradNorm:0.00534  GradNormST:0.01346  AvgTextLen:15.5  AvgSpecLen:63.2  StepTime:0.41  LR:0.000100\n",
      "   | > Step:29/68  GlobalStep:95940  TotalLoss:0.00210  PostnetLoss:0.00102  DecoderLoss:0.00108  StopLoss:0.06134  GradNorm:0.00380  GradNormST:0.01483  AvgTextLen:21.8  AvgSpecLen:83.4  StepTime:0.54  LR:0.000100\n",
      "   | > Step:39/68  GlobalStep:95950  TotalLoss:0.00230  PostnetLoss:0.00112  DecoderLoss:0.00118  StopLoss:0.03418  GradNorm:0.00305  GradNormST:0.01170  AvgTextLen:30.2  AvgSpecLen:110.4  StepTime:0.69  LR:0.000100\n",
      "   | > Step:49/68  GlobalStep:95960  TotalLoss:0.00245  PostnetLoss:0.00119  DecoderLoss:0.00126  StopLoss:0.03060  GradNorm:0.00302  GradNormST:0.00736  AvgTextLen:38.2  AvgSpecLen:127.7  StepTime:0.78  LR:0.000100\n",
      "   | > Step:59/68  GlobalStep:95970  TotalLoss:0.00261  PostnetLoss:0.00126  DecoderLoss:0.00135  StopLoss:0.04274  GradNorm:0.00341  GradNormST:0.00884  AvgTextLen:50.5  AvgSpecLen:155.9  StepTime:0.88  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:95979  AvgTotalLoss:0.04341  AvgPostnetLoss:0.00107  AvgDecoderLoss:0.00114  AvgStopLoss:0.04121  EpochTime:42.39  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.08497   PostnetLoss: 0.00493   DecoderLoss:0.00531  StopLoss: 0.07473  \n",
      "   | > TotalLoss: 0.06611   PostnetLoss: 0.00740   DecoderLoss:0.00793  StopLoss: 0.05079  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00107   Validation Loss: 0.00678\n",
      "\n",
      " > Epoch 391/1000\n",
      "   | > Step:0/68  GlobalStep:95980  TotalLoss:0.00161  PostnetLoss:0.00078  DecoderLoss:0.00084  StopLoss:0.06568  GradNorm:0.00832  GradNormST:0.01911  AvgTextLen:3.0  AvgSpecLen:25.3  StepTime:0.51  LR:0.000100\n",
      "   | > Step:10/68  GlobalStep:95990  TotalLoss:0.00172  PostnetLoss:0.00084  DecoderLoss:0.00088  StopLoss:0.03468  GradNorm:0.00396  GradNormST:0.01424  AvgTextLen:9.8  AvgSpecLen:48.6  StepTime:0.45  LR:0.000100\n",
      "   | > Step:20/68  GlobalStep:96000  TotalLoss:0.00196  PostnetLoss:0.00095  DecoderLoss:0.00101  StopLoss:0.03309  GradNorm:0.00407  GradNormST:0.01099  AvgTextLen:16.4  AvgSpecLen:64.8  StepTime:0.48  LR:0.000100\n",
      " | | > Checkpoint saving : /vm/TTS/models/cartman_models/mozilla-no-loc-fattn-stopnet-sigmoid-loss_masking-August-15-2019_11+26AM-de6aeb9/checkpoint_96000.pth.tar\n",
      "warning: audio amplitude out of range, auto clipped.\n",
      "   | > Step:30/68  GlobalStep:96010  TotalLoss:0.00213  PostnetLoss:0.00103  DecoderLoss:0.00110  StopLoss:0.03317  GradNorm:0.00408  GradNormST:0.00993  AvgTextLen:22.9  AvgSpecLen:85.8  StepTime:0.69  LR:0.000100\n",
      "   | > Step:40/68  GlobalStep:96020  TotalLoss:0.00229  PostnetLoss:0.00111  DecoderLoss:0.00118  StopLoss:0.04551  GradNorm:0.00320  GradNormST:0.00979  AvgTextLen:30.9  AvgSpecLen:105.3  StepTime:0.60  LR:0.000100\n",
      "   | > Step:50/68  GlobalStep:96030  TotalLoss:0.00246  PostnetLoss:0.00119  DecoderLoss:0.00127  StopLoss:0.03211  GradNorm:0.00279  GradNormST:0.00683  AvgTextLen:38.8  AvgSpecLen:132.0  StepTime:0.86  LR:0.000100\n",
      "   | > Step:60/68  GlobalStep:96040  TotalLoss:0.00271  PostnetLoss:0.00131  DecoderLoss:0.00140  StopLoss:0.02204  GradNorm:0.00382  GradNormST:0.00559  AvgTextLen:52.0  AvgSpecLen:172.7  StepTime:1.02  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:96048  AvgTotalLoss:0.04509  AvgPostnetLoss:0.00106  AvgDecoderLoss:0.00113  AvgStopLoss:0.04291  EpochTime:41.95  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.08555   PostnetLoss: 0.00504   DecoderLoss:0.00542  StopLoss: 0.07509  \n",
      "   | > TotalLoss: 0.06879   PostnetLoss: 0.00741   DecoderLoss:0.00792  StopLoss: 0.05346  \n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00106   Validation Loss: 0.00680\n",
      "\n",
      " > Epoch 392/1000\n",
      "   | > Step:1/68  GlobalStep:96050  TotalLoss:0.00155  PostnetLoss:0.00075  DecoderLoss:0.00080  StopLoss:0.05304  GradNorm:0.00551  GradNormST:0.02160  AvgTextLen:3.7  AvgSpecLen:28.6  StepTime:0.47  LR:0.000100\n",
      "   | > Step:11/68  GlobalStep:96060  TotalLoss:0.00167  PostnetLoss:0.00081  DecoderLoss:0.00085  StopLoss:0.04497  GradNorm:0.00361  GradNormST:0.01188  AvgTextLen:10.3  AvgSpecLen:49.4  StepTime:0.40  LR:0.000100\n",
      "   | > Step:21/68  GlobalStep:96070  TotalLoss:0.00192  PostnetLoss:0.00093  DecoderLoss:0.00098  StopLoss:0.04689  GradNorm:0.00362  GradNormST:0.01107  AvgTextLen:16.6  AvgSpecLen:66.3  StepTime:0.34  LR:0.000100\n",
      "   | > Step:31/68  GlobalStep:96080  TotalLoss:0.00209  PostnetLoss:0.00102  DecoderLoss:0.00107  StopLoss:0.03214  GradNorm:0.00355  GradNormST:0.00835  AvgTextLen:24.1  AvgSpecLen:82.7  StepTime:0.52  LR:0.000100\n",
      "   | > Step:41/68  GlobalStep:96090  TotalLoss:0.00238  PostnetLoss:0.00116  DecoderLoss:0.00122  StopLoss:0.03085  GradNorm:0.00373  GradNormST:0.01014  AvgTextLen:32.3  AvgSpecLen:109.4  StepTime:0.62  LR:0.000100\n",
      "   | > Step:51/68  GlobalStep:96100  TotalLoss:0.00244  PostnetLoss:0.00118  DecoderLoss:0.00126  StopLoss:0.02843  GradNorm:0.00292  GradNormST:0.00510  AvgTextLen:39.2  AvgSpecLen:126.9  StepTime:0.74  LR:0.000100\n",
      "   | > Step:61/68  GlobalStep:96110  TotalLoss:0.00269  PostnetLoss:0.00130  DecoderLoss:0.00139  StopLoss:0.02560  GradNorm:0.00405  GradNormST:0.00669  AvgTextLen:54.6  AvgSpecLen:165.5  StepTime:0.91  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:96117  AvgTotalLoss:0.04412  AvgPostnetLoss:0.00105  AvgDecoderLoss:0.00112  AvgStopLoss:0.04195  EpochTime:42.51  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.08618   PostnetLoss: 0.00488   DecoderLoss:0.00524  StopLoss: 0.07606  \n",
      "   | > TotalLoss: 0.06852   PostnetLoss: 0.00741   DecoderLoss:0.00794  StopLoss: 0.05317  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00105   Validation Loss: 0.00678\n",
      "\n",
      " > Epoch 393/1000\n",
      "   | > Step:2/68  GlobalStep:96120  TotalLoss:0.00138  PostnetLoss:0.00067  DecoderLoss:0.00071  StopLoss:0.04857  GradNorm:0.00370  GradNormST:0.01978  AvgTextLen:4.0  AvgSpecLen:31.3  StepTime:0.43  LR:0.000100\n",
      "   | > Step:12/68  GlobalStep:96130  TotalLoss:0.00169  PostnetLoss:0.00082  DecoderLoss:0.00087  StopLoss:0.04936  GradNorm:0.00370  GradNormST:0.01043  AvgTextLen:11.0  AvgSpecLen:49.3  StepTime:0.36  LR:0.000100\n",
      "   | > Step:22/68  GlobalStep:96140  TotalLoss:0.00196  PostnetLoss:0.00095  DecoderLoss:0.00101  StopLoss:0.04302  GradNorm:0.00330  GradNormST:0.00921  AvgTextLen:18.0  AvgSpecLen:72.0  StepTime:0.47  LR:0.000100\n",
      "   | > Step:32/68  GlobalStep:96150  TotalLoss:0.00215  PostnetLoss:0.00104  DecoderLoss:0.00110  StopLoss:0.04788  GradNorm:0.00341  GradNormST:0.01225  AvgTextLen:24.4  AvgSpecLen:91.5  StepTime:0.56  LR:0.000100\n",
      "   | > Step:42/68  GlobalStep:96160  TotalLoss:0.00229  PostnetLoss:0.00111  DecoderLoss:0.00118  StopLoss:0.04503  GradNorm:0.00302  GradNormST:0.01207  AvgTextLen:33.1  AvgSpecLen:109.7  StepTime:0.53  LR:0.000100\n",
      "   | > Step:52/68  GlobalStep:96170  TotalLoss:0.00251  PostnetLoss:0.00121  DecoderLoss:0.00129  StopLoss:0.03564  GradNorm:0.00290  GradNormST:0.00979  AvgTextLen:42.1  AvgSpecLen:137.5  StepTime:0.80  LR:0.000100\n",
      "   | > Step:62/68  GlobalStep:96180  TotalLoss:0.00276  PostnetLoss:0.00133  DecoderLoss:0.00143  StopLoss:0.03081  GradNorm:0.00360  GradNormST:0.01071  AvgTextLen:54.0  AvgSpecLen:174.7  StepTime:0.81  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:96186  AvgTotalLoss:0.04330  AvgPostnetLoss:0.00105  AvgDecoderLoss:0.00112  AvgStopLoss:0.04113  EpochTime:42.16  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.08359   PostnetLoss: 0.00495   DecoderLoss:0.00531  StopLoss: 0.07333  \n",
      "   | > TotalLoss: 0.06987   PostnetLoss: 0.00736   DecoderLoss:0.00787  StopLoss: 0.05465  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00105   Validation Loss: 0.00683\n",
      "\n",
      " > Epoch 394/1000\n",
      "   | > Step:3/68  GlobalStep:96190  TotalLoss:0.00146  PostnetLoss:0.00071  DecoderLoss:0.00075  StopLoss:0.06835  GradNorm:0.00429  GradNormST:0.01891  AvgTextLen:4.7  AvgSpecLen:29.7  StepTime:0.31  LR:0.000100\n",
      "   | > Step:13/68  GlobalStep:96200  TotalLoss:0.00169  PostnetLoss:0.00082  DecoderLoss:0.00087  StopLoss:0.05878  GradNorm:0.00343  GradNormST:0.02104  AvgTextLen:11.5  AvgSpecLen:55.2  StepTime:0.43  LR:0.000100\n",
      "   | > Step:23/68  GlobalStep:96210  TotalLoss:0.00202  PostnetLoss:0.00098  DecoderLoss:0.00104  StopLoss:0.04420  GradNorm:0.00348  GradNormST:0.00932  AvgTextLen:18.3  AvgSpecLen:72.2  StepTime:0.43  LR:0.000100\n",
      "   | > Step:33/68  GlobalStep:96220  TotalLoss:0.00217  PostnetLoss:0.00105  DecoderLoss:0.00112  StopLoss:0.04552  GradNorm:0.00328  GradNormST:0.01433  AvgTextLen:25.9  AvgSpecLen:88.9  StepTime:0.57  LR:0.000100\n",
      "   | > Step:43/68  GlobalStep:96230  TotalLoss:0.00232  PostnetLoss:0.00113  DecoderLoss:0.00120  StopLoss:0.04711  GradNorm:0.00315  GradNormST:0.00867  AvgTextLen:33.4  AvgSpecLen:115.7  StepTime:0.66  LR:0.000100\n",
      "   | > Step:53/68  GlobalStep:96240  TotalLoss:0.00249  PostnetLoss:0.00120  DecoderLoss:0.00128  StopLoss:0.03531  GradNorm:0.00306  GradNormST:0.00656  AvgTextLen:42.4  AvgSpecLen:137.6  StepTime:0.63  LR:0.000100\n",
      "   | > Step:63/68  GlobalStep:96250  TotalLoss:0.00281  PostnetLoss:0.00136  DecoderLoss:0.00145  StopLoss:0.02860  GradNorm:0.00320  GradNormST:0.00531  AvgTextLen:58.6  AvgSpecLen:185.1  StepTime:0.91  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:96255  AvgTotalLoss:0.04468  AvgPostnetLoss:0.00105  AvgDecoderLoss:0.00112  AvgStopLoss:0.04251  EpochTime:42.64  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.08708   PostnetLoss: 0.00493   DecoderLoss:0.00530  StopLoss: 0.07686  \n",
      "   | > TotalLoss: 0.07265   PostnetLoss: 0.00790   DecoderLoss:0.00844  StopLoss: 0.05631  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00105   Validation Loss: 0.00712\n",
      "\n",
      " > Epoch 395/1000\n",
      "   | > Step:4/68  GlobalStep:96260  TotalLoss:0.00153  PostnetLoss:0.00074  DecoderLoss:0.00079  StopLoss:0.07339  GradNorm:0.00561  GradNormST:0.02214  AvgTextLen:4.8  AvgSpecLen:31.1  StepTime:0.28  LR:0.000100\n",
      "   | > Step:14/68  GlobalStep:96270  TotalLoss:0.00176  PostnetLoss:0.00086  DecoderLoss:0.00090  StopLoss:0.04598  GradNorm:0.00366  GradNormST:0.01368  AvgTextLen:12.2  AvgSpecLen:52.0  StepTime:0.35  LR:0.000100\n",
      "   | > Step:24/68  GlobalStep:96280  TotalLoss:0.00208  PostnetLoss:0.00101  DecoderLoss:0.00107  StopLoss:0.05395  GradNorm:0.00330  GradNormST:0.01583  AvgTextLen:19.2  AvgSpecLen:79.0  StepTime:0.48  LR:0.000100\n",
      "   | > Step:34/68  GlobalStep:96290  TotalLoss:0.00222  PostnetLoss:0.00108  DecoderLoss:0.00114  StopLoss:0.03785  GradNorm:0.00326  GradNormST:0.00855  AvgTextLen:26.2  AvgSpecLen:95.9  StepTime:0.59  LR:0.000100\n",
      "   | > Step:44/68  GlobalStep:96300  TotalLoss:0.00236  PostnetLoss:0.00114  DecoderLoss:0.00122  StopLoss:0.03097  GradNorm:0.00307  GradNormST:0.00884  AvgTextLen:33.8  AvgSpecLen:116.3  StepTime:0.57  LR:0.000100\n",
      "   | > Step:54/68  GlobalStep:96310  TotalLoss:0.00253  PostnetLoss:0.00122  DecoderLoss:0.00131  StopLoss:0.03637  GradNorm:0.00293  GradNormST:0.00822  AvgTextLen:42.2  AvgSpecLen:144.9  StepTime:0.87  LR:0.000100\n",
      "   | > Step:64/68  GlobalStep:96320  TotalLoss:0.00267  PostnetLoss:0.00129  DecoderLoss:0.00138  StopLoss:0.02943  GradNorm:0.00410  GradNormST:0.00593  AvgTextLen:60.6  AvgSpecLen:177.6  StepTime:0.81  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:96324  AvgTotalLoss:0.04547  AvgPostnetLoss:0.00106  AvgDecoderLoss:0.00112  AvgStopLoss:0.04329  EpochTime:42.58  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.08669   PostnetLoss: 0.00501   DecoderLoss:0.00539  StopLoss: 0.07630  \n",
      "   | > TotalLoss: 0.06867   PostnetLoss: 0.00759   DecoderLoss:0.00811  StopLoss: 0.05297  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00106   Validation Loss: 0.00697\n",
      "\n",
      " > Epoch 396/1000\n",
      "   | > Step:5/68  GlobalStep:96330  TotalLoss:0.00161  PostnetLoss:0.00078  DecoderLoss:0.00083  StopLoss:0.06135  GradNorm:0.00655  GradNormST:0.01841  AvgTextLen:5.9  AvgSpecLen:33.2  StepTime:0.33  LR:0.000100\n",
      "   | > Step:15/68  GlobalStep:96340  TotalLoss:0.00182  PostnetLoss:0.00089  DecoderLoss:0.00093  StopLoss:0.04077  GradNorm:0.00373  GradNormST:0.01659  AvgTextLen:12.8  AvgSpecLen:59.1  StepTime:0.53  LR:0.000100\n",
      "   | > Step:25/68  GlobalStep:96350  TotalLoss:0.00198  PostnetLoss:0.00096  DecoderLoss:0.00101  StopLoss:0.04992  GradNorm:0.00328  GradNormST:0.01222  AvgTextLen:19.8  AvgSpecLen:73.3  StepTime:0.46  LR:0.000100\n",
      "   | > Step:35/68  GlobalStep:96360  TotalLoss:0.00221  PostnetLoss:0.00107  DecoderLoss:0.00114  StopLoss:0.05201  GradNorm:0.00317  GradNormST:0.01532  AvgTextLen:27.1  AvgSpecLen:104.7  StepTime:0.64  LR:0.000100\n",
      "   | > Step:45/68  GlobalStep:96370  TotalLoss:0.00232  PostnetLoss:0.00112  DecoderLoss:0.00120  StopLoss:0.04660  GradNorm:0.00344  GradNormST:0.01496  AvgTextLen:34.4  AvgSpecLen:118.6  StepTime:0.53  LR:0.000100\n",
      "   | > Step:55/68  GlobalStep:96380  TotalLoss:0.00260  PostnetLoss:0.00126  DecoderLoss:0.00134  StopLoss:0.03973  GradNorm:0.00318  GradNormST:0.00818  AvgTextLen:43.9  AvgSpecLen:145.2  StepTime:0.81  LR:0.000100\n",
      "   | > Step:65/68  GlobalStep:96390  TotalLoss:0.00286  PostnetLoss:0.00138  DecoderLoss:0.00148  StopLoss:0.03094  GradNorm:0.00342  GradNormST:0.00935  AvgTextLen:64.2  AvgSpecLen:207.4  StepTime:1.19  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:96393  AvgTotalLoss:0.04524  AvgPostnetLoss:0.00106  AvgDecoderLoss:0.00112  AvgStopLoss:0.04306  EpochTime:41.96  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.08874   PostnetLoss: 0.00509   DecoderLoss:0.00545  StopLoss: 0.07820  \n",
      "   | > TotalLoss: 0.07227   PostnetLoss: 0.00784   DecoderLoss:0.00835  StopLoss: 0.05609  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00106   Validation Loss: 0.00708\n",
      "\n",
      " > Epoch 397/1000\n",
      "   | > Step:6/68  GlobalStep:96400  TotalLoss:0.00152  PostnetLoss:0.00074  DecoderLoss:0.00078  StopLoss:0.04072  GradNorm:0.00395  GradNormST:0.01905  AvgTextLen:6.0  AvgSpecLen:34.8  StepTime:0.30  LR:0.000100\n",
      "   | > Step:16/68  GlobalStep:96410  TotalLoss:0.00182  PostnetLoss:0.00089  DecoderLoss:0.00093  StopLoss:0.04313  GradNorm:0.00408  GradNormST:0.01089  AvgTextLen:13.8  AvgSpecLen:60.6  StepTime:0.37  LR:0.000100\n",
      "   | > Step:26/68  GlobalStep:96420  TotalLoss:0.00202  PostnetLoss:0.00098  DecoderLoss:0.00104  StopLoss:0.04026  GradNorm:0.00343  GradNormST:0.01125  AvgTextLen:20.3  AvgSpecLen:77.5  StepTime:0.41  LR:0.000100\n",
      "   | > Step:36/68  GlobalStep:96430  TotalLoss:0.00219  PostnetLoss:0.00106  DecoderLoss:0.00113  StopLoss:0.05403  GradNorm:0.00305  GradNormST:0.01227  AvgTextLen:28.5  AvgSpecLen:100.2  StepTime:0.61  LR:0.000100\n",
      "   | > Step:46/68  GlobalStep:96440  TotalLoss:0.00239  PostnetLoss:0.00115  DecoderLoss:0.00123  StopLoss:0.04191  GradNorm:0.00304  GradNormST:0.01455  AvgTextLen:35.4  AvgSpecLen:122.6  StepTime:0.84  LR:0.000100\n",
      "   | > Step:56/68  GlobalStep:96450  TotalLoss:0.00257  PostnetLoss:0.00124  DecoderLoss:0.00132  StopLoss:0.02921  GradNorm:0.00373  GradNormST:0.01440  AvgTextLen:45.8  AvgSpecLen:145.4  StepTime:1.02  LR:0.000100\n",
      "   | > Step:66/68  GlobalStep:96460  TotalLoss:0.00304  PostnetLoss:0.00146  DecoderLoss:0.00158  StopLoss:0.03123  GradNorm:0.00382  GradNormST:0.01298  AvgTextLen:71.2  AvgSpecLen:220.8  StepTime:1.02  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:96462  AvgTotalLoss:0.04507  AvgPostnetLoss:0.00105  AvgDecoderLoss:0.00112  AvgStopLoss:0.04290  EpochTime:43.05  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.09391   PostnetLoss: 0.00504   DecoderLoss:0.00540  StopLoss: 0.08346  \n",
      "   | > TotalLoss: 0.07926   PostnetLoss: 0.00836   DecoderLoss:0.00890  StopLoss: 0.06200  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00105   Validation Loss: 0.00743\n",
      "\n",
      " > Epoch 398/1000\n",
      "   | > Step:7/68  GlobalStep:96470  TotalLoss:0.00162  PostnetLoss:0.00079  DecoderLoss:0.00083  StopLoss:0.05654  GradNorm:0.00412  GradNormST:0.02899  AvgTextLen:7.4  AvgSpecLen:44.6  StepTime:0.40  LR:0.000100\n",
      "   | > Step:17/68  GlobalStep:96480  TotalLoss:0.00184  PostnetLoss:0.00089  DecoderLoss:0.00094  StopLoss:0.05711  GradNorm:0.00359  GradNormST:0.01273  AvgTextLen:14.7  AvgSpecLen:66.0  StepTime:0.33  LR:0.000100\n",
      "   | > Step:27/68  GlobalStep:96490  TotalLoss:0.00208  PostnetLoss:0.00101  DecoderLoss:0.00107  StopLoss:0.04835  GradNorm:0.00315  GradNormST:0.00940  AvgTextLen:21.0  AvgSpecLen:80.2  StepTime:0.68  LR:0.000100\n",
      "   | > Step:37/68  GlobalStep:96500  TotalLoss:0.00225  PostnetLoss:0.00109  DecoderLoss:0.00116  StopLoss:0.03521  GradNorm:0.00357  GradNormST:0.00629  AvgTextLen:29.3  AvgSpecLen:102.4  StepTime:0.48  LR:0.000100\n",
      "   | > Step:47/68  GlobalStep:96510  TotalLoss:0.00244  PostnetLoss:0.00118  DecoderLoss:0.00126  StopLoss:0.03444  GradNorm:0.00310  GradNormST:0.00708  AvgTextLen:36.7  AvgSpecLen:126.9  StepTime:0.64  LR:0.000100\n",
      "   | > Step:57/68  GlobalStep:96520  TotalLoss:0.00264  PostnetLoss:0.00127  DecoderLoss:0.00136  StopLoss:0.03753  GradNorm:0.00315  GradNormST:0.00920  AvgTextLen:47.1  AvgSpecLen:161.0  StepTime:0.83  LR:0.000100\n",
      "   | > Step:67/68  GlobalStep:96530  TotalLoss:0.00301  PostnetLoss:0.00145  DecoderLoss:0.00156  StopLoss:0.02927  GradNorm:0.00302  GradNormST:0.01281  AvgTextLen:80.6  AvgSpecLen:246.5  StepTime:1.22  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:96531  AvgTotalLoss:0.04592  AvgPostnetLoss:0.00105  AvgDecoderLoss:0.00112  AvgStopLoss:0.04374  EpochTime:42.53  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.09302   PostnetLoss: 0.00511   DecoderLoss:0.00548  StopLoss: 0.08243  \n",
      "   | > TotalLoss: 0.07939   PostnetLoss: 0.00843   DecoderLoss:0.00897  StopLoss: 0.06199  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00105   Validation Loss: 0.00751\n",
      "\n",
      " > Epoch 399/1000\n",
      "   | > Step:8/68  GlobalStep:96540  TotalLoss:0.00166  PostnetLoss:0.00081  DecoderLoss:0.00085  StopLoss:0.05580  GradNorm:0.00428  GradNormST:0.01606  AvgTextLen:8.3  AvgSpecLen:44.0  StepTime:0.32  LR:0.000100\n",
      "   | > Step:18/68  GlobalStep:96550  TotalLoss:0.00182  PostnetLoss:0.00088  DecoderLoss:0.00093  StopLoss:0.07069  GradNorm:0.00353  GradNormST:0.01935  AvgTextLen:15.2  AvgSpecLen:64.5  StepTime:0.40  LR:0.000100\n",
      "   | > Step:28/68  GlobalStep:96560  TotalLoss:0.00207  PostnetLoss:0.00101  DecoderLoss:0.00106  StopLoss:0.04504  GradNorm:0.00313  GradNormST:0.01004  AvgTextLen:22.1  AvgSpecLen:85.4  StepTime:0.56  LR:0.000100\n",
      "   | > Step:38/68  GlobalStep:96570  TotalLoss:0.00223  PostnetLoss:0.00108  DecoderLoss:0.00115  StopLoss:0.04431  GradNorm:0.00317  GradNormST:0.01620  AvgTextLen:29.4  AvgSpecLen:104.1  StepTime:0.49  LR:0.000100\n",
      "   | > Step:48/68  GlobalStep:96580  TotalLoss:0.00242  PostnetLoss:0.00117  DecoderLoss:0.00125  StopLoss:0.03664  GradNorm:0.00297  GradNormST:0.01038  AvgTextLen:37.5  AvgSpecLen:123.8  StepTime:0.67  LR:0.000100\n",
      "   | > Step:58/68  GlobalStep:96590  TotalLoss:0.00265  PostnetLoss:0.00128  DecoderLoss:0.00137  StopLoss:0.03964  GradNorm:0.00361  GradNormST:0.01006  AvgTextLen:47.8  AvgSpecLen:158.0  StepTime:0.71  LR:0.000100\n",
      "   | > Step:68/68  GlobalStep:96600  TotalLoss:0.00294  PostnetLoss:0.00141  DecoderLoss:0.00153  StopLoss:0.02674  GradNorm:0.00302  GradNormST:0.01084  AvgTextLen:103.0  AvgSpecLen:308.9  StepTime:1.87  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:96600  AvgTotalLoss:0.04680  AvgPostnetLoss:0.00105  AvgDecoderLoss:0.00112  AvgStopLoss:0.04463  EpochTime:42.23  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.09139   PostnetLoss: 0.00510   DecoderLoss:0.00546  StopLoss: 0.08083  \n",
      "   | > TotalLoss: 0.07890   PostnetLoss: 0.00837   DecoderLoss:0.00890  StopLoss: 0.06164  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00105   Validation Loss: 0.00746\n",
      "\n",
      " > Epoch 400/1000\n",
      "   | > Step:9/68  GlobalStep:96610  TotalLoss:0.00158  PostnetLoss:0.00077  DecoderLoss:0.00081  StopLoss:0.03445  GradNorm:0.00368  GradNormST:0.01626  AvgTextLen:9.0  AvgSpecLen:42.1  StepTime:0.40  LR:0.000100\n",
      "   | > Step:19/68  GlobalStep:96620  TotalLoss:0.00183  PostnetLoss:0.00089  DecoderLoss:0.00094  StopLoss:0.05930  GradNorm:0.00336  GradNormST:0.01935  AvgTextLen:15.5  AvgSpecLen:63.2  StepTime:0.41  LR:0.000100\n",
      "   | > Step:29/68  GlobalStep:96630  TotalLoss:0.00204  PostnetLoss:0.00099  DecoderLoss:0.00105  StopLoss:0.05475  GradNorm:0.00328  GradNormST:0.01104  AvgTextLen:21.8  AvgSpecLen:83.4  StepTime:0.59  LR:0.000100\n",
      "   | > Step:39/68  GlobalStep:96640  TotalLoss:0.00227  PostnetLoss:0.00110  DecoderLoss:0.00117  StopLoss:0.03401  GradNorm:0.00336  GradNormST:0.01066  AvgTextLen:30.2  AvgSpecLen:110.4  StepTime:0.76  LR:0.000100\n",
      "   | > Step:49/68  GlobalStep:96650  TotalLoss:0.00242  PostnetLoss:0.00118  DecoderLoss:0.00125  StopLoss:0.03369  GradNorm:0.00293  GradNormST:0.00732  AvgTextLen:38.2  AvgSpecLen:127.7  StepTime:0.70  LR:0.000100\n",
      "   | > Step:59/68  GlobalStep:96660  TotalLoss:0.00260  PostnetLoss:0.00126  DecoderLoss:0.00135  StopLoss:0.04570  GradNorm:0.00334  GradNormST:0.01730  AvgTextLen:50.5  AvgSpecLen:155.9  StepTime:0.83  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:96669  AvgTotalLoss:0.04594  AvgPostnetLoss:0.00105  AvgDecoderLoss:0.00112  AvgStopLoss:0.04377  EpochTime:42.09  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.08958   PostnetLoss: 0.00505   DecoderLoss:0.00540  StopLoss: 0.07913  \n",
      "   | > TotalLoss: 0.07703   PostnetLoss: 0.00812   DecoderLoss:0.00863  StopLoss: 0.06028  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00105   Validation Loss: 0.00724\n",
      "\n",
      " > Epoch 401/1000\n",
      "   | > Step:0/68  GlobalStep:96670  TotalLoss:0.00153  PostnetLoss:0.00074  DecoderLoss:0.00080  StopLoss:0.07659  GradNorm:0.00646  GradNormST:0.02741  AvgTextLen:3.0  AvgSpecLen:25.3  StepTime:0.48  LR:0.000100\n",
      "   | > Step:10/68  GlobalStep:96680  TotalLoss:0.00169  PostnetLoss:0.00082  DecoderLoss:0.00086  StopLoss:0.03450  GradNorm:0.00375  GradNormST:0.01199  AvgTextLen:9.8  AvgSpecLen:48.6  StepTime:0.56  LR:0.000100\n",
      "   | > Step:20/68  GlobalStep:96690  TotalLoss:0.00193  PostnetLoss:0.00094  DecoderLoss:0.00099  StopLoss:0.04024  GradNorm:0.00341  GradNormST:0.01799  AvgTextLen:16.4  AvgSpecLen:64.8  StepTime:0.52  LR:0.000100\n",
      "   | > Step:30/68  GlobalStep:96700  TotalLoss:0.00209  PostnetLoss:0.00102  DecoderLoss:0.00108  StopLoss:0.04416  GradNorm:0.00311  GradNormST:0.00885  AvgTextLen:22.9  AvgSpecLen:85.8  StepTime:0.57  LR:0.000100\n",
      "   | > Step:40/68  GlobalStep:96710  TotalLoss:0.00225  PostnetLoss:0.00110  DecoderLoss:0.00116  StopLoss:0.05384  GradNorm:0.00310  GradNormST:0.01165  AvgTextLen:30.9  AvgSpecLen:105.3  StepTime:0.60  LR:0.000100\n",
      "   | > Step:50/68  GlobalStep:96720  TotalLoss:0.00244  PostnetLoss:0.00118  DecoderLoss:0.00126  StopLoss:0.02621  GradNorm:0.00305  GradNormST:0.00584  AvgTextLen:38.8  AvgSpecLen:132.0  StepTime:0.79  LR:0.000100\n",
      "   | > Step:60/68  GlobalStep:96730  TotalLoss:0.00270  PostnetLoss:0.00131  DecoderLoss:0.00140  StopLoss:0.02760  GradNorm:0.00315  GradNormST:0.00541  AvgTextLen:52.0  AvgSpecLen:172.7  StepTime:0.96  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:96738  AvgTotalLoss:0.04802  AvgPostnetLoss:0.00105  AvgDecoderLoss:0.00112  AvgStopLoss:0.04585  EpochTime:42.79  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.09175   PostnetLoss: 0.00503   DecoderLoss:0.00539  StopLoss: 0.08132  \n",
      "   | > TotalLoss: 0.07774   PostnetLoss: 0.00812   DecoderLoss:0.00862  StopLoss: 0.06101  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00105   Validation Loss: 0.00729\n",
      "\n",
      " > Epoch 402/1000\n",
      "   | > Step:1/68  GlobalStep:96740  TotalLoss:0.00155  PostnetLoss:0.00075  DecoderLoss:0.00080  StopLoss:0.04670  GradNorm:0.00492  GradNormST:0.02027  AvgTextLen:3.7  AvgSpecLen:28.6  StepTime:0.51  LR:0.000100\n",
      "   | > Step:11/68  GlobalStep:96750  TotalLoss:0.00164  PostnetLoss:0.00080  DecoderLoss:0.00084  StopLoss:0.06526  GradNorm:0.00396  GradNormST:0.03048  AvgTextLen:10.3  AvgSpecLen:49.4  StepTime:0.34  LR:0.000100\n",
      "   | > Step:21/68  GlobalStep:96760  TotalLoss:0.00193  PostnetLoss:0.00094  DecoderLoss:0.00099  StopLoss:0.05245  GradNorm:0.00370  GradNormST:0.01831  AvgTextLen:16.6  AvgSpecLen:66.3  StepTime:0.43  LR:0.000100\n",
      "   | > Step:31/68  GlobalStep:96770  TotalLoss:0.00208  PostnetLoss:0.00101  DecoderLoss:0.00107  StopLoss:0.04316  GradNorm:0.00303  GradNormST:0.00996  AvgTextLen:24.1  AvgSpecLen:82.7  StepTime:0.56  LR:0.000100\n",
      "   | > Step:41/68  GlobalStep:96780  TotalLoss:0.00232  PostnetLoss:0.00112  DecoderLoss:0.00119  StopLoss:0.03152  GradNorm:0.00292  GradNormST:0.00828  AvgTextLen:32.3  AvgSpecLen:109.4  StepTime:0.67  LR:0.000100\n",
      "   | > Step:51/68  GlobalStep:96790  TotalLoss:0.00244  PostnetLoss:0.00118  DecoderLoss:0.00126  StopLoss:0.03341  GradNorm:0.00295  GradNormST:0.00848  AvgTextLen:39.2  AvgSpecLen:126.9  StepTime:0.72  LR:0.000100\n",
      "   | > Step:61/68  GlobalStep:96800  TotalLoss:0.00267  PostnetLoss:0.00129  DecoderLoss:0.00138  StopLoss:0.03111  GradNorm:0.00398  GradNormST:0.00907  AvgTextLen:54.6  AvgSpecLen:165.5  StepTime:0.91  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:96807  AvgTotalLoss:0.04722  AvgPostnetLoss:0.00105  AvgDecoderLoss:0.00112  AvgStopLoss:0.04505  EpochTime:41.78  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.08766   PostnetLoss: 0.00503   DecoderLoss:0.00539  StopLoss: 0.07723  \n",
      "   | > TotalLoss: 0.07378   PostnetLoss: 0.00767   DecoderLoss:0.00817  StopLoss: 0.05795  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00105   Validation Loss: 0.00704\n",
      "\n",
      " > Epoch 403/1000\n",
      "   | > Step:2/68  GlobalStep:96810  TotalLoss:0.00140  PostnetLoss:0.00068  DecoderLoss:0.00072  StopLoss:0.06362  GradNorm:0.00457  GradNormST:0.01885  AvgTextLen:4.0  AvgSpecLen:31.3  StepTime:0.49  LR:0.000100\n",
      "   | > Step:12/68  GlobalStep:96820  TotalLoss:0.00167  PostnetLoss:0.00081  DecoderLoss:0.00086  StopLoss:0.04232  GradNorm:0.00393  GradNormST:0.01038  AvgTextLen:11.0  AvgSpecLen:49.3  StepTime:0.43  LR:0.000100\n",
      "   | > Step:22/68  GlobalStep:96830  TotalLoss:0.00195  PostnetLoss:0.00095  DecoderLoss:0.00100  StopLoss:0.04526  GradNorm:0.00319  GradNormST:0.01284  AvgTextLen:18.0  AvgSpecLen:72.0  StepTime:0.46  LR:0.000100\n",
      "   | > Step:32/68  GlobalStep:96840  TotalLoss:0.00211  PostnetLoss:0.00102  DecoderLoss:0.00109  StopLoss:0.05938  GradNorm:0.00294  GradNormST:0.01582  AvgTextLen:24.4  AvgSpecLen:91.5  StepTime:0.58  LR:0.000100\n",
      "   | > Step:42/68  GlobalStep:96850  TotalLoss:0.00228  PostnetLoss:0.00111  DecoderLoss:0.00118  StopLoss:0.04614  GradNorm:0.00276  GradNormST:0.01350  AvgTextLen:33.1  AvgSpecLen:109.7  StepTime:0.48  LR:0.000100\n",
      "   | > Step:52/68  GlobalStep:96860  TotalLoss:0.00247  PostnetLoss:0.00120  DecoderLoss:0.00127  StopLoss:0.04421  GradNorm:0.00303  GradNormST:0.01311  AvgTextLen:42.1  AvgSpecLen:137.5  StepTime:0.85  LR:0.000100\n",
      "   | > Step:62/68  GlobalStep:96870  TotalLoss:0.00273  PostnetLoss:0.00132  DecoderLoss:0.00141  StopLoss:0.03955  GradNorm:0.00325  GradNormST:0.00990  AvgTextLen:54.0  AvgSpecLen:174.7  StepTime:0.75  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:96876  AvgTotalLoss:0.04645  AvgPostnetLoss:0.00105  AvgDecoderLoss:0.00111  AvgStopLoss:0.04429  EpochTime:42.36  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.08285   PostnetLoss: 0.00500   DecoderLoss:0.00535  StopLoss: 0.07250  \n",
      "   | > TotalLoss: 0.07187   PostnetLoss: 0.00775   DecoderLoss:0.00825  StopLoss: 0.05586  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00105   Validation Loss: 0.00711\n",
      "\n",
      " > Epoch 404/1000\n",
      "   | > Step:3/68  GlobalStep:96880  TotalLoss:0.00145  PostnetLoss:0.00070  DecoderLoss:0.00074  StopLoss:0.06430  GradNorm:0.00444  GradNormST:0.02305  AvgTextLen:4.7  AvgSpecLen:29.7  StepTime:0.27  LR:0.000100\n",
      "   | > Step:13/68  GlobalStep:96890  TotalLoss:0.00170  PostnetLoss:0.00083  DecoderLoss:0.00087  StopLoss:0.07467  GradNorm:0.00446  GradNormST:0.02212  AvgTextLen:11.5  AvgSpecLen:55.2  StepTime:0.43  LR:0.000100\n",
      "   | > Step:23/68  GlobalStep:96900  TotalLoss:0.00198  PostnetLoss:0.00096  DecoderLoss:0.00102  StopLoss:0.05637  GradNorm:0.00331  GradNormST:0.01070  AvgTextLen:18.3  AvgSpecLen:72.2  StepTime:0.45  LR:0.000100\n",
      "   | > Step:33/68  GlobalStep:96910  TotalLoss:0.00214  PostnetLoss:0.00104  DecoderLoss:0.00110  StopLoss:0.04418  GradNorm:0.00310  GradNormST:0.01043  AvgTextLen:25.9  AvgSpecLen:88.9  StepTime:0.54  LR:0.000100\n",
      "   | > Step:43/68  GlobalStep:96920  TotalLoss:0.00231  PostnetLoss:0.00112  DecoderLoss:0.00119  StopLoss:0.06239  GradNorm:0.00296  GradNormST:0.01312  AvgTextLen:33.4  AvgSpecLen:115.7  StepTime:0.62  LR:0.000100\n",
      "   | > Step:53/68  GlobalStep:96930  TotalLoss:0.00245  PostnetLoss:0.00119  DecoderLoss:0.00127  StopLoss:0.03515  GradNorm:0.00305  GradNormST:0.00695  AvgTextLen:42.4  AvgSpecLen:137.6  StepTime:0.76  LR:0.000100\n",
      "   | > Step:63/68  GlobalStep:96940  TotalLoss:0.00280  PostnetLoss:0.00135  DecoderLoss:0.00145  StopLoss:0.02966  GradNorm:0.00312  GradNormST:0.01420  AvgTextLen:58.6  AvgSpecLen:185.1  StepTime:0.91  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:96945  AvgTotalLoss:0.04965  AvgPostnetLoss:0.00105  AvgDecoderLoss:0.00111  AvgStopLoss:0.04749  EpochTime:42.68  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.08221   PostnetLoss: 0.00518   DecoderLoss:0.00553  StopLoss: 0.07150  \n",
      "   | > TotalLoss: 0.07131   PostnetLoss: 0.00756   DecoderLoss:0.00806  StopLoss: 0.05570  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00105   Validation Loss: 0.00704\n",
      "\n",
      " > Epoch 405/1000\n",
      "   | > Step:4/68  GlobalStep:96950  TotalLoss:0.00145  PostnetLoss:0.00070  DecoderLoss:0.00075  StopLoss:0.08467  GradNorm:0.00382  GradNormST:0.03576  AvgTextLen:4.8  AvgSpecLen:31.1  StepTime:0.30  LR:0.000100\n",
      "   | > Step:14/68  GlobalStep:96960  TotalLoss:0.00178  PostnetLoss:0.00086  DecoderLoss:0.00091  StopLoss:0.05093  GradNorm:0.00419  GradNormST:0.01278  AvgTextLen:12.2  AvgSpecLen:52.0  StepTime:0.36  LR:0.000100\n",
      "   | > Step:24/68  GlobalStep:96970  TotalLoss:0.00206  PostnetLoss:0.00100  DecoderLoss:0.00106  StopLoss:0.04352  GradNorm:0.00393  GradNormST:0.01029  AvgTextLen:19.2  AvgSpecLen:79.0  StepTime:0.49  LR:0.000100\n",
      "   | > Step:34/68  GlobalStep:96980  TotalLoss:0.00221  PostnetLoss:0.00107  DecoderLoss:0.00113  StopLoss:0.03191  GradNorm:0.00339  GradNormST:0.00788  AvgTextLen:26.2  AvgSpecLen:95.9  StepTime:0.62  LR:0.000100\n",
      "   | > Step:44/68  GlobalStep:96990  TotalLoss:0.00235  PostnetLoss:0.00114  DecoderLoss:0.00121  StopLoss:0.04109  GradNorm:0.00337  GradNormST:0.01128  AvgTextLen:33.8  AvgSpecLen:116.3  StepTime:0.53  LR:0.000100\n",
      "   | > Step:54/68  GlobalStep:97000  TotalLoss:0.00254  PostnetLoss:0.00123  DecoderLoss:0.00131  StopLoss:0.03881  GradNorm:0.00398  GradNormST:0.00816  AvgTextLen:42.2  AvgSpecLen:144.9  StepTime:0.87  LR:0.000100\n",
      " | | > Checkpoint saving : /vm/TTS/models/cartman_models/mozilla-no-loc-fattn-stopnet-sigmoid-loss_masking-August-15-2019_11+26AM-de6aeb9/checkpoint_97000.pth.tar\n",
      "warning: audio amplitude out of range, auto clipped.\n",
      "   | > Step:64/68  GlobalStep:97010  TotalLoss:0.00266  PostnetLoss:0.00128  DecoderLoss:0.00138  StopLoss:0.03781  GradNorm:0.00382  GradNormST:0.01279  AvgTextLen:60.6  AvgSpecLen:177.6  StepTime:0.81  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:97014  AvgTotalLoss:0.04898  AvgPostnetLoss:0.00105  AvgDecoderLoss:0.00111  AvgStopLoss:0.04682  EpochTime:42.97  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.08524   PostnetLoss: 0.00501   DecoderLoss:0.00536  StopLoss: 0.07487  \n",
      "   | > TotalLoss: 0.07202   PostnetLoss: 0.00764   DecoderLoss:0.00813  StopLoss: 0.05625  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00105   Validation Loss: 0.00713\n",
      "\n",
      " > Epoch 406/1000\n",
      "   | > Step:5/68  GlobalStep:97020  TotalLoss:0.00152  PostnetLoss:0.00074  DecoderLoss:0.00078  StopLoss:0.06505  GradNorm:0.00514  GradNormST:0.02773  AvgTextLen:5.9  AvgSpecLen:33.2  StepTime:0.33  LR:0.000100\n",
      "   | > Step:15/68  GlobalStep:97030  TotalLoss:0.00184  PostnetLoss:0.00090  DecoderLoss:0.00094  StopLoss:0.04096  GradNorm:0.00425  GradNormST:0.01146  AvgTextLen:12.8  AvgSpecLen:59.1  StepTime:0.51  LR:0.000100\n",
      "   | > Step:25/68  GlobalStep:97040  TotalLoss:0.00200  PostnetLoss:0.00097  DecoderLoss:0.00103  StopLoss:0.05084  GradNorm:0.00376  GradNormST:0.01271  AvgTextLen:19.8  AvgSpecLen:73.3  StepTime:0.46  LR:0.000100\n",
      "   | > Step:35/68  GlobalStep:97050  TotalLoss:0.00224  PostnetLoss:0.00109  DecoderLoss:0.00115  StopLoss:0.08033  GradNorm:0.00363  GradNormST:0.03171  AvgTextLen:27.1  AvgSpecLen:104.7  StepTime:0.74  LR:0.000100\n",
      "   | > Step:45/68  GlobalStep:97060  TotalLoss:0.00232  PostnetLoss:0.00112  DecoderLoss:0.00120  StopLoss:0.05732  GradNorm:0.00292  GradNormST:0.01174  AvgTextLen:34.4  AvgSpecLen:118.6  StepTime:0.65  LR:0.000100\n",
      "   | > Step:55/68  GlobalStep:97070  TotalLoss:0.00258  PostnetLoss:0.00125  DecoderLoss:0.00133  StopLoss:0.04449  GradNorm:0.00323  GradNormST:0.00912  AvgTextLen:43.9  AvgSpecLen:145.2  StepTime:0.79  LR:0.000100\n",
      "   | > Step:65/68  GlobalStep:97080  TotalLoss:0.00287  PostnetLoss:0.00138  DecoderLoss:0.00149  StopLoss:0.04748  GradNorm:0.00332  GradNormST:0.02533  AvgTextLen:64.2  AvgSpecLen:207.4  StepTime:1.18  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:97083  AvgTotalLoss:0.04919  AvgPostnetLoss:0.00105  AvgDecoderLoss:0.00111  AvgStopLoss:0.04702  EpochTime:43.12  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.08067   PostnetLoss: 0.00505   DecoderLoss:0.00540  StopLoss: 0.07023  \n",
      "   | > TotalLoss: 0.06842   PostnetLoss: 0.00739   DecoderLoss:0.00788  StopLoss: 0.05315  \n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00105   Validation Loss: 0.00697\n",
      "\n",
      " > Epoch 407/1000\n",
      "   | > Step:6/68  GlobalStep:97090  TotalLoss:0.00151  PostnetLoss:0.00073  DecoderLoss:0.00077  StopLoss:0.05235  GradNorm:0.00404  GradNormST:0.02522  AvgTextLen:6.0  AvgSpecLen:34.8  StepTime:0.37  LR:0.000100\n",
      "   | > Step:16/68  GlobalStep:97100  TotalLoss:0.00182  PostnetLoss:0.00089  DecoderLoss:0.00093  StopLoss:0.05010  GradNorm:0.00448  GradNormST:0.01355  AvgTextLen:13.8  AvgSpecLen:60.6  StepTime:0.36  LR:0.000100\n",
      "   | > Step:26/68  GlobalStep:97110  TotalLoss:0.00203  PostnetLoss:0.00099  DecoderLoss:0.00104  StopLoss:0.03453  GradNorm:0.00425  GradNormST:0.00982  AvgTextLen:20.3  AvgSpecLen:77.5  StepTime:0.47  LR:0.000100\n",
      "   | > Step:36/68  GlobalStep:97120  TotalLoss:0.00219  PostnetLoss:0.00106  DecoderLoss:0.00113  StopLoss:0.06466  GradNorm:0.00345  GradNormST:0.01961  AvgTextLen:28.5  AvgSpecLen:100.2  StepTime:0.58  LR:0.000100\n",
      "   | > Step:46/68  GlobalStep:97130  TotalLoss:0.00236  PostnetLoss:0.00114  DecoderLoss:0.00121  StopLoss:0.03595  GradNorm:0.00344  GradNormST:0.00677  AvgTextLen:35.4  AvgSpecLen:122.6  StepTime:0.77  LR:0.000100\n",
      "   | > Step:56/68  GlobalStep:97140  TotalLoss:0.00257  PostnetLoss:0.00124  DecoderLoss:0.00132  StopLoss:0.03534  GradNorm:0.00372  GradNormST:0.01286  AvgTextLen:45.8  AvgSpecLen:145.4  StepTime:0.96  LR:0.000100\n",
      "   | > Step:66/68  GlobalStep:97150  TotalLoss:0.00302  PostnetLoss:0.00145  DecoderLoss:0.00157  StopLoss:0.03282  GradNorm:0.00364  GradNormST:0.01444  AvgTextLen:71.2  AvgSpecLen:220.8  StepTime:1.04  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:97152  AvgTotalLoss:0.04873  AvgPostnetLoss:0.00105  AvgDecoderLoss:0.00111  AvgStopLoss:0.04657  EpochTime:42.58  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.08449   PostnetLoss: 0.00506   DecoderLoss:0.00541  StopLoss: 0.07402  \n",
      "   | > TotalLoss: 0.06959   PostnetLoss: 0.00747   DecoderLoss:0.00797  StopLoss: 0.05415  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00105   Validation Loss: 0.00692\n",
      "\n",
      " > Epoch 408/1000\n",
      "   | > Step:7/68  GlobalStep:97160  TotalLoss:0.00157  PostnetLoss:0.00077  DecoderLoss:0.00081  StopLoss:0.03527  GradNorm:0.00358  GradNormST:0.01578  AvgTextLen:7.4  AvgSpecLen:44.6  StepTime:0.41  LR:0.000100\n",
      "   | > Step:17/68  GlobalStep:97170  TotalLoss:0.00182  PostnetLoss:0.00089  DecoderLoss:0.00093  StopLoss:0.06665  GradNorm:0.00378  GradNormST:0.01223  AvgTextLen:14.7  AvgSpecLen:66.0  StepTime:0.34  LR:0.000100\n",
      "   | > Step:27/68  GlobalStep:97180  TotalLoss:0.00209  PostnetLoss:0.00102  DecoderLoss:0.00107  StopLoss:0.04194  GradNorm:0.00456  GradNormST:0.00960  AvgTextLen:21.0  AvgSpecLen:80.2  StepTime:0.55  LR:0.000100\n",
      "   | > Step:37/68  GlobalStep:97190  TotalLoss:0.00227  PostnetLoss:0.00110  DecoderLoss:0.00117  StopLoss:0.04453  GradNorm:0.00396  GradNormST:0.00878  AvgTextLen:29.3  AvgSpecLen:102.4  StepTime:0.64  LR:0.000100\n",
      "   | > Step:47/68  GlobalStep:97200  TotalLoss:0.00242  PostnetLoss:0.00117  DecoderLoss:0.00125  StopLoss:0.03556  GradNorm:0.00373  GradNormST:0.00767  AvgTextLen:36.7  AvgSpecLen:126.9  StepTime:0.75  LR:0.000100\n",
      "   | > Step:57/68  GlobalStep:97210  TotalLoss:0.00264  PostnetLoss:0.00128  DecoderLoss:0.00136  StopLoss:0.03280  GradNorm:0.00392  GradNormST:0.00655  AvgTextLen:47.1  AvgSpecLen:161.0  StepTime:0.69  LR:0.000100\n",
      "   | > Step:67/68  GlobalStep:97220  TotalLoss:0.00302  PostnetLoss:0.00145  DecoderLoss:0.00156  StopLoss:0.03154  GradNorm:0.00307  GradNormST:0.01275  AvgTextLen:80.6  AvgSpecLen:246.5  StepTime:1.22  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:97221  AvgTotalLoss:0.04737  AvgPostnetLoss:0.00105  AvgDecoderLoss:0.00111  AvgStopLoss:0.04521  EpochTime:41.92  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.08490   PostnetLoss: 0.00499   DecoderLoss:0.00534  StopLoss: 0.07457  \n",
      "   | > TotalLoss: 0.06847   PostnetLoss: 0.00737   DecoderLoss:0.00786  StopLoss: 0.05324  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00105   Validation Loss: 0.00695\n",
      "\n",
      " > Epoch 409/1000\n",
      "   | > Step:8/68  GlobalStep:97230  TotalLoss:0.00170  PostnetLoss:0.00083  DecoderLoss:0.00087  StopLoss:0.04762  GradNorm:0.00449  GradNormST:0.01291  AvgTextLen:8.3  AvgSpecLen:44.0  StepTime:0.31  LR:0.000100\n",
      "   | > Step:18/68  GlobalStep:97240  TotalLoss:0.00180  PostnetLoss:0.00088  DecoderLoss:0.00092  StopLoss:0.05084  GradNorm:0.00399  GradNormST:0.01320  AvgTextLen:15.2  AvgSpecLen:64.5  StepTime:0.43  LR:0.000100\n",
      "   | > Step:28/68  GlobalStep:97250  TotalLoss:0.00208  PostnetLoss:0.00101  DecoderLoss:0.00107  StopLoss:0.04421  GradNorm:0.00387  GradNormST:0.01022  AvgTextLen:22.1  AvgSpecLen:85.4  StepTime:0.55  LR:0.000100\n",
      "   | > Step:38/68  GlobalStep:97260  TotalLoss:0.00222  PostnetLoss:0.00108  DecoderLoss:0.00114  StopLoss:0.04687  GradNorm:0.00422  GradNormST:0.01142  AvgTextLen:29.4  AvgSpecLen:104.1  StepTime:0.62  LR:0.000100\n",
      "   | > Step:48/68  GlobalStep:97270  TotalLoss:0.00247  PostnetLoss:0.00119  DecoderLoss:0.00127  StopLoss:0.03069  GradNorm:0.00446  GradNormST:0.00689  AvgTextLen:37.5  AvgSpecLen:123.8  StepTime:0.71  LR:0.000100\n",
      "   | > Step:58/68  GlobalStep:97280  TotalLoss:0.00268  PostnetLoss:0.00130  DecoderLoss:0.00138  StopLoss:0.04023  GradNorm:0.00461  GradNormST:0.01180  AvgTextLen:47.8  AvgSpecLen:158.0  StepTime:0.83  LR:0.000100\n",
      "   | > Step:68/68  GlobalStep:97290  TotalLoss:0.00295  PostnetLoss:0.00142  DecoderLoss:0.00153  StopLoss:0.03174  GradNorm:0.00299  GradNormST:0.01284  AvgTextLen:103.0  AvgSpecLen:308.9  StepTime:1.87  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:97290  AvgTotalLoss:0.04592  AvgPostnetLoss:0.00105  AvgDecoderLoss:0.00111  AvgStopLoss:0.04376  EpochTime:42.40  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.08141   PostnetLoss: 0.00505   DecoderLoss:0.00540  StopLoss: 0.07097  \n",
      "   | > TotalLoss: 0.07085   PostnetLoss: 0.00744   DecoderLoss:0.00795  StopLoss: 0.05546  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00105   Validation Loss: 0.00702\n",
      "\n",
      " > Epoch 410/1000\n",
      "   | > Step:9/68  GlobalStep:97300  TotalLoss:0.00157  PostnetLoss:0.00076  DecoderLoss:0.00080  StopLoss:0.03914  GradNorm:0.00360  GradNormST:0.01292  AvgTextLen:9.0  AvgSpecLen:42.1  StepTime:0.41  LR:0.000100\n",
      "   | > Step:19/68  GlobalStep:97310  TotalLoss:0.00183  PostnetLoss:0.00089  DecoderLoss:0.00094  StopLoss:0.06775  GradNorm:0.00367  GradNormST:0.02711  AvgTextLen:15.5  AvgSpecLen:63.2  StepTime:0.36  LR:0.000100\n",
      "   | > Step:29/68  GlobalStep:97320  TotalLoss:0.00207  PostnetLoss:0.00101  DecoderLoss:0.00107  StopLoss:0.06324  GradNorm:0.00430  GradNormST:0.01380  AvgTextLen:21.8  AvgSpecLen:83.4  StepTime:0.49  LR:0.000100\n",
      "   | > Step:39/68  GlobalStep:97330  TotalLoss:0.00228  PostnetLoss:0.00111  DecoderLoss:0.00118  StopLoss:0.03530  GradNorm:0.00414  GradNormST:0.01294  AvgTextLen:30.2  AvgSpecLen:110.4  StepTime:0.75  LR:0.000100\n",
      "   | > Step:49/68  GlobalStep:97340  TotalLoss:0.00246  PostnetLoss:0.00119  DecoderLoss:0.00127  StopLoss:0.03958  GradNorm:0.00438  GradNormST:0.00744  AvgTextLen:38.2  AvgSpecLen:127.7  StepTime:0.69  LR:0.000100\n",
      "   | > Step:59/68  GlobalStep:97350  TotalLoss:0.00265  PostnetLoss:0.00128  DecoderLoss:0.00137  StopLoss:0.03882  GradNorm:0.00441  GradNormST:0.00916  AvgTextLen:50.5  AvgSpecLen:155.9  StepTime:0.71  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:97359  AvgTotalLoss:0.04610  AvgPostnetLoss:0.00105  AvgDecoderLoss:0.00111  AvgStopLoss:0.04393  EpochTime:42.26  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.08121   PostnetLoss: 0.00492   DecoderLoss:0.00527  StopLoss: 0.07102  \n",
      "   | > TotalLoss: 0.07098   PostnetLoss: 0.00740   DecoderLoss:0.00792  StopLoss: 0.05566  \n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00105   Validation Loss: 0.00691\n",
      "\n",
      " > Epoch 411/1000\n",
      "   | > Step:0/68  GlobalStep:97360  TotalLoss:0.00155  PostnetLoss:0.00074  DecoderLoss:0.00081  StopLoss:0.07408  GradNorm:0.00672  GradNormST:0.02942  AvgTextLen:3.0  AvgSpecLen:25.3  StepTime:0.40  LR:0.000100\n",
      "   | > Step:10/68  GlobalStep:97370  TotalLoss:0.00167  PostnetLoss:0.00081  DecoderLoss:0.00086  StopLoss:0.03952  GradNorm:0.00364  GradNormST:0.01349  AvgTextLen:9.8  AvgSpecLen:48.6  StepTime:0.53  LR:0.000100\n",
      "   | > Step:20/68  GlobalStep:97380  TotalLoss:0.00193  PostnetLoss:0.00094  DecoderLoss:0.00099  StopLoss:0.04323  GradNorm:0.00359  GradNormST:0.01806  AvgTextLen:16.4  AvgSpecLen:64.8  StepTime:0.51  LR:0.000100\n",
      "   | > Step:30/68  GlobalStep:97390  TotalLoss:0.00212  PostnetLoss:0.00103  DecoderLoss:0.00109  StopLoss:0.04544  GradNorm:0.00428  GradNormST:0.01014  AvgTextLen:22.9  AvgSpecLen:85.8  StepTime:0.47  LR:0.000100\n",
      "   | > Step:40/68  GlobalStep:97400  TotalLoss:0.00229  PostnetLoss:0.00111  DecoderLoss:0.00118  StopLoss:0.04193  GradNorm:0.00494  GradNormST:0.00925  AvgTextLen:30.9  AvgSpecLen:105.3  StepTime:0.61  LR:0.000100\n",
      "   | > Step:50/68  GlobalStep:97410  TotalLoss:0.00246  PostnetLoss:0.00119  DecoderLoss:0.00127  StopLoss:0.03401  GradNorm:0.00409  GradNormST:0.01023  AvgTextLen:38.8  AvgSpecLen:132.0  StepTime:0.85  LR:0.000100\n",
      "   | > Step:60/68  GlobalStep:97420  TotalLoss:0.00270  PostnetLoss:0.00131  DecoderLoss:0.00140  StopLoss:0.02749  GradNorm:0.00444  GradNormST:0.00772  AvgTextLen:52.0  AvgSpecLen:172.7  StepTime:0.93  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:97428  AvgTotalLoss:0.04529  AvgPostnetLoss:0.00105  AvgDecoderLoss:0.00111  AvgStopLoss:0.04313  EpochTime:42.53  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.08482   PostnetLoss: 0.00497   DecoderLoss:0.00532  StopLoss: 0.07453  \n",
      "   | > TotalLoss: 0.07271   PostnetLoss: 0.00745   DecoderLoss:0.00795  StopLoss: 0.05731  \n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00105   Validation Loss: 0.00694\n",
      "\n",
      " > Epoch 412/1000\n",
      "   | > Step:1/68  GlobalStep:97430  TotalLoss:0.00152  PostnetLoss:0.00074  DecoderLoss:0.00078  StopLoss:0.03987  GradNorm:0.00552  GradNormST:0.01446  AvgTextLen:3.7  AvgSpecLen:28.6  StepTime:0.52  LR:0.000100\n",
      "   | > Step:11/68  GlobalStep:97440  TotalLoss:0.00165  PostnetLoss:0.00080  DecoderLoss:0.00085  StopLoss:0.04124  GradNorm:0.00360  GradNormST:0.01160  AvgTextLen:10.3  AvgSpecLen:49.4  StepTime:0.30  LR:0.000100\n",
      "   | > Step:21/68  GlobalStep:97450  TotalLoss:0.00188  PostnetLoss:0.00091  DecoderLoss:0.00097  StopLoss:0.04238  GradNorm:0.00350  GradNormST:0.01226  AvgTextLen:16.6  AvgSpecLen:66.3  StepTime:0.33  LR:0.000100\n",
      "   | > Step:31/68  GlobalStep:97460  TotalLoss:0.00208  PostnetLoss:0.00101  DecoderLoss:0.00107  StopLoss:0.03797  GradNorm:0.00383  GradNormST:0.01076  AvgTextLen:24.1  AvgSpecLen:82.7  StepTime:0.54  LR:0.000100\n",
      "   | > Step:41/68  GlobalStep:97470  TotalLoss:0.00234  PostnetLoss:0.00114  DecoderLoss:0.00121  StopLoss:0.03249  GradNorm:0.00411  GradNormST:0.01045  AvgTextLen:32.3  AvgSpecLen:109.4  StepTime:0.68  LR:0.000100\n",
      "   | > Step:51/68  GlobalStep:97480  TotalLoss:0.00242  PostnetLoss:0.00117  DecoderLoss:0.00125  StopLoss:0.03650  GradNorm:0.00405  GradNormST:0.01010  AvgTextLen:39.2  AvgSpecLen:126.9  StepTime:0.74  LR:0.000100\n",
      "   | > Step:61/68  GlobalStep:97490  TotalLoss:0.00270  PostnetLoss:0.00131  DecoderLoss:0.00139  StopLoss:0.03039  GradNorm:0.00468  GradNormST:0.00818  AvgTextLen:54.6  AvgSpecLen:165.5  StepTime:0.89  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:97497  AvgTotalLoss:0.04471  AvgPostnetLoss:0.00105  AvgDecoderLoss:0.00111  AvgStopLoss:0.04256  EpochTime:42.22  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.07971   PostnetLoss: 0.00507   DecoderLoss:0.00543  StopLoss: 0.06921  \n",
      "   | > TotalLoss: 0.06896   PostnetLoss: 0.00725   DecoderLoss:0.00776  StopLoss: 0.05394  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00105   Validation Loss: 0.00692\n",
      "\n",
      " > Epoch 413/1000\n",
      "   | > Step:2/68  GlobalStep:97500  TotalLoss:0.00142  PostnetLoss:0.00069  DecoderLoss:0.00073  StopLoss:0.05752  GradNorm:0.00412  GradNormST:0.01963  AvgTextLen:4.0  AvgSpecLen:31.3  StepTime:0.57  LR:0.000100\n",
      "   | > Step:12/68  GlobalStep:97510  TotalLoss:0.00166  PostnetLoss:0.00081  DecoderLoss:0.00085  StopLoss:0.04172  GradNorm:0.00388  GradNormST:0.01050  AvgTextLen:11.0  AvgSpecLen:49.3  StepTime:0.41  LR:0.000100\n",
      "   | > Step:22/68  GlobalStep:97520  TotalLoss:0.00191  PostnetLoss:0.00093  DecoderLoss:0.00098  StopLoss:0.04994  GradNorm:0.00335  GradNormST:0.01025  AvgTextLen:18.0  AvgSpecLen:72.0  StepTime:0.36  LR:0.000100\n",
      "   | > Step:32/68  GlobalStep:97530  TotalLoss:0.00212  PostnetLoss:0.00103  DecoderLoss:0.00109  StopLoss:0.05026  GradNorm:0.00374  GradNormST:0.00926  AvgTextLen:24.4  AvgSpecLen:91.5  StepTime:0.58  LR:0.000100\n",
      "   | > Step:42/68  GlobalStep:97540  TotalLoss:0.00231  PostnetLoss:0.00112  DecoderLoss:0.00119  StopLoss:0.04784  GradNorm:0.00364  GradNormST:0.01235  AvgTextLen:33.1  AvgSpecLen:109.7  StepTime:0.49  LR:0.000100\n",
      "   | > Step:52/68  GlobalStep:97550  TotalLoss:0.00253  PostnetLoss:0.00123  DecoderLoss:0.00130  StopLoss:0.03751  GradNorm:0.00472  GradNormST:0.01178  AvgTextLen:42.1  AvgSpecLen:137.5  StepTime:0.80  LR:0.000100\n",
      "   | > Step:62/68  GlobalStep:97560  TotalLoss:0.00275  PostnetLoss:0.00133  DecoderLoss:0.00142  StopLoss:0.03536  GradNorm:0.00441  GradNormST:0.00734  AvgTextLen:54.0  AvgSpecLen:174.7  StepTime:0.75  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:97566  AvgTotalLoss:0.04473  AvgPostnetLoss:0.00105  AvgDecoderLoss:0.00111  AvgStopLoss:0.04257  EpochTime:42.11  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.08028   PostnetLoss: 0.00505   DecoderLoss:0.00540  StopLoss: 0.06983  \n",
      "   | > TotalLoss: 0.07109   PostnetLoss: 0.00740   DecoderLoss:0.00791  StopLoss: 0.05579  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00105   Validation Loss: 0.00694\n",
      "\n",
      " > Epoch 414/1000\n",
      "   | > Step:3/68  GlobalStep:97570  TotalLoss:0.00146  PostnetLoss:0.00071  DecoderLoss:0.00075  StopLoss:0.06455  GradNorm:0.00410  GradNormST:0.01873  AvgTextLen:4.7  AvgSpecLen:29.7  StepTime:0.28  LR:0.000100\n",
      "   | > Step:13/68  GlobalStep:97580  TotalLoss:0.00164  PostnetLoss:0.00080  DecoderLoss:0.00084  StopLoss:0.08471  GradNorm:0.00392  GradNormST:0.02492  AvgTextLen:11.5  AvgSpecLen:55.2  StepTime:0.44  LR:0.000100\n",
      "   | > Step:23/68  GlobalStep:97590  TotalLoss:0.00201  PostnetLoss:0.00098  DecoderLoss:0.00103  StopLoss:0.04383  GradNorm:0.00399  GradNormST:0.00878  AvgTextLen:18.3  AvgSpecLen:72.2  StepTime:0.45  LR:0.000100\n",
      "   | > Step:33/68  GlobalStep:97600  TotalLoss:0.00218  PostnetLoss:0.00106  DecoderLoss:0.00112  StopLoss:0.04106  GradNorm:0.00424  GradNormST:0.01053  AvgTextLen:25.9  AvgSpecLen:88.9  StepTime:0.56  LR:0.000100\n",
      "   | > Step:43/68  GlobalStep:97610  TotalLoss:0.00233  PostnetLoss:0.00113  DecoderLoss:0.00120  StopLoss:0.04378  GradNorm:0.00347  GradNormST:0.00784  AvgTextLen:33.4  AvgSpecLen:115.7  StepTime:0.60  LR:0.000100\n",
      "   | > Step:53/68  GlobalStep:97620  TotalLoss:0.00247  PostnetLoss:0.00119  DecoderLoss:0.00127  StopLoss:0.03140  GradNorm:0.00348  GradNormST:0.00521  AvgTextLen:42.4  AvgSpecLen:137.6  StepTime:0.76  LR:0.000100\n",
      "   | > Step:63/68  GlobalStep:97630  TotalLoss:0.00280  PostnetLoss:0.00135  DecoderLoss:0.00144  StopLoss:0.02697  GradNorm:0.00403  GradNormST:0.00713  AvgTextLen:58.6  AvgSpecLen:185.1  StepTime:0.94  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:97635  AvgTotalLoss:0.04524  AvgPostnetLoss:0.00105  AvgDecoderLoss:0.00111  AvgStopLoss:0.04308  EpochTime:42.49  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.08291   PostnetLoss: 0.00501   DecoderLoss:0.00536  StopLoss: 0.07254  \n",
      "   | > TotalLoss: 0.06526   PostnetLoss: 0.00727   DecoderLoss:0.00779  StopLoss: 0.05021  \n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00105   Validation Loss: 0.00689\n",
      "\n",
      " > Epoch 415/1000\n",
      "   | > Step:4/68  GlobalStep:97640  TotalLoss:0.00144  PostnetLoss:0.00070  DecoderLoss:0.00074  StopLoss:0.08197  GradNorm:0.00388  GradNormST:0.03468  AvgTextLen:4.8  AvgSpecLen:31.1  StepTime:0.30  LR:0.000100\n",
      "   | > Step:14/68  GlobalStep:97650  TotalLoss:0.00172  PostnetLoss:0.00083  DecoderLoss:0.00088  StopLoss:0.04527  GradNorm:0.00389  GradNormST:0.01294  AvgTextLen:12.2  AvgSpecLen:52.0  StepTime:0.31  LR:0.000100\n",
      "   | > Step:24/68  GlobalStep:97660  TotalLoss:0.00204  PostnetLoss:0.00099  DecoderLoss:0.00105  StopLoss:0.04759  GradNorm:0.00335  GradNormST:0.00972  AvgTextLen:19.2  AvgSpecLen:79.0  StepTime:0.54  LR:0.000100\n",
      "   | > Step:34/68  GlobalStep:97670  TotalLoss:0.00221  PostnetLoss:0.00107  DecoderLoss:0.00114  StopLoss:0.03509  GradNorm:0.00400  GradNormST:0.00832  AvgTextLen:26.2  AvgSpecLen:95.9  StepTime:0.62  LR:0.000100\n",
      "   | > Step:44/68  GlobalStep:97680  TotalLoss:0.00238  PostnetLoss:0.00115  DecoderLoss:0.00123  StopLoss:0.03728  GradNorm:0.00431  GradNormST:0.01069  AvgTextLen:33.8  AvgSpecLen:116.3  StepTime:0.69  LR:0.000100\n",
      "   | > Step:54/68  GlobalStep:97690  TotalLoss:0.00252  PostnetLoss:0.00122  DecoderLoss:0.00130  StopLoss:0.03667  GradNorm:0.00354  GradNormST:0.00752  AvgTextLen:42.2  AvgSpecLen:144.9  StepTime:0.81  LR:0.000100\n",
      "   | > Step:64/68  GlobalStep:97700  TotalLoss:0.00263  PostnetLoss:0.00127  DecoderLoss:0.00136  StopLoss:0.03501  GradNorm:0.00310  GradNormST:0.00615  AvgTextLen:60.6  AvgSpecLen:177.6  StepTime:0.81  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:97704  AvgTotalLoss:0.04421  AvgPostnetLoss:0.00104  AvgDecoderLoss:0.00111  AvgStopLoss:0.04206  EpochTime:43.19  AvgStepTime:0.63\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.08461   PostnetLoss: 0.00503   DecoderLoss:0.00539  StopLoss: 0.07419  \n",
      "   | > TotalLoss: 0.07108   PostnetLoss: 0.00762   DecoderLoss:0.00813  StopLoss: 0.05534  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00104   Validation Loss: 0.00705\n",
      "\n",
      " > Epoch 416/1000\n",
      "   | > Step:5/68  GlobalStep:97710  TotalLoss:0.00150  PostnetLoss:0.00072  DecoderLoss:0.00078  StopLoss:0.04256  GradNorm:0.00361  GradNormST:0.01093  AvgTextLen:5.9  AvgSpecLen:33.2  StepTime:0.39  LR:0.000100\n",
      "   | > Step:15/68  GlobalStep:97720  TotalLoss:0.00182  PostnetLoss:0.00089  DecoderLoss:0.00093  StopLoss:0.03510  GradNorm:0.00392  GradNormST:0.00875  AvgTextLen:12.8  AvgSpecLen:59.1  StepTime:0.54  LR:0.000100\n",
      "   | > Step:25/68  GlobalStep:97730  TotalLoss:0.00197  PostnetLoss:0.00096  DecoderLoss:0.00101  StopLoss:0.04334  GradNorm:0.00364  GradNormST:0.01229  AvgTextLen:19.8  AvgSpecLen:73.3  StepTime:0.44  LR:0.000100\n",
      "   | > Step:35/68  GlobalStep:97740  TotalLoss:0.00221  PostnetLoss:0.00107  DecoderLoss:0.00114  StopLoss:0.03864  GradNorm:0.00371  GradNormST:0.01876  AvgTextLen:27.1  AvgSpecLen:104.7  StepTime:0.63  LR:0.000100\n",
      "   | > Step:45/68  GlobalStep:97750  TotalLoss:0.00230  PostnetLoss:0.00112  DecoderLoss:0.00119  StopLoss:0.05572  GradNorm:0.00301  GradNormST:0.01791  AvgTextLen:34.4  AvgSpecLen:118.6  StepTime:0.54  LR:0.000100\n",
      "   | > Step:55/68  GlobalStep:97760  TotalLoss:0.00258  PostnetLoss:0.00125  DecoderLoss:0.00133  StopLoss:0.03285  GradNorm:0.00377  GradNormST:0.00651  AvgTextLen:43.9  AvgSpecLen:145.2  StepTime:0.77  LR:0.000100\n",
      "   | > Step:65/68  GlobalStep:97770  TotalLoss:0.00282  PostnetLoss:0.00136  DecoderLoss:0.00146  StopLoss:0.02709  GradNorm:0.00331  GradNormST:0.00748  AvgTextLen:64.2  AvgSpecLen:207.4  StepTime:1.19  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:97773  AvgTotalLoss:0.04391  AvgPostnetLoss:0.00104  AvgDecoderLoss:0.00111  AvgStopLoss:0.04176  EpochTime:41.91  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.08343   PostnetLoss: 0.00504   DecoderLoss:0.00540  StopLoss: 0.07300  \n",
      "   | > TotalLoss: 0.07356   PostnetLoss: 0.00765   DecoderLoss:0.00819  StopLoss: 0.05772  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00104   Validation Loss: 0.00694\n",
      "\n",
      " > Epoch 417/1000\n",
      "   | > Step:6/68  GlobalStep:97780  TotalLoss:0.00148  PostnetLoss:0.00072  DecoderLoss:0.00076  StopLoss:0.04361  GradNorm:0.00433  GradNormST:0.01762  AvgTextLen:6.0  AvgSpecLen:34.8  StepTime:0.47  LR:0.000100\n",
      "   | > Step:16/68  GlobalStep:97790  TotalLoss:0.00181  PostnetLoss:0.00088  DecoderLoss:0.00093  StopLoss:0.04028  GradNorm:0.00373  GradNormST:0.01087  AvgTextLen:13.8  AvgSpecLen:60.6  StepTime:0.29  LR:0.000100\n",
      "   | > Step:26/68  GlobalStep:97800  TotalLoss:0.00198  PostnetLoss:0.00096  DecoderLoss:0.00101  StopLoss:0.03171  GradNorm:0.00349  GradNormST:0.00897  AvgTextLen:20.3  AvgSpecLen:77.5  StepTime:0.38  LR:0.000100\n",
      "   | > Step:36/68  GlobalStep:97810  TotalLoss:0.00219  PostnetLoss:0.00106  DecoderLoss:0.00113  StopLoss:0.05110  GradNorm:0.00380  GradNormST:0.00904  AvgTextLen:28.5  AvgSpecLen:100.2  StepTime:0.56  LR:0.000100\n",
      "   | > Step:46/68  GlobalStep:97820  TotalLoss:0.00236  PostnetLoss:0.00114  DecoderLoss:0.00121  StopLoss:0.03598  GradNorm:0.00341  GradNormST:0.00891  AvgTextLen:35.4  AvgSpecLen:122.6  StepTime:0.66  LR:0.000100\n",
      "   | > Step:56/68  GlobalStep:97830  TotalLoss:0.00255  PostnetLoss:0.00124  DecoderLoss:0.00132  StopLoss:0.04262  GradNorm:0.00341  GradNormST:0.01643  AvgTextLen:45.8  AvgSpecLen:145.4  StepTime:0.95  LR:0.000100\n",
      "   | > Step:66/68  GlobalStep:97840  TotalLoss:0.00297  PostnetLoss:0.00143  DecoderLoss:0.00154  StopLoss:0.03153  GradNorm:0.00537  GradNormST:0.01141  AvgTextLen:71.2  AvgSpecLen:220.8  StepTime:1.04  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:97842  AvgTotalLoss:0.04393  AvgPostnetLoss:0.00104  AvgDecoderLoss:0.00111  AvgStopLoss:0.04178  EpochTime:41.60  AvgStepTime:0.60\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.08321   PostnetLoss: 0.00508   DecoderLoss:0.00544  StopLoss: 0.07269  \n",
      "   | > TotalLoss: 0.06867   PostnetLoss: 0.00754   DecoderLoss:0.00802  StopLoss: 0.05312  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00104   Validation Loss: 0.00701\n",
      "\n",
      " > Epoch 418/1000\n",
      "   | > Step:7/68  GlobalStep:97850  TotalLoss:0.00164  PostnetLoss:0.00080  DecoderLoss:0.00084  StopLoss:0.03843  GradNorm:0.00539  GradNormST:0.01713  AvgTextLen:7.4  AvgSpecLen:44.6  StepTime:0.39  LR:0.000100\n",
      "   | > Step:17/68  GlobalStep:97860  TotalLoss:0.00181  PostnetLoss:0.00088  DecoderLoss:0.00093  StopLoss:0.05040  GradNorm:0.00371  GradNormST:0.01387  AvgTextLen:14.7  AvgSpecLen:66.0  StepTime:0.43  LR:0.000100\n",
      "   | > Step:27/68  GlobalStep:97870  TotalLoss:0.00204  PostnetLoss:0.00099  DecoderLoss:0.00104  StopLoss:0.05263  GradNorm:0.00329  GradNormST:0.01594  AvgTextLen:21.0  AvgSpecLen:80.2  StepTime:0.65  LR:0.000100\n",
      "   | > Step:37/68  GlobalStep:97880  TotalLoss:0.00221  PostnetLoss:0.00107  DecoderLoss:0.00114  StopLoss:0.04473  GradNorm:0.00329  GradNormST:0.00987  AvgTextLen:29.3  AvgSpecLen:102.4  StepTime:0.49  LR:0.000100\n",
      "   | > Step:47/68  GlobalStep:97890  TotalLoss:0.00241  PostnetLoss:0.00117  DecoderLoss:0.00124  StopLoss:0.04060  GradNorm:0.00327  GradNormST:0.00845  AvgTextLen:36.7  AvgSpecLen:126.9  StepTime:0.72  LR:0.000100\n",
      "   | > Step:57/68  GlobalStep:97900  TotalLoss:0.00263  PostnetLoss:0.00127  DecoderLoss:0.00136  StopLoss:0.03019  GradNorm:0.00356  GradNormST:0.00515  AvgTextLen:47.1  AvgSpecLen:161.0  StepTime:0.74  LR:0.000100\n",
      "   | > Step:67/68  GlobalStep:97910  TotalLoss:0.00300  PostnetLoss:0.00144  DecoderLoss:0.00156  StopLoss:0.03078  GradNorm:0.00325  GradNormST:0.01102  AvgTextLen:80.6  AvgSpecLen:246.5  StepTime:1.25  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:97911  AvgTotalLoss:0.04411  AvgPostnetLoss:0.00104  AvgDecoderLoss:0.00111  AvgStopLoss:0.04196  EpochTime:42.12  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.08344   PostnetLoss: 0.00505   DecoderLoss:0.00540  StopLoss: 0.07299  \n",
      "   | > TotalLoss: 0.06776   PostnetLoss: 0.00742   DecoderLoss:0.00791  StopLoss: 0.05243  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00104   Validation Loss: 0.00689\n",
      "\n",
      " > Epoch 419/1000\n",
      "   | > Step:8/68  GlobalStep:97920  TotalLoss:0.00161  PostnetLoss:0.00079  DecoderLoss:0.00083  StopLoss:0.04887  GradNorm:0.00503  GradNormST:0.01692  AvgTextLen:8.3  AvgSpecLen:44.0  StepTime:0.30  LR:0.000100\n",
      "   | > Step:18/68  GlobalStep:97930  TotalLoss:0.00185  PostnetLoss:0.00090  DecoderLoss:0.00095  StopLoss:0.04389  GradNorm:0.00442  GradNormST:0.01164  AvgTextLen:15.2  AvgSpecLen:64.5  StepTime:0.37  LR:0.000100\n",
      "   | > Step:28/68  GlobalStep:97940  TotalLoss:0.00205  PostnetLoss:0.00100  DecoderLoss:0.00105  StopLoss:0.04974  GradNorm:0.00376  GradNormST:0.01022  AvgTextLen:22.1  AvgSpecLen:85.4  StepTime:0.45  LR:0.000100\n",
      "   | > Step:38/68  GlobalStep:97950  TotalLoss:0.00219  PostnetLoss:0.00106  DecoderLoss:0.00113  StopLoss:0.03417  GradNorm:0.00305  GradNormST:0.00731  AvgTextLen:29.4  AvgSpecLen:104.1  StepTime:0.54  LR:0.000100\n",
      "   | > Step:48/68  GlobalStep:97960  TotalLoss:0.00242  PostnetLoss:0.00117  DecoderLoss:0.00124  StopLoss:0.03517  GradNorm:0.00286  GradNormST:0.00690  AvgTextLen:37.5  AvgSpecLen:123.8  StepTime:0.72  LR:0.000100\n",
      "   | > Step:58/68  GlobalStep:97970  TotalLoss:0.00264  PostnetLoss:0.00128  DecoderLoss:0.00136  StopLoss:0.04207  GradNorm:0.00300  GradNormST:0.01687  AvgTextLen:47.8  AvgSpecLen:158.0  StepTime:0.70  LR:0.000100\n",
      "   | > Step:68/68  GlobalStep:97980  TotalLoss:0.00295  PostnetLoss:0.00142  DecoderLoss:0.00153  StopLoss:0.02670  GradNorm:0.00429  GradNormST:0.01063  AvgTextLen:103.0  AvgSpecLen:308.9  StepTime:1.86  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:97980  AvgTotalLoss:0.04323  AvgPostnetLoss:0.00104  AvgDecoderLoss:0.00111  AvgStopLoss:0.04107  EpochTime:42.84  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.08089   PostnetLoss: 0.00496   DecoderLoss:0.00533  StopLoss: 0.07060  \n",
      "   | > TotalLoss: 0.07003   PostnetLoss: 0.00788   DecoderLoss:0.00841  StopLoss: 0.05374  \n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00104   Validation Loss: 0.00703\n",
      "\n",
      " > Epoch 420/1000\n",
      "   | > Step:9/68  GlobalStep:97990  TotalLoss:0.00164  PostnetLoss:0.00080  DecoderLoss:0.00084  StopLoss:0.04675  GradNorm:0.00463  GradNormST:0.01527  AvgTextLen:9.0  AvgSpecLen:42.1  StepTime:0.42  LR:0.000100\n",
      "   | > Step:19/68  GlobalStep:98000  TotalLoss:0.00189  PostnetLoss:0.00092  DecoderLoss:0.00097  StopLoss:0.05205  GradNorm:0.00565  GradNormST:0.01819  AvgTextLen:15.5  AvgSpecLen:63.2  StepTime:0.41  LR:0.000100\n",
      " | | > Checkpoint saving : /vm/TTS/models/cartman_models/mozilla-no-loc-fattn-stopnet-sigmoid-loss_masking-August-15-2019_11+26AM-de6aeb9/checkpoint_98000.pth.tar\n",
      "   | > Step:29/68  GlobalStep:98010  TotalLoss:0.00204  PostnetLoss:0.00099  DecoderLoss:0.00105  StopLoss:0.05363  GradNorm:0.00394  GradNormST:0.01023  AvgTextLen:21.8  AvgSpecLen:83.4  StepTime:0.48  LR:0.000100\n",
      "   | > Step:39/68  GlobalStep:98020  TotalLoss:0.00228  PostnetLoss:0.00111  DecoderLoss:0.00117  StopLoss:0.03174  GradNorm:0.00383  GradNormST:0.01112  AvgTextLen:30.2  AvgSpecLen:110.4  StepTime:0.62  LR:0.000100\n",
      "   | > Step:49/68  GlobalStep:98030  TotalLoss:0.00239  PostnetLoss:0.00116  DecoderLoss:0.00123  StopLoss:0.03416  GradNorm:0.00312  GradNormST:0.00761  AvgTextLen:38.2  AvgSpecLen:127.7  StepTime:0.75  LR:0.000100\n",
      "   | > Step:59/68  GlobalStep:98040  TotalLoss:0.00260  PostnetLoss:0.00126  DecoderLoss:0.00134  StopLoss:0.03786  GradNorm:0.00374  GradNormST:0.00828  AvgTextLen:50.5  AvgSpecLen:155.9  StepTime:0.80  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:98049  AvgTotalLoss:0.04228  AvgPostnetLoss:0.00105  AvgDecoderLoss:0.00111  AvgStopLoss:0.04011  EpochTime:42.51  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.08611   PostnetLoss: 0.00517   DecoderLoss:0.00554  StopLoss: 0.07540  \n",
      "   | > TotalLoss: 0.07229   PostnetLoss: 0.00781   DecoderLoss:0.00832  StopLoss: 0.05616  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00105   Validation Loss: 0.00708\n",
      "\n",
      " > Epoch 421/1000\n",
      "   | > Step:0/68  GlobalStep:98050  TotalLoss:0.00165  PostnetLoss:0.00080  DecoderLoss:0.00086  StopLoss:0.05851  GradNorm:0.00894  GradNormST:0.01868  AvgTextLen:3.0  AvgSpecLen:25.3  StepTime:0.53  LR:0.000100\n",
      "   | > Step:10/68  GlobalStep:98060  TotalLoss:0.00167  PostnetLoss:0.00081  DecoderLoss:0.00086  StopLoss:0.04194  GradNorm:0.00371  GradNormST:0.01763  AvgTextLen:9.8  AvgSpecLen:48.6  StepTime:0.51  LR:0.000100\n",
      "   | > Step:20/68  GlobalStep:98070  TotalLoss:0.00193  PostnetLoss:0.00094  DecoderLoss:0.00099  StopLoss:0.03438  GradNorm:0.00404  GradNormST:0.01271  AvgTextLen:16.4  AvgSpecLen:64.8  StepTime:0.43  LR:0.000100\n",
      "   | > Step:30/68  GlobalStep:98080  TotalLoss:0.00209  PostnetLoss:0.00102  DecoderLoss:0.00107  StopLoss:0.03507  GradNorm:0.00442  GradNormST:0.00642  AvgTextLen:22.9  AvgSpecLen:85.8  StepTime:0.50  LR:0.000100\n",
      "   | > Step:40/68  GlobalStep:98090  TotalLoss:0.00224  PostnetLoss:0.00109  DecoderLoss:0.00115  StopLoss:0.03842  GradNorm:0.00385  GradNormST:0.00885  AvgTextLen:30.9  AvgSpecLen:105.3  StepTime:0.61  LR:0.000100\n",
      "   | > Step:50/68  GlobalStep:98100  TotalLoss:0.00247  PostnetLoss:0.00119  DecoderLoss:0.00127  StopLoss:0.03161  GradNorm:0.00337  GradNormST:0.00959  AvgTextLen:38.8  AvgSpecLen:132.0  StepTime:0.70  LR:0.000100\n",
      "   | > Step:60/68  GlobalStep:98110  TotalLoss:0.00267  PostnetLoss:0.00129  DecoderLoss:0.00138  StopLoss:0.02813  GradNorm:0.00319  GradNormST:0.00622  AvgTextLen:52.0  AvgSpecLen:172.7  StepTime:0.92  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:98118  AvgTotalLoss:0.04303  AvgPostnetLoss:0.00105  AvgDecoderLoss:0.00111  AvgStopLoss:0.04087  EpochTime:41.71  AvgStepTime:0.60\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.08472   PostnetLoss: 0.00508   DecoderLoss:0.00541  StopLoss: 0.07424  \n",
      "   | > TotalLoss: 0.07055   PostnetLoss: 0.00773   DecoderLoss:0.00822  StopLoss: 0.05460  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00105   Validation Loss: 0.00709\n",
      "\n",
      " > Epoch 422/1000\n",
      "   | > Step:1/68  GlobalStep:98120  TotalLoss:0.00153  PostnetLoss:0.00074  DecoderLoss:0.00078  StopLoss:0.04969  GradNorm:0.00634  GradNormST:0.01866  AvgTextLen:3.7  AvgSpecLen:28.6  StepTime:0.24  LR:0.000100\n",
      "   | > Step:11/68  GlobalStep:98130  TotalLoss:0.00167  PostnetLoss:0.00082  DecoderLoss:0.00086  StopLoss:0.03316  GradNorm:0.00390  GradNormST:0.01695  AvgTextLen:10.3  AvgSpecLen:49.4  StepTime:0.39  LR:0.000100\n",
      "   | > Step:21/68  GlobalStep:98140  TotalLoss:0.00192  PostnetLoss:0.00093  DecoderLoss:0.00098  StopLoss:0.04888  GradNorm:0.00403  GradNormST:0.01250  AvgTextLen:16.6  AvgSpecLen:66.3  StepTime:0.43  LR:0.000100\n",
      "   | > Step:31/68  GlobalStep:98150  TotalLoss:0.00208  PostnetLoss:0.00101  DecoderLoss:0.00107  StopLoss:0.04026  GradNorm:0.00344  GradNormST:0.01000  AvgTextLen:24.1  AvgSpecLen:82.7  StepTime:0.47  LR:0.000100\n",
      "   | > Step:41/68  GlobalStep:98160  TotalLoss:0.00232  PostnetLoss:0.00113  DecoderLoss:0.00119  StopLoss:0.03198  GradNorm:0.00357  GradNormST:0.01229  AvgTextLen:32.3  AvgSpecLen:109.4  StepTime:0.52  LR:0.000100\n",
      "   | > Step:51/68  GlobalStep:98170  TotalLoss:0.00239  PostnetLoss:0.00116  DecoderLoss:0.00123  StopLoss:0.03136  GradNorm:0.00297  GradNormST:0.00652  AvgTextLen:39.2  AvgSpecLen:126.9  StepTime:0.61  LR:0.000100\n",
      "   | > Step:61/68  GlobalStep:98180  TotalLoss:0.00267  PostnetLoss:0.00129  DecoderLoss:0.00138  StopLoss:0.02807  GradNorm:0.00484  GradNormST:0.00766  AvgTextLen:54.6  AvgSpecLen:165.5  StepTime:0.91  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:98187  AvgTotalLoss:0.04270  AvgPostnetLoss:0.00105  AvgDecoderLoss:0.00111  AvgStopLoss:0.04055  EpochTime:41.68  AvgStepTime:0.60\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.08756   PostnetLoss: 0.00504   DecoderLoss:0.00538  StopLoss: 0.07715  \n",
      "   | > TotalLoss: 0.07695   PostnetLoss: 0.00803   DecoderLoss:0.00855  StopLoss: 0.06037  \n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00105   Validation Loss: 0.00727\n",
      "\n",
      " > Epoch 423/1000\n",
      "   | > Step:2/68  GlobalStep:98190  TotalLoss:0.00142  PostnetLoss:0.00069  DecoderLoss:0.00073  StopLoss:0.07519  GradNorm:0.00436  GradNormST:0.03132  AvgTextLen:4.0  AvgSpecLen:31.3  StepTime:0.36  LR:0.000100\n",
      "   | > Step:12/68  GlobalStep:98200  TotalLoss:0.00171  PostnetLoss:0.00083  DecoderLoss:0.00088  StopLoss:0.04145  GradNorm:0.00598  GradNormST:0.01001  AvgTextLen:11.0  AvgSpecLen:49.3  StepTime:0.42  LR:0.000100\n",
      "   | > Step:22/68  GlobalStep:98210  TotalLoss:0.00197  PostnetLoss:0.00096  DecoderLoss:0.00101  StopLoss:0.04135  GradNorm:0.00527  GradNormST:0.00878  AvgTextLen:18.0  AvgSpecLen:72.0  StepTime:0.48  LR:0.000100\n",
      "   | > Step:32/68  GlobalStep:98220  TotalLoss:0.00213  PostnetLoss:0.00104  DecoderLoss:0.00110  StopLoss:0.05341  GradNorm:0.00434  GradNormST:0.01279  AvgTextLen:24.4  AvgSpecLen:91.5  StepTime:0.56  LR:0.000100\n",
      "   | > Step:42/68  GlobalStep:98230  TotalLoss:0.00227  PostnetLoss:0.00110  DecoderLoss:0.00117  StopLoss:0.03765  GradNorm:0.00360  GradNormST:0.00981  AvgTextLen:33.1  AvgSpecLen:109.7  StepTime:0.60  LR:0.000100\n",
      "   | > Step:52/68  GlobalStep:98240  TotalLoss:0.00250  PostnetLoss:0.00121  DecoderLoss:0.00129  StopLoss:0.04543  GradNorm:0.00320  GradNormST:0.00911  AvgTextLen:42.1  AvgSpecLen:137.5  StepTime:0.67  LR:0.000100\n",
      "   | > Step:62/68  GlobalStep:98250  TotalLoss:0.00273  PostnetLoss:0.00132  DecoderLoss:0.00141  StopLoss:0.03159  GradNorm:0.00396  GradNormST:0.00884  AvgTextLen:54.0  AvgSpecLen:174.7  StepTime:0.75  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:98256  AvgTotalLoss:0.04330  AvgPostnetLoss:0.00105  AvgDecoderLoss:0.00112  AvgStopLoss:0.04113  EpochTime:42.57  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.08204   PostnetLoss: 0.00510   DecoderLoss:0.00543  StopLoss: 0.07151  \n",
      "   | > TotalLoss: 0.07334   PostnetLoss: 0.00790   DecoderLoss:0.00841  StopLoss: 0.05703  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00105   Validation Loss: 0.00709\n",
      "\n",
      " > Epoch 424/1000\n",
      "   | > Step:3/68  GlobalStep:98260  TotalLoss:0.00146  PostnetLoss:0.00071  DecoderLoss:0.00075  StopLoss:0.06044  GradNorm:0.00442  GradNormST:0.02356  AvgTextLen:4.7  AvgSpecLen:29.7  StepTime:0.27  LR:0.000100\n",
      "   | > Step:13/68  GlobalStep:98270  TotalLoss:0.00167  PostnetLoss:0.00081  DecoderLoss:0.00086  StopLoss:0.05583  GradNorm:0.00395  GradNormST:0.01483  AvgTextLen:11.5  AvgSpecLen:55.2  StepTime:0.40  LR:0.000100\n",
      "   | > Step:23/68  GlobalStep:98280  TotalLoss:0.00205  PostnetLoss:0.00100  DecoderLoss:0.00105  StopLoss:0.03942  GradNorm:0.00577  GradNormST:0.00880  AvgTextLen:18.3  AvgSpecLen:72.2  StepTime:0.43  LR:0.000100\n",
      "   | > Step:33/68  GlobalStep:98290  TotalLoss:0.00217  PostnetLoss:0.00106  DecoderLoss:0.00111  StopLoss:0.04199  GradNorm:0.00470  GradNormST:0.00944  AvgTextLen:25.9  AvgSpecLen:88.9  StepTime:0.54  LR:0.000100\n",
      "   | > Step:43/68  GlobalStep:98300  TotalLoss:0.00229  PostnetLoss:0.00111  DecoderLoss:0.00118  StopLoss:0.04106  GradNorm:0.00399  GradNormST:0.00822  AvgTextLen:33.4  AvgSpecLen:115.7  StepTime:0.63  LR:0.000100\n",
      "   | > Step:53/68  GlobalStep:98310  TotalLoss:0.00247  PostnetLoss:0.00120  DecoderLoss:0.00128  StopLoss:0.03167  GradNorm:0.00330  GradNormST:0.00593  AvgTextLen:42.4  AvgSpecLen:137.6  StepTime:0.77  LR:0.000100\n",
      "   | > Step:63/68  GlobalStep:98320  TotalLoss:0.00279  PostnetLoss:0.00135  DecoderLoss:0.00144  StopLoss:0.02773  GradNorm:0.00447  GradNormST:0.00869  AvgTextLen:58.6  AvgSpecLen:185.1  StepTime:0.90  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:98325  AvgTotalLoss:0.04161  AvgPostnetLoss:0.00105  AvgDecoderLoss:0.00111  AvgStopLoss:0.03945  EpochTime:42.62  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.08377   PostnetLoss: 0.00504   DecoderLoss:0.00537  StopLoss: 0.07336  \n",
      "   | > TotalLoss: 0.07590   PostnetLoss: 0.00834   DecoderLoss:0.00884  StopLoss: 0.05872  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00105   Validation Loss: 0.00749\n",
      "\n",
      " > Epoch 425/1000\n",
      "   | > Step:4/68  GlobalStep:98330  TotalLoss:0.00140  PostnetLoss:0.00068  DecoderLoss:0.00072  StopLoss:0.08607  GradNorm:0.00357  GradNormST:0.03335  AvgTextLen:4.8  AvgSpecLen:31.1  StepTime:0.29  LR:0.000100\n",
      "   | > Step:14/68  GlobalStep:98340  TotalLoss:0.00170  PostnetLoss:0.00082  DecoderLoss:0.00087  StopLoss:0.04156  GradNorm:0.00390  GradNormST:0.01162  AvgTextLen:12.2  AvgSpecLen:52.0  StepTime:0.36  LR:0.000100\n",
      "   | > Step:24/68  GlobalStep:98350  TotalLoss:0.00210  PostnetLoss:0.00102  DecoderLoss:0.00107  StopLoss:0.04277  GradNorm:0.00482  GradNormST:0.00888  AvgTextLen:19.2  AvgSpecLen:79.0  StepTime:0.39  LR:0.000100\n",
      "   | > Step:34/68  GlobalStep:98360  TotalLoss:0.00222  PostnetLoss:0.00108  DecoderLoss:0.00114  StopLoss:0.02726  GradNorm:0.00476  GradNormST:0.00655  AvgTextLen:26.2  AvgSpecLen:95.9  StepTime:0.61  LR:0.000100\n",
      "   | > Step:44/68  GlobalStep:98370  TotalLoss:0.00239  PostnetLoss:0.00116  DecoderLoss:0.00123  StopLoss:0.03175  GradNorm:0.00424  GradNormST:0.01092  AvgTextLen:33.8  AvgSpecLen:116.3  StepTime:0.69  LR:0.000100\n",
      "   | > Step:54/68  GlobalStep:98380  TotalLoss:0.00254  PostnetLoss:0.00123  DecoderLoss:0.00131  StopLoss:0.03559  GradNorm:0.00409  GradNormST:0.00778  AvgTextLen:42.2  AvgSpecLen:144.9  StepTime:0.85  LR:0.000100\n",
      "   | > Step:64/68  GlobalStep:98390  TotalLoss:0.00265  PostnetLoss:0.00128  DecoderLoss:0.00137  StopLoss:0.03262  GradNorm:0.00473  GradNormST:0.00902  AvgTextLen:60.6  AvgSpecLen:177.6  StepTime:0.80  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:98394  AvgTotalLoss:0.04311  AvgPostnetLoss:0.00104  AvgDecoderLoss:0.00111  AvgStopLoss:0.04096  EpochTime:42.78  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.08382   PostnetLoss: 0.00498   DecoderLoss:0.00531  StopLoss: 0.07354  \n",
      "   | > TotalLoss: 0.07758   PostnetLoss: 0.00824   DecoderLoss:0.00875  StopLoss: 0.06059  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00104   Validation Loss: 0.00737\n",
      "\n",
      " > Epoch 426/1000\n",
      "   | > Step:5/68  GlobalStep:98400  TotalLoss:0.00152  PostnetLoss:0.00073  DecoderLoss:0.00078  StopLoss:0.04465  GradNorm:0.00365  GradNormST:0.01135  AvgTextLen:5.9  AvgSpecLen:33.2  StepTime:0.40  LR:0.000100\n",
      "   | > Step:15/68  GlobalStep:98410  TotalLoss:0.00180  PostnetLoss:0.00087  DecoderLoss:0.00092  StopLoss:0.04047  GradNorm:0.00343  GradNormST:0.00935  AvgTextLen:12.8  AvgSpecLen:59.1  StepTime:0.44  LR:0.000100\n",
      "   | > Step:25/68  GlobalStep:98420  TotalLoss:0.00199  PostnetLoss:0.00097  DecoderLoss:0.00102  StopLoss:0.04861  GradNorm:0.00422  GradNormST:0.00862  AvgTextLen:19.8  AvgSpecLen:73.3  StepTime:0.47  LR:0.000100\n",
      "   | > Step:35/68  GlobalStep:98430  TotalLoss:0.00222  PostnetLoss:0.00108  DecoderLoss:0.00114  StopLoss:0.03299  GradNorm:0.00462  GradNormST:0.01238  AvgTextLen:27.1  AvgSpecLen:104.7  StepTime:0.75  LR:0.000100\n",
      "   | > Step:45/68  GlobalStep:98440  TotalLoss:0.00231  PostnetLoss:0.00112  DecoderLoss:0.00119  StopLoss:0.03897  GradNorm:0.00484  GradNormST:0.01116  AvgTextLen:34.4  AvgSpecLen:118.6  StepTime:0.58  LR:0.000100\n",
      "   | > Step:55/68  GlobalStep:98450  TotalLoss:0.00258  PostnetLoss:0.00125  DecoderLoss:0.00133  StopLoss:0.03137  GradNorm:0.00379  GradNormST:0.00800  AvgTextLen:43.9  AvgSpecLen:145.2  StepTime:0.85  LR:0.000100\n",
      "   | > Step:65/68  GlobalStep:98460  TotalLoss:0.00283  PostnetLoss:0.00136  DecoderLoss:0.00146  StopLoss:0.02764  GradNorm:0.00394  GradNormST:0.00739  AvgTextLen:64.2  AvgSpecLen:207.4  StepTime:1.19  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:98463  AvgTotalLoss:0.04350  AvgPostnetLoss:0.00104  AvgDecoderLoss:0.00111  AvgStopLoss:0.04135  EpochTime:42.45  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.08589   PostnetLoss: 0.00505   DecoderLoss:0.00538  StopLoss: 0.07546  \n",
      "   | > TotalLoss: 0.08198   PostnetLoss: 0.00884   DecoderLoss:0.00937  StopLoss: 0.06377  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00104   Validation Loss: 0.00781\n",
      "\n",
      " > Epoch 427/1000\n",
      "   | > Step:6/68  GlobalStep:98470  TotalLoss:0.00147  PostnetLoss:0.00072  DecoderLoss:0.00075  StopLoss:0.03702  GradNorm:0.00388  GradNormST:0.01116  AvgTextLen:6.0  AvgSpecLen:34.8  StepTime:0.37  LR:0.000100\n",
      "   | > Step:16/68  GlobalStep:98480  TotalLoss:0.00185  PostnetLoss:0.00090  DecoderLoss:0.00095  StopLoss:0.04027  GradNorm:0.00412  GradNormST:0.01121  AvgTextLen:13.8  AvgSpecLen:60.6  StepTime:0.29  LR:0.000100\n",
      "   | > Step:26/68  GlobalStep:98490  TotalLoss:0.00202  PostnetLoss:0.00099  DecoderLoss:0.00104  StopLoss:0.03837  GradNorm:0.00462  GradNormST:0.01138  AvgTextLen:20.3  AvgSpecLen:77.5  StepTime:0.38  LR:0.000100\n",
      "   | > Step:36/68  GlobalStep:98500  TotalLoss:0.00222  PostnetLoss:0.00108  DecoderLoss:0.00114  StopLoss:0.04752  GradNorm:0.00439  GradNormST:0.00950  AvgTextLen:28.5  AvgSpecLen:100.2  StepTime:0.47  LR:0.000100\n",
      "   | > Step:46/68  GlobalStep:98510  TotalLoss:0.00235  PostnetLoss:0.00114  DecoderLoss:0.00121  StopLoss:0.03509  GradNorm:0.00459  GradNormST:0.01119  AvgTextLen:35.4  AvgSpecLen:122.6  StepTime:0.78  LR:0.000100\n",
      "   | > Step:56/68  GlobalStep:98520  TotalLoss:0.00258  PostnetLoss:0.00125  DecoderLoss:0.00133  StopLoss:0.03134  GradNorm:0.00380  GradNormST:0.01198  AvgTextLen:45.8  AvgSpecLen:145.4  StepTime:0.83  LR:0.000100\n",
      "   | > Step:66/68  GlobalStep:98530  TotalLoss:0.00298  PostnetLoss:0.00144  DecoderLoss:0.00154  StopLoss:0.03087  GradNorm:0.00334  GradNormST:0.01174  AvgTextLen:71.2  AvgSpecLen:220.8  StepTime:1.07  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:98532  AvgTotalLoss:0.04447  AvgPostnetLoss:0.00104  AvgDecoderLoss:0.00111  AvgStopLoss:0.04232  EpochTime:41.49  AvgStepTime:0.60\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.08065   PostnetLoss: 0.00508   DecoderLoss:0.00541  StopLoss: 0.07017  \n",
      "   | > TotalLoss: 0.07726   PostnetLoss: 0.00851   DecoderLoss:0.00903  StopLoss: 0.05971  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00104   Validation Loss: 0.00757\n",
      "\n",
      " > Epoch 428/1000\n",
      "   | > Step:7/68  GlobalStep:98540  TotalLoss:0.00161  PostnetLoss:0.00078  DecoderLoss:0.00082  StopLoss:0.05553  GradNorm:0.00360  GradNormST:0.03174  AvgTextLen:7.4  AvgSpecLen:44.6  StepTime:0.40  LR:0.000100\n",
      "   | > Step:17/68  GlobalStep:98550  TotalLoss:0.00185  PostnetLoss:0.00090  DecoderLoss:0.00095  StopLoss:0.06069  GradNorm:0.00445  GradNormST:0.01394  AvgTextLen:14.7  AvgSpecLen:66.0  StepTime:0.33  LR:0.000100\n",
      "   | > Step:27/68  GlobalStep:98560  TotalLoss:0.00205  PostnetLoss:0.00100  DecoderLoss:0.00105  StopLoss:0.04826  GradNorm:0.00400  GradNormST:0.01744  AvgTextLen:21.0  AvgSpecLen:80.2  StepTime:0.64  LR:0.000100\n",
      "   | > Step:37/68  GlobalStep:98570  TotalLoss:0.00227  PostnetLoss:0.00110  DecoderLoss:0.00116  StopLoss:0.03541  GradNorm:0.00488  GradNormST:0.00956  AvgTextLen:29.3  AvgSpecLen:102.4  StepTime:0.57  LR:0.000100\n",
      "   | > Step:47/68  GlobalStep:98580  TotalLoss:0.00244  PostnetLoss:0.00118  DecoderLoss:0.00125  StopLoss:0.03481  GradNorm:0.00432  GradNormST:0.00810  AvgTextLen:36.7  AvgSpecLen:126.9  StepTime:0.58  LR:0.000100\n",
      "   | > Step:57/68  GlobalStep:98590  TotalLoss:0.00258  PostnetLoss:0.00125  DecoderLoss:0.00133  StopLoss:0.03167  GradNorm:0.00373  GradNormST:0.00927  AvgTextLen:47.1  AvgSpecLen:161.0  StepTime:0.69  LR:0.000100\n",
      "   | > Step:67/68  GlobalStep:98600  TotalLoss:0.00294  PostnetLoss:0.00141  DecoderLoss:0.00152  StopLoss:0.02717  GradNorm:0.00309  GradNormST:0.00977  AvgTextLen:80.6  AvgSpecLen:246.5  StepTime:1.23  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:98601  AvgTotalLoss:0.04318  AvgPostnetLoss:0.00104  AvgDecoderLoss:0.00111  AvgStopLoss:0.04103  EpochTime:41.53  AvgStepTime:0.60\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.07929   PostnetLoss: 0.00504   DecoderLoss:0.00537  StopLoss: 0.06887  \n",
      "   | > TotalLoss: 0.07289   PostnetLoss: 0.00809   DecoderLoss:0.00858  StopLoss: 0.05622  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00104   Validation Loss: 0.00720\n",
      "\n",
      " > Epoch 429/1000\n",
      "   | > Step:8/68  GlobalStep:98610  TotalLoss:0.00164  PostnetLoss:0.00080  DecoderLoss:0.00084  StopLoss:0.04885  GradNorm:0.00520  GradNormST:0.01217  AvgTextLen:8.3  AvgSpecLen:44.0  StepTime:0.31  LR:0.000100\n",
      "   | > Step:18/68  GlobalStep:98620  TotalLoss:0.00177  PostnetLoss:0.00086  DecoderLoss:0.00091  StopLoss:0.04573  GradNorm:0.00351  GradNormST:0.01383  AvgTextLen:15.2  AvgSpecLen:64.5  StepTime:0.42  LR:0.000100\n",
      "   | > Step:28/68  GlobalStep:98630  TotalLoss:0.00206  PostnetLoss:0.00100  DecoderLoss:0.00106  StopLoss:0.04369  GradNorm:0.00409  GradNormST:0.00880  AvgTextLen:22.1  AvgSpecLen:85.4  StepTime:0.46  LR:0.000100\n",
      "   | > Step:38/68  GlobalStep:98640  TotalLoss:0.00225  PostnetLoss:0.00109  DecoderLoss:0.00116  StopLoss:0.04127  GradNorm:0.00441  GradNormST:0.01691  AvgTextLen:29.4  AvgSpecLen:104.1  StepTime:0.61  LR:0.000100\n",
      "   | > Step:48/68  GlobalStep:98650  TotalLoss:0.00241  PostnetLoss:0.00117  DecoderLoss:0.00124  StopLoss:0.03267  GradNorm:0.00428  GradNormST:0.00779  AvgTextLen:37.5  AvgSpecLen:123.8  StepTime:0.60  LR:0.000100\n",
      "   | > Step:58/68  GlobalStep:98660  TotalLoss:0.00262  PostnetLoss:0.00127  DecoderLoss:0.00135  StopLoss:0.04091  GradNorm:0.00372  GradNormST:0.01464  AvgTextLen:47.8  AvgSpecLen:158.0  StepTime:0.70  LR:0.000100\n",
      "   | > Step:68/68  GlobalStep:98670  TotalLoss:0.00292  PostnetLoss:0.00140  DecoderLoss:0.00151  StopLoss:0.02354  GradNorm:0.00333  GradNormST:0.00843  AvgTextLen:103.0  AvgSpecLen:308.9  StepTime:1.85  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:98670  AvgTotalLoss:0.04414  AvgPostnetLoss:0.00104  AvgDecoderLoss:0.00110  AvgStopLoss:0.04199  EpochTime:41.30  AvgStepTime:0.60\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.08298   PostnetLoss: 0.00497   DecoderLoss:0.00530  StopLoss: 0.07270  \n",
      "   | > TotalLoss: 0.07379   PostnetLoss: 0.00804   DecoderLoss:0.00852  StopLoss: 0.05723  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00104   Validation Loss: 0.00722\n",
      "\n",
      " > Epoch 430/1000\n",
      "   | > Step:9/68  GlobalStep:98680  TotalLoss:0.00160  PostnetLoss:0.00078  DecoderLoss:0.00082  StopLoss:0.03141  GradNorm:0.00418  GradNormST:0.00821  AvgTextLen:9.0  AvgSpecLen:42.1  StepTime:0.32  LR:0.000100\n",
      "   | > Step:19/68  GlobalStep:98690  TotalLoss:0.00184  PostnetLoss:0.00090  DecoderLoss:0.00095  StopLoss:0.05869  GradNorm:0.00352  GradNormST:0.01553  AvgTextLen:15.5  AvgSpecLen:63.2  StepTime:0.33  LR:0.000100\n",
      "   | > Step:29/68  GlobalStep:98700  TotalLoss:0.00199  PostnetLoss:0.00097  DecoderLoss:0.00102  StopLoss:0.06043  GradNorm:0.00353  GradNormST:0.01279  AvgTextLen:21.8  AvgSpecLen:83.4  StepTime:0.58  LR:0.000100\n",
      "   | > Step:39/68  GlobalStep:98710  TotalLoss:0.00230  PostnetLoss:0.00112  DecoderLoss:0.00118  StopLoss:0.03388  GradNorm:0.00415  GradNormST:0.00820  AvgTextLen:30.2  AvgSpecLen:110.4  StepTime:0.67  LR:0.000100\n",
      "   | > Step:49/68  GlobalStep:98720  TotalLoss:0.00242  PostnetLoss:0.00117  DecoderLoss:0.00124  StopLoss:0.03856  GradNorm:0.00439  GradNormST:0.01150  AvgTextLen:38.2  AvgSpecLen:127.7  StepTime:0.73  LR:0.000100\n",
      "   | > Step:59/68  GlobalStep:98730  TotalLoss:0.00261  PostnetLoss:0.00126  DecoderLoss:0.00135  StopLoss:0.04437  GradNorm:0.00348  GradNormST:0.01479  AvgTextLen:50.5  AvgSpecLen:155.9  StepTime:0.71  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:98739  AvgTotalLoss:0.04341  AvgPostnetLoss:0.00104  AvgDecoderLoss:0.00110  AvgStopLoss:0.04127  EpochTime:43.39  AvgStepTime:0.63\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.07651   PostnetLoss: 0.00509   DecoderLoss:0.00542  StopLoss: 0.06600  \n",
      "   | > TotalLoss: 0.06583   PostnetLoss: 0.00746   DecoderLoss:0.00792  StopLoss: 0.05045  \n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00104   Validation Loss: 0.00699\n",
      "\n",
      " > Epoch 431/1000\n",
      "   | > Step:0/68  GlobalStep:98740  TotalLoss:0.00153  PostnetLoss:0.00074  DecoderLoss:0.00080  StopLoss:0.05241  GradNorm:0.00630  GradNormST:0.01586  AvgTextLen:3.0  AvgSpecLen:25.3  StepTime:0.38  LR:0.000100\n",
      "   | > Step:10/68  GlobalStep:98750  TotalLoss:0.00165  PostnetLoss:0.00080  DecoderLoss:0.00085  StopLoss:0.03850  GradNorm:0.00392  GradNormST:0.01055  AvgTextLen:9.8  AvgSpecLen:48.6  StepTime:0.54  LR:0.000100\n",
      "   | > Step:20/68  GlobalStep:98760  TotalLoss:0.00192  PostnetLoss:0.00093  DecoderLoss:0.00099  StopLoss:0.04222  GradNorm:0.00373  GradNormST:0.01999  AvgTextLen:16.4  AvgSpecLen:64.8  StepTime:0.46  LR:0.000100\n",
      "   | > Step:30/68  GlobalStep:98770  TotalLoss:0.00205  PostnetLoss:0.00100  DecoderLoss:0.00105  StopLoss:0.04384  GradNorm:0.00354  GradNormST:0.00855  AvgTextLen:22.9  AvgSpecLen:85.8  StepTime:0.49  LR:0.000100\n",
      "   | > Step:40/68  GlobalStep:98780  TotalLoss:0.00228  PostnetLoss:0.00111  DecoderLoss:0.00117  StopLoss:0.04275  GradNorm:0.00424  GradNormST:0.00941  AvgTextLen:30.9  AvgSpecLen:105.3  StepTime:0.49  LR:0.000100\n",
      "   | > Step:50/68  GlobalStep:98790  TotalLoss:0.00243  PostnetLoss:0.00118  DecoderLoss:0.00125  StopLoss:0.02991  GradNorm:0.00432  GradNormST:0.00986  AvgTextLen:38.8  AvgSpecLen:132.0  StepTime:0.86  LR:0.000100\n",
      "   | > Step:60/68  GlobalStep:98800  TotalLoss:0.00267  PostnetLoss:0.00129  DecoderLoss:0.00138  StopLoss:0.02494  GradNorm:0.00416  GradNormST:0.00539  AvgTextLen:52.0  AvgSpecLen:172.7  StepTime:0.82  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:98808  AvgTotalLoss:0.04416  AvgPostnetLoss:0.00104  AvgDecoderLoss:0.00110  AvgStopLoss:0.04203  EpochTime:42.48  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.07959   PostnetLoss: 0.00494   DecoderLoss:0.00528  StopLoss: 0.06937  \n",
      "   | > TotalLoss: 0.06749   PostnetLoss: 0.00748   DecoderLoss:0.00797  StopLoss: 0.05204  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00104   Validation Loss: 0.00690\n",
      "\n",
      " > Epoch 432/1000\n",
      "   | > Step:1/68  GlobalStep:98810  TotalLoss:0.00149  PostnetLoss:0.00072  DecoderLoss:0.00077  StopLoss:0.04904  GradNorm:0.00607  GradNormST:0.01634  AvgTextLen:3.7  AvgSpecLen:28.6  StepTime:0.43  LR:0.000100\n",
      "   | > Step:11/68  GlobalStep:98820  TotalLoss:0.00166  PostnetLoss:0.00081  DecoderLoss:0.00085  StopLoss:0.04144  GradNorm:0.00389  GradNormST:0.01519  AvgTextLen:10.3  AvgSpecLen:49.4  StepTime:0.30  LR:0.000100\n",
      "   | > Step:21/68  GlobalStep:98830  TotalLoss:0.00188  PostnetLoss:0.00092  DecoderLoss:0.00096  StopLoss:0.04511  GradNorm:0.00327  GradNormST:0.01600  AvgTextLen:16.6  AvgSpecLen:66.3  StepTime:0.42  LR:0.000100\n",
      "   | > Step:31/68  GlobalStep:98840  TotalLoss:0.00205  PostnetLoss:0.00100  DecoderLoss:0.00105  StopLoss:0.03468  GradNorm:0.00335  GradNormST:0.00743  AvgTextLen:24.1  AvgSpecLen:82.7  StepTime:0.53  LR:0.000100\n",
      "   | > Step:41/68  GlobalStep:98850  TotalLoss:0.00233  PostnetLoss:0.00113  DecoderLoss:0.00120  StopLoss:0.03591  GradNorm:0.00363  GradNormST:0.01060  AvgTextLen:32.3  AvgSpecLen:109.4  StepTime:0.65  LR:0.000100\n",
      "   | > Step:51/68  GlobalStep:98860  TotalLoss:0.00239  PostnetLoss:0.00116  DecoderLoss:0.00123  StopLoss:0.03368  GradNorm:0.00405  GradNormST:0.00676  AvgTextLen:39.2  AvgSpecLen:126.9  StepTime:0.74  LR:0.000100\n",
      "   | > Step:61/68  GlobalStep:98870  TotalLoss:0.00259  PostnetLoss:0.00125  DecoderLoss:0.00134  StopLoss:0.02900  GradNorm:0.00346  GradNormST:0.00698  AvgTextLen:54.6  AvgSpecLen:165.5  StepTime:0.92  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:98877  AvgTotalLoss:0.04402  AvgPostnetLoss:0.00103  AvgDecoderLoss:0.00110  AvgStopLoss:0.04188  EpochTime:42.85  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.08166   PostnetLoss: 0.00500   DecoderLoss:0.00534  StopLoss: 0.07132  \n",
      "   | > TotalLoss: 0.06885   PostnetLoss: 0.00753   DecoderLoss:0.00802  StopLoss: 0.05330  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00103   Validation Loss: 0.00701\n",
      "\n",
      " > Epoch 433/1000\n",
      "   | > Step:2/68  GlobalStep:98880  TotalLoss:0.00138  PostnetLoss:0.00067  DecoderLoss:0.00071  StopLoss:0.04917  GradNorm:0.00408  GradNormST:0.01828  AvgTextLen:4.0  AvgSpecLen:31.3  StepTime:0.36  LR:0.000100\n",
      "   | > Step:12/68  GlobalStep:98890  TotalLoss:0.00161  PostnetLoss:0.00078  DecoderLoss:0.00083  StopLoss:0.03736  GradNorm:0.00323  GradNormST:0.00729  AvgTextLen:11.0  AvgSpecLen:49.3  StepTime:0.43  LR:0.000100\n",
      "   | > Step:22/68  GlobalStep:98900  TotalLoss:0.00191  PostnetLoss:0.00093  DecoderLoss:0.00098  StopLoss:0.04067  GradNorm:0.00359  GradNormST:0.01299  AvgTextLen:18.0  AvgSpecLen:72.0  StepTime:0.45  LR:0.000100\n",
      "   | > Step:32/68  GlobalStep:98910  TotalLoss:0.00206  PostnetLoss:0.00100  DecoderLoss:0.00106  StopLoss:0.05439  GradNorm:0.00311  GradNormST:0.01455  AvgTextLen:24.4  AvgSpecLen:91.5  StepTime:0.54  LR:0.000100\n",
      "   | > Step:42/68  GlobalStep:98920  TotalLoss:0.00225  PostnetLoss:0.00109  DecoderLoss:0.00116  StopLoss:0.04506  GradNorm:0.00289  GradNormST:0.01244  AvgTextLen:33.1  AvgSpecLen:109.7  StepTime:0.60  LR:0.000100\n",
      "   | > Step:52/68  GlobalStep:98930  TotalLoss:0.00246  PostnetLoss:0.00119  DecoderLoss:0.00127  StopLoss:0.04115  GradNorm:0.00342  GradNormST:0.01034  AvgTextLen:42.1  AvgSpecLen:137.5  StepTime:0.67  LR:0.000100\n",
      "   | > Step:62/68  GlobalStep:98940  TotalLoss:0.00267  PostnetLoss:0.00129  DecoderLoss:0.00138  StopLoss:0.04174  GradNorm:0.00331  GradNormST:0.01113  AvgTextLen:54.0  AvgSpecLen:174.7  StepTime:0.75  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:98946  AvgTotalLoss:0.04518  AvgPostnetLoss:0.00103  AvgDecoderLoss:0.00109  AvgStopLoss:0.04305  EpochTime:41.50  AvgStepTime:0.60\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.07894   PostnetLoss: 0.00498   DecoderLoss:0.00533  StopLoss: 0.06863  \n",
      "   | > TotalLoss: 0.07185   PostnetLoss: 0.00774   DecoderLoss:0.00823  StopLoss: 0.05589  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00103   Validation Loss: 0.00709\n",
      "\n",
      " > Epoch 434/1000\n",
      "   | > Step:3/68  GlobalStep:98950  TotalLoss:0.00148  PostnetLoss:0.00072  DecoderLoss:0.00076  StopLoss:0.06602  GradNorm:0.00442  GradNormST:0.02403  AvgTextLen:4.7  AvgSpecLen:29.7  StepTime:0.31  LR:0.000100\n",
      "   | > Step:13/68  GlobalStep:98960  TotalLoss:0.00169  PostnetLoss:0.00083  DecoderLoss:0.00087  StopLoss:0.07475  GradNorm:0.00407  GradNormST:0.02530  AvgTextLen:11.5  AvgSpecLen:55.2  StepTime:0.44  LR:0.000100\n",
      "   | > Step:23/68  GlobalStep:98970  TotalLoss:0.00196  PostnetLoss:0.00095  DecoderLoss:0.00101  StopLoss:0.03488  GradNorm:0.00328  GradNormST:0.00833  AvgTextLen:18.3  AvgSpecLen:72.2  StepTime:0.43  LR:0.000100\n",
      "   | > Step:33/68  GlobalStep:98980  TotalLoss:0.00213  PostnetLoss:0.00103  DecoderLoss:0.00109  StopLoss:0.04011  GradNorm:0.00333  GradNormST:0.00749  AvgTextLen:25.9  AvgSpecLen:88.9  StepTime:0.45  LR:0.000100\n",
      "   | > Step:43/68  GlobalStep:98990  TotalLoss:0.00229  PostnetLoss:0.00111  DecoderLoss:0.00118  StopLoss:0.03833  GradNorm:0.00300  GradNormST:0.00927  AvgTextLen:33.4  AvgSpecLen:115.7  StepTime:0.62  LR:0.000100\n",
      "   | > Step:53/68  GlobalStep:99000  TotalLoss:0.00243  PostnetLoss:0.00118  DecoderLoss:0.00125  StopLoss:0.03102  GradNorm:0.00326  GradNormST:0.00614  AvgTextLen:42.4  AvgSpecLen:137.6  StepTime:0.79  LR:0.000100\n",
      " | | > Checkpoint saving : /vm/TTS/models/cartman_models/mozilla-no-loc-fattn-stopnet-sigmoid-loss_masking-August-15-2019_11+26AM-de6aeb9/checkpoint_99000.pth.tar\n",
      "warning: audio amplitude out of range, auto clipped.\n",
      "   | > Step:63/68  GlobalStep:99010  TotalLoss:0.00275  PostnetLoss:0.00133  DecoderLoss:0.00142  StopLoss:0.02331  GradNorm:0.00311  GradNormST:0.00562  AvgTextLen:58.6  AvgSpecLen:185.1  StepTime:0.90  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:99015  AvgTotalLoss:0.04231  AvgPostnetLoss:0.00104  AvgDecoderLoss:0.00110  AvgStopLoss:0.04017  EpochTime:42.85  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.07625   PostnetLoss: 0.00501   DecoderLoss:0.00536  StopLoss: 0.06588  \n",
      "   | > TotalLoss: 0.06748   PostnetLoss: 0.00745   DecoderLoss:0.00794  StopLoss: 0.05209  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00104   Validation Loss: 0.00686\n",
      "\n",
      " > Epoch 435/1000\n",
      "   | > Step:4/68  GlobalStep:99020  TotalLoss:0.00153  PostnetLoss:0.00074  DecoderLoss:0.00079  StopLoss:0.09349  GradNorm:0.00559  GradNormST:0.03434  AvgTextLen:4.8  AvgSpecLen:31.1  StepTime:0.30  LR:0.000100\n",
      "   | > Step:14/68  GlobalStep:99030  TotalLoss:0.00172  PostnetLoss:0.00084  DecoderLoss:0.00088  StopLoss:0.03865  GradNorm:0.00347  GradNormST:0.00902  AvgTextLen:12.2  AvgSpecLen:52.0  StepTime:0.28  LR:0.000100\n",
      "   | > Step:24/68  GlobalStep:99040  TotalLoss:0.00201  PostnetLoss:0.00097  DecoderLoss:0.00103  StopLoss:0.04120  GradNorm:0.00333  GradNormST:0.00865  AvgTextLen:19.2  AvgSpecLen:79.0  StepTime:0.48  LR:0.000100\n",
      "   | > Step:34/68  GlobalStep:99050  TotalLoss:0.00220  PostnetLoss:0.00106  DecoderLoss:0.00113  StopLoss:0.03410  GradNorm:0.00326  GradNormST:0.00756  AvgTextLen:26.2  AvgSpecLen:95.9  StepTime:0.57  LR:0.000100\n",
      "   | > Step:44/68  GlobalStep:99060  TotalLoss:0.00235  PostnetLoss:0.00114  DecoderLoss:0.00121  StopLoss:0.03130  GradNorm:0.00315  GradNormST:0.00706  AvgTextLen:33.8  AvgSpecLen:116.3  StepTime:0.67  LR:0.000100\n",
      "   | > Step:54/68  GlobalStep:99070  TotalLoss:0.00248  PostnetLoss:0.00120  DecoderLoss:0.00128  StopLoss:0.03379  GradNorm:0.00290  GradNormST:0.00892  AvgTextLen:42.2  AvgSpecLen:144.9  StepTime:0.82  LR:0.000100\n",
      "   | > Step:64/68  GlobalStep:99080  TotalLoss:0.00259  PostnetLoss:0.00125  DecoderLoss:0.00134  StopLoss:0.02823  GradNorm:0.00275  GradNormST:0.00556  AvgTextLen:60.6  AvgSpecLen:177.6  StepTime:0.80  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:99084  AvgTotalLoss:0.04182  AvgPostnetLoss:0.00104  AvgDecoderLoss:0.00110  AvgStopLoss:0.03968  EpochTime:42.73  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.07868   PostnetLoss: 0.00503   DecoderLoss:0.00538  StopLoss: 0.06827  \n",
      "   | > TotalLoss: 0.06872   PostnetLoss: 0.00756   DecoderLoss:0.00805  StopLoss: 0.05311  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00104   Validation Loss: 0.00694\n",
      "\n",
      " > Epoch 436/1000\n",
      "   | > Step:5/68  GlobalStep:99090  TotalLoss:0.00153  PostnetLoss:0.00074  DecoderLoss:0.00079  StopLoss:0.04689  GradNorm:0.00527  GradNormST:0.01474  AvgTextLen:5.9  AvgSpecLen:33.2  StepTime:0.33  LR:0.000100\n",
      "   | > Step:15/68  GlobalStep:99100  TotalLoss:0.00183  PostnetLoss:0.00089  DecoderLoss:0.00094  StopLoss:0.03754  GradNorm:0.00363  GradNormST:0.01165  AvgTextLen:12.8  AvgSpecLen:59.1  StepTime:0.55  LR:0.000100\n",
      "   | > Step:25/68  GlobalStep:99110  TotalLoss:0.00194  PostnetLoss:0.00094  DecoderLoss:0.00100  StopLoss:0.04842  GradNorm:0.00309  GradNormST:0.01302  AvgTextLen:19.8  AvgSpecLen:73.3  StepTime:0.36  LR:0.000100\n",
      "   | > Step:35/68  GlobalStep:99120  TotalLoss:0.00220  PostnetLoss:0.00107  DecoderLoss:0.00113  StopLoss:0.03098  GradNorm:0.00303  GradNormST:0.01041  AvgTextLen:27.1  AvgSpecLen:104.7  StepTime:0.74  LR:0.000100\n",
      "   | > Step:45/68  GlobalStep:99130  TotalLoss:0.00228  PostnetLoss:0.00110  DecoderLoss:0.00118  StopLoss:0.04155  GradNorm:0.00297  GradNormST:0.01704  AvgTextLen:34.4  AvgSpecLen:118.6  StepTime:0.63  LR:0.000100\n",
      "   | > Step:55/68  GlobalStep:99140  TotalLoss:0.00255  PostnetLoss:0.00123  DecoderLoss:0.00131  StopLoss:0.03484  GradNorm:0.00286  GradNormST:0.00702  AvgTextLen:43.9  AvgSpecLen:145.2  StepTime:0.77  LR:0.000100\n",
      "   | > Step:65/68  GlobalStep:99150  TotalLoss:0.00277  PostnetLoss:0.00134  DecoderLoss:0.00143  StopLoss:0.02849  GradNorm:0.00281  GradNormST:0.00821  AvgTextLen:64.2  AvgSpecLen:207.4  StepTime:1.18  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:99153  AvgTotalLoss:0.04261  AvgPostnetLoss:0.00104  AvgDecoderLoss:0.00110  AvgStopLoss:0.04048  EpochTime:42.80  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.07716   PostnetLoss: 0.00502   DecoderLoss:0.00537  StopLoss: 0.06677  \n",
      "   | > TotalLoss: 0.06843   PostnetLoss: 0.00768   DecoderLoss:0.00819  StopLoss: 0.05255  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00104   Validation Loss: 0.00703\n",
      "\n",
      " > Epoch 437/1000\n",
      "   | > Step:6/68  GlobalStep:99160  TotalLoss:0.00147  PostnetLoss:0.00072  DecoderLoss:0.00075  StopLoss:0.03910  GradNorm:0.00412  GradNormST:0.01015  AvgTextLen:6.0  AvgSpecLen:34.8  StepTime:0.42  LR:0.000100\n",
      "   | > Step:16/68  GlobalStep:99170  TotalLoss:0.00183  PostnetLoss:0.00089  DecoderLoss:0.00094  StopLoss:0.03718  GradNorm:0.00416  GradNormST:0.01034  AvgTextLen:13.8  AvgSpecLen:60.6  StepTime:0.29  LR:0.000100\n",
      "   | > Step:26/68  GlobalStep:99180  TotalLoss:0.00200  PostnetLoss:0.00097  DecoderLoss:0.00103  StopLoss:0.03171  GradNorm:0.00326  GradNormST:0.00851  AvgTextLen:20.3  AvgSpecLen:77.5  StepTime:0.46  LR:0.000100\n",
      "   | > Step:36/68  GlobalStep:99190  TotalLoss:0.00215  PostnetLoss:0.00104  DecoderLoss:0.00111  StopLoss:0.06576  GradNorm:0.00312  GradNormST:0.01487  AvgTextLen:28.5  AvgSpecLen:100.2  StepTime:0.58  LR:0.000100\n",
      "   | > Step:46/68  GlobalStep:99200  TotalLoss:0.00233  PostnetLoss:0.00113  DecoderLoss:0.00120  StopLoss:0.03555  GradNorm:0.00308  GradNormST:0.00752  AvgTextLen:35.4  AvgSpecLen:122.6  StepTime:0.69  LR:0.000100\n",
      "   | > Step:56/68  GlobalStep:99210  TotalLoss:0.00254  PostnetLoss:0.00123  DecoderLoss:0.00131  StopLoss:0.02559  GradNorm:0.01332  GradNormST:0.00710  AvgTextLen:45.8  AvgSpecLen:145.4  StepTime:0.96  LR:0.000100\n",
      "   | > Step:66/68  GlobalStep:99220  TotalLoss:0.00297  PostnetLoss:0.00143  DecoderLoss:0.00154  StopLoss:0.02785  GradNorm:0.00361  GradNormST:0.01366  AvgTextLen:71.2  AvgSpecLen:220.8  StepTime:1.04  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:99222  AvgTotalLoss:0.04127  AvgPostnetLoss:0.00103  AvgDecoderLoss:0.00110  AvgStopLoss:0.03914  EpochTime:42.71  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.07740   PostnetLoss: 0.00508   DecoderLoss:0.00543  StopLoss: 0.06689  \n",
      "   | > TotalLoss: 0.06909   PostnetLoss: 0.00746   DecoderLoss:0.00796  StopLoss: 0.05367  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00103   Validation Loss: 0.00701\n",
      "\n",
      " > Epoch 438/1000\n",
      "   | > Step:7/68  GlobalStep:99230  TotalLoss:0.00158  PostnetLoss:0.00077  DecoderLoss:0.00081  StopLoss:0.03410  GradNorm:0.00387  GradNormST:0.01079  AvgTextLen:7.4  AvgSpecLen:44.6  StepTime:0.33  LR:0.000100\n",
      "   | > Step:17/68  GlobalStep:99240  TotalLoss:0.00181  PostnetLoss:0.00088  DecoderLoss:0.00093  StopLoss:0.05318  GradNorm:0.00406  GradNormST:0.01025  AvgTextLen:14.7  AvgSpecLen:66.0  StepTime:0.42  LR:0.000100\n",
      "   | > Step:27/68  GlobalStep:99250  TotalLoss:0.00201  PostnetLoss:0.00098  DecoderLoss:0.00103  StopLoss:0.04926  GradNorm:0.00317  GradNormST:0.01624  AvgTextLen:21.0  AvgSpecLen:80.2  StepTime:0.67  LR:0.000100\n",
      "   | > Step:37/68  GlobalStep:99260  TotalLoss:0.00221  PostnetLoss:0.00108  DecoderLoss:0.00114  StopLoss:0.03353  GradNorm:0.00318  GradNormST:0.00581  AvgTextLen:29.3  AvgSpecLen:102.4  StepTime:0.57  LR:0.000100\n",
      "   | > Step:47/68  GlobalStep:99270  TotalLoss:0.00240  PostnetLoss:0.00116  DecoderLoss:0.00124  StopLoss:0.03535  GradNorm:0.00321  GradNormST:0.00635  AvgTextLen:36.7  AvgSpecLen:126.9  StepTime:0.70  LR:0.000100\n",
      "   | > Step:57/68  GlobalStep:99280  TotalLoss:0.00257  PostnetLoss:0.00125  DecoderLoss:0.00133  StopLoss:0.02523  GradNorm:0.00302  GradNormST:0.00454  AvgTextLen:47.1  AvgSpecLen:161.0  StepTime:0.90  LR:0.000100\n",
      "   | > Step:67/68  GlobalStep:99290  TotalLoss:0.00296  PostnetLoss:0.00142  DecoderLoss:0.00153  StopLoss:0.02627  GradNorm:0.00352  GradNormST:0.00964  AvgTextLen:80.6  AvgSpecLen:246.5  StepTime:1.22  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:99291  AvgTotalLoss:0.04144  AvgPostnetLoss:0.00104  AvgDecoderLoss:0.00110  AvgStopLoss:0.03930  EpochTime:42.11  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.07903   PostnetLoss: 0.00495   DecoderLoss:0.00530  StopLoss: 0.06877  \n",
      "   | > TotalLoss: 0.06861   PostnetLoss: 0.00791   DecoderLoss:0.00844  StopLoss: 0.05226  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00104   Validation Loss: 0.00711\n",
      "\n",
      " > Epoch 439/1000\n",
      "   | > Step:8/68  GlobalStep:99300  TotalLoss:0.00163  PostnetLoss:0.00080  DecoderLoss:0.00083  StopLoss:0.04387  GradNorm:0.00401  GradNormST:0.01114  AvgTextLen:8.3  AvgSpecLen:44.0  StepTime:0.31  LR:0.000100\n",
      "   | > Step:18/68  GlobalStep:99310  TotalLoss:0.00178  PostnetLoss:0.00087  DecoderLoss:0.00091  StopLoss:0.03936  GradNorm:0.00354  GradNormST:0.00847  AvgTextLen:15.2  AvgSpecLen:64.5  StepTime:0.43  LR:0.000100\n",
      "   | > Step:28/68  GlobalStep:99320  TotalLoss:0.00202  PostnetLoss:0.00098  DecoderLoss:0.00104  StopLoss:0.04338  GradNorm:0.00315  GradNormST:0.01008  AvgTextLen:22.1  AvgSpecLen:85.4  StepTime:0.54  LR:0.000100\n",
      "   | > Step:38/68  GlobalStep:99330  TotalLoss:0.00219  PostnetLoss:0.00106  DecoderLoss:0.00113  StopLoss:0.03154  GradNorm:0.00334  GradNormST:0.00668  AvgTextLen:29.4  AvgSpecLen:104.1  StepTime:0.64  LR:0.000100\n",
      "   | > Step:48/68  GlobalStep:99340  TotalLoss:0.00237  PostnetLoss:0.00115  DecoderLoss:0.00122  StopLoss:0.02745  GradNorm:0.00295  GradNormST:0.00507  AvgTextLen:37.5  AvgSpecLen:123.8  StepTime:0.72  LR:0.000100\n",
      "   | > Step:58/68  GlobalStep:99350  TotalLoss:0.00259  PostnetLoss:0.00126  DecoderLoss:0.00134  StopLoss:0.03623  GradNorm:0.00342  GradNormST:0.01137  AvgTextLen:47.8  AvgSpecLen:158.0  StepTime:0.91  LR:0.000100\n",
      "   | > Step:68/68  GlobalStep:99360  TotalLoss:0.00286  PostnetLoss:0.00138  DecoderLoss:0.00148  StopLoss:0.02420  GradNorm:0.00304  GradNormST:0.00843  AvgTextLen:103.0  AvgSpecLen:308.9  StepTime:1.85  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:99360  AvgTotalLoss:0.04061  AvgPostnetLoss:0.00103  AvgDecoderLoss:0.00110  AvgStopLoss:0.03848  EpochTime:42.78  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.07619   PostnetLoss: 0.00498   DecoderLoss:0.00534  StopLoss: 0.06587  \n",
      "   | > TotalLoss: 0.06342   PostnetLoss: 0.00739   DecoderLoss:0.00788  StopLoss: 0.04815  \n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00103   Validation Loss: 0.00691\n",
      "\n",
      " > Epoch 440/1000\n",
      "   | > Step:9/68  GlobalStep:99370  TotalLoss:0.00161  PostnetLoss:0.00078  DecoderLoss:0.00083  StopLoss:0.03703  GradNorm:0.00402  GradNormST:0.01048  AvgTextLen:9.0  AvgSpecLen:42.1  StepTime:0.38  LR:0.000100\n",
      "   | > Step:19/68  GlobalStep:99380  TotalLoss:0.00182  PostnetLoss:0.00088  DecoderLoss:0.00093  StopLoss:0.06305  GradNorm:0.00342  GradNormST:0.01937  AvgTextLen:15.5  AvgSpecLen:63.2  StepTime:0.42  LR:0.000100\n",
      "   | > Step:29/68  GlobalStep:99390  TotalLoss:0.00200  PostnetLoss:0.00097  DecoderLoss:0.00103  StopLoss:0.04940  GradNorm:0.00304  GradNormST:0.01071  AvgTextLen:21.8  AvgSpecLen:83.4  StepTime:0.55  LR:0.000100\n",
      "   | > Step:39/68  GlobalStep:99400  TotalLoss:0.00221  PostnetLoss:0.00108  DecoderLoss:0.00114  StopLoss:0.03085  GradNorm:0.00291  GradNormST:0.01043  AvgTextLen:30.2  AvgSpecLen:110.4  StepTime:0.67  LR:0.000100\n",
      "   | > Step:49/68  GlobalStep:99410  TotalLoss:0.00237  PostnetLoss:0.00115  DecoderLoss:0.00122  StopLoss:0.03095  GradNorm:0.00295  GradNormST:0.00674  AvgTextLen:38.2  AvgSpecLen:127.7  StepTime:0.73  LR:0.000100\n",
      "   | > Step:59/68  GlobalStep:99420  TotalLoss:0.00257  PostnetLoss:0.00124  DecoderLoss:0.00133  StopLoss:0.04141  GradNorm:0.00350  GradNormST:0.01094  AvgTextLen:50.5  AvgSpecLen:155.9  StepTime:0.70  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:99429  AvgTotalLoss:0.04088  AvgPostnetLoss:0.00104  AvgDecoderLoss:0.00110  AvgStopLoss:0.03875  EpochTime:42.74  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.07823   PostnetLoss: 0.00500   DecoderLoss:0.00535  StopLoss: 0.06788  \n",
      "   | > TotalLoss: 0.06841   PostnetLoss: 0.00766   DecoderLoss:0.00816  StopLoss: 0.05259  \n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00104   Validation Loss: 0.00707\n",
      "\n",
      " > Epoch 441/1000\n",
      "   | > Step:0/68  GlobalStep:99430  TotalLoss:0.00159  PostnetLoss:0.00076  DecoderLoss:0.00082  StopLoss:0.06397  GradNorm:0.00758  GradNormST:0.02680  AvgTextLen:3.0  AvgSpecLen:25.3  StepTime:0.39  LR:0.000100\n",
      "   | > Step:10/68  GlobalStep:99440  TotalLoss:0.00167  PostnetLoss:0.00081  DecoderLoss:0.00086  StopLoss:0.03284  GradNorm:0.00481  GradNormST:0.01244  AvgTextLen:9.8  AvgSpecLen:48.6  StepTime:0.52  LR:0.000100\n",
      "   | > Step:20/68  GlobalStep:99450  TotalLoss:0.00189  PostnetLoss:0.00092  DecoderLoss:0.00097  StopLoss:0.03396  GradNorm:0.00381  GradNormST:0.01726  AvgTextLen:16.4  AvgSpecLen:64.8  StepTime:0.49  LR:0.000100\n",
      "   | > Step:30/68  GlobalStep:99460  TotalLoss:0.00204  PostnetLoss:0.00099  DecoderLoss:0.00105  StopLoss:0.04543  GradNorm:0.00312  GradNormST:0.00858  AvgTextLen:22.9  AvgSpecLen:85.8  StepTime:0.48  LR:0.000100\n",
      "   | > Step:40/68  GlobalStep:99470  TotalLoss:0.00220  PostnetLoss:0.00107  DecoderLoss:0.00113  StopLoss:0.03712  GradNorm:0.00297  GradNormST:0.00917  AvgTextLen:30.9  AvgSpecLen:105.3  StepTime:0.60  LR:0.000100\n",
      "   | > Step:50/68  GlobalStep:99480  TotalLoss:0.00242  PostnetLoss:0.00117  DecoderLoss:0.00125  StopLoss:0.02848  GradNorm:0.00329  GradNormST:0.00609  AvgTextLen:38.8  AvgSpecLen:132.0  StepTime:0.79  LR:0.000100\n",
      "   | > Step:60/68  GlobalStep:99490  TotalLoss:0.00263  PostnetLoss:0.00128  DecoderLoss:0.00136  StopLoss:0.02709  GradNorm:0.00330  GradNormST:0.00561  AvgTextLen:52.0  AvgSpecLen:172.7  StepTime:0.94  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:99498  AvgTotalLoss:0.04024  AvgPostnetLoss:0.00103  AvgDecoderLoss:0.00109  AvgStopLoss:0.03812  EpochTime:42.85  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.08145   PostnetLoss: 0.00496   DecoderLoss:0.00532  StopLoss: 0.07117  \n",
      "   | > TotalLoss: 0.07126   PostnetLoss: 0.00801   DecoderLoss:0.00854  StopLoss: 0.05471  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00103   Validation Loss: 0.00729\n",
      "\n",
      " > Epoch 442/1000\n",
      "   | > Step:1/68  GlobalStep:99500  TotalLoss:0.00149  PostnetLoss:0.00072  DecoderLoss:0.00077  StopLoss:0.03551  GradNorm:0.00598  GradNormST:0.01176  AvgTextLen:3.7  AvgSpecLen:28.6  StepTime:0.40  LR:0.000100\n",
      "   | > Step:11/68  GlobalStep:99510  TotalLoss:0.00164  PostnetLoss:0.00080  DecoderLoss:0.00084  StopLoss:0.03432  GradNorm:0.00396  GradNormST:0.01457  AvgTextLen:10.3  AvgSpecLen:49.4  StepTime:0.40  LR:0.000100\n",
      "   | > Step:21/68  GlobalStep:99520  TotalLoss:0.00189  PostnetLoss:0.00092  DecoderLoss:0.00097  StopLoss:0.04059  GradNorm:0.00384  GradNormST:0.01737  AvgTextLen:16.6  AvgSpecLen:66.3  StepTime:0.43  LR:0.000100\n",
      "   | > Step:31/68  GlobalStep:99530  TotalLoss:0.00203  PostnetLoss:0.00099  DecoderLoss:0.00104  StopLoss:0.03714  GradNorm:0.00320  GradNormST:0.01122  AvgTextLen:24.1  AvgSpecLen:82.7  StepTime:0.52  LR:0.000100\n",
      "   | > Step:41/68  GlobalStep:99540  TotalLoss:0.00226  PostnetLoss:0.00110  DecoderLoss:0.00116  StopLoss:0.03162  GradNorm:0.00294  GradNormST:0.01278  AvgTextLen:32.3  AvgSpecLen:109.4  StepTime:0.62  LR:0.000100\n",
      "   | > Step:51/68  GlobalStep:99550  TotalLoss:0.00238  PostnetLoss:0.00115  DecoderLoss:0.00123  StopLoss:0.03314  GradNorm:0.00304  GradNormST:0.00590  AvgTextLen:39.2  AvgSpecLen:126.9  StepTime:0.74  LR:0.000100\n",
      "   | > Step:61/68  GlobalStep:99560  TotalLoss:0.00262  PostnetLoss:0.00127  DecoderLoss:0.00135  StopLoss:0.02839  GradNorm:0.00421  GradNormST:0.00810  AvgTextLen:54.6  AvgSpecLen:165.5  StepTime:0.93  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:99567  AvgTotalLoss:0.04241  AvgPostnetLoss:0.00103  AvgDecoderLoss:0.00109  AvgStopLoss:0.04029  EpochTime:42.12  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.08177   PostnetLoss: 0.00501   DecoderLoss:0.00536  StopLoss: 0.07140  \n",
      "   | > TotalLoss: 0.06898   PostnetLoss: 0.00781   DecoderLoss:0.00834  StopLoss: 0.05283  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00103   Validation Loss: 0.00712\n",
      "\n",
      " > Epoch 443/1000\n",
      "   | > Step:2/68  GlobalStep:99570  TotalLoss:0.00135  PostnetLoss:0.00066  DecoderLoss:0.00069  StopLoss:0.04147  GradNorm:0.00439  GradNormST:0.01712  AvgTextLen:4.0  AvgSpecLen:31.3  StepTime:0.34  LR:0.000100\n",
      "   | > Step:12/68  GlobalStep:99580  TotalLoss:0.00160  PostnetLoss:0.00078  DecoderLoss:0.00082  StopLoss:0.05130  GradNorm:0.00337  GradNormST:0.01808  AvgTextLen:11.0  AvgSpecLen:49.3  StepTime:0.44  LR:0.000100\n",
      "   | > Step:22/68  GlobalStep:99590  TotalLoss:0.00190  PostnetLoss:0.00092  DecoderLoss:0.00097  StopLoss:0.06149  GradNorm:0.00340  GradNormST:0.01923  AvgTextLen:18.0  AvgSpecLen:72.0  StepTime:0.46  LR:0.000100\n",
      "   | > Step:32/68  GlobalStep:99600  TotalLoss:0.00203  PostnetLoss:0.00099  DecoderLoss:0.00105  StopLoss:0.05235  GradNorm:0.00294  GradNormST:0.01227  AvgTextLen:24.4  AvgSpecLen:91.5  StepTime:0.59  LR:0.000100\n",
      "   | > Step:42/68  GlobalStep:99610  TotalLoss:0.00223  PostnetLoss:0.00108  DecoderLoss:0.00115  StopLoss:0.04682  GradNorm:0.00303  GradNormST:0.01189  AvgTextLen:33.1  AvgSpecLen:109.7  StepTime:0.63  LR:0.000100\n",
      "   | > Step:52/68  GlobalStep:99620  TotalLoss:0.00245  PostnetLoss:0.00119  DecoderLoss:0.00127  StopLoss:0.03826  GradNorm:0.00309  GradNormST:0.00682  AvgTextLen:42.1  AvgSpecLen:137.5  StepTime:0.80  LR:0.000100\n",
      "   | > Step:62/68  GlobalStep:99630  TotalLoss:0.00268  PostnetLoss:0.00129  DecoderLoss:0.00138  StopLoss:0.04033  GradNorm:0.00357  GradNormST:0.00906  AvgTextLen:54.0  AvgSpecLen:174.7  StepTime:0.74  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:99636  AvgTotalLoss:0.04371  AvgPostnetLoss:0.00103  AvgDecoderLoss:0.00109  AvgStopLoss:0.04159  EpochTime:41.89  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.08199   PostnetLoss: 0.00512   DecoderLoss:0.00548  StopLoss: 0.07139  \n",
      "   | > TotalLoss: 0.07614   PostnetLoss: 0.00837   DecoderLoss:0.00892  StopLoss: 0.05885  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00103   Validation Loss: 0.00754\n",
      "\n",
      " > Epoch 444/1000\n",
      "   | > Step:3/68  GlobalStep:99640  TotalLoss:0.00143  PostnetLoss:0.00069  DecoderLoss:0.00073  StopLoss:0.05602  GradNorm:0.00400  GradNormST:0.01960  AvgTextLen:4.7  AvgSpecLen:29.7  StepTime:0.36  LR:0.000100\n",
      "   | > Step:13/68  GlobalStep:99650  TotalLoss:0.00164  PostnetLoss:0.00080  DecoderLoss:0.00084  StopLoss:0.06623  GradNorm:0.00368  GradNormST:0.02020  AvgTextLen:11.5  AvgSpecLen:55.2  StepTime:0.42  LR:0.000100\n",
      "   | > Step:23/68  GlobalStep:99660  TotalLoss:0.00196  PostnetLoss:0.00096  DecoderLoss:0.00101  StopLoss:0.03589  GradNorm:0.00339  GradNormST:0.00986  AvgTextLen:18.3  AvgSpecLen:72.2  StepTime:0.35  LR:0.000100\n",
      "   | > Step:33/68  GlobalStep:99670  TotalLoss:0.00212  PostnetLoss:0.00103  DecoderLoss:0.00109  StopLoss:0.03550  GradNorm:0.00307  GradNormST:0.00806  AvgTextLen:25.9  AvgSpecLen:88.9  StepTime:0.44  LR:0.000100\n",
      "   | > Step:43/68  GlobalStep:99680  TotalLoss:0.00226  PostnetLoss:0.00109  DecoderLoss:0.00116  StopLoss:0.04232  GradNorm:0.00287  GradNormST:0.00669  AvgTextLen:33.4  AvgSpecLen:115.7  StepTime:0.63  LR:0.000100\n",
      "   | > Step:53/68  GlobalStep:99690  TotalLoss:0.00243  PostnetLoss:0.00117  DecoderLoss:0.00125  StopLoss:0.03272  GradNorm:0.00297  GradNormST:0.00723  AvgTextLen:42.4  AvgSpecLen:137.6  StepTime:0.63  LR:0.000100\n",
      "   | > Step:63/68  GlobalStep:99700  TotalLoss:0.00276  PostnetLoss:0.00134  DecoderLoss:0.00142  StopLoss:0.02509  GradNorm:0.00411  GradNormST:0.00466  AvgTextLen:58.6  AvgSpecLen:185.1  StepTime:0.90  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:99705  AvgTotalLoss:0.04266  AvgPostnetLoss:0.00103  AvgDecoderLoss:0.00109  AvgStopLoss:0.04055  EpochTime:41.25  AvgStepTime:0.60\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.08103   PostnetLoss: 0.00501   DecoderLoss:0.00538  StopLoss: 0.07064  \n",
      "   | > TotalLoss: 0.07727   PostnetLoss: 0.00809   DecoderLoss:0.00864  StopLoss: 0.06054  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00103   Validation Loss: 0.00742\n",
      "\n",
      " > Epoch 445/1000\n",
      "   | > Step:4/68  GlobalStep:99710  TotalLoss:0.00149  PostnetLoss:0.00073  DecoderLoss:0.00076  StopLoss:0.08094  GradNorm:0.00455  GradNormST:0.03533  AvgTextLen:4.8  AvgSpecLen:31.1  StepTime:0.22  LR:0.000100\n",
      "   | > Step:14/68  GlobalStep:99720  TotalLoss:0.00170  PostnetLoss:0.00083  DecoderLoss:0.00087  StopLoss:0.05233  GradNorm:0.00394  GradNormST:0.01332  AvgTextLen:12.2  AvgSpecLen:52.0  StepTime:0.34  LR:0.000100\n",
      "   | > Step:24/68  GlobalStep:99730  TotalLoss:0.00203  PostnetLoss:0.00099  DecoderLoss:0.00104  StopLoss:0.04065  GradNorm:0.00361  GradNormST:0.00719  AvgTextLen:19.2  AvgSpecLen:79.0  StepTime:0.50  LR:0.000100\n",
      "   | > Step:34/68  GlobalStep:99740  TotalLoss:0.00213  PostnetLoss:0.00103  DecoderLoss:0.00109  StopLoss:0.03495  GradNorm:0.00295  GradNormST:0.00758  AvgTextLen:26.2  AvgSpecLen:95.9  StepTime:0.59  LR:0.000100\n",
      "   | > Step:44/68  GlobalStep:99750  TotalLoss:0.00232  PostnetLoss:0.00113  DecoderLoss:0.00119  StopLoss:0.03252  GradNorm:0.00283  GradNormST:0.00888  AvgTextLen:33.8  AvgSpecLen:116.3  StepTime:0.53  LR:0.000100\n",
      "   | > Step:54/68  GlobalStep:99760  TotalLoss:0.00246  PostnetLoss:0.00119  DecoderLoss:0.00127  StopLoss:0.03393  GradNorm:0.00312  GradNormST:0.00700  AvgTextLen:42.2  AvgSpecLen:144.9  StepTime:0.87  LR:0.000100\n",
      "   | > Step:64/68  GlobalStep:99770  TotalLoss:0.00266  PostnetLoss:0.00128  DecoderLoss:0.00137  StopLoss:0.03130  GradNorm:0.00441  GradNormST:0.00833  AvgTextLen:60.6  AvgSpecLen:177.6  StepTime:0.85  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:99774  AvgTotalLoss:0.04263  AvgPostnetLoss:0.00103  AvgDecoderLoss:0.00109  AvgStopLoss:0.04052  EpochTime:43.03  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.08400   PostnetLoss: 0.00512   DecoderLoss:0.00548  StopLoss: 0.07340  \n",
      "   | > TotalLoss: 0.07848   PostnetLoss: 0.00877   DecoderLoss:0.00931  StopLoss: 0.06040  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00103   Validation Loss: 0.00779\n",
      "\n",
      " > Epoch 446/1000\n",
      "   | > Step:5/68  GlobalStep:99780  TotalLoss:0.00150  PostnetLoss:0.00073  DecoderLoss:0.00077  StopLoss:0.04770  GradNorm:0.00397  GradNormST:0.00908  AvgTextLen:5.9  AvgSpecLen:33.2  StepTime:0.36  LR:0.000100\n",
      "   | > Step:15/68  GlobalStep:99790  TotalLoss:0.00176  PostnetLoss:0.00086  DecoderLoss:0.00090  StopLoss:0.03870  GradNorm:0.00344  GradNormST:0.01203  AvgTextLen:12.8  AvgSpecLen:59.1  StepTime:0.45  LR:0.000100\n",
      "   | > Step:25/68  GlobalStep:99800  TotalLoss:0.00194  PostnetLoss:0.00094  DecoderLoss:0.00099  StopLoss:0.04093  GradNorm:0.00329  GradNormST:0.00940  AvgTextLen:19.8  AvgSpecLen:73.3  StepTime:0.46  LR:0.000100\n",
      "   | > Step:35/68  GlobalStep:99810  TotalLoss:0.00218  PostnetLoss:0.00106  DecoderLoss:0.00112  StopLoss:0.03624  GradNorm:0.00296  GradNormST:0.01300  AvgTextLen:27.1  AvgSpecLen:104.7  StepTime:0.63  LR:0.000100\n",
      "   | > Step:45/68  GlobalStep:99820  TotalLoss:0.00222  PostnetLoss:0.00108  DecoderLoss:0.00115  StopLoss:0.03755  GradNorm:0.00288  GradNormST:0.00989  AvgTextLen:34.4  AvgSpecLen:118.6  StepTime:0.65  LR:0.000100\n",
      "   | > Step:55/68  GlobalStep:99830  TotalLoss:0.00252  PostnetLoss:0.00122  DecoderLoss:0.00130  StopLoss:0.03065  GradNorm:0.00301  GradNormST:0.00549  AvgTextLen:43.9  AvgSpecLen:145.2  StepTime:0.77  LR:0.000100\n",
      "   | > Step:65/68  GlobalStep:99840  TotalLoss:0.00281  PostnetLoss:0.00136  DecoderLoss:0.00145  StopLoss:0.03078  GradNorm:0.00510  GradNormST:0.00873  AvgTextLen:64.2  AvgSpecLen:207.4  StepTime:1.18  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:99843  AvgTotalLoss:0.04175  AvgPostnetLoss:0.00102  AvgDecoderLoss:0.00109  AvgStopLoss:0.03964  EpochTime:42.81  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.08284   PostnetLoss: 0.00508   DecoderLoss:0.00544  StopLoss: 0.07231  \n",
      "   | > TotalLoss: 0.07513   PostnetLoss: 0.00853   DecoderLoss:0.00908  StopLoss: 0.05752  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00102   Validation Loss: 0.00773\n",
      "\n",
      " > Epoch 447/1000\n",
      "   | > Step:6/68  GlobalStep:99850  TotalLoss:0.00146  PostnetLoss:0.00071  DecoderLoss:0.00075  StopLoss:0.03458  GradNorm:0.00374  GradNormST:0.01110  AvgTextLen:6.0  AvgSpecLen:34.8  StepTime:0.32  LR:0.000100\n",
      "   | > Step:16/68  GlobalStep:99860  TotalLoss:0.00178  PostnetLoss:0.00086  DecoderLoss:0.00091  StopLoss:0.04332  GradNorm:0.00386  GradNormST:0.01347  AvgTextLen:13.8  AvgSpecLen:60.6  StepTime:0.32  LR:0.000100\n",
      "   | > Step:26/68  GlobalStep:99870  TotalLoss:0.00195  PostnetLoss:0.00095  DecoderLoss:0.00100  StopLoss:0.03179  GradNorm:0.00337  GradNormST:0.01007  AvgTextLen:20.3  AvgSpecLen:77.5  StepTime:0.48  LR:0.000100\n",
      "   | > Step:36/68  GlobalStep:99880  TotalLoss:0.00214  PostnetLoss:0.00104  DecoderLoss:0.00110  StopLoss:0.04926  GradNorm:0.00297  GradNormST:0.01035  AvgTextLen:28.5  AvgSpecLen:100.2  StepTime:0.57  LR:0.000100\n",
      "   | > Step:46/68  GlobalStep:99890  TotalLoss:0.00228  PostnetLoss:0.00110  DecoderLoss:0.00117  StopLoss:0.03213  GradNorm:0.00295  GradNormST:0.00830  AvgTextLen:35.4  AvgSpecLen:122.6  StepTime:0.66  LR:0.000100\n",
      "   | > Step:56/68  GlobalStep:99900  TotalLoss:0.00252  PostnetLoss:0.00122  DecoderLoss:0.00130  StopLoss:0.03308  GradNorm:0.00394  GradNormST:0.02180  AvgTextLen:45.8  AvgSpecLen:145.4  StepTime:0.96  LR:0.000100\n",
      "   | > Step:66/68  GlobalStep:99910  TotalLoss:0.00296  PostnetLoss:0.00143  DecoderLoss:0.00153  StopLoss:0.02452  GradNorm:0.00337  GradNormST:0.00981  AvgTextLen:71.2  AvgSpecLen:220.8  StepTime:1.03  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:99912  AvgTotalLoss:0.04062  AvgPostnetLoss:0.00102  AvgDecoderLoss:0.00109  AvgStopLoss:0.03851  EpochTime:42.11  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.08644   PostnetLoss: 0.00509   DecoderLoss:0.00545  StopLoss: 0.07590  \n",
      "   | > TotalLoss: 0.08125   PostnetLoss: 0.00902   DecoderLoss:0.00954  StopLoss: 0.06269  \n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00102   Validation Loss: 0.00799\n",
      "\n",
      " > Epoch 448/1000\n",
      "   | > Step:7/68  GlobalStep:99920  TotalLoss:0.00156  PostnetLoss:0.00076  DecoderLoss:0.00080  StopLoss:0.02867  GradNorm:0.00365  GradNormST:0.02130  AvgTextLen:7.4  AvgSpecLen:44.6  StepTime:0.39  LR:0.000100\n",
      "   | > Step:17/68  GlobalStep:99930  TotalLoss:0.00178  PostnetLoss:0.00087  DecoderLoss:0.00091  StopLoss:0.05632  GradNorm:0.00340  GradNormST:0.01314  AvgTextLen:14.7  AvgSpecLen:66.0  StepTime:0.42  LR:0.000100\n",
      "   | > Step:27/68  GlobalStep:99940  TotalLoss:0.00199  PostnetLoss:0.00097  DecoderLoss:0.00102  StopLoss:0.05174  GradNorm:0.00291  GradNormST:0.01614  AvgTextLen:21.0  AvgSpecLen:80.2  StepTime:0.53  LR:0.000100\n",
      "   | > Step:37/68  GlobalStep:99950  TotalLoss:0.00219  PostnetLoss:0.00106  DecoderLoss:0.00113  StopLoss:0.03106  GradNorm:0.00322  GradNormST:0.00659  AvgTextLen:29.3  AvgSpecLen:102.4  StepTime:0.58  LR:0.000100\n",
      "   | > Step:47/68  GlobalStep:99960  TotalLoss:0.00238  PostnetLoss:0.00115  DecoderLoss:0.00123  StopLoss:0.03323  GradNorm:0.00274  GradNormST:0.00819  AvgTextLen:36.7  AvgSpecLen:126.9  StepTime:0.76  LR:0.000100\n",
      "   | > Step:57/68  GlobalStep:99970  TotalLoss:0.00254  PostnetLoss:0.00123  DecoderLoss:0.00131  StopLoss:0.03192  GradNorm:0.00276  GradNormST:0.00758  AvgTextLen:47.1  AvgSpecLen:161.0  StepTime:0.78  LR:0.000100\n",
      "   | > Step:67/68  GlobalStep:99980  TotalLoss:0.00294  PostnetLoss:0.00142  DecoderLoss:0.00152  StopLoss:0.02791  GradNorm:0.00478  GradNormST:0.01229  AvgTextLen:80.6  AvgSpecLen:246.5  StepTime:1.21  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:99981  AvgTotalLoss:0.04053  AvgPostnetLoss:0.00102  AvgDecoderLoss:0.00108  AvgStopLoss:0.03843  EpochTime:42.16  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.07823   PostnetLoss: 0.00510   DecoderLoss:0.00546  StopLoss: 0.06768  \n",
      "   | > TotalLoss: 0.07339   PostnetLoss: 0.00843   DecoderLoss:0.00897  StopLoss: 0.05599  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00102   Validation Loss: 0.00759\n",
      "\n",
      " > Epoch 449/1000\n",
      "   | > Step:8/68  GlobalStep:99990  TotalLoss:0.00161  PostnetLoss:0.00079  DecoderLoss:0.00083  StopLoss:0.05434  GradNorm:0.00430  GradNormST:0.01844  AvgTextLen:8.3  AvgSpecLen:44.0  StepTime:0.24  LR:0.000100\n",
      "   | > Step:18/68  GlobalStep:100000  TotalLoss:0.00177  PostnetLoss:0.00086  DecoderLoss:0.00091  StopLoss:0.05333  GradNorm:0.00401  GradNormST:0.00988  AvgTextLen:15.2  AvgSpecLen:64.5  StepTime:0.41  LR:0.000100\n",
      " | | > Checkpoint saving : /vm/TTS/models/cartman_models/mozilla-no-loc-fattn-stopnet-sigmoid-loss_masking-August-15-2019_11+26AM-de6aeb9/checkpoint_100000.pth.tar\n",
      "   | > Step:28/68  GlobalStep:100010  TotalLoss:0.00202  PostnetLoss:0.00098  DecoderLoss:0.00104  StopLoss:0.04006  GradNorm:0.00343  GradNormST:0.00823  AvgTextLen:22.1  AvgSpecLen:85.4  StepTime:0.55  LR:0.000100\n",
      "   | > Step:38/68  GlobalStep:100020  TotalLoss:0.00217  PostnetLoss:0.00105  DecoderLoss:0.00112  StopLoss:0.02885  GradNorm:0.00326  GradNormST:0.00622  AvgTextLen:29.4  AvgSpecLen:104.1  StepTime:0.54  LR:0.000100\n",
      "   | > Step:48/68  GlobalStep:100030  TotalLoss:0.00234  PostnetLoss:0.00114  DecoderLoss:0.00121  StopLoss:0.03517  GradNorm:0.00299  GradNormST:0.00787  AvgTextLen:37.5  AvgSpecLen:123.8  StepTime:0.66  LR:0.000100\n",
      "   | > Step:58/68  GlobalStep:100040  TotalLoss:0.00259  PostnetLoss:0.00126  DecoderLoss:0.00134  StopLoss:0.03650  GradNorm:0.00347  GradNormST:0.01015  AvgTextLen:47.8  AvgSpecLen:158.0  StepTime:0.93  LR:0.000100\n",
      "   | > Step:68/68  GlobalStep:100050  TotalLoss:0.00290  PostnetLoss:0.00140  DecoderLoss:0.00150  StopLoss:0.02175  GradNorm:0.00277  GradNormST:0.00796  AvgTextLen:103.0  AvgSpecLen:308.9  StepTime:1.90  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:100050  AvgTotalLoss:0.04179  AvgPostnetLoss:0.00102  AvgDecoderLoss:0.00108  AvgStopLoss:0.03969  EpochTime:42.02  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.07960   PostnetLoss: 0.00512   DecoderLoss:0.00547  StopLoss: 0.06901  \n",
      "   | > TotalLoss: 0.07504   PostnetLoss: 0.00891   DecoderLoss:0.00940  StopLoss: 0.05673  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00102   Validation Loss: 0.00789\n",
      "\n",
      " > Epoch 450/1000\n",
      "   | > Step:9/68  GlobalStep:100060  TotalLoss:0.00156  PostnetLoss:0.00076  DecoderLoss:0.00080  StopLoss:0.03720  GradNorm:0.00390  GradNormST:0.01337  AvgTextLen:9.0  AvgSpecLen:42.1  StepTime:0.40  LR:0.000100\n",
      "   | > Step:19/68  GlobalStep:100070  TotalLoss:0.00175  PostnetLoss:0.00086  DecoderLoss:0.00090  StopLoss:0.05037  GradNorm:0.00322  GradNormST:0.01895  AvgTextLen:15.5  AvgSpecLen:63.2  StepTime:0.36  LR:0.000100\n",
      "   | > Step:29/68  GlobalStep:100080  TotalLoss:0.00196  PostnetLoss:0.00096  DecoderLoss:0.00101  StopLoss:0.05105  GradNorm:0.00333  GradNormST:0.01246  AvgTextLen:21.8  AvgSpecLen:83.4  StepTime:0.59  LR:0.000100\n",
      "   | > Step:39/68  GlobalStep:100090  TotalLoss:0.00221  PostnetLoss:0.00107  DecoderLoss:0.00114  StopLoss:0.03052  GradNorm:0.00299  GradNormST:0.01085  AvgTextLen:30.2  AvgSpecLen:110.4  StepTime:0.64  LR:0.000100\n",
      "   | > Step:49/68  GlobalStep:100100  TotalLoss:0.00235  PostnetLoss:0.00114  DecoderLoss:0.00121  StopLoss:0.02828  GradNorm:0.00279  GradNormST:0.00622  AvgTextLen:38.2  AvgSpecLen:127.7  StepTime:0.72  LR:0.000100\n",
      "   | > Step:59/68  GlobalStep:100110  TotalLoss:0.00252  PostnetLoss:0.00122  DecoderLoss:0.00130  StopLoss:0.03482  GradNorm:0.00294  GradNormST:0.00859  AvgTextLen:50.5  AvgSpecLen:155.9  StepTime:0.86  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:100119  AvgTotalLoss:0.04103  AvgPostnetLoss:0.00102  AvgDecoderLoss:0.00108  AvgStopLoss:0.03893  EpochTime:42.58  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.07519   PostnetLoss: 0.00505   DecoderLoss:0.00541  StopLoss: 0.06474  \n",
      "   | > TotalLoss: 0.07136   PostnetLoss: 0.00817   DecoderLoss:0.00870  StopLoss: 0.05449  \n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00102   Validation Loss: 0.00742\n",
      "\n",
      " > Epoch 451/1000\n",
      "   | > Step:0/68  GlobalStep:100120  TotalLoss:0.00158  PostnetLoss:0.00076  DecoderLoss:0.00082  StopLoss:0.05750  GradNorm:0.00662  GradNormST:0.01756  AvgTextLen:3.0  AvgSpecLen:25.3  StepTime:0.54  LR:0.000100\n",
      "   | > Step:10/68  GlobalStep:100130  TotalLoss:0.00163  PostnetLoss:0.00079  DecoderLoss:0.00084  StopLoss:0.04784  GradNorm:0.00392  GradNormST:0.01463  AvgTextLen:9.8  AvgSpecLen:48.6  StepTime:0.48  LR:0.000100\n",
      "   | > Step:20/68  GlobalStep:100140  TotalLoss:0.00188  PostnetLoss:0.00092  DecoderLoss:0.00097  StopLoss:0.03259  GradNorm:0.00364  GradNormST:0.01752  AvgTextLen:16.4  AvgSpecLen:64.8  StepTime:0.47  LR:0.000100\n",
      "   | > Step:30/68  GlobalStep:100150  TotalLoss:0.00201  PostnetLoss:0.00098  DecoderLoss:0.00103  StopLoss:0.03867  GradNorm:0.00328  GradNormST:0.00745  AvgTextLen:22.9  AvgSpecLen:85.8  StepTime:0.57  LR:0.000100\n",
      "   | > Step:40/68  GlobalStep:100160  TotalLoss:0.00219  PostnetLoss:0.00107  DecoderLoss:0.00113  StopLoss:0.03895  GradNorm:0.00289  GradNormST:0.00880  AvgTextLen:30.9  AvgSpecLen:105.3  StepTime:0.57  LR:0.000100\n",
      "   | > Step:50/68  GlobalStep:100170  TotalLoss:0.00239  PostnetLoss:0.00115  DecoderLoss:0.00123  StopLoss:0.02828  GradNorm:0.00289  GradNormST:0.00846  AvgTextLen:38.8  AvgSpecLen:132.0  StepTime:0.67  LR:0.000100\n",
      "   | > Step:60/68  GlobalStep:100180  TotalLoss:0.00261  PostnetLoss:0.00126  DecoderLoss:0.00135  StopLoss:0.02798  GradNorm:0.00405  GradNormST:0.00718  AvgTextLen:52.0  AvgSpecLen:172.7  StepTime:0.78  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:100188  AvgTotalLoss:0.04144  AvgPostnetLoss:0.00102  AvgDecoderLoss:0.00108  AvgStopLoss:0.03933  EpochTime:41.84  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.07405   PostnetLoss: 0.00499   DecoderLoss:0.00535  StopLoss: 0.06371  \n",
      "   | > TotalLoss: 0.06867   PostnetLoss: 0.00781   DecoderLoss:0.00830  StopLoss: 0.05256  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00102   Validation Loss: 0.00719\n",
      "\n",
      " > Epoch 452/1000\n",
      "   | > Step:1/68  GlobalStep:100190  TotalLoss:0.00148  PostnetLoss:0.00072  DecoderLoss:0.00077  StopLoss:0.03542  GradNorm:0.00464  GradNormST:0.01211  AvgTextLen:3.7  AvgSpecLen:28.6  StepTime:0.27  LR:0.000100\n",
      "   | > Step:11/68  GlobalStep:100200  TotalLoss:0.00165  PostnetLoss:0.00081  DecoderLoss:0.00085  StopLoss:0.03787  GradNorm:0.00388  GradNormST:0.00844  AvgTextLen:10.3  AvgSpecLen:49.4  StepTime:0.39  LR:0.000100\n",
      "   | > Step:21/68  GlobalStep:100210  TotalLoss:0.00185  PostnetLoss:0.00090  DecoderLoss:0.00095  StopLoss:0.03747  GradNorm:0.00341  GradNormST:0.01115  AvgTextLen:16.6  AvgSpecLen:66.3  StepTime:0.43  LR:0.000100\n",
      "   | > Step:31/68  GlobalStep:100220  TotalLoss:0.00202  PostnetLoss:0.00098  DecoderLoss:0.00104  StopLoss:0.03466  GradNorm:0.00324  GradNormST:0.01272  AvgTextLen:24.1  AvgSpecLen:82.7  StepTime:0.53  LR:0.000100\n",
      "   | > Step:41/68  GlobalStep:100230  TotalLoss:0.00227  PostnetLoss:0.00111  DecoderLoss:0.00117  StopLoss:0.02759  GradNorm:0.00373  GradNormST:0.00878  AvgTextLen:32.3  AvgSpecLen:109.4  StepTime:0.63  LR:0.000100\n",
      "   | > Step:51/68  GlobalStep:100240  TotalLoss:0.00236  PostnetLoss:0.00114  DecoderLoss:0.00122  StopLoss:0.02917  GradNorm:0.00317  GradNormST:0.00750  AvgTextLen:39.2  AvgSpecLen:126.9  StepTime:0.79  LR:0.000100\n",
      "   | > Step:61/68  GlobalStep:100250  TotalLoss:0.00254  PostnetLoss:0.00123  DecoderLoss:0.00131  StopLoss:0.02466  GradNorm:0.00293  GradNormST:0.00834  AvgTextLen:54.6  AvgSpecLen:165.5  StepTime:0.90  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:100257  AvgTotalLoss:0.04209  AvgPostnetLoss:0.00102  AvgDecoderLoss:0.00108  AvgStopLoss:0.03999  EpochTime:42.56  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.07849   PostnetLoss: 0.00506   DecoderLoss:0.00541  StopLoss: 0.06802  \n",
      "   | > TotalLoss: 0.07585   PostnetLoss: 0.00798   DecoderLoss:0.00851  StopLoss: 0.05936  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00102   Validation Loss: 0.00728\n",
      "\n",
      " > Epoch 453/1000\n",
      "   | > Step:2/68  GlobalStep:100260  TotalLoss:0.00137  PostnetLoss:0.00067  DecoderLoss:0.00070  StopLoss:0.06011  GradNorm:0.00390  GradNormST:0.01948  AvgTextLen:4.0  AvgSpecLen:31.3  StepTime:0.45  LR:0.000100\n",
      "   | > Step:12/68  GlobalStep:100270  TotalLoss:0.00162  PostnetLoss:0.00079  DecoderLoss:0.00083  StopLoss:0.04178  GradNorm:0.00426  GradNormST:0.00836  AvgTextLen:11.0  AvgSpecLen:49.3  StepTime:0.45  LR:0.000100\n",
      "   | > Step:22/68  GlobalStep:100280  TotalLoss:0.00190  PostnetLoss:0.00092  DecoderLoss:0.00097  StopLoss:0.04628  GradNorm:0.00335  GradNormST:0.01461  AvgTextLen:18.0  AvgSpecLen:72.0  StepTime:0.46  LR:0.000100\n",
      "   | > Step:32/68  GlobalStep:100290  TotalLoss:0.00206  PostnetLoss:0.00100  DecoderLoss:0.00106  StopLoss:0.05237  GradNorm:0.00324  GradNormST:0.01127  AvgTextLen:24.4  AvgSpecLen:91.5  StepTime:0.46  LR:0.000100\n",
      "   | > Step:42/68  GlobalStep:100300  TotalLoss:0.00224  PostnetLoss:0.00109  DecoderLoss:0.00115  StopLoss:0.04124  GradNorm:0.00317  GradNormST:0.01523  AvgTextLen:33.1  AvgSpecLen:109.7  StepTime:0.64  LR:0.000100\n",
      "   | > Step:52/68  GlobalStep:100310  TotalLoss:0.00240  PostnetLoss:0.00117  DecoderLoss:0.00124  StopLoss:0.03918  GradNorm:0.00294  GradNormST:0.00925  AvgTextLen:42.1  AvgSpecLen:137.5  StepTime:0.80  LR:0.000100\n",
      "   | > Step:62/68  GlobalStep:100320  TotalLoss:0.00262  PostnetLoss:0.00127  DecoderLoss:0.00135  StopLoss:0.03362  GradNorm:0.00258  GradNormST:0.00873  AvgTextLen:54.0  AvgSpecLen:174.7  StepTime:0.76  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:100326  AvgTotalLoss:0.04401  AvgPostnetLoss:0.00102  AvgDecoderLoss:0.00108  AvgStopLoss:0.04191  EpochTime:42.62  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.07541   PostnetLoss: 0.00499   DecoderLoss:0.00534  StopLoss: 0.06509  \n",
      "   | > TotalLoss: 0.06958   PostnetLoss: 0.00791   DecoderLoss:0.00840  StopLoss: 0.05327  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00102   Validation Loss: 0.00720\n",
      "\n",
      " > Epoch 454/1000\n",
      "   | > Step:3/68  GlobalStep:100330  TotalLoss:0.00140  PostnetLoss:0.00068  DecoderLoss:0.00071  StopLoss:0.08296  GradNorm:0.00365  GradNormST:0.04096  AvgTextLen:4.7  AvgSpecLen:29.7  StepTime:0.30  LR:0.000100\n",
      "   | > Step:13/68  GlobalStep:100340  TotalLoss:0.00161  PostnetLoss:0.00079  DecoderLoss:0.00083  StopLoss:0.05981  GradNorm:0.00345  GradNormST:0.02101  AvgTextLen:11.5  AvgSpecLen:55.2  StepTime:0.35  LR:0.000100\n",
      "   | > Step:23/68  GlobalStep:100350  TotalLoss:0.00192  PostnetLoss:0.00093  DecoderLoss:0.00098  StopLoss:0.04448  GradNorm:0.00314  GradNormST:0.01140  AvgTextLen:18.3  AvgSpecLen:72.2  StepTime:0.37  LR:0.000100\n",
      "   | > Step:33/68  GlobalStep:100360  TotalLoss:0.00211  PostnetLoss:0.00103  DecoderLoss:0.00108  StopLoss:0.04057  GradNorm:0.00314  GradNormST:0.01233  AvgTextLen:25.9  AvgSpecLen:88.9  StepTime:0.54  LR:0.000100\n",
      "   | > Step:43/68  GlobalStep:100370  TotalLoss:0.00222  PostnetLoss:0.00108  DecoderLoss:0.00114  StopLoss:0.04471  GradNorm:0.00286  GradNormST:0.01100  AvgTextLen:33.4  AvgSpecLen:115.7  StepTime:0.53  LR:0.000100\n",
      "   | > Step:53/68  GlobalStep:100380  TotalLoss:0.00238  PostnetLoss:0.00115  DecoderLoss:0.00123  StopLoss:0.03502  GradNorm:0.00300  GradNormST:0.01070  AvgTextLen:42.4  AvgSpecLen:137.6  StepTime:0.81  LR:0.000100\n",
      "   | > Step:63/68  GlobalStep:100390  TotalLoss:0.00271  PostnetLoss:0.00131  DecoderLoss:0.00140  StopLoss:0.02308  GradNorm:0.00263  GradNormST:0.00647  AvgTextLen:58.6  AvgSpecLen:185.1  StepTime:0.90  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:100395  AvgTotalLoss:0.04264  AvgPostnetLoss:0.00101  AvgDecoderLoss:0.00108  AvgStopLoss:0.04055  EpochTime:42.70  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.07559   PostnetLoss: 0.00496   DecoderLoss:0.00532  StopLoss: 0.06530  \n",
      "   | > TotalLoss: 0.07380   PostnetLoss: 0.00776   DecoderLoss:0.00828  StopLoss: 0.05776  \n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00101   Validation Loss: 0.00724\n",
      "\n",
      " > Epoch 455/1000\n",
      "   | > Step:4/68  GlobalStep:100400  TotalLoss:0.00142  PostnetLoss:0.00069  DecoderLoss:0.00073  StopLoss:0.07792  GradNorm:0.00348  GradNormST:0.03636  AvgTextLen:4.8  AvgSpecLen:31.1  StepTime:0.27  LR:0.000100\n",
      "   | > Step:14/68  GlobalStep:100410  TotalLoss:0.00171  PostnetLoss:0.00083  DecoderLoss:0.00088  StopLoss:0.04657  GradNorm:0.00369  GradNormST:0.01296  AvgTextLen:12.2  AvgSpecLen:52.0  StepTime:0.26  LR:0.000100\n",
      "   | > Step:24/68  GlobalStep:100420  TotalLoss:0.00197  PostnetLoss:0.00096  DecoderLoss:0.00101  StopLoss:0.04397  GradNorm:0.00309  GradNormST:0.00831  AvgTextLen:19.2  AvgSpecLen:79.0  StepTime:0.39  LR:0.000100\n",
      "   | > Step:34/68  GlobalStep:100430  TotalLoss:0.00212  PostnetLoss:0.00103  DecoderLoss:0.00109  StopLoss:0.03006  GradNorm:0.00299  GradNormST:0.00749  AvgTextLen:26.2  AvgSpecLen:95.9  StepTime:0.62  LR:0.000100\n",
      "   | > Step:44/68  GlobalStep:100440  TotalLoss:0.00227  PostnetLoss:0.00110  DecoderLoss:0.00117  StopLoss:0.02594  GradNorm:0.00285  GradNormST:0.00538  AvgTextLen:33.8  AvgSpecLen:116.3  StepTime:0.66  LR:0.000100\n",
      "   | > Step:54/68  GlobalStep:100450  TotalLoss:0.00246  PostnetLoss:0.00119  DecoderLoss:0.00127  StopLoss:0.03347  GradNorm:0.00324  GradNormST:0.00860  AvgTextLen:42.2  AvgSpecLen:144.9  StepTime:0.82  LR:0.000100\n",
      "   | > Step:64/68  GlobalStep:100460  TotalLoss:0.00254  PostnetLoss:0.00123  DecoderLoss:0.00131  StopLoss:0.03264  GradNorm:0.00247  GradNormST:0.00778  AvgTextLen:60.6  AvgSpecLen:177.6  StepTime:0.80  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:100464  AvgTotalLoss:0.04319  AvgPostnetLoss:0.00101  AvgDecoderLoss:0.00107  AvgStopLoss:0.04110  EpochTime:41.69  AvgStepTime:0.60\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.07973   PostnetLoss: 0.00503   DecoderLoss:0.00538  StopLoss: 0.06933  \n",
      "   | > TotalLoss: 0.07339   PostnetLoss: 0.00794   DecoderLoss:0.00843  StopLoss: 0.05701  \n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00101   Validation Loss: 0.00735\n",
      "\n",
      " > Epoch 456/1000\n",
      "   | > Step:5/68  GlobalStep:100470  TotalLoss:0.00146  PostnetLoss:0.00071  DecoderLoss:0.00075  StopLoss:0.04053  GradNorm:0.00356  GradNormST:0.01013  AvgTextLen:5.9  AvgSpecLen:33.2  StepTime:0.33  LR:0.000100\n",
      "   | > Step:15/68  GlobalStep:100480  TotalLoss:0.00178  PostnetLoss:0.00087  DecoderLoss:0.00091  StopLoss:0.03373  GradNorm:0.00393  GradNormST:0.00877  AvgTextLen:12.8  AvgSpecLen:59.1  StepTime:0.43  LR:0.000100\n",
      "   | > Step:25/68  GlobalStep:100490  TotalLoss:0.00196  PostnetLoss:0.00095  DecoderLoss:0.00101  StopLoss:0.04822  GradNorm:0.00324  GradNormST:0.01040  AvgTextLen:19.8  AvgSpecLen:73.3  StepTime:0.56  LR:0.000100\n",
      "   | > Step:35/68  GlobalStep:100500  TotalLoss:0.00214  PostnetLoss:0.00104  DecoderLoss:0.00110  StopLoss:0.04056  GradNorm:0.00299  GradNormST:0.01983  AvgTextLen:27.1  AvgSpecLen:104.7  StepTime:0.76  LR:0.000100\n",
      "   | > Step:45/68  GlobalStep:100510  TotalLoss:0.00223  PostnetLoss:0.00108  DecoderLoss:0.00115  StopLoss:0.04118  GradNorm:0.00293  GradNormST:0.01590  AvgTextLen:34.4  AvgSpecLen:118.6  StepTime:0.64  LR:0.000100\n",
      "   | > Step:55/68  GlobalStep:100520  TotalLoss:0.00251  PostnetLoss:0.00122  DecoderLoss:0.00130  StopLoss:0.03394  GradNorm:0.00295  GradNormST:0.00684  AvgTextLen:43.9  AvgSpecLen:145.2  StepTime:0.87  LR:0.000100\n",
      "   | > Step:65/68  GlobalStep:100530  TotalLoss:0.00274  PostnetLoss:0.00132  DecoderLoss:0.00141  StopLoss:0.03094  GradNorm:0.00304  GradNormST:0.01129  AvgTextLen:64.2  AvgSpecLen:207.4  StepTime:1.20  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:100533  AvgTotalLoss:0.04273  AvgPostnetLoss:0.00101  AvgDecoderLoss:0.00108  AvgStopLoss:0.04064  EpochTime:42.85  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.07728   PostnetLoss: 0.00506   DecoderLoss:0.00542  StopLoss: 0.06679  \n",
      "   | > TotalLoss: 0.07271   PostnetLoss: 0.00797   DecoderLoss:0.00848  StopLoss: 0.05626  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00101   Validation Loss: 0.00726\n",
      "\n",
      " > Epoch 457/1000\n",
      "   | > Step:6/68  GlobalStep:100540  TotalLoss:0.00146  PostnetLoss:0.00071  DecoderLoss:0.00075  StopLoss:0.03182  GradNorm:0.00397  GradNormST:0.01111  AvgTextLen:6.0  AvgSpecLen:34.8  StepTime:0.32  LR:0.000100\n",
      "   | > Step:16/68  GlobalStep:100550  TotalLoss:0.00178  PostnetLoss:0.00087  DecoderLoss:0.00091  StopLoss:0.04520  GradNorm:0.00380  GradNormST:0.01420  AvgTextLen:13.8  AvgSpecLen:60.6  StepTime:0.37  LR:0.000100\n",
      "   | > Step:26/68  GlobalStep:100560  TotalLoss:0.00199  PostnetLoss:0.00097  DecoderLoss:0.00102  StopLoss:0.03385  GradNorm:0.00344  GradNormST:0.00839  AvgTextLen:20.3  AvgSpecLen:77.5  StepTime:0.47  LR:0.000100\n",
      "   | > Step:36/68  GlobalStep:100570  TotalLoss:0.00209  PostnetLoss:0.00101  DecoderLoss:0.00107  StopLoss:0.07394  GradNorm:0.00296  GradNormST:0.01643  AvgTextLen:28.5  AvgSpecLen:100.2  StepTime:0.55  LR:0.000100\n",
      "   | > Step:46/68  GlobalStep:100580  TotalLoss:0.00227  PostnetLoss:0.00110  DecoderLoss:0.00117  StopLoss:0.03737  GradNorm:0.00286  GradNormST:0.01151  AvgTextLen:35.4  AvgSpecLen:122.6  StepTime:0.88  LR:0.000100\n",
      "   | > Step:56/68  GlobalStep:100590  TotalLoss:0.00247  PostnetLoss:0.00119  DecoderLoss:0.00127  StopLoss:0.03175  GradNorm:0.00275  GradNormST:0.01242  AvgTextLen:45.8  AvgSpecLen:145.4  StepTime:1.04  LR:0.000100\n",
      "   | > Step:66/68  GlobalStep:100600  TotalLoss:0.00289  PostnetLoss:0.00139  DecoderLoss:0.00149  StopLoss:0.02616  GradNorm:0.00259  GradNormST:0.01142  AvgTextLen:71.2  AvgSpecLen:220.8  StepTime:1.03  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:100602  AvgTotalLoss:0.04236  AvgPostnetLoss:0.00101  AvgDecoderLoss:0.00108  AvgStopLoss:0.04027  EpochTime:43.67  AvgStepTime:0.63\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.07531   PostnetLoss: 0.00509   DecoderLoss:0.00545  StopLoss: 0.06477  \n",
      "   | > TotalLoss: 0.06935   PostnetLoss: 0.00792   DecoderLoss:0.00843  StopLoss: 0.05300  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00101   Validation Loss: 0.00735\n",
      "\n",
      " > Epoch 458/1000\n",
      "   | > Step:7/68  GlobalStep:100610  TotalLoss:0.00155  PostnetLoss:0.00076  DecoderLoss:0.00079  StopLoss:0.03466  GradNorm:0.00365  GradNormST:0.01879  AvgTextLen:7.4  AvgSpecLen:44.6  StepTime:0.40  LR:0.000100\n",
      "   | > Step:17/68  GlobalStep:100620  TotalLoss:0.00174  PostnetLoss:0.00085  DecoderLoss:0.00089  StopLoss:0.05305  GradNorm:0.00361  GradNormST:0.01010  AvgTextLen:14.7  AvgSpecLen:66.0  StepTime:0.42  LR:0.000100\n",
      "   | > Step:27/68  GlobalStep:100630  TotalLoss:0.00199  PostnetLoss:0.00097  DecoderLoss:0.00102  StopLoss:0.05481  GradNorm:0.00337  GradNormST:0.02878  AvgTextLen:21.0  AvgSpecLen:80.2  StepTime:0.64  LR:0.000100\n",
      "   | > Step:37/68  GlobalStep:100640  TotalLoss:0.00215  PostnetLoss:0.00104  DecoderLoss:0.00110  StopLoss:0.03439  GradNorm:0.00319  GradNormST:0.00587  AvgTextLen:29.3  AvgSpecLen:102.4  StepTime:0.56  LR:0.000100\n",
      "   | > Step:47/68  GlobalStep:100650  TotalLoss:0.00236  PostnetLoss:0.00115  DecoderLoss:0.00122  StopLoss:0.03394  GradNorm:0.00317  GradNormST:0.00662  AvgTextLen:36.7  AvgSpecLen:126.9  StepTime:0.75  LR:0.000100\n",
      "   | > Step:57/68  GlobalStep:100660  TotalLoss:0.00255  PostnetLoss:0.00123  DecoderLoss:0.00132  StopLoss:0.03333  GradNorm:0.00274  GradNormST:0.00719  AvgTextLen:47.1  AvgSpecLen:161.0  StepTime:0.84  LR:0.000100\n",
      "   | > Step:67/68  GlobalStep:100670  TotalLoss:0.00289  PostnetLoss:0.00139  DecoderLoss:0.00150  StopLoss:0.02826  GradNorm:0.00263  GradNormST:0.00871  AvgTextLen:80.6  AvgSpecLen:246.5  StepTime:1.22  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:100671  AvgTotalLoss:0.04331  AvgPostnetLoss:0.00101  AvgDecoderLoss:0.00108  AvgStopLoss:0.04122  EpochTime:43.12  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.08444   PostnetLoss: 0.00510   DecoderLoss:0.00547  StopLoss: 0.07387  \n",
      "   | > TotalLoss: 0.07319   PostnetLoss: 0.00803   DecoderLoss:0.00858  StopLoss: 0.05659  \n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00101   Validation Loss: 0.00736\n",
      "\n",
      " > Epoch 459/1000\n",
      "   | > Step:8/68  GlobalStep:100680  TotalLoss:0.00158  PostnetLoss:0.00077  DecoderLoss:0.00081  StopLoss:0.05445  GradNorm:0.00437  GradNormST:0.01756  AvgTextLen:8.3  AvgSpecLen:44.0  StepTime:0.25  LR:0.000100\n",
      "   | > Step:18/68  GlobalStep:100690  TotalLoss:0.00176  PostnetLoss:0.00086  DecoderLoss:0.00090  StopLoss:0.04342  GradNorm:0.00349  GradNormST:0.01090  AvgTextLen:15.2  AvgSpecLen:64.5  StepTime:0.32  LR:0.000100\n",
      "   | > Step:28/68  GlobalStep:100700  TotalLoss:0.00200  PostnetLoss:0.00097  DecoderLoss:0.00103  StopLoss:0.04036  GradNorm:0.00338  GradNormST:0.01006  AvgTextLen:22.1  AvgSpecLen:85.4  StepTime:0.55  LR:0.000100\n",
      "   | > Step:38/68  GlobalStep:100710  TotalLoss:0.00215  PostnetLoss:0.00104  DecoderLoss:0.00111  StopLoss:0.03246  GradNorm:0.00343  GradNormST:0.00988  AvgTextLen:29.4  AvgSpecLen:104.1  StepTime:0.61  LR:0.000100\n",
      "   | > Step:48/68  GlobalStep:100720  TotalLoss:0.00233  PostnetLoss:0.00113  DecoderLoss:0.00120  StopLoss:0.03684  GradNorm:0.00311  GradNormST:0.00761  AvgTextLen:37.5  AvgSpecLen:123.8  StepTime:0.67  LR:0.000100\n",
      "   | > Step:58/68  GlobalStep:100730  TotalLoss:0.00255  PostnetLoss:0.00123  DecoderLoss:0.00131  StopLoss:0.03718  GradNorm:0.00296  GradNormST:0.01076  AvgTextLen:47.8  AvgSpecLen:158.0  StepTime:0.71  LR:0.000100\n",
      "   | > Step:68/68  GlobalStep:100740  TotalLoss:0.00287  PostnetLoss:0.00139  DecoderLoss:0.00149  StopLoss:0.02380  GradNorm:0.00358  GradNormST:0.00782  AvgTextLen:103.0  AvgSpecLen:308.9  StepTime:1.87  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:100740  AvgTotalLoss:0.04213  AvgPostnetLoss:0.00102  AvgDecoderLoss:0.00108  AvgStopLoss:0.04004  EpochTime:42.37  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.07757   PostnetLoss: 0.00508   DecoderLoss:0.00545  StopLoss: 0.06704  \n",
      "   | > TotalLoss: 0.07318   PostnetLoss: 0.00817   DecoderLoss:0.00870  StopLoss: 0.05631  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00102   Validation Loss: 0.00747\n",
      "\n",
      " > Epoch 460/1000\n",
      "   | > Step:9/68  GlobalStep:100750  TotalLoss:0.00155  PostnetLoss:0.00076  DecoderLoss:0.00079  StopLoss:0.03306  GradNorm:0.00451  GradNormST:0.01278  AvgTextLen:9.0  AvgSpecLen:42.1  StepTime:0.36  LR:0.000100\n",
      "   | > Step:19/68  GlobalStep:100760  TotalLoss:0.00179  PostnetLoss:0.00087  DecoderLoss:0.00092  StopLoss:0.05060  GradNorm:0.00357  GradNormST:0.01847  AvgTextLen:15.5  AvgSpecLen:63.2  StepTime:0.43  LR:0.000100\n",
      "   | > Step:29/68  GlobalStep:100770  TotalLoss:0.00198  PostnetLoss:0.00097  DecoderLoss:0.00102  StopLoss:0.05382  GradNorm:0.00372  GradNormST:0.01195  AvgTextLen:21.8  AvgSpecLen:83.4  StepTime:0.59  LR:0.000100\n",
      "   | > Step:39/68  GlobalStep:100780  TotalLoss:0.00219  PostnetLoss:0.00106  DecoderLoss:0.00113  StopLoss:0.02966  GradNorm:0.00326  GradNormST:0.00989  AvgTextLen:30.2  AvgSpecLen:110.4  StepTime:0.67  LR:0.000100\n",
      "   | > Step:49/68  GlobalStep:100790  TotalLoss:0.00232  PostnetLoss:0.00113  DecoderLoss:0.00119  StopLoss:0.02879  GradNorm:0.00298  GradNormST:0.00643  AvgTextLen:38.2  AvgSpecLen:127.7  StepTime:0.59  LR:0.000100\n",
      "   | > Step:59/68  GlobalStep:100800  TotalLoss:0.00252  PostnetLoss:0.00122  DecoderLoss:0.00130  StopLoss:0.03929  GradNorm:0.00287  GradNormST:0.01038  AvgTextLen:50.5  AvgSpecLen:155.9  StepTime:0.87  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:100809  AvgTotalLoss:0.04130  AvgPostnetLoss:0.00102  AvgDecoderLoss:0.00108  AvgStopLoss:0.03921  EpochTime:42.24  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.07504   PostnetLoss: 0.00512   DecoderLoss:0.00549  StopLoss: 0.06443  \n",
      "   | > TotalLoss: 0.06536   PostnetLoss: 0.00765   DecoderLoss:0.00816  StopLoss: 0.04956  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00102   Validation Loss: 0.00713\n",
      "\n",
      " > Epoch 461/1000\n",
      "   | > Step:0/68  GlobalStep:100810  TotalLoss:0.00156  PostnetLoss:0.00076  DecoderLoss:0.00081  StopLoss:0.06115  GradNorm:0.00779  GradNormST:0.01656  AvgTextLen:3.0  AvgSpecLen:25.3  StepTime:0.38  LR:0.000100\n",
      "   | > Step:10/68  GlobalStep:100820  TotalLoss:0.00163  PostnetLoss:0.00079  DecoderLoss:0.00083  StopLoss:0.02997  GradNorm:0.00358  GradNormST:0.01045  AvgTextLen:9.8  AvgSpecLen:48.6  StepTime:0.55  LR:0.000100\n",
      "   | > Step:20/68  GlobalStep:100830  TotalLoss:0.00190  PostnetLoss:0.00092  DecoderLoss:0.00097  StopLoss:0.04022  GradNorm:0.00401  GradNormST:0.01505  AvgTextLen:16.4  AvgSpecLen:64.8  StepTime:0.51  LR:0.000100\n",
      "   | > Step:30/68  GlobalStep:100840  TotalLoss:0.00205  PostnetLoss:0.00100  DecoderLoss:0.00105  StopLoss:0.03831  GradNorm:0.00421  GradNormST:0.00961  AvgTextLen:22.9  AvgSpecLen:85.8  StepTime:0.56  LR:0.000100\n",
      "   | > Step:40/68  GlobalStep:100850  TotalLoss:0.00221  PostnetLoss:0.00107  DecoderLoss:0.00113  StopLoss:0.03932  GradNorm:0.00362  GradNormST:0.00772  AvgTextLen:30.9  AvgSpecLen:105.3  StepTime:0.59  LR:0.000100\n",
      "   | > Step:50/68  GlobalStep:100860  TotalLoss:0.00238  PostnetLoss:0.00115  DecoderLoss:0.00123  StopLoss:0.03378  GradNorm:0.00338  GradNormST:0.00912  AvgTextLen:38.8  AvgSpecLen:132.0  StepTime:0.79  LR:0.000100\n",
      "   | > Step:60/68  GlobalStep:100870  TotalLoss:0.00259  PostnetLoss:0.00126  DecoderLoss:0.00134  StopLoss:0.02237  GradNorm:0.00333  GradNormST:0.00508  AvgTextLen:52.0  AvgSpecLen:172.7  StepTime:0.81  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:100878  AvgTotalLoss:0.04128  AvgPostnetLoss:0.00102  AvgDecoderLoss:0.00108  AvgStopLoss:0.03917  EpochTime:42.57  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.07544   PostnetLoss: 0.00506   DecoderLoss:0.00541  StopLoss: 0.06497  \n",
      "   | > TotalLoss: 0.07395   PostnetLoss: 0.00784   DecoderLoss:0.00836  StopLoss: 0.05774  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00102   Validation Loss: 0.00722\n",
      "\n",
      " > Epoch 462/1000\n",
      "   | > Step:1/68  GlobalStep:100880  TotalLoss:0.00147  PostnetLoss:0.00071  DecoderLoss:0.00076  StopLoss:0.04183  GradNorm:0.00501  GradNormST:0.01345  AvgTextLen:3.7  AvgSpecLen:28.6  StepTime:0.26  LR:0.000100\n",
      "   | > Step:11/68  GlobalStep:100890  TotalLoss:0.00161  PostnetLoss:0.00078  DecoderLoss:0.00082  StopLoss:0.03744  GradNorm:0.00382  GradNormST:0.00965  AvgTextLen:10.3  AvgSpecLen:49.4  StepTime:0.30  LR:0.000100\n",
      "   | > Step:21/68  GlobalStep:100900  TotalLoss:0.00182  PostnetLoss:0.00089  DecoderLoss:0.00093  StopLoss:0.04114  GradNorm:0.00345  GradNormST:0.01017  AvgTextLen:16.6  AvgSpecLen:66.3  StepTime:0.42  LR:0.000100\n",
      "   | > Step:31/68  GlobalStep:100910  TotalLoss:0.00205  PostnetLoss:0.00100  DecoderLoss:0.00105  StopLoss:0.03405  GradNorm:0.00404  GradNormST:0.00848  AvgTextLen:24.1  AvgSpecLen:82.7  StepTime:0.56  LR:0.000100\n",
      "   | > Step:41/68  GlobalStep:100920  TotalLoss:0.00229  PostnetLoss:0.00111  DecoderLoss:0.00118  StopLoss:0.03209  GradNorm:0.00417  GradNormST:0.00934  AvgTextLen:32.3  AvgSpecLen:109.4  StepTime:0.56  LR:0.000100\n",
      "   | > Step:51/68  GlobalStep:100930  TotalLoss:0.00237  PostnetLoss:0.00115  DecoderLoss:0.00122  StopLoss:0.03561  GradNorm:0.00395  GradNormST:0.00568  AvgTextLen:39.2  AvgSpecLen:126.9  StepTime:0.69  LR:0.000100\n",
      "   | > Step:61/68  GlobalStep:100940  TotalLoss:0.00258  PostnetLoss:0.00125  DecoderLoss:0.00133  StopLoss:0.02675  GradNorm:0.00324  GradNormST:0.00772  AvgTextLen:54.6  AvgSpecLen:165.5  StepTime:0.93  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:100947  AvgTotalLoss:0.04205  AvgPostnetLoss:0.00102  AvgDecoderLoss:0.00108  AvgStopLoss:0.03995  EpochTime:42.21  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.07864   PostnetLoss: 0.00503   DecoderLoss:0.00539  StopLoss: 0.06822  \n",
      "   | > TotalLoss: 0.07302   PostnetLoss: 0.00801   DecoderLoss:0.00854  StopLoss: 0.05646  \n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00102   Validation Loss: 0.00737\n",
      "\n",
      " > Epoch 463/1000\n",
      "   | > Step:2/68  GlobalStep:100950  TotalLoss:0.00141  PostnetLoss:0.00068  DecoderLoss:0.00072  StopLoss:0.06438  GradNorm:0.00490  GradNormST:0.02225  AvgTextLen:4.0  AvgSpecLen:31.3  StepTime:0.44  LR:0.000100\n",
      "   | > Step:12/68  GlobalStep:100960  TotalLoss:0.00162  PostnetLoss:0.00079  DecoderLoss:0.00083  StopLoss:0.03792  GradNorm:0.00404  GradNormST:0.01099  AvgTextLen:11.0  AvgSpecLen:49.3  StepTime:0.45  LR:0.000100\n",
      "   | > Step:22/68  GlobalStep:100970  TotalLoss:0.00184  PostnetLoss:0.00090  DecoderLoss:0.00094  StopLoss:0.05608  GradNorm:0.00314  GradNormST:0.01414  AvgTextLen:18.0  AvgSpecLen:72.0  StepTime:0.36  LR:0.000100\n",
      "   | > Step:32/68  GlobalStep:100980  TotalLoss:0.00205  PostnetLoss:0.00100  DecoderLoss:0.00105  StopLoss:0.05342  GradNorm:0.00364  GradNormST:0.00856  AvgTextLen:24.4  AvgSpecLen:91.5  StepTime:0.56  LR:0.000100\n",
      "   | > Step:42/68  GlobalStep:100990  TotalLoss:0.00226  PostnetLoss:0.00110  DecoderLoss:0.00116  StopLoss:0.04077  GradNorm:0.00448  GradNormST:0.01554  AvgTextLen:33.1  AvgSpecLen:109.7  StepTime:0.64  LR:0.000100\n",
      "   | > Step:52/68  GlobalStep:101000  TotalLoss:0.00246  PostnetLoss:0.00119  DecoderLoss:0.00127  StopLoss:0.03968  GradNorm:0.00470  GradNormST:0.00689  AvgTextLen:42.1  AvgSpecLen:137.5  StepTime:0.68  LR:0.000100\n",
      " | | > Checkpoint saving : /vm/TTS/models/cartman_models/mozilla-no-loc-fattn-stopnet-sigmoid-loss_masking-August-15-2019_11+26AM-de6aeb9/checkpoint_101000.pth.tar\n",
      "warning: audio amplitude out of range, auto clipped.\n",
      "   | > Step:62/68  GlobalStep:101010  TotalLoss:0.00265  PostnetLoss:0.00128  DecoderLoss:0.00137  StopLoss:0.03651  GradNorm:0.00388  GradNormST:0.00854  AvgTextLen:54.0  AvgSpecLen:174.7  StepTime:0.75  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:101016  AvgTotalLoss:0.04303  AvgPostnetLoss:0.00102  AvgDecoderLoss:0.00108  AvgStopLoss:0.04092  EpochTime:42.41  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.07666   PostnetLoss: 0.00509   DecoderLoss:0.00545  StopLoss: 0.06612  \n",
      "   | > TotalLoss: 0.06962   PostnetLoss: 0.00785   DecoderLoss:0.00835  StopLoss: 0.05342  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00102   Validation Loss: 0.00727\n",
      "\n",
      " > Epoch 464/1000\n",
      "   | > Step:3/68  GlobalStep:101020  TotalLoss:0.00140  PostnetLoss:0.00068  DecoderLoss:0.00072  StopLoss:0.06497  GradNorm:0.00457  GradNormST:0.02422  AvgTextLen:4.7  AvgSpecLen:29.7  StepTime:0.28  LR:0.000100\n",
      "   | > Step:13/68  GlobalStep:101030  TotalLoss:0.00160  PostnetLoss:0.00078  DecoderLoss:0.00082  StopLoss:0.05510  GradNorm:0.00330  GradNormST:0.02779  AvgTextLen:11.5  AvgSpecLen:55.2  StepTime:0.42  LR:0.000100\n",
      "   | > Step:23/68  GlobalStep:101040  TotalLoss:0.00192  PostnetLoss:0.00094  DecoderLoss:0.00098  StopLoss:0.04006  GradNorm:0.00326  GradNormST:0.00890  AvgTextLen:18.3  AvgSpecLen:72.2  StepTime:0.44  LR:0.000100\n",
      "   | > Step:33/68  GlobalStep:101050  TotalLoss:0.00207  PostnetLoss:0.00101  DecoderLoss:0.00106  StopLoss:0.04526  GradNorm:0.00367  GradNormST:0.01020  AvgTextLen:25.9  AvgSpecLen:88.9  StepTime:0.52  LR:0.000100\n",
      "   | > Step:43/68  GlobalStep:101060  TotalLoss:0.00226  PostnetLoss:0.00110  DecoderLoss:0.00116  StopLoss:0.04141  GradNorm:0.00383  GradNormST:0.00931  AvgTextLen:33.4  AvgSpecLen:115.7  StepTime:0.56  LR:0.000100\n",
      "   | > Step:53/68  GlobalStep:101070  TotalLoss:0.00243  PostnetLoss:0.00118  DecoderLoss:0.00125  StopLoss:0.03441  GradNorm:0.00465  GradNormST:0.00772  AvgTextLen:42.4  AvgSpecLen:137.6  StepTime:0.76  LR:0.000100\n",
      "   | > Step:63/68  GlobalStep:101080  TotalLoss:0.00275  PostnetLoss:0.00133  DecoderLoss:0.00142  StopLoss:0.02797  GradNorm:0.00459  GradNormST:0.00492  AvgTextLen:58.6  AvgSpecLen:185.1  StepTime:0.90  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:101085  AvgTotalLoss:0.04360  AvgPostnetLoss:0.00102  AvgDecoderLoss:0.00108  AvgStopLoss:0.04150  EpochTime:42.71  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.08455   PostnetLoss: 0.00510   DecoderLoss:0.00545  StopLoss: 0.07400  \n",
      "   | > TotalLoss: 0.07173   PostnetLoss: 0.00791   DecoderLoss:0.00842  StopLoss: 0.05540  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00102   Validation Loss: 0.00739\n",
      "\n",
      " > Epoch 465/1000\n",
      "   | > Step:4/68  GlobalStep:101090  TotalLoss:0.00144  PostnetLoss:0.00070  DecoderLoss:0.00074  StopLoss:0.06158  GradNorm:0.00386  GradNormST:0.02657  AvgTextLen:4.8  AvgSpecLen:31.1  StepTime:0.24  LR:0.000100\n",
      "   | > Step:14/68  GlobalStep:101100  TotalLoss:0.00168  PostnetLoss:0.00082  DecoderLoss:0.00086  StopLoss:0.04328  GradNorm:0.00347  GradNormST:0.01449  AvgTextLen:12.2  AvgSpecLen:52.0  StepTime:0.26  LR:0.000100\n",
      "   | > Step:24/68  GlobalStep:101110  TotalLoss:0.00197  PostnetLoss:0.00096  DecoderLoss:0.00101  StopLoss:0.04920  GradNorm:0.00311  GradNormST:0.01026  AvgTextLen:19.2  AvgSpecLen:79.0  StepTime:0.48  LR:0.000100\n",
      "   | > Step:34/68  GlobalStep:101120  TotalLoss:0.00210  PostnetLoss:0.00102  DecoderLoss:0.00108  StopLoss:0.03058  GradNorm:0.00317  GradNormST:0.00788  AvgTextLen:26.2  AvgSpecLen:95.9  StepTime:0.62  LR:0.000100\n",
      "   | > Step:44/68  GlobalStep:101130  TotalLoss:0.00231  PostnetLoss:0.00112  DecoderLoss:0.00119  StopLoss:0.03511  GradNorm:0.00389  GradNormST:0.00891  AvgTextLen:33.8  AvgSpecLen:116.3  StepTime:0.74  LR:0.000100\n",
      "   | > Step:54/68  GlobalStep:101140  TotalLoss:0.00247  PostnetLoss:0.00119  DecoderLoss:0.00127  StopLoss:0.04168  GradNorm:0.00421  GradNormST:0.01030  AvgTextLen:42.2  AvgSpecLen:144.9  StepTime:0.71  LR:0.000100\n",
      "   | > Step:64/68  GlobalStep:101150  TotalLoss:0.00259  PostnetLoss:0.00125  DecoderLoss:0.00133  StopLoss:0.03047  GradNorm:0.00395  GradNormST:0.00848  AvgTextLen:60.6  AvgSpecLen:177.6  StepTime:0.86  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:101154  AvgTotalLoss:0.04417  AvgPostnetLoss:0.00102  AvgDecoderLoss:0.00108  AvgStopLoss:0.04207  EpochTime:42.13  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.08120   PostnetLoss: 0.00511   DecoderLoss:0.00547  StopLoss: 0.07062  \n",
      "   | > TotalLoss: 0.07945   PostnetLoss: 0.00846   DecoderLoss:0.00899  StopLoss: 0.06200  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00102   Validation Loss: 0.00774\n",
      "\n",
      " > Epoch 466/1000\n",
      "   | > Step:5/68  GlobalStep:101160  TotalLoss:0.00144  PostnetLoss:0.00070  DecoderLoss:0.00074  StopLoss:0.07807  GradNorm:0.00351  GradNormST:0.04020  AvgTextLen:5.9  AvgSpecLen:33.2  StepTime:0.32  LR:0.000100\n",
      "   | > Step:15/68  GlobalStep:101170  TotalLoss:0.00178  PostnetLoss:0.00087  DecoderLoss:0.00091  StopLoss:0.03205  GradNorm:0.00348  GradNormST:0.00722  AvgTextLen:12.8  AvgSpecLen:59.1  StepTime:0.55  LR:0.000100\n",
      "   | > Step:25/68  GlobalStep:101180  TotalLoss:0.00191  PostnetLoss:0.00093  DecoderLoss:0.00098  StopLoss:0.04943  GradNorm:0.00336  GradNormST:0.01317  AvgTextLen:19.8  AvgSpecLen:73.3  StepTime:0.48  LR:0.000100\n",
      "   | > Step:35/68  GlobalStep:101190  TotalLoss:0.00216  PostnetLoss:0.00105  DecoderLoss:0.00111  StopLoss:0.03750  GradNorm:0.00392  GradNormST:0.01715  AvgTextLen:27.1  AvgSpecLen:104.7  StepTime:0.67  LR:0.000100\n",
      "   | > Step:45/68  GlobalStep:101200  TotalLoss:0.00224  PostnetLoss:0.00109  DecoderLoss:0.00115  StopLoss:0.04456  GradNorm:0.00328  GradNormST:0.01548  AvgTextLen:34.4  AvgSpecLen:118.6  StepTime:0.70  LR:0.000100\n",
      "   | > Step:55/68  GlobalStep:101210  TotalLoss:0.00254  PostnetLoss:0.00123  DecoderLoss:0.00131  StopLoss:0.03351  GradNorm:0.00456  GradNormST:0.00665  AvgTextLen:43.9  AvgSpecLen:145.2  StepTime:0.80  LR:0.000100\n",
      "   | > Step:65/68  GlobalStep:101220  TotalLoss:0.00277  PostnetLoss:0.00134  DecoderLoss:0.00143  StopLoss:0.03008  GradNorm:0.00497  GradNormST:0.00895  AvgTextLen:64.2  AvgSpecLen:207.4  StepTime:1.19  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:101223  AvgTotalLoss:0.04397  AvgPostnetLoss:0.00102  AvgDecoderLoss:0.00108  AvgStopLoss:0.04187  EpochTime:42.33  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.08594   PostnetLoss: 0.00513   DecoderLoss:0.00549  StopLoss: 0.07531  \n",
      "   | > TotalLoss: 0.07922   PostnetLoss: 0.00850   DecoderLoss:0.00906  StopLoss: 0.06165  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00102   Validation Loss: 0.00779\n",
      "\n",
      " > Epoch 467/1000\n",
      "   | > Step:6/68  GlobalStep:101230  TotalLoss:0.00145  PostnetLoss:0.00071  DecoderLoss:0.00074  StopLoss:0.03224  GradNorm:0.00350  GradNormST:0.01389  AvgTextLen:6.0  AvgSpecLen:34.8  StepTime:0.35  LR:0.000100\n",
      "   | > Step:16/68  GlobalStep:101240  TotalLoss:0.00177  PostnetLoss:0.00086  DecoderLoss:0.00091  StopLoss:0.03851  GradNorm:0.00341  GradNormST:0.01157  AvgTextLen:13.8  AvgSpecLen:60.6  StepTime:0.29  LR:0.000100\n",
      "   | > Step:26/68  GlobalStep:101250  TotalLoss:0.00197  PostnetLoss:0.00096  DecoderLoss:0.00101  StopLoss:0.03289  GradNorm:0.00321  GradNormST:0.00815  AvgTextLen:20.3  AvgSpecLen:77.5  StepTime:0.52  LR:0.000100\n",
      "   | > Step:36/68  GlobalStep:101260  TotalLoss:0.00214  PostnetLoss:0.00104  DecoderLoss:0.00110  StopLoss:0.07391  GradNorm:0.00376  GradNormST:0.01735  AvgTextLen:28.5  AvgSpecLen:100.2  StepTime:0.58  LR:0.000100\n",
      "   | > Step:46/68  GlobalStep:101270  TotalLoss:0.00227  PostnetLoss:0.00110  DecoderLoss:0.00117  StopLoss:0.03151  GradNorm:0.00336  GradNormST:0.00753  AvgTextLen:35.4  AvgSpecLen:122.6  StepTime:0.78  LR:0.000100\n",
      "   | > Step:56/68  GlobalStep:101280  TotalLoss:0.00253  PostnetLoss:0.00123  DecoderLoss:0.00130  StopLoss:0.03893  GradNorm:0.00502  GradNormST:0.02141  AvgTextLen:45.8  AvgSpecLen:145.4  StepTime:0.99  LR:0.000100\n",
      "   | > Step:66/68  GlobalStep:101290  TotalLoss:0.00292  PostnetLoss:0.00141  DecoderLoss:0.00151  StopLoss:0.02498  GradNorm:0.00406  GradNormST:0.00969  AvgTextLen:71.2  AvgSpecLen:220.8  StepTime:1.07  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:101292  AvgTotalLoss:0.04407  AvgPostnetLoss:0.00102  AvgDecoderLoss:0.00108  AvgStopLoss:0.04198  EpochTime:43.49  AvgStepTime:0.63\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.08336   PostnetLoss: 0.00515   DecoderLoss:0.00551  StopLoss: 0.07270  \n",
      "   | > TotalLoss: 0.07998   PostnetLoss: 0.00898   DecoderLoss:0.00951  StopLoss: 0.06149  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00102   Validation Loss: 0.00810\n",
      "\n",
      " > Epoch 468/1000\n",
      "   | > Step:7/68  GlobalStep:101300  TotalLoss:0.00156  PostnetLoss:0.00076  DecoderLoss:0.00080  StopLoss:0.04731  GradNorm:0.00351  GradNormST:0.02366  AvgTextLen:7.4  AvgSpecLen:44.6  StepTime:0.34  LR:0.000100\n",
      "   | > Step:17/68  GlobalStep:101310  TotalLoss:0.00176  PostnetLoss:0.00086  DecoderLoss:0.00090  StopLoss:0.05363  GradNorm:0.00327  GradNormST:0.01173  AvgTextLen:14.7  AvgSpecLen:66.0  StepTime:0.43  LR:0.000100\n",
      "   | > Step:27/68  GlobalStep:101320  TotalLoss:0.00201  PostnetLoss:0.00098  DecoderLoss:0.00103  StopLoss:0.05002  GradNorm:0.00303  GradNormST:0.02003  AvgTextLen:21.0  AvgSpecLen:80.2  StepTime:0.66  LR:0.000100\n",
      "   | > Step:37/68  GlobalStep:101330  TotalLoss:0.00214  PostnetLoss:0.00104  DecoderLoss:0.00110  StopLoss:0.03518  GradNorm:0.00310  GradNormST:0.00612  AvgTextLen:29.3  AvgSpecLen:102.4  StepTime:0.52  LR:0.000100\n",
      "   | > Step:47/68  GlobalStep:101340  TotalLoss:0.00235  PostnetLoss:0.00114  DecoderLoss:0.00121  StopLoss:0.03570  GradNorm:0.00427  GradNormST:0.00724  AvgTextLen:36.7  AvgSpecLen:126.9  StepTime:0.64  LR:0.000100\n",
      "   | > Step:57/68  GlobalStep:101350  TotalLoss:0.00260  PostnetLoss:0.00126  DecoderLoss:0.00134  StopLoss:0.02916  GradNorm:0.00501  GradNormST:0.00880  AvgTextLen:47.1  AvgSpecLen:161.0  StepTime:0.91  LR:0.000100\n",
      "   | > Step:67/68  GlobalStep:101360  TotalLoss:0.00299  PostnetLoss:0.00145  DecoderLoss:0.00154  StopLoss:0.02676  GradNorm:0.00705  GradNormST:0.01263  AvgTextLen:80.6  AvgSpecLen:246.5  StepTime:1.27  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:101361  AvgTotalLoss:0.04595  AvgPostnetLoss:0.00102  AvgDecoderLoss:0.00108  AvgStopLoss:0.04384  EpochTime:44.37  AvgStepTime:0.64\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.08853   PostnetLoss: 0.00522   DecoderLoss:0.00558  StopLoss: 0.07773  \n",
      "   | > TotalLoss: 0.08009   PostnetLoss: 0.00877   DecoderLoss:0.00930  StopLoss: 0.06202  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00102   Validation Loss: 0.00798\n",
      "\n",
      " > Epoch 469/1000\n",
      "   | > Step:8/68  GlobalStep:101370  TotalLoss:0.00159  PostnetLoss:0.00077  DecoderLoss:0.00081  StopLoss:0.04645  GradNorm:0.00426  GradNormST:0.01175  AvgTextLen:8.3  AvgSpecLen:44.0  StepTime:0.30  LR:0.000100\n",
      "   | > Step:18/68  GlobalStep:101380  TotalLoss:0.00175  PostnetLoss:0.00085  DecoderLoss:0.00090  StopLoss:0.06437  GradNorm:0.00317  GradNormST:0.02298  AvgTextLen:15.2  AvgSpecLen:64.5  StepTime:0.42  LR:0.000100\n",
      "   | > Step:28/68  GlobalStep:101390  TotalLoss:0.00198  PostnetLoss:0.00096  DecoderLoss:0.00101  StopLoss:0.05696  GradNorm:0.00302  GradNormST:0.02358  AvgTextLen:22.1  AvgSpecLen:85.4  StepTime:0.55  LR:0.000100\n",
      "   | > Step:38/68  GlobalStep:101400  TotalLoss:0.00215  PostnetLoss:0.00105  DecoderLoss:0.00111  StopLoss:0.04821  GradNorm:0.00342  GradNormST:0.02063  AvgTextLen:29.4  AvgSpecLen:104.1  StepTime:0.51  LR:0.000100\n",
      "   | > Step:48/68  GlobalStep:101410  TotalLoss:0.00238  PostnetLoss:0.00116  DecoderLoss:0.00122  StopLoss:0.03817  GradNorm:0.00396  GradNormST:0.01202  AvgTextLen:37.5  AvgSpecLen:123.8  StepTime:0.72  LR:0.000100\n",
      "   | > Step:58/68  GlobalStep:101420  TotalLoss:0.00264  PostnetLoss:0.00128  DecoderLoss:0.00136  StopLoss:0.05033  GradNorm:0.00594  GradNormST:0.01831  AvgTextLen:47.8  AvgSpecLen:158.0  StepTime:0.83  LR:0.000100\n",
      "   | > Step:68/68  GlobalStep:101430  TotalLoss:0.00290  PostnetLoss:0.00140  DecoderLoss:0.00150  StopLoss:0.04552  GradNorm:0.00411  GradNormST:0.02354  AvgTextLen:103.0  AvgSpecLen:308.9  StepTime:1.87  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:101430  AvgTotalLoss:0.05645  AvgPostnetLoss:0.00102  AvgDecoderLoss:0.00109  AvgStopLoss:0.05434  EpochTime:43.18  AvgStepTime:0.63\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.09327   PostnetLoss: 0.00514   DecoderLoss:0.00550  StopLoss: 0.08264  \n",
      "   | > TotalLoss: 0.08118   PostnetLoss: 0.00907   DecoderLoss:0.00958  StopLoss: 0.06253  \n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00102   Validation Loss: 0.00817\n",
      "\n",
      " > Epoch 470/1000\n",
      "   | > Step:9/68  GlobalStep:101440  TotalLoss:0.00153  PostnetLoss:0.00075  DecoderLoss:0.00078  StopLoss:0.05063  GradNorm:0.00387  GradNormST:0.02023  AvgTextLen:9.0  AvgSpecLen:42.1  StepTime:0.42  LR:0.000100\n",
      "   | > Step:19/68  GlobalStep:101450  TotalLoss:0.00180  PostnetLoss:0.00088  DecoderLoss:0.00092  StopLoss:0.08858  GradNorm:0.00349  GradNormST:0.02829  AvgTextLen:15.5  AvgSpecLen:63.2  StepTime:0.48  LR:0.000100\n",
      "   | > Step:29/68  GlobalStep:101460  TotalLoss:0.00196  PostnetLoss:0.00095  DecoderLoss:0.00101  StopLoss:0.07535  GradNorm:0.00296  GradNormST:0.02267  AvgTextLen:21.8  AvgSpecLen:83.4  StepTime:0.63  LR:0.000100\n",
      "   | > Step:39/68  GlobalStep:101470  TotalLoss:0.00219  PostnetLoss:0.00107  DecoderLoss:0.00113  StopLoss:0.05191  GradNorm:0.00330  GradNormST:0.02174  AvgTextLen:30.2  AvgSpecLen:110.4  StepTime:0.69  LR:0.000100\n",
      "   | > Step:49/68  GlobalStep:101480  TotalLoss:0.00235  PostnetLoss:0.00114  DecoderLoss:0.00121  StopLoss:0.04555  GradNorm:0.00376  GradNormST:0.01303  AvgTextLen:38.2  AvgSpecLen:127.7  StepTime:0.77  LR:0.000100\n",
      "   | > Step:59/68  GlobalStep:101490  TotalLoss:0.00257  PostnetLoss:0.00125  DecoderLoss:0.00133  StopLoss:0.04661  GradNorm:0.00539  GradNormST:0.01854  AvgTextLen:50.5  AvgSpecLen:155.9  StepTime:0.93  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:101499  AvgTotalLoss:0.05920  AvgPostnetLoss:0.00102  AvgDecoderLoss:0.00108  AvgStopLoss:0.05710  EpochTime:44.05  AvgStepTime:0.64\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.08158   PostnetLoss: 0.00521   DecoderLoss:0.00556  StopLoss: 0.07082  \n",
      "   | > TotalLoss: 0.08092   PostnetLoss: 0.00840   DecoderLoss:0.00891  StopLoss: 0.06361  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00102   Validation Loss: 0.00774\n",
      "\n",
      " > Epoch 471/1000\n",
      "   | > Step:0/68  GlobalStep:101500  TotalLoss:0.00168  PostnetLoss:0.00082  DecoderLoss:0.00086  StopLoss:0.06555  GradNorm:0.01004  GradNormST:0.02069  AvgTextLen:3.0  AvgSpecLen:25.3  StepTime:0.44  LR:0.000100\n",
      "   | > Step:10/68  GlobalStep:101510  TotalLoss:0.00165  PostnetLoss:0.00080  DecoderLoss:0.00085  StopLoss:0.03936  GradNorm:0.00406  GradNormST:0.00994  AvgTextLen:9.8  AvgSpecLen:48.6  StepTime:0.48  LR:0.000100\n",
      "   | > Step:20/68  GlobalStep:101520  TotalLoss:0.00190  PostnetLoss:0.00092  DecoderLoss:0.00097  StopLoss:0.03876  GradNorm:0.00370  GradNormST:0.01481  AvgTextLen:16.4  AvgSpecLen:64.8  StepTime:0.52  LR:0.000100\n",
      "   | > Step:30/68  GlobalStep:101530  TotalLoss:0.00204  PostnetLoss:0.00099  DecoderLoss:0.00105  StopLoss:0.04806  GradNorm:0.00337  GradNormST:0.01296  AvgTextLen:22.9  AvgSpecLen:85.8  StepTime:0.59  LR:0.000100\n",
      "   | > Step:40/68  GlobalStep:101540  TotalLoss:0.00219  PostnetLoss:0.00106  DecoderLoss:0.00112  StopLoss:0.04229  GradNorm:0.00353  GradNormST:0.01176  AvgTextLen:30.9  AvgSpecLen:105.3  StepTime:0.49  LR:0.000100\n",
      "   | > Step:50/68  GlobalStep:101550  TotalLoss:0.00241  PostnetLoss:0.00117  DecoderLoss:0.00124  StopLoss:0.04398  GradNorm:0.00384  GradNormST:0.01138  AvgTextLen:38.8  AvgSpecLen:132.0  StepTime:0.79  LR:0.000100\n",
      "   | > Step:60/68  GlobalStep:101560  TotalLoss:0.00268  PostnetLoss:0.00130  DecoderLoss:0.00138  StopLoss:0.03346  GradNorm:0.00572  GradNormST:0.00969  AvgTextLen:52.0  AvgSpecLen:172.7  StepTime:0.99  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:101568  AvgTotalLoss:0.05539  AvgPostnetLoss:0.00103  AvgDecoderLoss:0.00109  AvgStopLoss:0.05327  EpochTime:42.49  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.09362   PostnetLoss: 0.00505   DecoderLoss:0.00539  StopLoss: 0.08318  \n",
      "   | > TotalLoss: 0.09093   PostnetLoss: 0.00906   DecoderLoss:0.00955  StopLoss: 0.07231  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00103   Validation Loss: 0.00806\n",
      "\n",
      " > Epoch 472/1000\n",
      "   | > Step:1/68  GlobalStep:101570  TotalLoss:0.00153  PostnetLoss:0.00074  DecoderLoss:0.00078  StopLoss:0.03982  GradNorm:0.00584  GradNormST:0.01418  AvgTextLen:3.7  AvgSpecLen:28.6  StepTime:0.41  LR:0.000100\n",
      "   | > Step:11/68  GlobalStep:101580  TotalLoss:0.00165  PostnetLoss:0.00081  DecoderLoss:0.00084  StopLoss:0.03697  GradNorm:0.00565  GradNormST:0.01193  AvgTextLen:10.3  AvgSpecLen:49.4  StepTime:0.38  LR:0.000100\n",
      "   | > Step:21/68  GlobalStep:101590  TotalLoss:0.00184  PostnetLoss:0.00090  DecoderLoss:0.00094  StopLoss:0.07730  GradNorm:0.00323  GradNormST:0.03388  AvgTextLen:16.6  AvgSpecLen:66.3  StepTime:0.43  LR:0.000100\n",
      "   | > Step:31/68  GlobalStep:101600  TotalLoss:0.00201  PostnetLoss:0.00098  DecoderLoss:0.00103  StopLoss:0.04434  GradNorm:0.00318  GradNormST:0.01620  AvgTextLen:24.1  AvgSpecLen:82.7  StepTime:0.54  LR:0.000100\n",
      "   | > Step:41/68  GlobalStep:101610  TotalLoss:0.00224  PostnetLoss:0.00109  DecoderLoss:0.00115  StopLoss:0.03139  GradNorm:0.00299  GradNormST:0.00917  AvgTextLen:32.3  AvgSpecLen:109.4  StepTime:0.62  LR:0.000100\n",
      "   | > Step:51/68  GlobalStep:101620  TotalLoss:0.00236  PostnetLoss:0.00114  DecoderLoss:0.00122  StopLoss:0.07007  GradNorm:0.00338  GradNormST:0.02719  AvgTextLen:39.2  AvgSpecLen:126.9  StepTime:0.63  LR:0.000100\n",
      "   | > Step:61/68  GlobalStep:101630  TotalLoss:0.00261  PostnetLoss:0.00126  DecoderLoss:0.00134  StopLoss:0.03284  GradNorm:0.00486  GradNormST:0.00849  AvgTextLen:54.6  AvgSpecLen:165.5  StepTime:0.89  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:101637  AvgTotalLoss:0.05029  AvgPostnetLoss:0.00102  AvgDecoderLoss:0.00108  AvgStopLoss:0.04819  EpochTime:42.69  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.08638   PostnetLoss: 0.00495   DecoderLoss:0.00529  StopLoss: 0.07615  \n",
      "   | > TotalLoss: 0.08229   PostnetLoss: 0.00831   DecoderLoss:0.00879  StopLoss: 0.06518  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00102   Validation Loss: 0.00760\n",
      "\n",
      " > Epoch 473/1000\n",
      "   | > Step:2/68  GlobalStep:101640  TotalLoss:0.00139  PostnetLoss:0.00067  DecoderLoss:0.00071  StopLoss:0.04305  GradNorm:0.00469  GradNormST:0.01773  AvgTextLen:4.0  AvgSpecLen:31.3  StepTime:0.32  LR:0.000100\n",
      "   | > Step:12/68  GlobalStep:101650  TotalLoss:0.00162  PostnetLoss:0.00079  DecoderLoss:0.00083  StopLoss:0.04447  GradNorm:0.00391  GradNormST:0.01375  AvgTextLen:11.0  AvgSpecLen:49.3  StepTime:0.36  LR:0.000100\n",
      "   | > Step:22/68  GlobalStep:101660  TotalLoss:0.00187  PostnetLoss:0.00091  DecoderLoss:0.00096  StopLoss:0.06733  GradNorm:0.00333  GradNormST:0.02014  AvgTextLen:18.0  AvgSpecLen:72.0  StepTime:0.37  LR:0.000100\n",
      "   | > Step:32/68  GlobalStep:101670  TotalLoss:0.00206  PostnetLoss:0.00100  DecoderLoss:0.00106  StopLoss:0.05995  GradNorm:0.00321  GradNormST:0.01275  AvgTextLen:24.4  AvgSpecLen:91.5  StepTime:0.50  LR:0.000100\n",
      "   | > Step:42/68  GlobalStep:101680  TotalLoss:0.00223  PostnetLoss:0.00108  DecoderLoss:0.00115  StopLoss:0.04528  GradNorm:0.00296  GradNormST:0.01888  AvgTextLen:33.1  AvgSpecLen:109.7  StepTime:0.60  LR:0.000100\n",
      "   | > Step:52/68  GlobalStep:101690  TotalLoss:0.00242  PostnetLoss:0.00117  DecoderLoss:0.00125  StopLoss:0.04478  GradNorm:0.00375  GradNormST:0.01146  AvgTextLen:42.1  AvgSpecLen:137.5  StepTime:0.79  LR:0.000100\n",
      "   | > Step:62/68  GlobalStep:101700  TotalLoss:0.00264  PostnetLoss:0.00128  DecoderLoss:0.00136  StopLoss:0.03388  GradNorm:0.00369  GradNormST:0.01374  AvgTextLen:54.0  AvgSpecLen:174.7  StepTime:0.76  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:101706  AvgTotalLoss:0.05081  AvgPostnetLoss:0.00102  AvgDecoderLoss:0.00108  AvgStopLoss:0.04871  EpochTime:42.09  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.08322   PostnetLoss: 0.00506   DecoderLoss:0.00540  StopLoss: 0.07275  \n",
      "   | > TotalLoss: 0.07831   PostnetLoss: 0.00812   DecoderLoss:0.00857  StopLoss: 0.06162  \n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00102   Validation Loss: 0.00749\n",
      "\n",
      " > Epoch 474/1000\n",
      "   | > Step:3/68  GlobalStep:101710  TotalLoss:0.00149  PostnetLoss:0.00072  DecoderLoss:0.00076  StopLoss:0.05527  GradNorm:0.00574  GradNormST:0.01825  AvgTextLen:4.7  AvgSpecLen:29.7  StepTime:0.31  LR:0.000100\n",
      "   | > Step:13/68  GlobalStep:101720  TotalLoss:0.00165  PostnetLoss:0.00080  DecoderLoss:0.00084  StopLoss:0.07331  GradNorm:0.00466  GradNormST:0.02508  AvgTextLen:11.5  AvgSpecLen:55.2  StepTime:0.42  LR:0.000100\n",
      "   | > Step:23/68  GlobalStep:101730  TotalLoss:0.00193  PostnetLoss:0.00094  DecoderLoss:0.00099  StopLoss:0.05117  GradNorm:0.00337  GradNormST:0.01371  AvgTextLen:18.3  AvgSpecLen:72.2  StepTime:0.43  LR:0.000100\n",
      "   | > Step:33/68  GlobalStep:101740  TotalLoss:0.00211  PostnetLoss:0.00103  DecoderLoss:0.00108  StopLoss:0.04639  GradNorm:0.00390  GradNormST:0.01250  AvgTextLen:25.9  AvgSpecLen:88.9  StepTime:0.43  LR:0.000100\n",
      "   | > Step:43/68  GlobalStep:101750  TotalLoss:0.00224  PostnetLoss:0.00109  DecoderLoss:0.00115  StopLoss:0.04769  GradNorm:0.00343  GradNormST:0.01239  AvgTextLen:33.4  AvgSpecLen:115.7  StepTime:0.67  LR:0.000100\n",
      "   | > Step:53/68  GlobalStep:101760  TotalLoss:0.00241  PostnetLoss:0.00117  DecoderLoss:0.00124  StopLoss:0.03924  GradNorm:0.00352  GradNormST:0.01140  AvgTextLen:42.4  AvgSpecLen:137.6  StepTime:0.63  LR:0.000100\n",
      "   | > Step:63/68  GlobalStep:101770  TotalLoss:0.00269  PostnetLoss:0.00130  DecoderLoss:0.00139  StopLoss:0.02742  GradNorm:0.00298  GradNormST:0.00548  AvgTextLen:58.6  AvgSpecLen:185.1  StepTime:0.91  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:101775  AvgTotalLoss:0.05127  AvgPostnetLoss:0.00101  AvgDecoderLoss:0.00107  AvgStopLoss:0.04918  EpochTime:42.52  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.08602   PostnetLoss: 0.00501   DecoderLoss:0.00534  StopLoss: 0.07567  \n",
      "   | > TotalLoss: 0.08425   PostnetLoss: 0.00809   DecoderLoss:0.00859  StopLoss: 0.06757  \n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00101   Validation Loss: 0.00747\n",
      "\n",
      " > Epoch 475/1000\n",
      "   | > Step:4/68  GlobalStep:101780  TotalLoss:0.00144  PostnetLoss:0.00070  DecoderLoss:0.00074  StopLoss:0.06740  GradNorm:0.00442  GradNormST:0.02837  AvgTextLen:4.8  AvgSpecLen:31.1  StepTime:0.28  LR:0.000100\n",
      "   | > Step:14/68  GlobalStep:101790  TotalLoss:0.00174  PostnetLoss:0.00085  DecoderLoss:0.00089  StopLoss:0.04415  GradNorm:0.00453  GradNormST:0.01863  AvgTextLen:12.2  AvgSpecLen:52.0  StepTime:0.26  LR:0.000100\n",
      "   | > Step:24/68  GlobalStep:101800  TotalLoss:0.00198  PostnetLoss:0.00096  DecoderLoss:0.00101  StopLoss:0.05153  GradNorm:0.00363  GradNormST:0.01224  AvgTextLen:19.2  AvgSpecLen:79.0  StepTime:0.49  LR:0.000100\n",
      "   | > Step:34/68  GlobalStep:101810  TotalLoss:0.00211  PostnetLoss:0.00103  DecoderLoss:0.00108  StopLoss:0.03721  GradNorm:0.00318  GradNormST:0.01180  AvgTextLen:26.2  AvgSpecLen:95.9  StepTime:0.59  LR:0.000100\n",
      "   | > Step:44/68  GlobalStep:101820  TotalLoss:0.00228  PostnetLoss:0.00111  DecoderLoss:0.00118  StopLoss:0.04285  GradNorm:0.00303  GradNormST:0.01256  AvgTextLen:33.8  AvgSpecLen:116.3  StepTime:0.68  LR:0.000100\n",
      "   | > Step:54/68  GlobalStep:101830  TotalLoss:0.00245  PostnetLoss:0.00118  DecoderLoss:0.00127  StopLoss:0.04158  GradNorm:0.00375  GradNormST:0.01047  AvgTextLen:42.2  AvgSpecLen:144.9  StepTime:0.81  LR:0.000100\n",
      "   | > Step:64/68  GlobalStep:101840  TotalLoss:0.00255  PostnetLoss:0.00123  DecoderLoss:0.00132  StopLoss:0.02864  GradNorm:0.00339  GradNormST:0.01044  AvgTextLen:60.6  AvgSpecLen:177.6  StepTime:0.84  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:101844  AvgTotalLoss:0.04664  AvgPostnetLoss:0.00101  AvgDecoderLoss:0.00107  AvgStopLoss:0.04456  EpochTime:43.14  AvgStepTime:0.63\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.08465   PostnetLoss: 0.00507   DecoderLoss:0.00541  StopLoss: 0.07417  \n",
      "   | > TotalLoss: 0.08368   PostnetLoss: 0.00847   DecoderLoss:0.00893  StopLoss: 0.06628  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00101   Validation Loss: 0.00768\n",
      "\n",
      " > Epoch 476/1000\n",
      "   | > Step:5/68  GlobalStep:101850  TotalLoss:0.00144  PostnetLoss:0.00070  DecoderLoss:0.00074  StopLoss:0.04155  GradNorm:0.00393  GradNormST:0.00985  AvgTextLen:5.9  AvgSpecLen:33.2  StepTime:0.33  LR:0.000100\n",
      "   | > Step:15/68  GlobalStep:101860  TotalLoss:0.00184  PostnetLoss:0.00090  DecoderLoss:0.00094  StopLoss:0.04666  GradNorm:0.00592  GradNormST:0.01137  AvgTextLen:12.8  AvgSpecLen:59.1  StepTime:0.50  LR:0.000100\n",
      "   | > Step:25/68  GlobalStep:101870  TotalLoss:0.00191  PostnetLoss:0.00093  DecoderLoss:0.00098  StopLoss:0.03935  GradNorm:0.00419  GradNormST:0.01040  AvgTextLen:19.8  AvgSpecLen:73.3  StepTime:0.41  LR:0.000100\n",
      "   | > Step:35/68  GlobalStep:101880  TotalLoss:0.00214  PostnetLoss:0.00104  DecoderLoss:0.00110  StopLoss:0.04615  GradNorm:0.00347  GradNormST:0.02021  AvgTextLen:27.1  AvgSpecLen:104.7  StepTime:0.63  LR:0.000100\n",
      "   | > Step:45/68  GlobalStep:101890  TotalLoss:0.00220  PostnetLoss:0.00107  DecoderLoss:0.00114  StopLoss:0.04815  GradNorm:0.00272  GradNormST:0.02245  AvgTextLen:34.4  AvgSpecLen:118.6  StepTime:0.53  LR:0.000100\n",
      "   | > Step:55/68  GlobalStep:101900  TotalLoss:0.00247  PostnetLoss:0.00120  DecoderLoss:0.00127  StopLoss:0.03623  GradNorm:0.00327  GradNormST:0.00716  AvgTextLen:43.9  AvgSpecLen:145.2  StepTime:0.81  LR:0.000100\n",
      "   | > Step:65/68  GlobalStep:101910  TotalLoss:0.00272  PostnetLoss:0.00131  DecoderLoss:0.00141  StopLoss:0.04046  GradNorm:0.00291  GradNormST:0.01771  AvgTextLen:64.2  AvgSpecLen:207.4  StepTime:1.19  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:101913  AvgTotalLoss:0.04557  AvgPostnetLoss:0.00101  AvgDecoderLoss:0.00107  AvgStopLoss:0.04349  EpochTime:41.81  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.07994   PostnetLoss: 0.00505   DecoderLoss:0.00540  StopLoss: 0.06949  \n",
      "   | > TotalLoss: 0.08049   PostnetLoss: 0.00833   DecoderLoss:0.00881  StopLoss: 0.06335  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00101   Validation Loss: 0.00761\n",
      "\n",
      " > Epoch 477/1000\n",
      "   | > Step:6/68  GlobalStep:101920  TotalLoss:0.00144  PostnetLoss:0.00070  DecoderLoss:0.00074  StopLoss:0.05329  GradNorm:0.00373  GradNormST:0.02195  AvgTextLen:6.0  AvgSpecLen:34.8  StepTime:0.34  LR:0.000100\n",
      "   | > Step:16/68  GlobalStep:101930  TotalLoss:0.00182  PostnetLoss:0.00089  DecoderLoss:0.00093  StopLoss:0.04875  GradNorm:0.00515  GradNormST:0.01398  AvgTextLen:13.8  AvgSpecLen:60.6  StepTime:0.36  LR:0.000100\n",
      "   | > Step:26/68  GlobalStep:101940  TotalLoss:0.00198  PostnetLoss:0.00097  DecoderLoss:0.00101  StopLoss:0.04630  GradNorm:0.00435  GradNormST:0.01309  AvgTextLen:20.3  AvgSpecLen:77.5  StepTime:0.47  LR:0.000100\n",
      "   | > Step:36/68  GlobalStep:101950  TotalLoss:0.00214  PostnetLoss:0.00104  DecoderLoss:0.00110  StopLoss:0.06845  GradNorm:0.00388  GradNormST:0.01563  AvgTextLen:28.5  AvgSpecLen:100.2  StepTime:0.57  LR:0.000100\n",
      "   | > Step:46/68  GlobalStep:101960  TotalLoss:0.00225  PostnetLoss:0.00109  DecoderLoss:0.00116  StopLoss:0.03934  GradNorm:0.00334  GradNormST:0.00978  AvgTextLen:35.4  AvgSpecLen:122.6  StepTime:0.84  LR:0.000100\n",
      "   | > Step:56/68  GlobalStep:101970  TotalLoss:0.00247  PostnetLoss:0.00120  DecoderLoss:0.00128  StopLoss:0.03795  GradNorm:0.00310  GradNormST:0.01453  AvgTextLen:45.8  AvgSpecLen:145.4  StepTime:0.84  LR:0.000100\n",
      "   | > Step:66/68  GlobalStep:101980  TotalLoss:0.00287  PostnetLoss:0.00138  DecoderLoss:0.00148  StopLoss:0.03264  GradNorm:0.00312  GradNormST:0.01440  AvgTextLen:71.2  AvgSpecLen:220.8  StepTime:1.03  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:101982  AvgTotalLoss:0.04907  AvgPostnetLoss:0.00102  AvgDecoderLoss:0.00108  AvgStopLoss:0.04698  EpochTime:41.90  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.08387   PostnetLoss: 0.00513   DecoderLoss:0.00546  StopLoss: 0.07328  \n",
      "   | > TotalLoss: 0.08073   PostnetLoss: 0.00829   DecoderLoss:0.00878  StopLoss: 0.06365  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00102   Validation Loss: 0.00753\n",
      "\n",
      " > Epoch 478/1000\n",
      "   | > Step:7/68  GlobalStep:101990  TotalLoss:0.00155  PostnetLoss:0.00076  DecoderLoss:0.00080  StopLoss:0.05037  GradNorm:0.00374  GradNormST:0.02501  AvgTextLen:7.4  AvgSpecLen:44.6  StepTime:0.40  LR:0.000100\n",
      "   | > Step:17/68  GlobalStep:102000  TotalLoss:0.00177  PostnetLoss:0.00087  DecoderLoss:0.00091  StopLoss:0.06708  GradNorm:0.00422  GradNormST:0.01719  AvgTextLen:14.7  AvgSpecLen:66.0  StepTime:0.35  LR:0.000100\n",
      " | | > Checkpoint saving : /vm/TTS/models/cartman_models/mozilla-no-loc-fattn-stopnet-sigmoid-loss_masking-August-15-2019_11+26AM-de6aeb9/checkpoint_102000.pth.tar\n",
      "warning: audio amplitude out of range, auto clipped.\n",
      "   | > Step:27/68  GlobalStep:102010  TotalLoss:0.00203  PostnetLoss:0.00099  DecoderLoss:0.00104  StopLoss:0.05806  GradNorm:0.00434  GradNormST:0.02030  AvgTextLen:21.0  AvgSpecLen:80.2  StepTime:0.68  LR:0.000100\n",
      "   | > Step:37/68  GlobalStep:102020  TotalLoss:0.00217  PostnetLoss:0.00106  DecoderLoss:0.00111  StopLoss:0.05082  GradNorm:0.00429  GradNormST:0.01027  AvgTextLen:29.3  AvgSpecLen:102.4  StepTime:0.62  LR:0.000100\n",
      "   | > Step:47/68  GlobalStep:102030  TotalLoss:0.00236  PostnetLoss:0.00115  DecoderLoss:0.00121  StopLoss:0.05411  GradNorm:0.00414  GradNormST:0.01884  AvgTextLen:36.7  AvgSpecLen:126.9  StepTime:0.58  LR:0.000100\n",
      "   | > Step:57/68  GlobalStep:102040  TotalLoss:0.00254  PostnetLoss:0.00123  DecoderLoss:0.00131  StopLoss:0.03235  GradNorm:0.00351  GradNormST:0.00670  AvgTextLen:47.1  AvgSpecLen:161.0  StepTime:0.90  LR:0.000100\n",
      "   | > Step:67/68  GlobalStep:102050  TotalLoss:0.00288  PostnetLoss:0.00139  DecoderLoss:0.00149  StopLoss:0.04446  GradNorm:0.00288  GradNormST:0.02816  AvgTextLen:80.6  AvgSpecLen:246.5  StepTime:1.21  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:102051  AvgTotalLoss:0.04858  AvgPostnetLoss:0.00102  AvgDecoderLoss:0.00108  AvgStopLoss:0.04649  EpochTime:42.57  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.08258   PostnetLoss: 0.00515   DecoderLoss:0.00548  StopLoss: 0.07195  \n",
      "   | > TotalLoss: 0.07751   PostnetLoss: 0.00791   DecoderLoss:0.00839  StopLoss: 0.06121  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00102   Validation Loss: 0.00738\n",
      "\n",
      " > Epoch 479/1000\n",
      "   | > Step:8/68  GlobalStep:102060  TotalLoss:0.00162  PostnetLoss:0.00079  DecoderLoss:0.00082  StopLoss:0.04586  GradNorm:0.00428  GradNormST:0.01446  AvgTextLen:8.3  AvgSpecLen:44.0  StepTime:0.30  LR:0.000100\n",
      "   | > Step:18/68  GlobalStep:102070  TotalLoss:0.00183  PostnetLoss:0.00089  DecoderLoss:0.00094  StopLoss:0.03941  GradNorm:0.00416  GradNormST:0.00955  AvgTextLen:15.2  AvgSpecLen:64.5  StepTime:0.44  LR:0.000100\n",
      "   | > Step:28/68  GlobalStep:102080  TotalLoss:0.00213  PostnetLoss:0.00104  DecoderLoss:0.00109  StopLoss:0.05148  GradNorm:0.00435  GradNormST:0.01492  AvgTextLen:22.1  AvgSpecLen:85.4  StepTime:0.57  LR:0.000100\n",
      "   | > Step:38/68  GlobalStep:102090  TotalLoss:0.00229  PostnetLoss:0.00111  DecoderLoss:0.00118  StopLoss:0.04988  GradNorm:0.00470  GradNormST:0.01678  AvgTextLen:29.4  AvgSpecLen:104.1  StepTime:0.61  LR:0.000100\n",
      "   | > Step:48/68  GlobalStep:102100  TotalLoss:0.00239  PostnetLoss:0.00116  DecoderLoss:0.00123  StopLoss:0.03511  GradNorm:0.00349  GradNormST:0.01317  AvgTextLen:37.5  AvgSpecLen:123.8  StepTime:0.68  LR:0.000100\n",
      "   | > Step:58/68  GlobalStep:102110  TotalLoss:0.00260  PostnetLoss:0.00126  DecoderLoss:0.00134  StopLoss:0.04026  GradNorm:0.00362  GradNormST:0.01380  AvgTextLen:47.8  AvgSpecLen:158.0  StepTime:0.71  LR:0.000100\n",
      "   | > Step:68/68  GlobalStep:102120  TotalLoss:0.00306  PostnetLoss:0.00147  DecoderLoss:0.00158  StopLoss:0.09827  GradNorm:0.00457  GradNormST:0.13539  AvgTextLen:103.0  AvgSpecLen:308.9  StepTime:1.88  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:102120  AvgTotalLoss:0.04908  AvgPostnetLoss:0.00105  AvgDecoderLoss:0.00111  AvgStopLoss:0.04692  EpochTime:42.69  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.07874   PostnetLoss: 0.00504   DecoderLoss:0.00538  StopLoss: 0.06832  \n",
      "   | > TotalLoss: 0.07242   PostnetLoss: 0.00770   DecoderLoss:0.00818  StopLoss: 0.05654  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00105   Validation Loss: 0.00720\n",
      "\n",
      " > Epoch 480/1000\n",
      "   | > Step:9/68  GlobalStep:102130  TotalLoss:0.00158  PostnetLoss:0.00077  DecoderLoss:0.00081  StopLoss:0.05736  GradNorm:0.00403  GradNormST:0.02549  AvgTextLen:9.0  AvgSpecLen:42.1  StepTime:0.44  LR:0.000100\n",
      "   | > Step:19/68  GlobalStep:102140  TotalLoss:0.00183  PostnetLoss:0.00089  DecoderLoss:0.00093  StopLoss:0.06007  GradNorm:0.00477  GradNormST:0.02083  AvgTextLen:15.5  AvgSpecLen:63.2  StepTime:0.44  LR:0.000100\n",
      "   | > Step:29/68  GlobalStep:102150  TotalLoss:0.00201  PostnetLoss:0.00098  DecoderLoss:0.00103  StopLoss:0.05476  GradNorm:0.00398  GradNormST:0.01076  AvgTextLen:21.8  AvgSpecLen:83.4  StepTime:0.55  LR:0.000100\n",
      "   | > Step:39/68  GlobalStep:102160  TotalLoss:0.00223  PostnetLoss:0.00109  DecoderLoss:0.00115  StopLoss:0.03245  GradNorm:0.00419  GradNormST:0.00858  AvgTextLen:30.2  AvgSpecLen:110.4  StepTime:0.65  LR:0.000100\n",
      "   | > Step:49/68  GlobalStep:102170  TotalLoss:0.00236  PostnetLoss:0.00115  DecoderLoss:0.00122  StopLoss:0.03707  GradNorm:0.00377  GradNormST:0.00992  AvgTextLen:38.2  AvgSpecLen:127.7  StepTime:0.73  LR:0.000100\n",
      "   | > Step:59/68  GlobalStep:102180  TotalLoss:0.00255  PostnetLoss:0.00123  DecoderLoss:0.00132  StopLoss:0.04826  GradNorm:0.00317  GradNormST:0.04619  AvgTextLen:50.5  AvgSpecLen:155.9  StepTime:0.90  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:102189  AvgTotalLoss:0.05011  AvgPostnetLoss:0.00103  AvgDecoderLoss:0.00109  AvgStopLoss:0.04799  EpochTime:42.73  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.08297   PostnetLoss: 0.00509   DecoderLoss:0.00543  StopLoss: 0.07244  \n",
      "   | > TotalLoss: 0.07483   PostnetLoss: 0.00777   DecoderLoss:0.00826  StopLoss: 0.05880  \n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00103   Validation Loss: 0.00727\n",
      "\n",
      " > Epoch 481/1000\n",
      "   | > Step:0/68  GlobalStep:102190  TotalLoss:0.00149  PostnetLoss:0.00071  DecoderLoss:0.00078  StopLoss:0.05546  GradNorm:0.00575  GradNormST:0.02560  AvgTextLen:3.0  AvgSpecLen:25.3  StepTime:0.36  LR:0.000100\n",
      "   | > Step:10/68  GlobalStep:102200  TotalLoss:0.00164  PostnetLoss:0.00080  DecoderLoss:0.00084  StopLoss:0.06378  GradNorm:0.00344  GradNormST:0.03047  AvgTextLen:9.8  AvgSpecLen:48.6  StepTime:0.54  LR:0.000100\n",
      "   | > Step:20/68  GlobalStep:102210  TotalLoss:0.00188  PostnetLoss:0.00092  DecoderLoss:0.00096  StopLoss:0.03721  GradNorm:0.00408  GradNormST:0.01957  AvgTextLen:16.4  AvgSpecLen:64.8  StepTime:0.39  LR:0.000100\n",
      "   | > Step:30/68  GlobalStep:102220  TotalLoss:0.00204  PostnetLoss:0.00099  DecoderLoss:0.00105  StopLoss:0.05634  GradNorm:0.00415  GradNormST:0.02615  AvgTextLen:22.9  AvgSpecLen:85.8  StepTime:0.50  LR:0.000100\n",
      "   | > Step:40/68  GlobalStep:102230  TotalLoss:0.00221  PostnetLoss:0.00108  DecoderLoss:0.00114  StopLoss:0.06509  GradNorm:0.00439  GradNormST:0.02509  AvgTextLen:30.9  AvgSpecLen:105.3  StepTime:0.60  LR:0.000100\n",
      "   | > Step:50/68  GlobalStep:102240  TotalLoss:0.00242  PostnetLoss:0.00117  DecoderLoss:0.00125  StopLoss:0.05806  GradNorm:0.00424  GradNormST:0.03836  AvgTextLen:38.8  AvgSpecLen:132.0  StepTime:0.71  LR:0.000100\n",
      "   | > Step:60/68  GlobalStep:102250  TotalLoss:0.00266  PostnetLoss:0.00129  DecoderLoss:0.00137  StopLoss:0.07754  GradNorm:0.00507  GradNormST:0.04539  AvgTextLen:52.0  AvgSpecLen:172.7  StepTime:0.79  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:102258  AvgTotalLoss:0.06529  AvgPostnetLoss:0.00102  AvgDecoderLoss:0.00108  AvgStopLoss:0.06319  EpochTime:42.06  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.08006   PostnetLoss: 0.00522   DecoderLoss:0.00556  StopLoss: 0.06928  \n",
      "   | > TotalLoss: 0.07089   PostnetLoss: 0.00726   DecoderLoss:0.00773  StopLoss: 0.05590  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00102   Validation Loss: 0.00706\n",
      "\n",
      " > Epoch 482/1000\n",
      "   | > Step:1/68  GlobalStep:102260  TotalLoss:0.00150  PostnetLoss:0.00073  DecoderLoss:0.00077  StopLoss:0.04851  GradNorm:0.00498  GradNormST:0.01482  AvgTextLen:3.7  AvgSpecLen:28.6  StepTime:0.25  LR:0.000100\n",
      "   | > Step:11/68  GlobalStep:102270  TotalLoss:0.00165  PostnetLoss:0.00081  DecoderLoss:0.00084  StopLoss:0.05812  GradNorm:0.00355  GradNormST:0.03051  AvgTextLen:10.3  AvgSpecLen:49.4  StepTime:0.30  LR:0.000100\n",
      "   | > Step:21/68  GlobalStep:102280  TotalLoss:0.00184  PostnetLoss:0.00090  DecoderLoss:0.00094  StopLoss:0.05039  GradNorm:0.00366  GradNormST:0.01268  AvgTextLen:16.6  AvgSpecLen:66.3  StepTime:0.46  LR:0.000100\n",
      "   | > Step:31/68  GlobalStep:102290  TotalLoss:0.00204  PostnetLoss:0.00099  DecoderLoss:0.00105  StopLoss:0.11043  GradNorm:0.00385  GradNormST:0.05552  AvgTextLen:24.1  AvgSpecLen:82.7  StepTime:0.52  LR:0.000100\n",
      "   | > Step:41/68  GlobalStep:102300  TotalLoss:0.00225  PostnetLoss:0.00110  DecoderLoss:0.00116  StopLoss:0.06976  GradNorm:0.00379  GradNormST:0.03945  AvgTextLen:32.3  AvgSpecLen:109.4  StepTime:0.64  LR:0.000100\n",
      "   | > Step:51/68  GlobalStep:102310  TotalLoss:0.00236  PostnetLoss:0.00115  DecoderLoss:0.00122  StopLoss:0.06118  GradNorm:0.00380  GradNormST:0.03325  AvgTextLen:39.2  AvgSpecLen:126.9  StepTime:0.72  LR:0.000100\n",
      "   | > Step:61/68  GlobalStep:102320  TotalLoss:0.00255  PostnetLoss:0.00123  DecoderLoss:0.00132  StopLoss:0.09804  GradNorm:0.00384  GradNormST:0.05942  AvgTextLen:54.6  AvgSpecLen:165.5  StepTime:0.90  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:102327  AvgTotalLoss:0.07861  AvgPostnetLoss:0.00102  AvgDecoderLoss:0.00108  AvgStopLoss:0.07652  EpochTime:42.01  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.08162   PostnetLoss: 0.00516   DecoderLoss:0.00549  StopLoss: 0.07097  \n",
      "   | > TotalLoss: 0.06604   PostnetLoss: 0.00730   DecoderLoss:0.00777  StopLoss: 0.05096  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00102   Validation Loss: 0.00700\n",
      "\n",
      " > Epoch 483/1000\n",
      "   | > Step:2/68  GlobalStep:102330  TotalLoss:0.00137  PostnetLoss:0.00067  DecoderLoss:0.00070  StopLoss:0.03675  GradNorm:0.00386  GradNormST:0.01415  AvgTextLen:4.0  AvgSpecLen:31.3  StepTime:0.33  LR:0.000100\n",
      "   | > Step:12/68  GlobalStep:102340  TotalLoss:0.00161  PostnetLoss:0.00078  DecoderLoss:0.00082  StopLoss:0.04741  GradNorm:0.00349  GradNormST:0.01179  AvgTextLen:11.0  AvgSpecLen:49.3  StepTime:0.37  LR:0.000100\n",
      "   | > Step:22/68  GlobalStep:102350  TotalLoss:0.00189  PostnetLoss:0.00092  DecoderLoss:0.00097  StopLoss:0.06343  GradNorm:0.00341  GradNormST:0.01665  AvgTextLen:18.0  AvgSpecLen:72.0  StepTime:0.46  LR:0.000100\n",
      "   | > Step:32/68  GlobalStep:102360  TotalLoss:0.00208  PostnetLoss:0.00101  DecoderLoss:0.00107  StopLoss:0.09056  GradNorm:0.00357  GradNormST:0.04074  AvgTextLen:24.4  AvgSpecLen:91.5  StepTime:0.57  LR:0.000100\n",
      "   | > Step:42/68  GlobalStep:102370  TotalLoss:0.00223  PostnetLoss:0.00108  DecoderLoss:0.00115  StopLoss:0.09361  GradNorm:0.00332  GradNormST:0.05384  AvgTextLen:33.1  AvgSpecLen:109.7  StepTime:0.49  LR:0.000100\n",
      "   | > Step:52/68  GlobalStep:102380  TotalLoss:0.00243  PostnetLoss:0.00118  DecoderLoss:0.00125  StopLoss:0.05918  GradNorm:0.00308  GradNormST:0.02015  AvgTextLen:42.1  AvgSpecLen:137.5  StepTime:0.67  LR:0.000100\n",
      "   | > Step:62/68  GlobalStep:102390  TotalLoss:0.00265  PostnetLoss:0.00128  DecoderLoss:0.00137  StopLoss:0.09965  GradNorm:0.00365  GradNormST:0.06512  AvgTextLen:54.0  AvgSpecLen:174.7  StepTime:0.75  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:102396  AvgTotalLoss:0.06810  AvgPostnetLoss:0.00103  AvgDecoderLoss:0.00109  AvgStopLoss:0.06598  EpochTime:42.40  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.08483   PostnetLoss: 0.00511   DecoderLoss:0.00545  StopLoss: 0.07427  \n",
      "   | > TotalLoss: 0.07103   PostnetLoss: 0.00738   DecoderLoss:0.00787  StopLoss: 0.05578  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00103   Validation Loss: 0.00707\n",
      "\n",
      " > Epoch 484/1000\n",
      "   | > Step:3/68  GlobalStep:102400  TotalLoss:0.00139  PostnetLoss:0.00068  DecoderLoss:0.00072  StopLoss:0.06199  GradNorm:0.00375  GradNormST:0.02297  AvgTextLen:4.7  AvgSpecLen:29.7  StepTime:0.32  LR:0.000100\n",
      "   | > Step:13/68  GlobalStep:102410  TotalLoss:0.00162  PostnetLoss:0.00079  DecoderLoss:0.00083  StopLoss:0.06919  GradNorm:0.00396  GradNormST:0.03184  AvgTextLen:11.5  AvgSpecLen:55.2  StepTime:0.42  LR:0.000100\n",
      "   | > Step:23/68  GlobalStep:102420  TotalLoss:0.00192  PostnetLoss:0.00093  DecoderLoss:0.00098  StopLoss:0.04494  GradNorm:0.00332  GradNormST:0.01418  AvgTextLen:18.3  AvgSpecLen:72.2  StepTime:0.43  LR:0.000100\n",
      "   | > Step:33/68  GlobalStep:102430  TotalLoss:0.00211  PostnetLoss:0.00103  DecoderLoss:0.00108  StopLoss:0.06868  GradNorm:0.00346  GradNormST:0.02784  AvgTextLen:25.9  AvgSpecLen:88.9  StepTime:0.57  LR:0.000100\n",
      "   | > Step:43/68  GlobalStep:102440  TotalLoss:0.00225  PostnetLoss:0.00109  DecoderLoss:0.00116  StopLoss:0.06081  GradNorm:0.00306  GradNormST:0.01582  AvgTextLen:33.4  AvgSpecLen:115.7  StepTime:0.60  LR:0.000100\n",
      "   | > Step:53/68  GlobalStep:102450  TotalLoss:0.00234  PostnetLoss:0.00113  DecoderLoss:0.00121  StopLoss:0.04608  GradNorm:0.00283  GradNormST:0.02085  AvgTextLen:42.4  AvgSpecLen:137.6  StepTime:0.74  LR:0.000100\n",
      "   | > Step:63/68  GlobalStep:102460  TotalLoss:0.00268  PostnetLoss:0.00129  DecoderLoss:0.00138  StopLoss:0.04077  GradNorm:0.00273  GradNormST:0.02060  AvgTextLen:58.6  AvgSpecLen:185.1  StepTime:0.92  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:102465  AvgTotalLoss:0.05415  AvgPostnetLoss:0.00101  AvgDecoderLoss:0.00107  AvgStopLoss:0.05206  EpochTime:42.38  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.08521   PostnetLoss: 0.00514   DecoderLoss:0.00549  StopLoss: 0.07459  \n",
      "   | > TotalLoss: 0.07196   PostnetLoss: 0.00773   DecoderLoss:0.00822  StopLoss: 0.05600  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00101   Validation Loss: 0.00729\n",
      "\n",
      " > Epoch 485/1000\n",
      "   | > Step:4/68  GlobalStep:102470  TotalLoss:0.00143  PostnetLoss:0.00069  DecoderLoss:0.00073  StopLoss:0.07185  GradNorm:0.00424  GradNormST:0.02879  AvgTextLen:4.8  AvgSpecLen:31.1  StepTime:0.21  LR:0.000100\n",
      "   | > Step:14/68  GlobalStep:102480  TotalLoss:0.00170  PostnetLoss:0.00083  DecoderLoss:0.00087  StopLoss:0.05097  GradNorm:0.00351  GradNormST:0.01201  AvgTextLen:12.2  AvgSpecLen:52.0  StepTime:0.36  LR:0.000100\n",
      "   | > Step:24/68  GlobalStep:102490  TotalLoss:0.00197  PostnetLoss:0.00096  DecoderLoss:0.00101  StopLoss:0.05349  GradNorm:0.00338  GradNormST:0.01547  AvgTextLen:19.2  AvgSpecLen:79.0  StepTime:0.50  LR:0.000100\n",
      "   | > Step:34/68  GlobalStep:102500  TotalLoss:0.00212  PostnetLoss:0.00103  DecoderLoss:0.00109  StopLoss:0.04282  GradNorm:0.00308  GradNormST:0.01558  AvgTextLen:26.2  AvgSpecLen:95.9  StepTime:0.63  LR:0.000100\n",
      "   | > Step:44/68  GlobalStep:102510  TotalLoss:0.00228  PostnetLoss:0.00111  DecoderLoss:0.00117  StopLoss:0.07902  GradNorm:0.00305  GradNormST:0.03554  AvgTextLen:33.8  AvgSpecLen:116.3  StepTime:0.73  LR:0.000100\n",
      "   | > Step:54/68  GlobalStep:102520  TotalLoss:0.00241  PostnetLoss:0.00117  DecoderLoss:0.00124  StopLoss:0.04045  GradNorm:0.00283  GradNormST:0.01122  AvgTextLen:42.2  AvgSpecLen:144.9  StepTime:0.81  LR:0.000100\n",
      "   | > Step:64/68  GlobalStep:102530  TotalLoss:0.00253  PostnetLoss:0.00122  DecoderLoss:0.00131  StopLoss:0.04435  GradNorm:0.00259  GradNormST:0.01920  AvgTextLen:60.6  AvgSpecLen:177.6  StepTime:0.81  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:102534  AvgTotalLoss:0.06129  AvgPostnetLoss:0.00101  AvgDecoderLoss:0.00107  AvgStopLoss:0.05921  EpochTime:42.51  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.08335   PostnetLoss: 0.00517   DecoderLoss:0.00553  StopLoss: 0.07265  \n",
      "   | > TotalLoss: 0.07314   PostnetLoss: 0.00797   DecoderLoss:0.00846  StopLoss: 0.05671  \n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00101   Validation Loss: 0.00743\n",
      "\n",
      " > Epoch 486/1000\n",
      "   | > Step:5/68  GlobalStep:102540  TotalLoss:0.00146  PostnetLoss:0.00071  DecoderLoss:0.00075  StopLoss:0.18819  GradNorm:0.00489  GradNormST:0.12515  AvgTextLen:5.9  AvgSpecLen:33.2  StepTime:0.37  LR:0.000100\n",
      "   | > Step:15/68  GlobalStep:102550  TotalLoss:0.00178  PostnetLoss:0.00086  DecoderLoss:0.00091  StopLoss:0.06918  GradNorm:0.00356  GradNormST:0.03625  AvgTextLen:12.8  AvgSpecLen:59.1  StepTime:0.44  LR:0.000100\n",
      "   | > Step:25/68  GlobalStep:102560  TotalLoss:0.00199  PostnetLoss:0.00097  DecoderLoss:0.00102  StopLoss:0.06775  GradNorm:0.00389  GradNormST:0.03314  AvgTextLen:19.8  AvgSpecLen:73.3  StepTime:0.63  LR:0.000100\n",
      "   | > Step:35/68  GlobalStep:102570  TotalLoss:0.00217  PostnetLoss:0.00105  DecoderLoss:0.00111  StopLoss:0.11322  GradNorm:0.00347  GradNormST:0.07884  AvgTextLen:27.1  AvgSpecLen:104.7  StepTime:0.75  LR:0.000100\n",
      "   | > Step:45/68  GlobalStep:102580  TotalLoss:0.00225  PostnetLoss:0.00109  DecoderLoss:0.00116  StopLoss:0.10269  GradNorm:0.00286  GradNormST:0.06586  AvgTextLen:34.4  AvgSpecLen:118.6  StepTime:0.66  LR:0.000100\n",
      "   | > Step:55/68  GlobalStep:102590  TotalLoss:0.00251  PostnetLoss:0.00122  DecoderLoss:0.00129  StopLoss:0.16302  GradNorm:0.00290  GradNormST:0.15132  AvgTextLen:43.9  AvgSpecLen:145.2  StepTime:0.77  LR:0.000100\n",
      "   | > Step:65/68  GlobalStep:102600  TotalLoss:0.00274  PostnetLoss:0.00132  DecoderLoss:0.00142  StopLoss:0.18351  GradNorm:0.00267  GradNormST:0.19702  AvgTextLen:64.2  AvgSpecLen:207.4  StepTime:1.18  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:102603  AvgTotalLoss:0.12538  AvgPostnetLoss:0.00102  AvgDecoderLoss:0.00108  AvgStopLoss:0.12327  EpochTime:42.74  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.08420   PostnetLoss: 0.00515   DecoderLoss:0.00549  StopLoss: 0.07355  \n",
      "   | > TotalLoss: 0.07149   PostnetLoss: 0.00796   DecoderLoss:0.00843  StopLoss: 0.05510  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00102   Validation Loss: 0.00739\n",
      "\n",
      " > Epoch 487/1000\n",
      "   | > Step:6/68  GlobalStep:102610  TotalLoss:0.00154  PostnetLoss:0.00076  DecoderLoss:0.00079  StopLoss:0.04186  GradNorm:0.00596  GradNormST:0.01749  AvgTextLen:6.0  AvgSpecLen:34.8  StepTime:0.37  LR:0.000100\n",
      "   | > Step:16/68  GlobalStep:102620  TotalLoss:0.00181  PostnetLoss:0.00088  DecoderLoss:0.00093  StopLoss:0.04225  GradNorm:0.00457  GradNormST:0.01426  AvgTextLen:13.8  AvgSpecLen:60.6  StepTime:0.38  LR:0.000100\n",
      "   | > Step:26/68  GlobalStep:102630  TotalLoss:0.00201  PostnetLoss:0.00098  DecoderLoss:0.00103  StopLoss:0.05424  GradNorm:0.00383  GradNormST:0.02740  AvgTextLen:20.3  AvgSpecLen:77.5  StepTime:0.50  LR:0.000100\n",
      "   | > Step:36/68  GlobalStep:102640  TotalLoss:0.00214  PostnetLoss:0.00104  DecoderLoss:0.00110  StopLoss:0.05536  GradNorm:0.00371  GradNormST:0.02571  AvgTextLen:28.5  AvgSpecLen:100.2  StepTime:0.47  LR:0.000100\n",
      "   | > Step:46/68  GlobalStep:102650  TotalLoss:0.00229  PostnetLoss:0.00111  DecoderLoss:0.00117  StopLoss:0.08340  GradNorm:0.00324  GradNormST:0.07721  AvgTextLen:35.4  AvgSpecLen:122.6  StepTime:0.96  LR:0.000100\n",
      "   | > Step:56/68  GlobalStep:102660  TotalLoss:0.00248  PostnetLoss:0.00120  DecoderLoss:0.00128  StopLoss:0.07309  GradNorm:0.00286  GradNormST:0.06245  AvgTextLen:45.8  AvgSpecLen:145.4  StepTime:1.04  LR:0.000100\n",
      "   | > Step:66/68  GlobalStep:102670  TotalLoss:0.00288  PostnetLoss:0.00139  DecoderLoss:0.00149  StopLoss:0.18643  GradNorm:0.00273  GradNormST:0.18055  AvgTextLen:71.2  AvgSpecLen:220.8  StepTime:1.04  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:102672  AvgTotalLoss:0.08740  AvgPostnetLoss:0.00102  AvgDecoderLoss:0.00108  AvgStopLoss:0.08531  EpochTime:43.41  AvgStepTime:0.63\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.09161   PostnetLoss: 0.00525   DecoderLoss:0.00559  StopLoss: 0.08077  \n",
      "   | > TotalLoss: 0.07059   PostnetLoss: 0.00795   DecoderLoss:0.00842  StopLoss: 0.05422  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00102   Validation Loss: 0.00739\n",
      "\n",
      " > Epoch 488/1000\n",
      "   | > Step:7/68  GlobalStep:102680  TotalLoss:0.00154  PostnetLoss:0.00075  DecoderLoss:0.00079  StopLoss:0.07338  GradNorm:0.00452  GradNormST:0.03876  AvgTextLen:7.4  AvgSpecLen:44.6  StepTime:0.39  LR:0.000100\n",
      "   | > Step:17/68  GlobalStep:102690  TotalLoss:0.00179  PostnetLoss:0.00087  DecoderLoss:0.00092  StopLoss:0.07949  GradNorm:0.00454  GradNormST:0.02857  AvgTextLen:14.7  AvgSpecLen:66.0  StepTime:0.45  LR:0.000100\n",
      "   | > Step:27/68  GlobalStep:102700  TotalLoss:0.00205  PostnetLoss:0.00100  DecoderLoss:0.00105  StopLoss:0.05746  GradNorm:0.00516  GradNormST:0.02085  AvgTextLen:21.0  AvgSpecLen:80.2  StepTime:0.57  LR:0.000100\n",
      "   | > Step:37/68  GlobalStep:102710  TotalLoss:0.00217  PostnetLoss:0.00106  DecoderLoss:0.00112  StopLoss:0.06198  GradNorm:0.00419  GradNormST:0.04904  AvgTextLen:29.3  AvgSpecLen:102.4  StepTime:0.50  LR:0.000100\n",
      "   | > Step:47/68  GlobalStep:102720  TotalLoss:0.00237  PostnetLoss:0.00115  DecoderLoss:0.00122  StopLoss:0.06722  GradNorm:0.00338  GradNormST:0.04362  AvgTextLen:36.7  AvgSpecLen:126.9  StepTime:0.70  LR:0.000100\n",
      "   | > Step:57/68  GlobalStep:102730  TotalLoss:0.00251  PostnetLoss:0.00121  DecoderLoss:0.00129  StopLoss:0.03775  GradNorm:0.00261  GradNormST:0.01768  AvgTextLen:47.1  AvgSpecLen:161.0  StepTime:0.91  LR:0.000100\n",
      "   | > Step:67/68  GlobalStep:102740  TotalLoss:0.00287  PostnetLoss:0.00139  DecoderLoss:0.00149  StopLoss:0.22741  GradNorm:0.00292  GradNormST:0.23789  AvgTextLen:80.6  AvgSpecLen:246.5  StepTime:1.21  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:102741  AvgTotalLoss:0.08008  AvgPostnetLoss:0.00101  AvgDecoderLoss:0.00107  AvgStopLoss:0.07800  EpochTime:42.61  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.09011   PostnetLoss: 0.00525   DecoderLoss:0.00559  StopLoss: 0.07927  \n",
      "   | > TotalLoss: 0.06898   PostnetLoss: 0.00798   DecoderLoss:0.00846  StopLoss: 0.05254  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00101   Validation Loss: 0.00737\n",
      "\n",
      " > Epoch 489/1000\n",
      "   | > Step:8/68  GlobalStep:102750  TotalLoss:0.00158  PostnetLoss:0.00077  DecoderLoss:0.00081  StopLoss:0.04462  GradNorm:0.00471  GradNormST:0.01570  AvgTextLen:8.3  AvgSpecLen:44.0  StepTime:0.31  LR:0.000100\n",
      "   | > Step:18/68  GlobalStep:102760  TotalLoss:0.00175  PostnetLoss:0.00085  DecoderLoss:0.00090  StopLoss:0.06534  GradNorm:0.00395  GradNormST:0.02345  AvgTextLen:15.2  AvgSpecLen:64.5  StepTime:0.40  LR:0.000100\n",
      "   | > Step:28/68  GlobalStep:102770  TotalLoss:0.00204  PostnetLoss:0.00100  DecoderLoss:0.00105  StopLoss:0.11794  GradNorm:0.00470  GradNormST:0.06735  AvgTextLen:22.1  AvgSpecLen:85.4  StepTime:0.57  LR:0.000100\n",
      "   | > Step:38/68  GlobalStep:102780  TotalLoss:0.00217  PostnetLoss:0.00106  DecoderLoss:0.00111  StopLoss:0.06007  GradNorm:0.00472  GradNormST:0.03108  AvgTextLen:29.4  AvgSpecLen:104.1  StepTime:0.60  LR:0.000100\n",
      "   | > Step:48/68  GlobalStep:102790  TotalLoss:0.00233  PostnetLoss:0.00113  DecoderLoss:0.00120  StopLoss:0.05983  GradNorm:0.00396  GradNormST:0.03301  AvgTextLen:37.5  AvgSpecLen:123.8  StepTime:0.68  LR:0.000100\n",
      "   | > Step:58/68  GlobalStep:102800  TotalLoss:0.00252  PostnetLoss:0.00122  DecoderLoss:0.00130  StopLoss:0.08205  GradNorm:0.00295  GradNormST:0.05910  AvgTextLen:47.8  AvgSpecLen:158.0  StepTime:0.74  LR:0.000100\n",
      "   | > Step:68/68  GlobalStep:102810  TotalLoss:0.00288  PostnetLoss:0.00139  DecoderLoss:0.00149  StopLoss:0.20630  GradNorm:0.00297  GradNormST:0.23516  AvgTextLen:103.0  AvgSpecLen:308.9  StepTime:1.87  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:102810  AvgTotalLoss:0.07746  AvgPostnetLoss:0.00101  AvgDecoderLoss:0.00107  AvgStopLoss:0.07539  EpochTime:42.09  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.09302   PostnetLoss: 0.00518   DecoderLoss:0.00553  StopLoss: 0.08231  \n",
      "   | > TotalLoss: 0.07223   PostnetLoss: 0.00826   DecoderLoss:0.00875  StopLoss: 0.05523  \n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00101   Validation Loss: 0.00761\n",
      "\n",
      " > Epoch 490/1000\n",
      "   | > Step:9/68  GlobalStep:102820  TotalLoss:0.00152  PostnetLoss:0.00074  DecoderLoss:0.00078  StopLoss:0.05909  GradNorm:0.00354  GradNormST:0.02608  AvgTextLen:9.0  AvgSpecLen:42.1  StepTime:0.47  LR:0.000100\n",
      "   | > Step:19/68  GlobalStep:102830  TotalLoss:0.00175  PostnetLoss:0.00085  DecoderLoss:0.00089  StopLoss:0.06323  GradNorm:0.00417  GradNormST:0.02358  AvgTextLen:15.5  AvgSpecLen:63.2  StepTime:0.42  LR:0.000100\n",
      "   | > Step:29/68  GlobalStep:102840  TotalLoss:0.00198  PostnetLoss:0.00096  DecoderLoss:0.00101  StopLoss:0.05956  GradNorm:0.00454  GradNormST:0.01538  AvgTextLen:21.8  AvgSpecLen:83.4  StepTime:0.50  LR:0.000100\n",
      "   | > Step:39/68  GlobalStep:102850  TotalLoss:0.00222  PostnetLoss:0.00108  DecoderLoss:0.00114  StopLoss:0.06708  GradNorm:0.00488  GradNormST:0.03522  AvgTextLen:30.2  AvgSpecLen:110.4  StepTime:0.62  LR:0.000100\n",
      "   | > Step:49/68  GlobalStep:102860  TotalLoss:0.00233  PostnetLoss:0.00113  DecoderLoss:0.00120  StopLoss:0.09041  GradNorm:0.00415  GradNormST:0.06364  AvgTextLen:38.2  AvgSpecLen:127.7  StepTime:0.76  LR:0.000100\n",
      "   | > Step:59/68  GlobalStep:102870  TotalLoss:0.00252  PostnetLoss:0.00122  DecoderLoss:0.00130  StopLoss:0.08743  GradNorm:0.00363  GradNormST:0.07933  AvgTextLen:50.5  AvgSpecLen:155.9  StepTime:0.93  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:102879  AvgTotalLoss:0.08133  AvgPostnetLoss:0.00100  AvgDecoderLoss:0.00106  AvgStopLoss:0.07926  EpochTime:42.52  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.08663   PostnetLoss: 0.00523   DecoderLoss:0.00557  StopLoss: 0.07582  \n",
      "   | > TotalLoss: 0.07649   PostnetLoss: 0.00818   DecoderLoss:0.00866  StopLoss: 0.05964  \n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00100   Validation Loss: 0.00767\n",
      "\n",
      " > Epoch 491/1000\n",
      "   | > Step:0/68  GlobalStep:102880  TotalLoss:0.00146  PostnetLoss:0.00070  DecoderLoss:0.00076  StopLoss:0.07272  GradNorm:0.00510  GradNormST:0.02310  AvgTextLen:3.0  AvgSpecLen:25.3  StepTime:0.54  LR:0.000100\n",
      "   | > Step:10/68  GlobalStep:102890  TotalLoss:0.00158  PostnetLoss:0.00077  DecoderLoss:0.00081  StopLoss:0.03846  GradNorm:0.00348  GradNormST:0.01017  AvgTextLen:9.8  AvgSpecLen:48.6  StepTime:0.53  LR:0.000100\n",
      "   | > Step:20/68  GlobalStep:102900  TotalLoss:0.00182  PostnetLoss:0.00089  DecoderLoss:0.00093  StopLoss:0.04901  GradNorm:0.00349  GradNormST:0.02086  AvgTextLen:16.4  AvgSpecLen:64.8  StepTime:0.49  LR:0.000100\n",
      "   | > Step:30/68  GlobalStep:102910  TotalLoss:0.00198  PostnetLoss:0.00097  DecoderLoss:0.00102  StopLoss:0.06035  GradNorm:0.00413  GradNormST:0.02710  AvgTextLen:22.9  AvgSpecLen:85.8  StepTime:0.56  LR:0.000100\n",
      "   | > Step:40/68  GlobalStep:102920  TotalLoss:0.00222  PostnetLoss:0.00108  DecoderLoss:0.00114  StopLoss:0.07006  GradNorm:0.00472  GradNormST:0.03125  AvgTextLen:30.9  AvgSpecLen:105.3  StepTime:0.49  LR:0.000100\n",
      "   | > Step:50/68  GlobalStep:102930  TotalLoss:0.00239  PostnetLoss:0.00116  DecoderLoss:0.00123  StopLoss:0.05167  GradNorm:0.00479  GradNormST:0.03472  AvgTextLen:38.8  AvgSpecLen:132.0  StepTime:0.80  LR:0.000100\n",
      "   | > Step:60/68  GlobalStep:102940  TotalLoss:0.00255  PostnetLoss:0.00124  DecoderLoss:0.00132  StopLoss:0.05466  GradNorm:0.00349  GradNormST:0.03063  AvgTextLen:52.0  AvgSpecLen:172.7  StepTime:0.99  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:102948  AvgTotalLoss:0.07038  AvgPostnetLoss:0.00100  AvgDecoderLoss:0.00106  AvgStopLoss:0.06832  EpochTime:42.70  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.08552   PostnetLoss: 0.00514   DecoderLoss:0.00548  StopLoss: 0.07490  \n",
      "   | > TotalLoss: 0.07396   PostnetLoss: 0.00843   DecoderLoss:0.00892  StopLoss: 0.05661  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00100   Validation Loss: 0.00763\n",
      "\n",
      " > Epoch 492/1000\n",
      "   | > Step:1/68  GlobalStep:102950  TotalLoss:0.00144  PostnetLoss:0.00070  DecoderLoss:0.00074  StopLoss:0.04888  GradNorm:0.00421  GradNormST:0.01846  AvgTextLen:3.7  AvgSpecLen:28.6  StepTime:0.35  LR:0.000100\n",
      "   | > Step:11/68  GlobalStep:102960  TotalLoss:0.00155  PostnetLoss:0.00076  DecoderLoss:0.00079  StopLoss:0.05335  GradNorm:0.00325  GradNormST:0.02473  AvgTextLen:10.3  AvgSpecLen:49.4  StepTime:0.34  LR:0.000100\n",
      "   | > Step:21/68  GlobalStep:102970  TotalLoss:0.00179  PostnetLoss:0.00087  DecoderLoss:0.00092  StopLoss:0.06265  GradNorm:0.00322  GradNormST:0.01934  AvgTextLen:16.6  AvgSpecLen:66.3  StepTime:0.43  LR:0.000100\n",
      "   | > Step:31/68  GlobalStep:102980  TotalLoss:0.00200  PostnetLoss:0.00097  DecoderLoss:0.00103  StopLoss:0.05016  GradNorm:0.00385  GradNormST:0.02795  AvgTextLen:24.1  AvgSpecLen:82.7  StepTime:0.53  LR:0.000100\n",
      "   | > Step:41/68  GlobalStep:102990  TotalLoss:0.00223  PostnetLoss:0.00108  DecoderLoss:0.00114  StopLoss:0.04944  GradNorm:0.00391  GradNormST:0.02228  AvgTextLen:32.3  AvgSpecLen:109.4  StepTime:0.53  LR:0.000100\n",
      "   | > Step:51/68  GlobalStep:103000  TotalLoss:0.00234  PostnetLoss:0.00113  DecoderLoss:0.00120  StopLoss:0.06770  GradNorm:0.00397  GradNormST:0.03814  AvgTextLen:39.2  AvgSpecLen:126.9  StepTime:0.74  LR:0.000100\n",
      " | | > Checkpoint saving : /vm/TTS/models/cartman_models/mozilla-no-loc-fattn-stopnet-sigmoid-loss_masking-August-15-2019_11+26AM-de6aeb9/checkpoint_103000.pth.tar\n",
      "   | > Step:61/68  GlobalStep:103010  TotalLoss:0.00255  PostnetLoss:0.00123  DecoderLoss:0.00131  StopLoss:0.11332  GradNorm:0.00392  GradNormST:0.07021  AvgTextLen:54.6  AvgSpecLen:165.5  StepTime:0.89  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:103017  AvgTotalLoss:0.07500  AvgPostnetLoss:0.00100  AvgDecoderLoss:0.00105  AvgStopLoss:0.07294  EpochTime:42.41  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.08419   PostnetLoss: 0.00519   DecoderLoss:0.00554  StopLoss: 0.07346  \n",
      "   | > TotalLoss: 0.07362   PostnetLoss: 0.00818   DecoderLoss:0.00867  StopLoss: 0.05678  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00100   Validation Loss: 0.00754\n",
      "\n",
      " > Epoch 493/1000\n",
      "   | > Step:2/68  GlobalStep:103020  TotalLoss:0.00137  PostnetLoss:0.00067  DecoderLoss:0.00071  StopLoss:0.06416  GradNorm:0.00440  GradNormST:0.02162  AvgTextLen:4.0  AvgSpecLen:31.3  StepTime:0.31  LR:0.000100\n",
      "   | > Step:12/68  GlobalStep:103030  TotalLoss:0.00157  PostnetLoss:0.00076  DecoderLoss:0.00080  StopLoss:0.06540  GradNorm:0.00328  GradNormST:0.02844  AvgTextLen:11.0  AvgSpecLen:49.3  StepTime:0.42  LR:0.000100\n",
      "   | > Step:22/68  GlobalStep:103040  TotalLoss:0.00185  PostnetLoss:0.00090  DecoderLoss:0.00095  StopLoss:0.06260  GradNorm:0.00315  GradNormST:0.02620  AvgTextLen:18.0  AvgSpecLen:72.0  StepTime:0.46  LR:0.000100\n",
      "   | > Step:32/68  GlobalStep:103050  TotalLoss:0.00202  PostnetLoss:0.00098  DecoderLoss:0.00104  StopLoss:0.14658  GradNorm:0.00310  GradNormST:0.08877  AvgTextLen:24.4  AvgSpecLen:91.5  StepTime:0.56  LR:0.000100\n",
      "   | > Step:42/68  GlobalStep:103060  TotalLoss:0.00222  PostnetLoss:0.00108  DecoderLoss:0.00114  StopLoss:0.07108  GradNorm:0.00336  GradNormST:0.02978  AvgTextLen:33.1  AvgSpecLen:109.7  StepTime:0.61  LR:0.000100\n",
      "   | > Step:52/68  GlobalStep:103070  TotalLoss:0.00244  PostnetLoss:0.00119  DecoderLoss:0.00126  StopLoss:0.08030  GradNorm:0.00413  GradNormST:0.03127  AvgTextLen:42.1  AvgSpecLen:137.5  StepTime:0.80  LR:0.000100\n",
      "   | > Step:62/68  GlobalStep:103080  TotalLoss:0.00260  PostnetLoss:0.00126  DecoderLoss:0.00134  StopLoss:0.06364  GradNorm:0.00373  GradNormST:0.03856  AvgTextLen:54.0  AvgSpecLen:174.7  StepTime:0.75  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:103086  AvgTotalLoss:0.07760  AvgPostnetLoss:0.00100  AvgDecoderLoss:0.00106  AvgStopLoss:0.07554  EpochTime:42.52  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.08674   PostnetLoss: 0.00523   DecoderLoss:0.00557  StopLoss: 0.07594  \n",
      "   | > TotalLoss: 0.07111   PostnetLoss: 0.00811   DecoderLoss:0.00858  StopLoss: 0.05442  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00100   Validation Loss: 0.00739\n",
      "\n",
      " > Epoch 494/1000\n",
      "   | > Step:3/68  GlobalStep:103090  TotalLoss:0.00140  PostnetLoss:0.00068  DecoderLoss:0.00072  StopLoss:0.06635  GradNorm:0.00397  GradNormST:0.02264  AvgTextLen:4.7  AvgSpecLen:29.7  StepTime:0.32  LR:0.000100\n",
      "   | > Step:13/68  GlobalStep:103100  TotalLoss:0.00160  PostnetLoss:0.00078  DecoderLoss:0.00082  StopLoss:0.08354  GradNorm:0.00384  GradNormST:0.02823  AvgTextLen:11.5  AvgSpecLen:55.2  StepTime:0.39  LR:0.000100\n",
      "   | > Step:23/68  GlobalStep:103110  TotalLoss:0.00189  PostnetLoss:0.00092  DecoderLoss:0.00097  StopLoss:0.06241  GradNorm:0.00324  GradNormST:0.02149  AvgTextLen:18.3  AvgSpecLen:72.2  StepTime:0.45  LR:0.000100\n",
      "   | > Step:33/68  GlobalStep:103120  TotalLoss:0.00208  PostnetLoss:0.00101  DecoderLoss:0.00107  StopLoss:0.06415  GradNorm:0.00362  GradNormST:0.02155  AvgTextLen:25.9  AvgSpecLen:88.9  StepTime:0.53  LR:0.000100\n",
      "   | > Step:43/68  GlobalStep:103130  TotalLoss:0.00224  PostnetLoss:0.00109  DecoderLoss:0.00115  StopLoss:0.09112  GradNorm:0.00345  GradNormST:0.03978  AvgTextLen:33.4  AvgSpecLen:115.7  StepTime:0.55  LR:0.000100\n",
      "   | > Step:53/68  GlobalStep:103140  TotalLoss:0.00236  PostnetLoss:0.00115  DecoderLoss:0.00122  StopLoss:0.06408  GradNorm:0.00355  GradNormST:0.03227  AvgTextLen:42.4  AvgSpecLen:137.6  StepTime:0.81  LR:0.000100\n",
      "   | > Step:63/68  GlobalStep:103150  TotalLoss:0.00268  PostnetLoss:0.00129  DecoderLoss:0.00138  StopLoss:0.05186  GradNorm:0.00383  GradNormST:0.03126  AvgTextLen:58.6  AvgSpecLen:185.1  StepTime:0.90  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:103155  AvgTotalLoss:0.06396  AvgPostnetLoss:0.00100  AvgDecoderLoss:0.00106  AvgStopLoss:0.06190  EpochTime:42.16  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.08347   PostnetLoss: 0.00512   DecoderLoss:0.00546  StopLoss: 0.07289  \n",
      "   | > TotalLoss: 0.07421   PostnetLoss: 0.00827   DecoderLoss:0.00877  StopLoss: 0.05717  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00100   Validation Loss: 0.00759\n",
      "\n",
      " > Epoch 495/1000\n",
      "   | > Step:4/68  GlobalStep:103160  TotalLoss:0.00138  PostnetLoss:0.00067  DecoderLoss:0.00071  StopLoss:0.08587  GradNorm:0.00378  GradNormST:0.03340  AvgTextLen:4.8  AvgSpecLen:31.1  StepTime:0.28  LR:0.000100\n",
      "   | > Step:14/68  GlobalStep:103170  TotalLoss:0.00166  PostnetLoss:0.00081  DecoderLoss:0.00085  StopLoss:0.05167  GradNorm:0.00355  GradNormST:0.02124  AvgTextLen:12.2  AvgSpecLen:52.0  StepTime:0.36  LR:0.000100\n",
      "   | > Step:24/68  GlobalStep:103180  TotalLoss:0.00194  PostnetLoss:0.00095  DecoderLoss:0.00100  StopLoss:0.05729  GradNorm:0.00309  GradNormST:0.01746  AvgTextLen:19.2  AvgSpecLen:79.0  StepTime:0.49  LR:0.000100\n",
      "   | > Step:34/68  GlobalStep:103190  TotalLoss:0.00210  PostnetLoss:0.00102  DecoderLoss:0.00108  StopLoss:0.04843  GradNorm:0.00331  GradNormST:0.01785  AvgTextLen:26.2  AvgSpecLen:95.9  StepTime:0.57  LR:0.000100\n",
      "   | > Step:44/68  GlobalStep:103200  TotalLoss:0.00229  PostnetLoss:0.00111  DecoderLoss:0.00118  StopLoss:0.04655  GradNorm:0.00387  GradNormST:0.01848  AvgTextLen:33.8  AvgSpecLen:116.3  StepTime:0.53  LR:0.000100\n",
      "   | > Step:54/68  GlobalStep:103210  TotalLoss:0.00243  PostnetLoss:0.00118  DecoderLoss:0.00125  StopLoss:0.04574  GradNorm:0.00373  GradNormST:0.01533  AvgTextLen:42.2  AvgSpecLen:144.9  StepTime:0.88  LR:0.000100\n",
      "   | > Step:64/68  GlobalStep:103220  TotalLoss:0.00252  PostnetLoss:0.00122  DecoderLoss:0.00130  StopLoss:0.04511  GradNorm:0.00331  GradNormST:0.01773  AvgTextLen:60.6  AvgSpecLen:177.6  StepTime:0.80  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:103224  AvgTotalLoss:0.06367  AvgPostnetLoss:0.00100  AvgDecoderLoss:0.00106  AvgStopLoss:0.06162  EpochTime:42.38  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.08247   PostnetLoss: 0.00509   DecoderLoss:0.00542  StopLoss: 0.07197  \n",
      "   | > TotalLoss: 0.07217   PostnetLoss: 0.00803   DecoderLoss:0.00851  StopLoss: 0.05563  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00100   Validation Loss: 0.00742\n",
      "\n",
      " > Epoch 496/1000\n",
      "   | > Step:5/68  GlobalStep:103230  TotalLoss:0.00144  PostnetLoss:0.00070  DecoderLoss:0.00074  StopLoss:0.06427  GradNorm:0.00333  GradNormST:0.03128  AvgTextLen:5.9  AvgSpecLen:33.2  StepTime:0.32  LR:0.000100\n",
      "   | > Step:15/68  GlobalStep:103240  TotalLoss:0.00173  PostnetLoss:0.00085  DecoderLoss:0.00089  StopLoss:0.05575  GradNorm:0.00330  GradNormST:0.01745  AvgTextLen:12.8  AvgSpecLen:59.1  StepTime:0.55  LR:0.000100\n",
      "   | > Step:25/68  GlobalStep:103250  TotalLoss:0.00188  PostnetLoss:0.00092  DecoderLoss:0.00096  StopLoss:0.05318  GradNorm:0.00324  GradNormST:0.01718  AvgTextLen:19.8  AvgSpecLen:73.3  StepTime:0.36  LR:0.000100\n",
      "   | > Step:35/68  GlobalStep:103260  TotalLoss:0.00211  PostnetLoss:0.00103  DecoderLoss:0.00108  StopLoss:0.05953  GradNorm:0.00321  GradNormST:0.02231  AvgTextLen:27.1  AvgSpecLen:104.7  StepTime:0.80  LR:0.000100\n",
      "   | > Step:45/68  GlobalStep:103270  TotalLoss:0.00222  PostnetLoss:0.00108  DecoderLoss:0.00115  StopLoss:0.06959  GradNorm:0.00328  GradNormST:0.02177  AvgTextLen:34.4  AvgSpecLen:118.6  StepTime:0.65  LR:0.000100\n",
      "   | > Step:55/68  GlobalStep:103280  TotalLoss:0.00249  PostnetLoss:0.00121  DecoderLoss:0.00128  StopLoss:0.04324  GradNorm:0.00408  GradNormST:0.01719  AvgTextLen:43.9  AvgSpecLen:145.2  StepTime:0.85  LR:0.000100\n",
      "   | > Step:65/68  GlobalStep:103290  TotalLoss:0.00271  PostnetLoss:0.00131  DecoderLoss:0.00140  StopLoss:0.06931  GradNorm:0.00316  GradNormST:0.03991  AvgTextLen:64.2  AvgSpecLen:207.4  StepTime:1.20  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:103293  AvgTotalLoss:0.06522  AvgPostnetLoss:0.00100  AvgDecoderLoss:0.00106  AvgStopLoss:0.06317  EpochTime:42.99  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.08199   PostnetLoss: 0.00517   DecoderLoss:0.00551  StopLoss: 0.07130  \n",
      "   | > TotalLoss: 0.07231   PostnetLoss: 0.00819   DecoderLoss:0.00869  StopLoss: 0.05543  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00100   Validation Loss: 0.00757\n",
      "\n",
      " > Epoch 497/1000\n",
      "   | > Step:6/68  GlobalStep:103300  TotalLoss:0.00146  PostnetLoss:0.00071  DecoderLoss:0.00075  StopLoss:0.04453  GradNorm:0.00385  GradNormST:0.02553  AvgTextLen:6.0  AvgSpecLen:34.8  StepTime:0.42  LR:0.000100\n",
      "   | > Step:16/68  GlobalStep:103310  TotalLoss:0.00177  PostnetLoss:0.00086  DecoderLoss:0.00091  StopLoss:0.05055  GradNorm:0.00369  GradNormST:0.02111  AvgTextLen:13.8  AvgSpecLen:60.6  StepTime:0.37  LR:0.000100\n",
      "   | > Step:26/68  GlobalStep:103320  TotalLoss:0.00192  PostnetLoss:0.00094  DecoderLoss:0.00099  StopLoss:0.04356  GradNorm:0.00310  GradNormST:0.01866  AvgTextLen:20.3  AvgSpecLen:77.5  StepTime:0.48  LR:0.000100\n",
      "   | > Step:36/68  GlobalStep:103330  TotalLoss:0.00208  PostnetLoss:0.00101  DecoderLoss:0.00107  StopLoss:0.05090  GradNorm:0.00295  GradNormST:0.01264  AvgTextLen:28.5  AvgSpecLen:100.2  StepTime:0.56  LR:0.000100\n",
      "   | > Step:46/68  GlobalStep:103340  TotalLoss:0.00223  PostnetLoss:0.00108  DecoderLoss:0.00115  StopLoss:0.04913  GradNorm:0.00314  GradNormST:0.01761  AvgTextLen:35.4  AvgSpecLen:122.6  StepTime:0.84  LR:0.000100\n",
      "   | > Step:56/68  GlobalStep:103350  TotalLoss:0.00246  PostnetLoss:0.00119  DecoderLoss:0.00127  StopLoss:0.04133  GradNorm:0.00409  GradNormST:0.01846  AvgTextLen:45.8  AvgSpecLen:145.4  StepTime:0.84  LR:0.000100\n",
      "   | > Step:66/68  GlobalStep:103360  TotalLoss:0.00288  PostnetLoss:0.00139  DecoderLoss:0.00149  StopLoss:0.09805  GradNorm:0.00385  GradNormST:0.08602  AvgTextLen:71.2  AvgSpecLen:220.8  StepTime:1.04  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:103362  AvgTotalLoss:0.06052  AvgPostnetLoss:0.00100  AvgDecoderLoss:0.00106  AvgStopLoss:0.05847  EpochTime:42.65  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.08106   PostnetLoss: 0.00510   DecoderLoss:0.00544  StopLoss: 0.07052  \n",
      "   | > TotalLoss: 0.07313   PostnetLoss: 0.00820   DecoderLoss:0.00869  StopLoss: 0.05624  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00100   Validation Loss: 0.00749\n",
      "\n",
      " > Epoch 498/1000\n",
      "   | > Step:7/68  GlobalStep:103370  TotalLoss:0.00151  PostnetLoss:0.00074  DecoderLoss:0.00077  StopLoss:0.08783  GradNorm:0.00381  GradNormST:0.03604  AvgTextLen:7.4  AvgSpecLen:44.6  StepTime:0.33  LR:0.000100\n",
      "   | > Step:17/68  GlobalStep:103380  TotalLoss:0.00173  PostnetLoss:0.00084  DecoderLoss:0.00089  StopLoss:0.10578  GradNorm:0.00337  GradNormST:0.05200  AvgTextLen:14.7  AvgSpecLen:66.0  StepTime:0.34  LR:0.000100\n",
      "   | > Step:27/68  GlobalStep:103390  TotalLoss:0.00199  PostnetLoss:0.00097  DecoderLoss:0.00102  StopLoss:0.06349  GradNorm:0.00308  GradNormST:0.02578  AvgTextLen:21.0  AvgSpecLen:80.2  StepTime:0.53  LR:0.000100\n",
      "   | > Step:37/68  GlobalStep:103400  TotalLoss:0.00213  PostnetLoss:0.00104  DecoderLoss:0.00109  StopLoss:0.06436  GradNorm:0.00303  GradNormST:0.03264  AvgTextLen:29.3  AvgSpecLen:102.4  StepTime:0.60  LR:0.000100\n",
      "   | > Step:47/68  GlobalStep:103410  TotalLoss:0.00232  PostnetLoss:0.00113  DecoderLoss:0.00120  StopLoss:0.06452  GradNorm:0.00312  GradNormST:0.02337  AvgTextLen:36.7  AvgSpecLen:126.9  StepTime:0.72  LR:0.000100\n",
      "   | > Step:57/68  GlobalStep:103420  TotalLoss:0.00251  PostnetLoss:0.00121  DecoderLoss:0.00129  StopLoss:0.04092  GradNorm:0.00326  GradNormST:0.01337  AvgTextLen:47.1  AvgSpecLen:161.0  StepTime:0.85  LR:0.000100\n",
      "   | > Step:67/68  GlobalStep:103430  TotalLoss:0.00290  PostnetLoss:0.00140  DecoderLoss:0.00150  StopLoss:0.03050  GradNorm:0.00288  GradNormST:0.01278  AvgTextLen:80.6  AvgSpecLen:246.5  StepTime:1.21  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:103431  AvgTotalLoss:0.06910  AvgPostnetLoss:0.00100  AvgDecoderLoss:0.00106  AvgStopLoss:0.06705  EpochTime:42.08  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.08365   PostnetLoss: 0.00526   DecoderLoss:0.00560  StopLoss: 0.07278  \n",
      "   | > TotalLoss: 0.07209   PostnetLoss: 0.00773   DecoderLoss:0.00819  StopLoss: 0.05617  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00100   Validation Loss: 0.00743\n",
      "\n",
      " > Epoch 499/1000\n",
      "   | > Step:8/68  GlobalStep:103440  TotalLoss:0.00158  PostnetLoss:0.00077  DecoderLoss:0.00081  StopLoss:0.11203  GradNorm:0.00410  GradNormST:0.03941  AvgTextLen:8.3  AvgSpecLen:44.0  StepTime:0.34  LR:0.000100\n",
      "   | > Step:18/68  GlobalStep:103450  TotalLoss:0.00175  PostnetLoss:0.00085  DecoderLoss:0.00090  StopLoss:0.10542  GradNorm:0.00352  GradNormST:0.04576  AvgTextLen:15.2  AvgSpecLen:64.5  StepTime:0.42  LR:0.000100\n",
      "   | > Step:28/68  GlobalStep:103460  TotalLoss:0.00199  PostnetLoss:0.00097  DecoderLoss:0.00102  StopLoss:0.11702  GradNorm:0.00336  GradNormST:0.05232  AvgTextLen:22.1  AvgSpecLen:85.4  StepTime:0.44  LR:0.000100\n",
      "   | > Step:38/68  GlobalStep:103470  TotalLoss:0.00213  PostnetLoss:0.00104  DecoderLoss:0.00110  StopLoss:0.08991  GradNorm:0.00296  GradNormST:0.04637  AvgTextLen:29.4  AvgSpecLen:104.1  StepTime:0.65  LR:0.000100\n",
      "   | > Step:48/68  GlobalStep:103480  TotalLoss:0.00234  PostnetLoss:0.00114  DecoderLoss:0.00121  StopLoss:0.10251  GradNorm:0.00300  GradNormST:0.05675  AvgTextLen:37.5  AvgSpecLen:123.8  StepTime:0.59  LR:0.000100\n",
      "   | > Step:58/68  GlobalStep:103490  TotalLoss:0.00255  PostnetLoss:0.00124  DecoderLoss:0.00131  StopLoss:0.08261  GradNorm:0.00381  GradNormST:0.04300  AvgTextLen:47.8  AvgSpecLen:158.0  StepTime:0.85  LR:0.000100\n",
      "   | > Step:68/68  GlobalStep:103500  TotalLoss:0.00286  PostnetLoss:0.00138  DecoderLoss:0.00148  StopLoss:0.02409  GradNorm:0.00272  GradNormST:0.01418  AvgTextLen:103.0  AvgSpecLen:308.9  StepTime:1.87  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:103500  AvgTotalLoss:0.09337  AvgPostnetLoss:0.00101  AvgDecoderLoss:0.00107  AvgStopLoss:0.09128  EpochTime:42.97  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.09873   PostnetLoss: 0.00501   DecoderLoss:0.00533  StopLoss: 0.08839  \n",
      "   | > TotalLoss: 0.07234   PostnetLoss: 0.00771   DecoderLoss:0.00817  StopLoss: 0.05646  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00101   Validation Loss: 0.00736\n",
      "\n",
      " > Epoch 500/1000\n",
      "   | > Step:9/68  GlobalStep:103510  TotalLoss:0.00153  PostnetLoss:0.00075  DecoderLoss:0.00079  StopLoss:0.05777  GradNorm:0.00347  GradNormST:0.02188  AvgTextLen:9.0  AvgSpecLen:42.1  StepTime:0.32  LR:0.000100\n",
      "   | > Step:19/68  GlobalStep:103520  TotalLoss:0.00174  PostnetLoss:0.00085  DecoderLoss:0.00089  StopLoss:0.09374  GradNorm:0.00337  GradNormST:0.03484  AvgTextLen:15.5  AvgSpecLen:63.2  StepTime:0.44  LR:0.000100\n",
      "   | > Step:29/68  GlobalStep:103530  TotalLoss:0.00195  PostnetLoss:0.00095  DecoderLoss:0.00100  StopLoss:0.07777  GradNorm:0.00296  GradNormST:0.01917  AvgTextLen:21.8  AvgSpecLen:83.4  StepTime:0.59  LR:0.000100\n",
      "   | > Step:39/68  GlobalStep:103540  TotalLoss:0.00217  PostnetLoss:0.00106  DecoderLoss:0.00112  StopLoss:0.07408  GradNorm:0.00293  GradNormST:0.03791  AvgTextLen:30.2  AvgSpecLen:110.4  StepTime:0.52  LR:0.000100\n",
      "   | > Step:49/68  GlobalStep:103550  TotalLoss:0.00233  PostnetLoss:0.00113  DecoderLoss:0.00120  StopLoss:0.05939  GradNorm:0.00295  GradNormST:0.02612  AvgTextLen:38.2  AvgSpecLen:127.7  StepTime:0.75  LR:0.000100\n",
      "   | > Step:59/68  GlobalStep:103560  TotalLoss:0.00250  PostnetLoss:0.00121  DecoderLoss:0.00129  StopLoss:0.05469  GradNorm:0.00306  GradNormST:0.02254  AvgTextLen:50.5  AvgSpecLen:155.9  StepTime:0.93  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:103569  AvgTotalLoss:0.07499  AvgPostnetLoss:0.00100  AvgDecoderLoss:0.00106  AvgStopLoss:0.07293  EpochTime:42.30  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.08965   PostnetLoss: 0.00514   DecoderLoss:0.00546  StopLoss: 0.07905  \n",
      "   | > TotalLoss: 0.07184   PostnetLoss: 0.00770   DecoderLoss:0.00813  StopLoss: 0.05602  \n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00100   Validation Loss: 0.00731\n",
      "\n",
      " > Epoch 501/1000\n",
      "   | > Step:0/68  GlobalStep:103570  TotalLoss:0.00151  PostnetLoss:0.00073  DecoderLoss:0.00078  StopLoss:0.06765  GradNorm:0.00670  GradNormST:0.02513  AvgTextLen:3.0  AvgSpecLen:25.3  StepTime:0.43  LR:0.000100\n",
      "   | > Step:10/68  GlobalStep:103580  TotalLoss:0.00157  PostnetLoss:0.00077  DecoderLoss:0.00081  StopLoss:0.06215  GradNorm:0.00381  GradNormST:0.02923  AvgTextLen:9.8  AvgSpecLen:48.6  StepTime:0.55  LR:0.000100\n",
      "   | > Step:20/68  GlobalStep:103590  TotalLoss:0.00179  PostnetLoss:0.00087  DecoderLoss:0.00092  StopLoss:0.09097  GradNorm:0.00330  GradNormST:0.04876  AvgTextLen:16.4  AvgSpecLen:64.8  StepTime:0.52  LR:0.000100\n",
      "   | > Step:30/68  GlobalStep:103600  TotalLoss:0.00199  PostnetLoss:0.00097  DecoderLoss:0.00102  StopLoss:0.08833  GradNorm:0.00302  GradNormST:0.04621  AvgTextLen:22.9  AvgSpecLen:85.8  StepTime:0.55  LR:0.000100\n",
      "   | > Step:40/68  GlobalStep:103610  TotalLoss:0.00214  PostnetLoss:0.00104  DecoderLoss:0.00110  StopLoss:0.09162  GradNorm:0.00285  GradNormST:0.04768  AvgTextLen:30.9  AvgSpecLen:105.3  StepTime:0.60  LR:0.000100\n",
      "   | > Step:50/68  GlobalStep:103620  TotalLoss:0.00235  PostnetLoss:0.00114  DecoderLoss:0.00121  StopLoss:0.05079  GradNorm:0.00288  GradNormST:0.01806  AvgTextLen:38.8  AvgSpecLen:132.0  StepTime:0.68  LR:0.000100\n",
      "   | > Step:60/68  GlobalStep:103630  TotalLoss:0.00256  PostnetLoss:0.00124  DecoderLoss:0.00132  StopLoss:0.05386  GradNorm:0.00327  GradNormST:0.01793  AvgTextLen:52.0  AvgSpecLen:172.7  StepTime:0.99  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:103638  AvgTotalLoss:0.06991  AvgPostnetLoss:0.00100  AvgDecoderLoss:0.00106  AvgStopLoss:0.06786  EpochTime:42.43  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.09504   PostnetLoss: 0.00521   DecoderLoss:0.00553  StopLoss: 0.08431  \n",
      "   | > TotalLoss: 0.06875   PostnetLoss: 0.00761   DecoderLoss:0.00806  StopLoss: 0.05308  \n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00100   Validation Loss: 0.00729\n",
      "\n",
      " > Epoch 502/1000\n",
      "   | > Step:1/68  GlobalStep:103640  TotalLoss:0.00139  PostnetLoss:0.00067  DecoderLoss:0.00072  StopLoss:0.04796  GradNorm:0.00453  GradNormST:0.02200  AvgTextLen:3.7  AvgSpecLen:28.6  StepTime:0.29  LR:0.000100\n",
      "   | > Step:11/68  GlobalStep:103650  TotalLoss:0.00158  PostnetLoss:0.00077  DecoderLoss:0.00081  StopLoss:0.08340  GradNorm:0.00360  GradNormST:0.02869  AvgTextLen:10.3  AvgSpecLen:49.4  StepTime:0.39  LR:0.000100\n",
      "   | > Step:21/68  GlobalStep:103660  TotalLoss:0.00178  PostnetLoss:0.00087  DecoderLoss:0.00091  StopLoss:0.06778  GradNorm:0.00326  GradNormST:0.02153  AvgTextLen:16.6  AvgSpecLen:66.3  StepTime:0.43  LR:0.000100\n",
      "   | > Step:31/68  GlobalStep:103670  TotalLoss:0.00198  PostnetLoss:0.00096  DecoderLoss:0.00102  StopLoss:0.07107  GradNorm:0.00299  GradNormST:0.02457  AvgTextLen:24.1  AvgSpecLen:82.7  StepTime:0.56  LR:0.000100\n",
      "   | > Step:41/68  GlobalStep:103680  TotalLoss:0.00219  PostnetLoss:0.00106  DecoderLoss:0.00113  StopLoss:0.05795  GradNorm:0.00288  GradNormST:0.02650  AvgTextLen:32.3  AvgSpecLen:109.4  StepTime:0.64  LR:0.000100\n",
      "   | > Step:51/68  GlobalStep:103690  TotalLoss:0.00231  PostnetLoss:0.00112  DecoderLoss:0.00119  StopLoss:0.07442  GradNorm:0.00273  GradNormST:0.03082  AvgTextLen:39.2  AvgSpecLen:126.9  StepTime:0.79  LR:0.000100\n",
      "   | > Step:61/68  GlobalStep:103700  TotalLoss:0.00252  PostnetLoss:0.00122  DecoderLoss:0.00130  StopLoss:0.05980  GradNorm:0.00275  GradNormST:0.02559  AvgTextLen:54.6  AvgSpecLen:165.5  StepTime:0.93  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:103707  AvgTotalLoss:0.06499  AvgPostnetLoss:0.00100  AvgDecoderLoss:0.00105  AvgStopLoss:0.06295  EpochTime:42.36  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.09059   PostnetLoss: 0.00517   DecoderLoss:0.00548  StopLoss: 0.07994  \n",
      "   | > TotalLoss: 0.07156   PostnetLoss: 0.00781   DecoderLoss:0.00827  StopLoss: 0.05548  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00100   Validation Loss: 0.00736\n",
      "\n",
      " > Epoch 503/1000\n",
      "   | > Step:2/68  GlobalStep:103710  TotalLoss:0.00133  PostnetLoss:0.00065  DecoderLoss:0.00068  StopLoss:0.07552  GradNorm:0.00377  GradNormST:0.02635  AvgTextLen:4.0  AvgSpecLen:31.3  StepTime:0.50  LR:0.000100\n",
      "   | > Step:12/68  GlobalStep:103720  TotalLoss:0.00155  PostnetLoss:0.00076  DecoderLoss:0.00080  StopLoss:0.05345  GradNorm:0.00323  GradNormST:0.01615  AvgTextLen:11.0  AvgSpecLen:49.3  StepTime:0.42  LR:0.000100\n",
      "   | > Step:22/68  GlobalStep:103730  TotalLoss:0.00180  PostnetLoss:0.00088  DecoderLoss:0.00092  StopLoss:0.07074  GradNorm:0.00337  GradNormST:0.02119  AvgTextLen:18.0  AvgSpecLen:72.0  StepTime:0.36  LR:0.000100\n",
      "   | > Step:32/68  GlobalStep:103740  TotalLoss:0.00198  PostnetLoss:0.00096  DecoderLoss:0.00102  StopLoss:0.08883  GradNorm:0.00290  GradNormST:0.02312  AvgTextLen:24.4  AvgSpecLen:91.5  StepTime:0.56  LR:0.000100\n",
      "   | > Step:42/68  GlobalStep:103750  TotalLoss:0.00214  PostnetLoss:0.00104  DecoderLoss:0.00110  StopLoss:0.06449  GradNorm:0.00281  GradNormST:0.01626  AvgTextLen:33.1  AvgSpecLen:109.7  StepTime:0.58  LR:0.000100\n",
      "   | > Step:52/68  GlobalStep:103760  TotalLoss:0.00238  PostnetLoss:0.00115  DecoderLoss:0.00122  StopLoss:0.06302  GradNorm:0.00289  GradNormST:0.02185  AvgTextLen:42.1  AvgSpecLen:137.5  StepTime:0.87  LR:0.000100\n",
      "   | > Step:62/68  GlobalStep:103770  TotalLoss:0.00261  PostnetLoss:0.00126  DecoderLoss:0.00135  StopLoss:0.06037  GradNorm:0.00279  GradNormST:0.02177  AvgTextLen:54.0  AvgSpecLen:174.7  StepTime:0.75  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:103776  AvgTotalLoss:0.06485  AvgPostnetLoss:0.00099  AvgDecoderLoss:0.00105  AvgStopLoss:0.06281  EpochTime:43.37  AvgStepTime:0.63\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.08107   PostnetLoss: 0.00513   DecoderLoss:0.00546  StopLoss: 0.07048  \n",
      "   | > TotalLoss: 0.07013   PostnetLoss: 0.00774   DecoderLoss:0.00821  StopLoss: 0.05417  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00099   Validation Loss: 0.00735\n",
      "\n",
      " > Epoch 504/1000\n",
      "   | > Step:3/68  GlobalStep:103780  TotalLoss:0.00138  PostnetLoss:0.00067  DecoderLoss:0.00071  StopLoss:0.08361  GradNorm:0.00363  GradNormST:0.02949  AvgTextLen:4.7  AvgSpecLen:29.7  StepTime:0.30  LR:0.000100\n",
      "   | > Step:13/68  GlobalStep:103790  TotalLoss:0.00156  PostnetLoss:0.00076  DecoderLoss:0.00080  StopLoss:0.08612  GradNorm:0.00326  GradNormST:0.02747  AvgTextLen:11.5  AvgSpecLen:55.2  StepTime:0.41  LR:0.000100\n",
      "   | > Step:23/68  GlobalStep:103800  TotalLoss:0.00186  PostnetLoss:0.00091  DecoderLoss:0.00095  StopLoss:0.05786  GradNorm:0.00336  GradNormST:0.01759  AvgTextLen:18.3  AvgSpecLen:72.2  StepTime:0.39  LR:0.000100\n",
      "   | > Step:33/68  GlobalStep:103810  TotalLoss:0.00204  PostnetLoss:0.00099  DecoderLoss:0.00105  StopLoss:0.05260  GradNorm:0.00303  GradNormST:0.01628  AvgTextLen:25.9  AvgSpecLen:88.9  StepTime:0.43  LR:0.000100\n",
      "   | > Step:43/68  GlobalStep:103820  TotalLoss:0.00218  PostnetLoss:0.00106  DecoderLoss:0.00112  StopLoss:0.07249  GradNorm:0.00273  GradNormST:0.02002  AvgTextLen:33.4  AvgSpecLen:115.7  StepTime:0.68  LR:0.000100\n",
      "   | > Step:53/68  GlobalStep:103830  TotalLoss:0.00233  PostnetLoss:0.00113  DecoderLoss:0.00120  StopLoss:0.05367  GradNorm:0.00269  GradNormST:0.01765  AvgTextLen:42.4  AvgSpecLen:137.6  StepTime:0.74  LR:0.000100\n",
      "   | > Step:63/68  GlobalStep:103840  TotalLoss:0.00266  PostnetLoss:0.00129  DecoderLoss:0.00138  StopLoss:0.04006  GradNorm:0.00274  GradNormST:0.01581  AvgTextLen:58.6  AvgSpecLen:185.1  StepTime:0.91  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:103845  AvgTotalLoss:0.05958  AvgPostnetLoss:0.00099  AvgDecoderLoss:0.00105  AvgStopLoss:0.05753  EpochTime:43.00  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.09297   PostnetLoss: 0.00512   DecoderLoss:0.00544  StopLoss: 0.08241  \n",
      "   | > TotalLoss: 0.07127   PostnetLoss: 0.00773   DecoderLoss:0.00819  StopLoss: 0.05535  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00099   Validation Loss: 0.00738\n",
      "\n",
      " > Epoch 505/1000\n",
      "   | > Step:4/68  GlobalStep:103850  TotalLoss:0.00141  PostnetLoss:0.00069  DecoderLoss:0.00073  StopLoss:0.09098  GradNorm:0.00437  GradNormST:0.04036  AvgTextLen:4.8  AvgSpecLen:31.1  StepTime:0.27  LR:0.000100\n",
      "   | > Step:14/68  GlobalStep:103860  TotalLoss:0.00164  PostnetLoss:0.00080  DecoderLoss:0.00084  StopLoss:0.04630  GradNorm:0.00363  GradNormST:0.01733  AvgTextLen:12.2  AvgSpecLen:52.0  StepTime:0.31  LR:0.000100\n",
      "   | > Step:24/68  GlobalStep:103870  TotalLoss:0.00194  PostnetLoss:0.00094  DecoderLoss:0.00100  StopLoss:0.06066  GradNorm:0.00322  GradNormST:0.02067  AvgTextLen:19.2  AvgSpecLen:79.0  StepTime:0.39  LR:0.000100\n",
      "   | > Step:34/68  GlobalStep:103880  TotalLoss:0.00205  PostnetLoss:0.00100  DecoderLoss:0.00105  StopLoss:0.04579  GradNorm:0.00286  GradNormST:0.01333  AvgTextLen:26.2  AvgSpecLen:95.9  StepTime:0.59  LR:0.000100\n",
      "   | > Step:44/68  GlobalStep:103890  TotalLoss:0.00220  PostnetLoss:0.00107  DecoderLoss:0.00113  StopLoss:0.04788  GradNorm:0.00280  GradNormST:0.01607  AvgTextLen:33.8  AvgSpecLen:116.3  StepTime:0.53  LR:0.000100\n",
      "   | > Step:54/68  GlobalStep:103900  TotalLoss:0.00241  PostnetLoss:0.00117  DecoderLoss:0.00124  StopLoss:0.04651  GradNorm:0.00285  GradNormST:0.01481  AvgTextLen:42.2  AvgSpecLen:144.9  StepTime:0.68  LR:0.000100\n",
      "   | > Step:64/68  GlobalStep:103910  TotalLoss:0.00251  PostnetLoss:0.00121  DecoderLoss:0.00130  StopLoss:0.03567  GradNorm:0.00257  GradNormST:0.01454  AvgTextLen:60.6  AvgSpecLen:177.6  StepTime:0.80  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:103914  AvgTotalLoss:0.05955  AvgPostnetLoss:0.00099  AvgDecoderLoss:0.00105  AvgStopLoss:0.05751  EpochTime:43.00  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.09234   PostnetLoss: 0.00520   DecoderLoss:0.00554  StopLoss: 0.08160  \n",
      "   | > TotalLoss: 0.06963   PostnetLoss: 0.00768   DecoderLoss:0.00816  StopLoss: 0.05379  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00099   Validation Loss: 0.00731\n",
      "\n",
      " > Epoch 506/1000\n",
      "   | > Step:5/68  GlobalStep:103920  TotalLoss:0.00144  PostnetLoss:0.00070  DecoderLoss:0.00074  StopLoss:0.05054  GradNorm:0.00373  GradNormST:0.01921  AvgTextLen:5.9  AvgSpecLen:33.2  StepTime:0.36  LR:0.000100\n",
      "   | > Step:15/68  GlobalStep:103930  TotalLoss:0.00171  PostnetLoss:0.00083  DecoderLoss:0.00088  StopLoss:0.06283  GradNorm:0.00333  GradNormST:0.02201  AvgTextLen:12.8  AvgSpecLen:59.1  StepTime:0.50  LR:0.000100\n",
      "   | > Step:25/68  GlobalStep:103940  TotalLoss:0.00187  PostnetLoss:0.00091  DecoderLoss:0.00096  StopLoss:0.06420  GradNorm:0.00310  GradNormST:0.01987  AvgTextLen:19.8  AvgSpecLen:73.3  StepTime:0.46  LR:0.000100\n",
      "   | > Step:35/68  GlobalStep:103950  TotalLoss:0.00210  PostnetLoss:0.00102  DecoderLoss:0.00108  StopLoss:0.05127  GradNorm:0.00301  GradNormST:0.01806  AvgTextLen:27.1  AvgSpecLen:104.7  StepTime:0.75  LR:0.000100\n",
      "   | > Step:45/68  GlobalStep:103960  TotalLoss:0.00219  PostnetLoss:0.00107  DecoderLoss:0.00113  StopLoss:0.07891  GradNorm:0.00277  GradNormST:0.02354  AvgTextLen:34.4  AvgSpecLen:118.6  StepTime:0.69  LR:0.000100\n",
      "   | > Step:55/68  GlobalStep:103970  TotalLoss:0.00244  PostnetLoss:0.00118  DecoderLoss:0.00126  StopLoss:0.04389  GradNorm:0.00277  GradNormST:0.01649  AvgTextLen:43.9  AvgSpecLen:145.2  StepTime:0.80  LR:0.000100\n",
      "   | > Step:65/68  GlobalStep:103980  TotalLoss:0.00269  PostnetLoss:0.00130  DecoderLoss:0.00139  StopLoss:0.03340  GradNorm:0.00287  GradNormST:0.00920  AvgTextLen:64.2  AvgSpecLen:207.4  StepTime:1.17  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:103983  AvgTotalLoss:0.06065  AvgPostnetLoss:0.00099  AvgDecoderLoss:0.00105  AvgStopLoss:0.05862  EpochTime:42.35  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.08538   PostnetLoss: 0.00512   DecoderLoss:0.00545  StopLoss: 0.07481  \n",
      "   | > TotalLoss: 0.07142   PostnetLoss: 0.00780   DecoderLoss:0.00827  StopLoss: 0.05535  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00099   Validation Loss: 0.00734\n",
      "\n",
      " > Epoch 507/1000\n",
      "   | > Step:6/68  GlobalStep:103990  TotalLoss:0.00147  PostnetLoss:0.00072  DecoderLoss:0.00075  StopLoss:0.03657  GradNorm:0.00378  GradNormST:0.01289  AvgTextLen:6.0  AvgSpecLen:34.8  StepTime:0.37  LR:0.000100\n",
      "   | > Step:16/68  GlobalStep:104000  TotalLoss:0.00174  PostnetLoss:0.00085  DecoderLoss:0.00089  StopLoss:0.05779  GradNorm:0.00325  GradNormST:0.01972  AvgTextLen:13.8  AvgSpecLen:60.6  StepTime:0.39  LR:0.000100\n",
      " | | > Checkpoint saving : /vm/TTS/models/cartman_models/mozilla-no-loc-fattn-stopnet-sigmoid-loss_masking-August-15-2019_11+26AM-de6aeb9/checkpoint_104000.pth.tar\n",
      "   | > Step:26/68  GlobalStep:104010  TotalLoss:0.00190  PostnetLoss:0.00093  DecoderLoss:0.00097  StopLoss:0.07443  GradNorm:0.00301  GradNormST:0.02547  AvgTextLen:20.3  AvgSpecLen:77.5  StepTime:0.51  LR:0.000100\n",
      "   | > Step:36/68  GlobalStep:104020  TotalLoss:0.00208  PostnetLoss:0.00101  DecoderLoss:0.00107  StopLoss:0.06263  GradNorm:0.00292  GradNormST:0.01377  AvgTextLen:28.5  AvgSpecLen:100.2  StepTime:0.58  LR:0.000100\n",
      "   | > Step:46/68  GlobalStep:104030  TotalLoss:0.00222  PostnetLoss:0.00108  DecoderLoss:0.00114  StopLoss:0.06604  GradNorm:0.00282  GradNormST:0.02460  AvgTextLen:35.4  AvgSpecLen:122.6  StepTime:0.79  LR:0.000100\n",
      "   | > Step:56/68  GlobalStep:104040  TotalLoss:0.00242  PostnetLoss:0.00117  DecoderLoss:0.00125  StopLoss:0.03973  GradNorm:0.00272  GradNormST:0.01638  AvgTextLen:45.8  AvgSpecLen:145.4  StepTime:0.95  LR:0.000100\n",
      "   | > Step:66/68  GlobalStep:104050  TotalLoss:0.00284  PostnetLoss:0.00137  DecoderLoss:0.00147  StopLoss:0.02833  GradNorm:0.00293  GradNormST:0.01414  AvgTextLen:71.2  AvgSpecLen:220.8  StepTime:1.03  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:104052  AvgTotalLoss:0.05700  AvgPostnetLoss:0.00099  AvgDecoderLoss:0.00105  AvgStopLoss:0.05496  EpochTime:42.46  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.09079   PostnetLoss: 0.00525   DecoderLoss:0.00558  StopLoss: 0.07996  \n",
      "   | > TotalLoss: 0.07518   PostnetLoss: 0.00813   DecoderLoss:0.00860  StopLoss: 0.05845  \n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00099   Validation Loss: 0.00755\n",
      "\n",
      " > Epoch 508/1000\n",
      "   | > Step:7/68  GlobalStep:104060  TotalLoss:0.00154  PostnetLoss:0.00075  DecoderLoss:0.00079  StopLoss:0.04550  GradNorm:0.00384  GradNormST:0.01878  AvgTextLen:7.4  AvgSpecLen:44.6  StepTime:0.40  LR:0.000100\n",
      "   | > Step:17/68  GlobalStep:104070  TotalLoss:0.00169  PostnetLoss:0.00083  DecoderLoss:0.00087  StopLoss:0.06245  GradNorm:0.00314  GradNormST:0.01748  AvgTextLen:14.7  AvgSpecLen:66.0  StepTime:0.46  LR:0.000100\n",
      "   | > Step:27/68  GlobalStep:104080  TotalLoss:0.00194  PostnetLoss:0.00094  DecoderLoss:0.00099  StopLoss:0.05364  GradNorm:0.00301  GradNormST:0.01107  AvgTextLen:21.0  AvgSpecLen:80.2  StepTime:0.63  LR:0.000100\n",
      "   | > Step:37/68  GlobalStep:104090  TotalLoss:0.00211  PostnetLoss:0.00103  DecoderLoss:0.00108  StopLoss:0.04504  GradNorm:0.00286  GradNormST:0.01212  AvgTextLen:29.3  AvgSpecLen:102.4  StepTime:0.57  LR:0.000100\n",
      "   | > Step:47/68  GlobalStep:104100  TotalLoss:0.00230  PostnetLoss:0.00112  DecoderLoss:0.00119  StopLoss:0.04957  GradNorm:0.00282  GradNormST:0.01658  AvgTextLen:36.7  AvgSpecLen:126.9  StepTime:0.75  LR:0.000100\n",
      "   | > Step:57/68  GlobalStep:104110  TotalLoss:0.00248  PostnetLoss:0.00120  DecoderLoss:0.00128  StopLoss:0.03905  GradNorm:0.00308  GradNormST:0.01001  AvgTextLen:47.1  AvgSpecLen:161.0  StepTime:0.90  LR:0.000100\n",
      "   | > Step:67/68  GlobalStep:104120  TotalLoss:0.00283  PostnetLoss:0.00137  DecoderLoss:0.00147  StopLoss:0.03650  GradNorm:0.00324  GradNormST:0.02100  AvgTextLen:80.6  AvgSpecLen:246.5  StepTime:1.21  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:104121  AvgTotalLoss:0.05886  AvgPostnetLoss:0.00099  AvgDecoderLoss:0.00105  AvgStopLoss:0.05683  EpochTime:41.80  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.08655   PostnetLoss: 0.00530   DecoderLoss:0.00564  StopLoss: 0.07561  \n",
      "   | > TotalLoss: 0.07334   PostnetLoss: 0.00795   DecoderLoss:0.00843  StopLoss: 0.05696  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00099   Validation Loss: 0.00745\n",
      "\n",
      " > Epoch 509/1000\n",
      "   | > Step:8/68  GlobalStep:104130  TotalLoss:0.00157  PostnetLoss:0.00077  DecoderLoss:0.00080  StopLoss:0.06236  GradNorm:0.00443  GradNormST:0.01775  AvgTextLen:8.3  AvgSpecLen:44.0  StepTime:0.29  LR:0.000100\n",
      "   | > Step:18/68  GlobalStep:104140  TotalLoss:0.00170  PostnetLoss:0.00083  DecoderLoss:0.00087  StopLoss:0.05564  GradNorm:0.00319  GradNormST:0.01659  AvgTextLen:15.2  AvgSpecLen:64.5  StepTime:0.33  LR:0.000100\n",
      "   | > Step:28/68  GlobalStep:104150  TotalLoss:0.00195  PostnetLoss:0.00095  DecoderLoss:0.00100  StopLoss:0.05462  GradNorm:0.00298  GradNormST:0.01364  AvgTextLen:22.1  AvgSpecLen:85.4  StepTime:0.53  LR:0.000100\n",
      "   | > Step:38/68  GlobalStep:104160  TotalLoss:0.00212  PostnetLoss:0.00103  DecoderLoss:0.00109  StopLoss:0.04286  GradNorm:0.00300  GradNormST:0.01182  AvgTextLen:29.4  AvgSpecLen:104.1  StepTime:0.50  LR:0.000100\n",
      "   | > Step:48/68  GlobalStep:104170  TotalLoss:0.00230  PostnetLoss:0.00112  DecoderLoss:0.00118  StopLoss:0.06181  GradNorm:0.00280  GradNormST:0.02063  AvgTextLen:37.5  AvgSpecLen:123.8  StepTime:0.68  LR:0.000100\n",
      "   | > Step:58/68  GlobalStep:104180  TotalLoss:0.00246  PostnetLoss:0.00119  DecoderLoss:0.00126  StopLoss:0.05002  GradNorm:0.00273  GradNormST:0.02053  AvgTextLen:47.8  AvgSpecLen:158.0  StepTime:0.85  LR:0.000100\n",
      "   | > Step:68/68  GlobalStep:104190  TotalLoss:0.00279  PostnetLoss:0.00135  DecoderLoss:0.00144  StopLoss:0.02197  GradNorm:0.00269  GradNormST:0.01273  AvgTextLen:103.0  AvgSpecLen:308.9  StepTime:1.87  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:104190  AvgTotalLoss:0.05551  AvgPostnetLoss:0.00099  AvgDecoderLoss:0.00105  AvgStopLoss:0.05348  EpochTime:41.49  AvgStepTime:0.60\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.08672   PostnetLoss: 0.00521   DecoderLoss:0.00554  StopLoss: 0.07598  \n",
      "   | > TotalLoss: 0.07355   PostnetLoss: 0.00843   DecoderLoss:0.00891  StopLoss: 0.05622  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00099   Validation Loss: 0.00767\n",
      "\n",
      " > Epoch 510/1000\n",
      "   | > Step:9/68  GlobalStep:104200  TotalLoss:0.00151  PostnetLoss:0.00074  DecoderLoss:0.00078  StopLoss:0.04553  GradNorm:0.00424  GradNormST:0.01681  AvgTextLen:9.0  AvgSpecLen:42.1  StepTime:0.43  LR:0.000100\n",
      "   | > Step:19/68  GlobalStep:104210  TotalLoss:0.00173  PostnetLoss:0.00084  DecoderLoss:0.00089  StopLoss:0.06557  GradNorm:0.00353  GradNormST:0.01648  AvgTextLen:15.5  AvgSpecLen:63.2  StepTime:0.48  LR:0.000100\n",
      "   | > Step:29/68  GlobalStep:104220  TotalLoss:0.00192  PostnetLoss:0.00094  DecoderLoss:0.00099  StopLoss:0.05675  GradNorm:0.00315  GradNormST:0.01262  AvgTextLen:21.8  AvgSpecLen:83.4  StepTime:0.56  LR:0.000100\n",
      "   | > Step:39/68  GlobalStep:104230  TotalLoss:0.00215  PostnetLoss:0.00105  DecoderLoss:0.00111  StopLoss:0.05030  GradNorm:0.00306  GradNormST:0.01962  AvgTextLen:30.2  AvgSpecLen:110.4  StepTime:0.58  LR:0.000100\n",
      "   | > Step:49/68  GlobalStep:104240  TotalLoss:0.00231  PostnetLoss:0.00112  DecoderLoss:0.00119  StopLoss:0.05229  GradNorm:0.00280  GradNormST:0.01661  AvgTextLen:38.2  AvgSpecLen:127.7  StepTime:0.72  LR:0.000100\n",
      "   | > Step:59/68  GlobalStep:104250  TotalLoss:0.00246  PostnetLoss:0.00119  DecoderLoss:0.00127  StopLoss:0.04749  GradNorm:0.00272  GradNormST:0.01429  AvgTextLen:50.5  AvgSpecLen:155.9  StepTime:0.71  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:104259  AvgTotalLoss:0.05593  AvgPostnetLoss:0.00099  AvgDecoderLoss:0.00105  AvgStopLoss:0.05389  EpochTime:42.24  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.08446   PostnetLoss: 0.00516   DecoderLoss:0.00549  StopLoss: 0.07381  \n",
      "   | > TotalLoss: 0.07249   PostnetLoss: 0.00812   DecoderLoss:0.00859  StopLoss: 0.05578  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00099   Validation Loss: 0.00749\n",
      "\n",
      " > Epoch 511/1000\n",
      "   | > Step:0/68  GlobalStep:104260  TotalLoss:0.00148  PostnetLoss:0.00071  DecoderLoss:0.00077  StopLoss:0.07756  GradNorm:0.00574  GradNormST:0.01972  AvgTextLen:3.0  AvgSpecLen:25.3  StepTime:0.40  LR:0.000100\n",
      "   | > Step:10/68  GlobalStep:104270  TotalLoss:0.00158  PostnetLoss:0.00077  DecoderLoss:0.00081  StopLoss:0.05870  GradNorm:0.00398  GradNormST:0.02379  AvgTextLen:9.8  AvgSpecLen:48.6  StepTime:0.52  LR:0.000100\n",
      "   | > Step:20/68  GlobalStep:104280  TotalLoss:0.00182  PostnetLoss:0.00089  DecoderLoss:0.00093  StopLoss:0.03851  GradNorm:0.00349  GradNormST:0.01615  AvgTextLen:16.4  AvgSpecLen:64.8  StepTime:0.39  LR:0.000100\n",
      "   | > Step:30/68  GlobalStep:104290  TotalLoss:0.00198  PostnetLoss:0.00097  DecoderLoss:0.00102  StopLoss:0.05051  GradNorm:0.00333  GradNormST:0.01319  AvgTextLen:22.9  AvgSpecLen:85.8  StepTime:0.57  LR:0.000100\n",
      "   | > Step:40/68  GlobalStep:104300  TotalLoss:0.00215  PostnetLoss:0.00104  DecoderLoss:0.00110  StopLoss:0.05538  GradNorm:0.00296  GradNormST:0.01721  AvgTextLen:30.9  AvgSpecLen:105.3  StepTime:0.50  LR:0.000100\n",
      "   | > Step:50/68  GlobalStep:104310  TotalLoss:0.00229  PostnetLoss:0.00111  DecoderLoss:0.00118  StopLoss:0.04675  GradNorm:0.00267  GradNormST:0.01302  AvgTextLen:38.8  AvgSpecLen:132.0  StepTime:0.79  LR:0.000100\n",
      "   | > Step:60/68  GlobalStep:104320  TotalLoss:0.00253  PostnetLoss:0.00122  DecoderLoss:0.00130  StopLoss:0.04642  GradNorm:0.00271  GradNormST:0.01568  AvgTextLen:52.0  AvgSpecLen:172.7  StepTime:0.94  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:104328  AvgTotalLoss:0.05212  AvgPostnetLoss:0.00099  AvgDecoderLoss:0.00105  AvgStopLoss:0.05009  EpochTime:42.76  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.09029   PostnetLoss: 0.00535   DecoderLoss:0.00568  StopLoss: 0.07925  \n",
      "   | > TotalLoss: 0.07181   PostnetLoss: 0.00798   DecoderLoss:0.00844  StopLoss: 0.05538  \n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00099   Validation Loss: 0.00740\n",
      "\n",
      " > Epoch 512/1000\n",
      "   | > Step:1/68  GlobalStep:104330  TotalLoss:0.00144  PostnetLoss:0.00070  DecoderLoss:0.00074  StopLoss:0.04758  GradNorm:0.00475  GradNormST:0.02230  AvgTextLen:3.7  AvgSpecLen:28.6  StepTime:0.22  LR:0.000100\n",
      "   | > Step:11/68  GlobalStep:104340  TotalLoss:0.00158  PostnetLoss:0.00077  DecoderLoss:0.00081  StopLoss:0.07507  GradNorm:0.00335  GradNormST:0.02665  AvgTextLen:10.3  AvgSpecLen:49.4  StepTime:0.41  LR:0.000100\n",
      "   | > Step:21/68  GlobalStep:104350  TotalLoss:0.00177  PostnetLoss:0.00086  DecoderLoss:0.00091  StopLoss:0.05262  GradNorm:0.00336  GradNormST:0.01758  AvgTextLen:16.6  AvgSpecLen:66.3  StepTime:0.33  LR:0.000100\n",
      "   | > Step:31/68  GlobalStep:104360  TotalLoss:0.00197  PostnetLoss:0.00096  DecoderLoss:0.00101  StopLoss:0.06684  GradNorm:0.00329  GradNormST:0.03119  AvgTextLen:24.1  AvgSpecLen:82.7  StepTime:0.54  LR:0.000100\n",
      "   | > Step:41/68  GlobalStep:104370  TotalLoss:0.00221  PostnetLoss:0.00107  DecoderLoss:0.00113  StopLoss:0.05591  GradNorm:0.00298  GradNormST:0.02426  AvgTextLen:32.3  AvgSpecLen:109.4  StepTime:0.62  LR:0.000100\n",
      "   | > Step:51/68  GlobalStep:104380  TotalLoss:0.00226  PostnetLoss:0.00110  DecoderLoss:0.00116  StopLoss:0.03611  GradNorm:0.00273  GradNormST:0.01226  AvgTextLen:39.2  AvgSpecLen:126.9  StepTime:0.74  LR:0.000100\n",
      "   | > Step:61/68  GlobalStep:104390  TotalLoss:0.00250  PostnetLoss:0.00121  DecoderLoss:0.00129  StopLoss:0.06796  GradNorm:0.00266  GradNormST:0.04808  AvgTextLen:54.6  AvgSpecLen:165.5  StepTime:0.95  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:104397  AvgTotalLoss:0.05526  AvgPostnetLoss:0.00098  AvgDecoderLoss:0.00104  AvgStopLoss:0.05324  EpochTime:41.77  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.08800   PostnetLoss: 0.00519   DecoderLoss:0.00552  StopLoss: 0.07729  \n",
      "   | > TotalLoss: 0.07188   PostnetLoss: 0.00796   DecoderLoss:0.00846  StopLoss: 0.05547  \n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00098   Validation Loss: 0.00744\n",
      "\n",
      " > Epoch 513/1000\n",
      "   | > Step:2/68  GlobalStep:104400  TotalLoss:0.00133  PostnetLoss:0.00065  DecoderLoss:0.00068  StopLoss:0.04491  GradNorm:0.00356  GradNormST:0.02031  AvgTextLen:4.0  AvgSpecLen:31.3  StepTime:0.34  LR:0.000100\n",
      "   | > Step:12/68  GlobalStep:104410  TotalLoss:0.00157  PostnetLoss:0.00076  DecoderLoss:0.00080  StopLoss:0.05749  GradNorm:0.00367  GradNormST:0.01699  AvgTextLen:11.0  AvgSpecLen:49.3  StepTime:0.39  LR:0.000100\n",
      "   | > Step:22/68  GlobalStep:104420  TotalLoss:0.00181  PostnetLoss:0.00089  DecoderLoss:0.00093  StopLoss:0.04719  GradNorm:0.00348  GradNormST:0.01186  AvgTextLen:18.0  AvgSpecLen:72.0  StepTime:0.46  LR:0.000100\n",
      "   | > Step:32/68  GlobalStep:104430  TotalLoss:0.00200  PostnetLoss:0.00097  DecoderLoss:0.00103  StopLoss:0.06488  GradNorm:0.00298  GradNormST:0.02436  AvgTextLen:24.4  AvgSpecLen:91.5  StepTime:0.55  LR:0.000100\n",
      "   | > Step:42/68  GlobalStep:104440  TotalLoss:0.00218  PostnetLoss:0.00105  DecoderLoss:0.00112  StopLoss:0.05168  GradNorm:0.00312  GradNormST:0.01651  AvgTextLen:33.1  AvgSpecLen:109.7  StepTime:0.59  LR:0.000100\n",
      "   | > Step:52/68  GlobalStep:104450  TotalLoss:0.00237  PostnetLoss:0.00115  DecoderLoss:0.00122  StopLoss:0.05168  GradNorm:0.00277  GradNormST:0.02770  AvgTextLen:42.1  AvgSpecLen:137.5  StepTime:0.67  LR:0.000100\n",
      "   | > Step:62/68  GlobalStep:104460  TotalLoss:0.00256  PostnetLoss:0.00124  DecoderLoss:0.00132  StopLoss:0.06114  GradNorm:0.00266  GradNormST:0.04780  AvgTextLen:54.0  AvgSpecLen:174.7  StepTime:0.76  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:104466  AvgTotalLoss:0.05456  AvgPostnetLoss:0.00099  AvgDecoderLoss:0.00104  AvgStopLoss:0.05253  EpochTime:42.15  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.08813   PostnetLoss: 0.00518   DecoderLoss:0.00551  StopLoss: 0.07744  \n",
      "   | > TotalLoss: 0.07569   PostnetLoss: 0.00822   DecoderLoss:0.00871  StopLoss: 0.05875  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00099   Validation Loss: 0.00765\n",
      "\n",
      " > Epoch 514/1000\n",
      "   | > Step:3/68  GlobalStep:104470  TotalLoss:0.00136  PostnetLoss:0.00066  DecoderLoss:0.00070  StopLoss:0.06692  GradNorm:0.00380  GradNormST:0.02607  AvgTextLen:4.7  AvgSpecLen:29.7  StepTime:0.34  LR:0.000100\n",
      "   | > Step:13/68  GlobalStep:104480  TotalLoss:0.00159  PostnetLoss:0.00077  DecoderLoss:0.00081  StopLoss:0.06182  GradNorm:0.00352  GradNormST:0.02320  AvgTextLen:11.5  AvgSpecLen:55.2  StepTime:0.42  LR:0.000100\n",
      "   | > Step:23/68  GlobalStep:104490  TotalLoss:0.00189  PostnetLoss:0.00092  DecoderLoss:0.00097  StopLoss:0.04822  GradNorm:0.00348  GradNormST:0.01540  AvgTextLen:18.3  AvgSpecLen:72.2  StepTime:0.43  LR:0.000100\n",
      "   | > Step:33/68  GlobalStep:104500  TotalLoss:0.00202  PostnetLoss:0.00099  DecoderLoss:0.00104  StopLoss:0.06346  GradNorm:0.00374  GradNormST:0.01684  AvgTextLen:25.9  AvgSpecLen:88.9  StepTime:0.51  LR:0.000100\n",
      "   | > Step:43/68  GlobalStep:104510  TotalLoss:0.00220  PostnetLoss:0.00106  DecoderLoss:0.00113  StopLoss:0.04977  GradNorm:0.00334  GradNormST:0.01678  AvgTextLen:33.4  AvgSpecLen:115.7  StepTime:0.62  LR:0.000100\n",
      "   | > Step:53/68  GlobalStep:104520  TotalLoss:0.00231  PostnetLoss:0.00112  DecoderLoss:0.00119  StopLoss:0.04172  GradNorm:0.00276  GradNormST:0.01708  AvgTextLen:42.4  AvgSpecLen:137.6  StepTime:0.64  LR:0.000100\n",
      "   | > Step:63/68  GlobalStep:104530  TotalLoss:0.00262  PostnetLoss:0.00127  DecoderLoss:0.00135  StopLoss:0.05782  GradNorm:0.00268  GradNormST:0.03651  AvgTextLen:58.6  AvgSpecLen:185.1  StepTime:0.92  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:104535  AvgTotalLoss:0.05274  AvgPostnetLoss:0.00099  AvgDecoderLoss:0.00105  AvgStopLoss:0.05070  EpochTime:42.31  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.07975   PostnetLoss: 0.00511   DecoderLoss:0.00544  StopLoss: 0.06919  \n",
      "   | > TotalLoss: 0.06891   PostnetLoss: 0.00795   DecoderLoss:0.00843  StopLoss: 0.05252  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00099   Validation Loss: 0.00748\n",
      "\n",
      " > Epoch 515/1000\n",
      "   | > Step:4/68  GlobalStep:104540  TotalLoss:0.00141  PostnetLoss:0.00068  DecoderLoss:0.00072  StopLoss:0.08596  GradNorm:0.00413  GradNormST:0.03559  AvgTextLen:4.8  AvgSpecLen:31.1  StepTime:0.27  LR:0.000100\n",
      "   | > Step:14/68  GlobalStep:104550  TotalLoss:0.00165  PostnetLoss:0.00080  DecoderLoss:0.00085  StopLoss:0.04816  GradNorm:0.00344  GradNormST:0.01752  AvgTextLen:12.2  AvgSpecLen:52.0  StepTime:0.34  LR:0.000100\n",
      "   | > Step:24/68  GlobalStep:104560  TotalLoss:0.00194  PostnetLoss:0.00094  DecoderLoss:0.00099  StopLoss:0.04836  GradNorm:0.00344  GradNormST:0.01321  AvgTextLen:19.2  AvgSpecLen:79.0  StepTime:0.49  LR:0.000100\n",
      "   | > Step:34/68  GlobalStep:104570  TotalLoss:0.00206  PostnetLoss:0.00100  DecoderLoss:0.00106  StopLoss:0.03770  GradNorm:0.00334  GradNormST:0.01463  AvgTextLen:26.2  AvgSpecLen:95.9  StepTime:0.57  LR:0.000100\n",
      "   | > Step:44/68  GlobalStep:104580  TotalLoss:0.00223  PostnetLoss:0.00108  DecoderLoss:0.00115  StopLoss:0.04144  GradNorm:0.00335  GradNormST:0.01272  AvgTextLen:33.8  AvgSpecLen:116.3  StepTime:0.65  LR:0.000100\n",
      "   | > Step:54/68  GlobalStep:104590  TotalLoss:0.00238  PostnetLoss:0.00116  DecoderLoss:0.00123  StopLoss:0.05433  GradNorm:0.00300  GradNormST:0.02051  AvgTextLen:42.2  AvgSpecLen:144.9  StepTime:0.79  LR:0.000100\n",
      "   | > Step:64/68  GlobalStep:104600  TotalLoss:0.00249  PostnetLoss:0.00120  DecoderLoss:0.00128  StopLoss:0.04404  GradNorm:0.00263  GradNormST:0.02095  AvgTextLen:60.6  AvgSpecLen:177.6  StepTime:0.80  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:104604  AvgTotalLoss:0.05331  AvgPostnetLoss:0.00099  AvgDecoderLoss:0.00104  AvgStopLoss:0.05128  EpochTime:42.87  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.08704   PostnetLoss: 0.00525   DecoderLoss:0.00558  StopLoss: 0.07622  \n",
      "   | > TotalLoss: 0.06921   PostnetLoss: 0.00805   DecoderLoss:0.00852  StopLoss: 0.05264  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00099   Validation Loss: 0.00757\n",
      "\n",
      " > Epoch 516/1000\n",
      "   | > Step:5/68  GlobalStep:104610  TotalLoss:0.00146  PostnetLoss:0.00071  DecoderLoss:0.00075  StopLoss:0.05850  GradNorm:0.00371  GradNormST:0.01603  AvgTextLen:5.9  AvgSpecLen:33.2  StepTime:0.38  LR:0.000100\n",
      "   | > Step:15/68  GlobalStep:104620  TotalLoss:0.00173  PostnetLoss:0.00084  DecoderLoss:0.00089  StopLoss:0.05397  GradNorm:0.00330  GradNormST:0.01749  AvgTextLen:12.8  AvgSpecLen:59.1  StepTime:0.54  LR:0.000100\n",
      "   | > Step:25/68  GlobalStep:104630  TotalLoss:0.00188  PostnetLoss:0.00091  DecoderLoss:0.00096  StopLoss:0.04736  GradNorm:0.00340  GradNormST:0.01200  AvgTextLen:19.8  AvgSpecLen:73.3  StepTime:0.48  LR:0.000100\n",
      "   | > Step:35/68  GlobalStep:104640  TotalLoss:0.00209  PostnetLoss:0.00102  DecoderLoss:0.00108  StopLoss:0.05677  GradNorm:0.00360  GradNormST:0.01492  AvgTextLen:27.1  AvgSpecLen:104.7  StepTime:0.74  LR:0.000100\n",
      "   | > Step:45/68  GlobalStep:104650  TotalLoss:0.00221  PostnetLoss:0.00107  DecoderLoss:0.00114  StopLoss:0.05218  GradNorm:0.00349  GradNormST:0.01576  AvgTextLen:34.4  AvgSpecLen:118.6  StepTime:0.69  LR:0.000100\n",
      "   | > Step:55/68  GlobalStep:104660  TotalLoss:0.00246  PostnetLoss:0.00119  DecoderLoss:0.00126  StopLoss:0.04540  GradNorm:0.00338  GradNormST:0.01506  AvgTextLen:43.9  AvgSpecLen:145.2  StepTime:0.86  LR:0.000100\n",
      "   | > Step:65/68  GlobalStep:104670  TotalLoss:0.00268  PostnetLoss:0.00129  DecoderLoss:0.00138  StopLoss:0.05792  GradNorm:0.00275  GradNormST:0.03919  AvgTextLen:64.2  AvgSpecLen:207.4  StepTime:1.20  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:104673  AvgTotalLoss:0.05522  AvgPostnetLoss:0.00099  AvgDecoderLoss:0.00105  AvgStopLoss:0.05318  EpochTime:42.67  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.08750   PostnetLoss: 0.00514   DecoderLoss:0.00546  StopLoss: 0.07690  \n",
      "   | > TotalLoss: 0.07154   PostnetLoss: 0.00789   DecoderLoss:0.00838  StopLoss: 0.05527  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00099   Validation Loss: 0.00749\n",
      "\n",
      " > Epoch 517/1000\n",
      "   | > Step:6/68  GlobalStep:104680  TotalLoss:0.00143  PostnetLoss:0.00070  DecoderLoss:0.00073  StopLoss:0.04052  GradNorm:0.00438  GradNormST:0.01268  AvgTextLen:6.0  AvgSpecLen:34.8  StepTime:0.32  LR:0.000100\n",
      "   | > Step:16/68  GlobalStep:104690  TotalLoss:0.00173  PostnetLoss:0.00084  DecoderLoss:0.00089  StopLoss:0.05018  GradNorm:0.00332  GradNormST:0.01738  AvgTextLen:13.8  AvgSpecLen:60.6  StepTime:0.32  LR:0.000100\n",
      "   | > Step:26/68  GlobalStep:104700  TotalLoss:0.00189  PostnetLoss:0.00092  DecoderLoss:0.00097  StopLoss:0.03802  GradNorm:0.00305  GradNormST:0.01174  AvgTextLen:20.3  AvgSpecLen:77.5  StepTime:0.38  LR:0.000100\n",
      "   | > Step:36/68  GlobalStep:104710  TotalLoss:0.00205  PostnetLoss:0.00100  DecoderLoss:0.00105  StopLoss:0.05696  GradNorm:0.00286  GradNormST:0.01229  AvgTextLen:28.5  AvgSpecLen:100.2  StepTime:0.60  LR:0.000100\n",
      "   | > Step:46/68  GlobalStep:104720  TotalLoss:0.00223  PostnetLoss:0.00108  DecoderLoss:0.00115  StopLoss:0.04231  GradNorm:0.00369  GradNormST:0.00989  AvgTextLen:35.4  AvgSpecLen:122.6  StepTime:0.79  LR:0.000100\n",
      "   | > Step:56/68  GlobalStep:104730  TotalLoss:0.00245  PostnetLoss:0.00119  DecoderLoss:0.00126  StopLoss:0.04835  GradNorm:0.00371  GradNormST:0.01799  AvgTextLen:45.8  AvgSpecLen:145.4  StepTime:0.83  LR:0.000100\n",
      "   | > Step:66/68  GlobalStep:104740  TotalLoss:0.00280  PostnetLoss:0.00135  DecoderLoss:0.00145  StopLoss:0.06104  GradNorm:0.00280  GradNormST:0.03336  AvgTextLen:71.2  AvgSpecLen:220.8  StepTime:1.03  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:104742  AvgTotalLoss:0.05497  AvgPostnetLoss:0.00099  AvgDecoderLoss:0.00104  AvgStopLoss:0.05294  EpochTime:41.51  AvgStepTime:0.60\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.07672   PostnetLoss: 0.00500   DecoderLoss:0.00532  StopLoss: 0.06641  \n",
      "   | > TotalLoss: 0.06718   PostnetLoss: 0.00791   DecoderLoss:0.00837  StopLoss: 0.05089  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00099   Validation Loss: 0.00749\n",
      "\n",
      " > Epoch 518/1000\n",
      "   | > Step:7/68  GlobalStep:104750  TotalLoss:0.00154  PostnetLoss:0.00075  DecoderLoss:0.00079  StopLoss:0.04385  GradNorm:0.00385  GradNormST:0.01467  AvgTextLen:7.4  AvgSpecLen:44.6  StepTime:0.35  LR:0.000100\n",
      "   | > Step:17/68  GlobalStep:104760  TotalLoss:0.00182  PostnetLoss:0.00089  DecoderLoss:0.00093  StopLoss:0.06420  GradNorm:0.00368  GradNormST:0.01632  AvgTextLen:14.7  AvgSpecLen:66.0  StepTime:0.44  LR:0.000100\n",
      "   | > Step:27/68  GlobalStep:104770  TotalLoss:0.00202  PostnetLoss:0.00099  DecoderLoss:0.00104  StopLoss:0.06367  GradNorm:0.00326  GradNormST:0.01629  AvgTextLen:21.0  AvgSpecLen:80.2  StepTime:0.64  LR:0.000100\n",
      "   | > Step:37/68  GlobalStep:104780  TotalLoss:0.00224  PostnetLoss:0.00109  DecoderLoss:0.00115  StopLoss:0.04746  GradNorm:0.00443  GradNormST:0.01325  AvgTextLen:29.3  AvgSpecLen:102.4  StepTime:0.63  LR:0.000100\n",
      "   | > Step:47/68  GlobalStep:104790  TotalLoss:0.00246  PostnetLoss:0.00119  DecoderLoss:0.00127  StopLoss:0.05944  GradNorm:0.00397  GradNormST:0.02632  AvgTextLen:36.7  AvgSpecLen:126.9  StepTime:0.71  LR:0.000100\n",
      "   | > Step:57/68  GlobalStep:104800  TotalLoss:0.00263  PostnetLoss:0.00127  DecoderLoss:0.00135  StopLoss:0.07237  GradNorm:0.00345  GradNormST:0.05856  AvgTextLen:47.1  AvgSpecLen:161.0  StepTime:0.81  LR:0.000100\n",
      "   | > Step:67/68  GlobalStep:104810  TotalLoss:0.00292  PostnetLoss:0.00141  DecoderLoss:0.00151  StopLoss:0.07355  GradNorm:0.00335  GradNormST:0.06960  AvgTextLen:80.6  AvgSpecLen:246.5  StepTime:1.22  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:104811  AvgTotalLoss:0.06014  AvgPostnetLoss:0.00103  AvgDecoderLoss:0.00109  AvgStopLoss:0.05801  EpochTime:43.01  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.08268   PostnetLoss: 0.00505   DecoderLoss:0.00540  StopLoss: 0.07223  \n",
      "   | > TotalLoss: 0.06556   PostnetLoss: 0.00796   DecoderLoss:0.00846  StopLoss: 0.04914  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00103   Validation Loss: 0.00754\n",
      "\n",
      " > Epoch 519/1000\n",
      "   | > Step:8/68  GlobalStep:104820  TotalLoss:0.00159  PostnetLoss:0.00078  DecoderLoss:0.00082  StopLoss:0.05758  GradNorm:0.00392  GradNormST:0.01717  AvgTextLen:8.3  AvgSpecLen:44.0  StepTime:0.30  LR:0.000100\n",
      "   | > Step:18/68  GlobalStep:104830  TotalLoss:0.00177  PostnetLoss:0.00086  DecoderLoss:0.00091  StopLoss:0.05195  GradNorm:0.00333  GradNormST:0.01512  AvgTextLen:15.2  AvgSpecLen:64.5  StepTime:0.41  LR:0.000100\n",
      "   | > Step:28/68  GlobalStep:104840  TotalLoss:0.00197  PostnetLoss:0.00096  DecoderLoss:0.00101  StopLoss:0.04635  GradNorm:0.00338  GradNormST:0.01166  AvgTextLen:22.1  AvgSpecLen:85.4  StepTime:0.54  LR:0.000100\n",
      "   | > Step:38/68  GlobalStep:104850  TotalLoss:0.00212  PostnetLoss:0.00103  DecoderLoss:0.00109  StopLoss:0.04673  GradNorm:0.00323  GradNormST:0.01563  AvgTextLen:29.4  AvgSpecLen:104.1  StepTime:0.49  LR:0.000100\n",
      "   | > Step:48/68  GlobalStep:104860  TotalLoss:0.00231  PostnetLoss:0.00112  DecoderLoss:0.00119  StopLoss:0.04180  GradNorm:0.00332  GradNormST:0.01230  AvgTextLen:37.5  AvgSpecLen:123.8  StepTime:0.65  LR:0.000100\n",
      "   | > Step:58/68  GlobalStep:104870  TotalLoss:0.00252  PostnetLoss:0.00122  DecoderLoss:0.00129  StopLoss:0.04792  GradNorm:0.00345  GradNormST:0.02522  AvgTextLen:47.8  AvgSpecLen:158.0  StepTime:0.85  LR:0.000100\n",
      "   | > Step:68/68  GlobalStep:104880  TotalLoss:0.00298  PostnetLoss:0.00144  DecoderLoss:0.00155  StopLoss:0.03312  GradNorm:0.00427  GradNormST:0.02227  AvgTextLen:103.0  AvgSpecLen:308.9  StepTime:1.87  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:104880  AvgTotalLoss:0.05276  AvgPostnetLoss:0.00101  AvgDecoderLoss:0.00107  AvgStopLoss:0.05068  EpochTime:43.24  AvgStepTime:0.63\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.08345   PostnetLoss: 0.00524   DecoderLoss:0.00558  StopLoss: 0.07263  \n",
      "   | > TotalLoss: 0.06526   PostnetLoss: 0.00788   DecoderLoss:0.00836  StopLoss: 0.04902  \n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00101   Validation Loss: 0.00753\n",
      "\n",
      " > Epoch 520/1000\n",
      "   | > Step:9/68  GlobalStep:104890  TotalLoss:0.00151  PostnetLoss:0.00074  DecoderLoss:0.00077  StopLoss:0.05350  GradNorm:0.00356  GradNormST:0.02382  AvgTextLen:9.0  AvgSpecLen:42.1  StepTime:0.43  LR:0.000100\n",
      "   | > Step:19/68  GlobalStep:104900  TotalLoss:0.00173  PostnetLoss:0.00084  DecoderLoss:0.00089  StopLoss:0.07179  GradNorm:0.00324  GradNormST:0.02256  AvgTextLen:15.5  AvgSpecLen:63.2  StepTime:0.36  LR:0.000100\n",
      "   | > Step:29/68  GlobalStep:104910  TotalLoss:0.00192  PostnetLoss:0.00093  DecoderLoss:0.00098  StopLoss:0.05574  GradNorm:0.00289  GradNormST:0.01061  AvgTextLen:21.8  AvgSpecLen:83.4  StepTime:0.46  LR:0.000100\n",
      "   | > Step:39/68  GlobalStep:104920  TotalLoss:0.00213  PostnetLoss:0.00104  DecoderLoss:0.00110  StopLoss:0.04575  GradNorm:0.00280  GradNormST:0.01430  AvgTextLen:30.2  AvgSpecLen:110.4  StepTime:0.57  LR:0.000100\n",
      "   | > Step:49/68  GlobalStep:104930  TotalLoss:0.00230  PostnetLoss:0.00112  DecoderLoss:0.00119  StopLoss:0.04741  GradNorm:0.00316  GradNormST:0.02464  AvgTextLen:38.2  AvgSpecLen:127.7  StepTime:0.70  LR:0.000100\n",
      "   | > Step:59/68  GlobalStep:104940  TotalLoss:0.00250  PostnetLoss:0.00121  DecoderLoss:0.00129  StopLoss:0.04535  GradNorm:0.00320  GradNormST:0.02113  AvgTextLen:50.5  AvgSpecLen:155.9  StepTime:0.98  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:104949  AvgTotalLoss:0.05219  AvgPostnetLoss:0.00100  AvgDecoderLoss:0.00105  AvgStopLoss:0.05014  EpochTime:42.03  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.08034   PostnetLoss: 0.00529   DecoderLoss:0.00562  StopLoss: 0.06943  \n",
      "   | > TotalLoss: 0.06821   PostnetLoss: 0.00802   DecoderLoss:0.00849  StopLoss: 0.05170  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00100   Validation Loss: 0.00764\n",
      "\n",
      " > Epoch 521/1000\n",
      "   | > Step:0/68  GlobalStep:104950  TotalLoss:0.00144  PostnetLoss:0.00069  DecoderLoss:0.00075  StopLoss:0.07108  GradNorm:0.00500  GradNormST:0.02185  AvgTextLen:3.0  AvgSpecLen:25.3  StepTime:0.53  LR:0.000100\n",
      "   | > Step:10/68  GlobalStep:104960  TotalLoss:0.00156  PostnetLoss:0.00076  DecoderLoss:0.00080  StopLoss:0.04183  GradNorm:0.00351  GradNormST:0.01413  AvgTextLen:9.8  AvgSpecLen:48.6  StepTime:0.57  LR:0.000100\n",
      "   | > Step:20/68  GlobalStep:104970  TotalLoss:0.00181  PostnetLoss:0.00088  DecoderLoss:0.00093  StopLoss:0.04941  GradNorm:0.00313  GradNormST:0.02115  AvgTextLen:16.4  AvgSpecLen:64.8  StepTime:0.48  LR:0.000100\n",
      "   | > Step:30/68  GlobalStep:104980  TotalLoss:0.00197  PostnetLoss:0.00096  DecoderLoss:0.00101  StopLoss:0.04290  GradNorm:0.00294  GradNormST:0.01197  AvgTextLen:22.9  AvgSpecLen:85.8  StepTime:0.56  LR:0.000100\n",
      "   | > Step:40/68  GlobalStep:104990  TotalLoss:0.00212  PostnetLoss:0.00103  DecoderLoss:0.00109  StopLoss:0.03921  GradNorm:0.00290  GradNormST:0.01498  AvgTextLen:30.9  AvgSpecLen:105.3  StepTime:0.49  LR:0.000100\n",
      "   | > Step:50/68  GlobalStep:105000  TotalLoss:0.00232  PostnetLoss:0.00112  DecoderLoss:0.00120  StopLoss:0.04179  GradNorm:0.00295  GradNormST:0.01508  AvgTextLen:38.8  AvgSpecLen:132.0  StepTime:0.67  LR:0.000100\n",
      " | | > Checkpoint saving : /vm/TTS/models/cartman_models/mozilla-no-loc-fattn-stopnet-sigmoid-loss_masking-August-15-2019_11+26AM-de6aeb9/checkpoint_105000.pth.tar\n",
      "warning: audio amplitude out of range, auto clipped.\n",
      "   | > Step:60/68  GlobalStep:105010  TotalLoss:0.00254  PostnetLoss:0.00123  DecoderLoss:0.00131  StopLoss:0.02920  GradNorm:0.00320  GradNormST:0.01116  AvgTextLen:52.0  AvgSpecLen:172.7  StepTime:0.99  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:105018  AvgTotalLoss:0.05175  AvgPostnetLoss:0.00099  AvgDecoderLoss:0.00105  AvgStopLoss:0.04971  EpochTime:42.58  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.08789   PostnetLoss: 0.00521   DecoderLoss:0.00554  StopLoss: 0.07715  \n",
      "   | > TotalLoss: 0.06792   PostnetLoss: 0.00805   DecoderLoss:0.00854  StopLoss: 0.05133  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00099   Validation Loss: 0.00755\n",
      "\n",
      " > Epoch 522/1000\n",
      "   | > Step:1/68  GlobalStep:105020  TotalLoss:0.00140  PostnetLoss:0.00068  DecoderLoss:0.00072  StopLoss:0.03850  GradNorm:0.00458  GradNormST:0.01577  AvgTextLen:3.7  AvgSpecLen:28.6  StepTime:0.25  LR:0.000100\n",
      "   | > Step:11/68  GlobalStep:105030  TotalLoss:0.00160  PostnetLoss:0.00078  DecoderLoss:0.00082  StopLoss:0.03732  GradNorm:0.00351  GradNormST:0.01535  AvgTextLen:10.3  AvgSpecLen:49.4  StepTime:0.36  LR:0.000100\n",
      "   | > Step:21/68  GlobalStep:105040  TotalLoss:0.00185  PostnetLoss:0.00090  DecoderLoss:0.00095  StopLoss:0.05293  GradNorm:0.00333  GradNormST:0.01895  AvgTextLen:16.6  AvgSpecLen:66.3  StepTime:0.46  LR:0.000100\n",
      "   | > Step:31/68  GlobalStep:105050  TotalLoss:0.00198  PostnetLoss:0.00097  DecoderLoss:0.00102  StopLoss:0.05494  GradNorm:0.00312  GradNormST:0.02273  AvgTextLen:24.1  AvgSpecLen:82.7  StepTime:0.43  LR:0.000100\n",
      "   | > Step:41/68  GlobalStep:105060  TotalLoss:0.00224  PostnetLoss:0.00109  DecoderLoss:0.00115  StopLoss:0.04325  GradNorm:0.00297  GradNormST:0.01805  AvgTextLen:32.3  AvgSpecLen:109.4  StepTime:0.53  LR:0.000100\n",
      "   | > Step:51/68  GlobalStep:105070  TotalLoss:0.00240  PostnetLoss:0.00117  DecoderLoss:0.00124  StopLoss:0.04807  GradNorm:0.00355  GradNormST:0.01834  AvgTextLen:39.2  AvgSpecLen:126.9  StepTime:0.75  LR:0.000100\n",
      "   | > Step:61/68  GlobalStep:105080  TotalLoss:0.00256  PostnetLoss:0.00124  DecoderLoss:0.00132  StopLoss:0.03930  GradNorm:0.00597  GradNormST:0.01838  AvgTextLen:54.6  AvgSpecLen:165.5  StepTime:0.90  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:105087  AvgTotalLoss:0.05191  AvgPostnetLoss:0.00101  AvgDecoderLoss:0.00106  AvgStopLoss:0.04984  EpochTime:42.24  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.09141   PostnetLoss: 0.00528   DecoderLoss:0.00561  StopLoss: 0.08052  \n",
      "   | > TotalLoss: 0.07051   PostnetLoss: 0.00802   DecoderLoss:0.00849  StopLoss: 0.05400  \n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00101   Validation Loss: 0.00758\n",
      "\n",
      " > Epoch 523/1000\n",
      "   | > Step:2/68  GlobalStep:105090  TotalLoss:0.00135  PostnetLoss:0.00065  DecoderLoss:0.00069  StopLoss:0.04382  GradNorm:0.00379  GradNormST:0.01586  AvgTextLen:4.0  AvgSpecLen:31.3  StepTime:0.45  LR:0.000100\n",
      "   | > Step:12/68  GlobalStep:105100  TotalLoss:0.00158  PostnetLoss:0.00077  DecoderLoss:0.00081  StopLoss:0.04566  GradNorm:0.00340  GradNormST:0.01554  AvgTextLen:11.0  AvgSpecLen:49.3  StepTime:0.34  LR:0.000100\n",
      "   | > Step:22/68  GlobalStep:105110  TotalLoss:0.00185  PostnetLoss:0.00090  DecoderLoss:0.00095  StopLoss:0.06634  GradNorm:0.00322  GradNormST:0.01812  AvgTextLen:18.0  AvgSpecLen:72.0  StepTime:0.46  LR:0.000100\n",
      "   | > Step:32/68  GlobalStep:105120  TotalLoss:0.00200  PostnetLoss:0.00097  DecoderLoss:0.00103  StopLoss:0.05533  GradNorm:0.00299  GradNormST:0.01718  AvgTextLen:24.4  AvgSpecLen:91.5  StepTime:0.56  LR:0.000100\n",
      "   | > Step:42/68  GlobalStep:105130  TotalLoss:0.00217  PostnetLoss:0.00106  DecoderLoss:0.00112  StopLoss:0.04797  GradNorm:0.00290  GradNormST:0.01491  AvgTextLen:33.1  AvgSpecLen:109.7  StepTime:0.48  LR:0.000100\n",
      "   | > Step:52/68  GlobalStep:105140  TotalLoss:0.00237  PostnetLoss:0.00115  DecoderLoss:0.00122  StopLoss:0.05286  GradNorm:0.00304  GradNormST:0.01470  AvgTextLen:42.1  AvgSpecLen:137.5  StepTime:0.87  LR:0.000100\n",
      "   | > Step:62/68  GlobalStep:105150  TotalLoss:0.00259  PostnetLoss:0.00126  DecoderLoss:0.00134  StopLoss:0.04921  GradNorm:0.00287  GradNormST:0.02585  AvgTextLen:54.0  AvgSpecLen:174.7  StepTime:0.76  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:105156  AvgTotalLoss:0.05107  AvgPostnetLoss:0.00100  AvgDecoderLoss:0.00106  AvgStopLoss:0.04901  EpochTime:42.96  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.08821   PostnetLoss: 0.00538   DecoderLoss:0.00572  StopLoss: 0.07711  \n",
      "   | > TotalLoss: 0.06810   PostnetLoss: 0.00796   DecoderLoss:0.00844  StopLoss: 0.05171  \n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00100   Validation Loss: 0.00746\n",
      "\n",
      " > Epoch 524/1000\n",
      "   | > Step:3/68  GlobalStep:105160  TotalLoss:0.00138  PostnetLoss:0.00067  DecoderLoss:0.00071  StopLoss:0.07060  GradNorm:0.00406  GradNormST:0.01823  AvgTextLen:4.7  AvgSpecLen:29.7  StepTime:0.27  LR:0.000100\n",
      "   | > Step:13/68  GlobalStep:105170  TotalLoss:0.00160  PostnetLoss:0.00078  DecoderLoss:0.00082  StopLoss:0.08732  GradNorm:0.01354  GradNormST:0.02702  AvgTextLen:11.5  AvgSpecLen:55.2  StepTime:0.33  LR:0.000100\n",
      "   | > Step:23/68  GlobalStep:105180  TotalLoss:0.00190  PostnetLoss:0.00092  DecoderLoss:0.00097  StopLoss:0.05498  GradNorm:0.00324  GradNormST:0.01431  AvgTextLen:18.3  AvgSpecLen:72.2  StepTime:0.45  LR:0.000100\n",
      "   | > Step:33/68  GlobalStep:105190  TotalLoss:0.00205  PostnetLoss:0.00100  DecoderLoss:0.00105  StopLoss:0.05809  GradNorm:0.00301  GradNormST:0.01603  AvgTextLen:25.9  AvgSpecLen:88.9  StepTime:0.51  LR:0.000100\n",
      "   | > Step:43/68  GlobalStep:105200  TotalLoss:0.00221  PostnetLoss:0.00107  DecoderLoss:0.00114  StopLoss:0.04711  GradNorm:0.00298  GradNormST:0.01241  AvgTextLen:33.4  AvgSpecLen:115.7  StepTime:0.66  LR:0.000100\n",
      "   | > Step:53/68  GlobalStep:105210  TotalLoss:0.00230  PostnetLoss:0.00112  DecoderLoss:0.00118  StopLoss:0.03819  GradNorm:0.00278  GradNormST:0.01066  AvgTextLen:42.4  AvgSpecLen:137.6  StepTime:0.74  LR:0.000100\n",
      "   | > Step:63/68  GlobalStep:105220  TotalLoss:0.00270  PostnetLoss:0.00131  DecoderLoss:0.00139  StopLoss:0.03816  GradNorm:0.00305  GradNormST:0.02292  AvgTextLen:58.6  AvgSpecLen:185.1  StepTime:0.92  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:105225  AvgTotalLoss:0.05034  AvgPostnetLoss:0.00099  AvgDecoderLoss:0.00105  AvgStopLoss:0.04830  EpochTime:42.90  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.08775   PostnetLoss: 0.00533   DecoderLoss:0.00566  StopLoss: 0.07676  \n",
      "   | > TotalLoss: 0.06655   PostnetLoss: 0.00789   DecoderLoss:0.00835  StopLoss: 0.05031  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00099   Validation Loss: 0.00743\n",
      "\n",
      " > Epoch 525/1000\n",
      "   | > Step:4/68  GlobalStep:105230  TotalLoss:0.00136  PostnetLoss:0.00066  DecoderLoss:0.00070  StopLoss:0.07791  GradNorm:0.00365  GradNormST:0.02856  AvgTextLen:4.8  AvgSpecLen:31.1  StepTime:0.26  LR:0.000100\n",
      "   | > Step:14/68  GlobalStep:105240  TotalLoss:0.00166  PostnetLoss:0.00081  DecoderLoss:0.00085  StopLoss:0.03944  GradNorm:0.00335  GradNormST:0.01536  AvgTextLen:12.2  AvgSpecLen:52.0  StepTime:0.34  LR:0.000100\n",
      "   | > Step:24/68  GlobalStep:105250  TotalLoss:0.00194  PostnetLoss:0.00095  DecoderLoss:0.00100  StopLoss:0.04938  GradNorm:0.00307  GradNormST:0.01374  AvgTextLen:19.2  AvgSpecLen:79.0  StepTime:0.49  LR:0.000100\n",
      "   | > Step:34/68  GlobalStep:105260  TotalLoss:0.00211  PostnetLoss:0.00102  DecoderLoss:0.00108  StopLoss:0.03646  GradNorm:0.00330  GradNormST:0.01421  AvgTextLen:26.2  AvgSpecLen:95.9  StepTime:0.58  LR:0.000100\n",
      "   | > Step:44/68  GlobalStep:105270  TotalLoss:0.00223  PostnetLoss:0.00108  DecoderLoss:0.00114  StopLoss:0.03661  GradNorm:0.00304  GradNormST:0.01394  AvgTextLen:33.8  AvgSpecLen:116.3  StepTime:0.68  LR:0.000100\n",
      "   | > Step:54/68  GlobalStep:105280  TotalLoss:0.00239  PostnetLoss:0.00116  DecoderLoss:0.00123  StopLoss:0.04733  GradNorm:0.00300  GradNormST:0.01516  AvgTextLen:42.2  AvgSpecLen:144.9  StepTime:0.82  LR:0.000100\n",
      "   | > Step:64/68  GlobalStep:105290  TotalLoss:0.00249  PostnetLoss:0.00120  DecoderLoss:0.00129  StopLoss:0.03539  GradNorm:0.00264  GradNormST:0.01817  AvgTextLen:60.6  AvgSpecLen:177.6  StepTime:0.81  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:105294  AvgTotalLoss:0.05015  AvgPostnetLoss:0.00099  AvgDecoderLoss:0.00105  AvgStopLoss:0.04811  EpochTime:43.18  AvgStepTime:0.63\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.08246   PostnetLoss: 0.00521   DecoderLoss:0.00553  StopLoss: 0.07172  \n",
      "   | > TotalLoss: 0.06855   PostnetLoss: 0.00779   DecoderLoss:0.00826  StopLoss: 0.05249  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00099   Validation Loss: 0.00740\n",
      "\n",
      " > Epoch 526/1000\n",
      "   | > Step:5/68  GlobalStep:105300  TotalLoss:0.00141  PostnetLoss:0.00068  DecoderLoss:0.00073  StopLoss:0.05370  GradNorm:0.00370  GradNormST:0.01643  AvgTextLen:5.9  AvgSpecLen:33.2  StepTime:0.34  LR:0.000100\n",
      "   | > Step:15/68  GlobalStep:105310  TotalLoss:0.00171  PostnetLoss:0.00084  DecoderLoss:0.00088  StopLoss:0.04625  GradNorm:0.00344  GradNormST:0.01676  AvgTextLen:12.8  AvgSpecLen:59.1  StepTime:0.52  LR:0.000100\n",
      "   | > Step:25/68  GlobalStep:105320  TotalLoss:0.00186  PostnetLoss:0.00091  DecoderLoss:0.00096  StopLoss:0.05808  GradNorm:0.00339  GradNormST:0.01435  AvgTextLen:19.8  AvgSpecLen:73.3  StepTime:0.48  LR:0.000100\n",
      "   | > Step:35/68  GlobalStep:105330  TotalLoss:0.00211  PostnetLoss:0.00103  DecoderLoss:0.00108  StopLoss:0.04349  GradNorm:0.00308  GradNormST:0.01656  AvgTextLen:27.1  AvgSpecLen:104.7  StepTime:0.74  LR:0.000100\n",
      "   | > Step:45/68  GlobalStep:105340  TotalLoss:0.00218  PostnetLoss:0.00106  DecoderLoss:0.00112  StopLoss:0.04613  GradNorm:0.00313  GradNormST:0.02006  AvgTextLen:34.4  AvgSpecLen:118.6  StepTime:0.69  LR:0.000100\n",
      "   | > Step:55/68  GlobalStep:105350  TotalLoss:0.00243  PostnetLoss:0.00118  DecoderLoss:0.00125  StopLoss:0.04441  GradNorm:0.00293  GradNormST:0.01265  AvgTextLen:43.9  AvgSpecLen:145.2  StepTime:0.78  LR:0.000100\n",
      "   | > Step:65/68  GlobalStep:105360  TotalLoss:0.00268  PostnetLoss:0.00130  DecoderLoss:0.00138  StopLoss:0.03429  GradNorm:0.00314  GradNormST:0.01720  AvgTextLen:64.2  AvgSpecLen:207.4  StepTime:1.18  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:105363  AvgTotalLoss:0.04944  AvgPostnetLoss:0.00099  AvgDecoderLoss:0.00104  AvgStopLoss:0.04741  EpochTime:42.42  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.09116   PostnetLoss: 0.00525   DecoderLoss:0.00557  StopLoss: 0.08033  \n",
      "   | > TotalLoss: 0.07435   PostnetLoss: 0.00816   DecoderLoss:0.00865  StopLoss: 0.05754  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00099   Validation Loss: 0.00756\n",
      "\n",
      " > Epoch 527/1000\n",
      "   | > Step:6/68  GlobalStep:105370  TotalLoss:0.00140  PostnetLoss:0.00068  DecoderLoss:0.00072  StopLoss:0.04026  GradNorm:0.00353  GradNormST:0.01614  AvgTextLen:6.0  AvgSpecLen:34.8  StepTime:0.30  LR:0.000100\n",
      "   | > Step:16/68  GlobalStep:105380  TotalLoss:0.00174  PostnetLoss:0.00085  DecoderLoss:0.00089  StopLoss:0.04377  GradNorm:0.00366  GradNormST:0.01438  AvgTextLen:13.8  AvgSpecLen:60.6  StepTime:0.36  LR:0.000100\n",
      "   | > Step:26/68  GlobalStep:105390  TotalLoss:0.00191  PostnetLoss:0.00093  DecoderLoss:0.00098  StopLoss:0.04023  GradNorm:0.00319  GradNormST:0.01873  AvgTextLen:20.3  AvgSpecLen:77.5  StepTime:0.50  LR:0.000100\n",
      "   | > Step:36/68  GlobalStep:105400  TotalLoss:0.00205  PostnetLoss:0.00100  DecoderLoss:0.00105  StopLoss:0.05325  GradNorm:0.00278  GradNormST:0.01394  AvgTextLen:28.5  AvgSpecLen:100.2  StepTime:0.47  LR:0.000100\n",
      "   | > Step:46/68  GlobalStep:105410  TotalLoss:0.00220  PostnetLoss:0.00107  DecoderLoss:0.00113  StopLoss:0.04231  GradNorm:0.00299  GradNormST:0.01394  AvgTextLen:35.4  AvgSpecLen:122.6  StepTime:0.77  LR:0.000100\n",
      "   | > Step:56/68  GlobalStep:105420  TotalLoss:0.00244  PostnetLoss:0.00119  DecoderLoss:0.00126  StopLoss:0.03445  GradNorm:0.00334  GradNormST:0.01170  AvgTextLen:45.8  AvgSpecLen:145.4  StepTime:0.96  LR:0.000100\n",
      "   | > Step:66/68  GlobalStep:105430  TotalLoss:0.00283  PostnetLoss:0.00136  DecoderLoss:0.00146  StopLoss:0.03786  GradNorm:0.00322  GradNormST:0.02316  AvgTextLen:71.2  AvgSpecLen:220.8  StepTime:1.04  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:105432  AvgTotalLoss:0.05054  AvgPostnetLoss:0.00099  AvgDecoderLoss:0.00104  AvgStopLoss:0.04851  EpochTime:43.08  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.08404   PostnetLoss: 0.00530   DecoderLoss:0.00563  StopLoss: 0.07311  \n",
      "   | > TotalLoss: 0.06759   PostnetLoss: 0.00809   DecoderLoss:0.00857  StopLoss: 0.05093  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00099   Validation Loss: 0.00745\n",
      "\n",
      " > Epoch 528/1000\n",
      "   | > Step:7/68  GlobalStep:105440  TotalLoss:0.00152  PostnetLoss:0.00074  DecoderLoss:0.00078  StopLoss:0.04720  GradNorm:0.00359  GradNormST:0.01696  AvgTextLen:7.4  AvgSpecLen:44.6  StepTime:0.39  LR:0.000100\n",
      "   | > Step:17/68  GlobalStep:105450  TotalLoss:0.00169  PostnetLoss:0.00082  DecoderLoss:0.00087  StopLoss:0.05359  GradNorm:0.00333  GradNormST:0.01222  AvgTextLen:14.7  AvgSpecLen:66.0  StepTime:0.36  LR:0.000100\n",
      "   | > Step:27/68  GlobalStep:105460  TotalLoss:0.00194  PostnetLoss:0.00095  DecoderLoss:0.00099  StopLoss:0.04296  GradNorm:0.00318  GradNormST:0.00971  AvgTextLen:21.0  AvgSpecLen:80.2  StepTime:0.66  LR:0.000100\n",
      "   | > Step:37/68  GlobalStep:105470  TotalLoss:0.00210  PostnetLoss:0.00102  DecoderLoss:0.00108  StopLoss:0.04782  GradNorm:0.00300  GradNormST:0.01308  AvgTextLen:29.3  AvgSpecLen:102.4  StepTime:0.49  LR:0.000100\n",
      "   | > Step:47/68  GlobalStep:105480  TotalLoss:0.00228  PostnetLoss:0.00111  DecoderLoss:0.00117  StopLoss:0.04091  GradNorm:0.00332  GradNormST:0.01060  AvgTextLen:36.7  AvgSpecLen:126.9  StepTime:0.58  LR:0.000100\n",
      "   | > Step:57/68  GlobalStep:105490  TotalLoss:0.00249  PostnetLoss:0.00121  DecoderLoss:0.00128  StopLoss:0.03489  GradNorm:0.00347  GradNormST:0.00885  AvgTextLen:47.1  AvgSpecLen:161.0  StepTime:0.91  LR:0.000100\n",
      "   | > Step:67/68  GlobalStep:105500  TotalLoss:0.00282  PostnetLoss:0.00136  DecoderLoss:0.00146  StopLoss:0.02786  GradNorm:0.00310  GradNormST:0.01551  AvgTextLen:80.6  AvgSpecLen:246.5  StepTime:1.22  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:105501  AvgTotalLoss:0.04710  AvgPostnetLoss:0.00098  AvgDecoderLoss:0.00104  AvgStopLoss:0.04507  EpochTime:42.15  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.08680   PostnetLoss: 0.00548   DecoderLoss:0.00582  StopLoss: 0.07551  \n",
      "   | > TotalLoss: 0.06976   PostnetLoss: 0.00800   DecoderLoss:0.00849  StopLoss: 0.05328  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00098   Validation Loss: 0.00746\n",
      "\n",
      " > Epoch 529/1000\n",
      "   | > Step:8/68  GlobalStep:105510  TotalLoss:0.00156  PostnetLoss:0.00076  DecoderLoss:0.00080  StopLoss:0.04475  GradNorm:0.00408  GradNormST:0.02276  AvgTextLen:8.3  AvgSpecLen:44.0  StepTime:0.31  LR:0.000100\n",
      "   | > Step:18/68  GlobalStep:105520  TotalLoss:0.00170  PostnetLoss:0.00083  DecoderLoss:0.00087  StopLoss:0.04822  GradNorm:0.00327  GradNormST:0.01330  AvgTextLen:15.2  AvgSpecLen:64.5  StepTime:0.41  LR:0.000100\n",
      "   | > Step:28/68  GlobalStep:105530  TotalLoss:0.00192  PostnetLoss:0.00094  DecoderLoss:0.00099  StopLoss:0.04982  GradNorm:0.00314  GradNormST:0.01091  AvgTextLen:22.1  AvgSpecLen:85.4  StepTime:0.58  LR:0.000100\n",
      "   | > Step:38/68  GlobalStep:105540  TotalLoss:0.00203  PostnetLoss:0.00099  DecoderLoss:0.00105  StopLoss:0.04188  GradNorm:0.00294  GradNormST:0.00934  AvgTextLen:29.4  AvgSpecLen:104.1  StepTime:0.65  LR:0.000100\n",
      "   | > Step:48/68  GlobalStep:105550  TotalLoss:0.00229  PostnetLoss:0.00111  DecoderLoss:0.00118  StopLoss:0.03439  GradNorm:0.00304  GradNormST:0.00923  AvgTextLen:37.5  AvgSpecLen:123.8  StepTime:0.66  LR:0.000100\n",
      "   | > Step:58/68  GlobalStep:105560  TotalLoss:0.00248  PostnetLoss:0.00120  DecoderLoss:0.00127  StopLoss:0.04023  GradNorm:0.00353  GradNormST:0.01375  AvgTextLen:47.8  AvgSpecLen:158.0  StepTime:0.71  LR:0.000100\n",
      "   | > Step:68/68  GlobalStep:105570  TotalLoss:0.00282  PostnetLoss:0.00136  DecoderLoss:0.00146  StopLoss:0.02820  GradNorm:0.00285  GradNormST:0.01137  AvgTextLen:103.0  AvgSpecLen:308.9  StepTime:1.86  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:105570  AvgTotalLoss:0.04832  AvgPostnetLoss:0.00098  AvgDecoderLoss:0.00104  AvgStopLoss:0.04630  EpochTime:42.59  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.08580   PostnetLoss: 0.00534   DecoderLoss:0.00568  StopLoss: 0.07478  \n",
      "   | > TotalLoss: 0.06941   PostnetLoss: 0.00799   DecoderLoss:0.00848  StopLoss: 0.05295  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00098   Validation Loss: 0.00748\n",
      "\n",
      " > Epoch 530/1000\n",
      "   | > Step:9/68  GlobalStep:105580  TotalLoss:0.00149  PostnetLoss:0.00073  DecoderLoss:0.00076  StopLoss:0.04358  GradNorm:0.00352  GradNormST:0.01554  AvgTextLen:9.0  AvgSpecLen:42.1  StepTime:0.32  LR:0.000100\n",
      "   | > Step:19/68  GlobalStep:105590  TotalLoss:0.00171  PostnetLoss:0.00083  DecoderLoss:0.00087  StopLoss:0.05336  GradNorm:0.00314  GradNormST:0.02270  AvgTextLen:15.5  AvgSpecLen:63.2  StepTime:0.33  LR:0.000100\n",
      "   | > Step:29/68  GlobalStep:105600  TotalLoss:0.00190  PostnetLoss:0.00093  DecoderLoss:0.00098  StopLoss:0.05862  GradNorm:0.00307  GradNormST:0.01153  AvgTextLen:21.8  AvgSpecLen:83.4  StepTime:0.59  LR:0.000100\n",
      "   | > Step:39/68  GlobalStep:105610  TotalLoss:0.00211  PostnetLoss:0.00103  DecoderLoss:0.00108  StopLoss:0.04081  GradNorm:0.00294  GradNormST:0.01224  AvgTextLen:30.2  AvgSpecLen:110.4  StepTime:0.68  LR:0.000100\n",
      "   | > Step:49/68  GlobalStep:105620  TotalLoss:0.00226  PostnetLoss:0.00110  DecoderLoss:0.00116  StopLoss:0.03728  GradNorm:0.00291  GradNormST:0.00928  AvgTextLen:38.2  AvgSpecLen:127.7  StepTime:0.73  LR:0.000100\n",
      "   | > Step:59/68  GlobalStep:105630  TotalLoss:0.00246  PostnetLoss:0.00119  DecoderLoss:0.00127  StopLoss:0.04286  GradNorm:0.00335  GradNormST:0.01075  AvgTextLen:50.5  AvgSpecLen:155.9  StepTime:0.82  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:105639  AvgTotalLoss:0.04616  AvgPostnetLoss:0.00098  AvgDecoderLoss:0.00104  AvgStopLoss:0.04414  EpochTime:42.37  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.08494   PostnetLoss: 0.00530   DecoderLoss:0.00563  StopLoss: 0.07401  \n",
      "   | > TotalLoss: 0.06639   PostnetLoss: 0.00810   DecoderLoss:0.00858  StopLoss: 0.04971  \n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00098   Validation Loss: 0.00752\n",
      "\n",
      " > Epoch 531/1000\n",
      "   | > Step:0/68  GlobalStep:105640  TotalLoss:0.00144  PostnetLoss:0.00069  DecoderLoss:0.00075  StopLoss:0.08978  GradNorm:0.00551  GradNormST:0.03523  AvgTextLen:3.0  AvgSpecLen:25.3  StepTime:0.48  LR:0.000100\n",
      "   | > Step:10/68  GlobalStep:105650  TotalLoss:0.00155  PostnetLoss:0.00076  DecoderLoss:0.00080  StopLoss:0.05595  GradNorm:0.00359  GradNormST:0.01905  AvgTextLen:9.8  AvgSpecLen:48.6  StepTime:0.66  LR:0.000100\n",
      "   | > Step:20/68  GlobalStep:105660  TotalLoss:0.00182  PostnetLoss:0.00089  DecoderLoss:0.00093  StopLoss:0.04412  GradNorm:0.00324  GradNormST:0.01936  AvgTextLen:16.4  AvgSpecLen:64.8  StepTime:0.49  LR:0.000100\n",
      "   | > Step:30/68  GlobalStep:105670  TotalLoss:0.00197  PostnetLoss:0.00096  DecoderLoss:0.00101  StopLoss:0.04075  GradNorm:0.00302  GradNormST:0.00887  AvgTextLen:22.9  AvgSpecLen:85.8  StepTime:0.57  LR:0.000100\n",
      "   | > Step:40/68  GlobalStep:105680  TotalLoss:0.00213  PostnetLoss:0.00104  DecoderLoss:0.00110  StopLoss:0.04662  GradNorm:0.00332  GradNormST:0.01319  AvgTextLen:30.9  AvgSpecLen:105.3  StepTime:0.58  LR:0.000100\n",
      "   | > Step:50/68  GlobalStep:105690  TotalLoss:0.00229  PostnetLoss:0.00111  DecoderLoss:0.00118  StopLoss:0.03628  GradNorm:0.00320  GradNormST:0.01058  AvgTextLen:38.8  AvgSpecLen:132.0  StepTime:0.70  LR:0.000100\n",
      "   | > Step:60/68  GlobalStep:105700  TotalLoss:0.00253  PostnetLoss:0.00123  DecoderLoss:0.00130  StopLoss:0.03441  GradNorm:0.00345  GradNormST:0.01240  AvgTextLen:52.0  AvgSpecLen:172.7  StepTime:0.91  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:105708  AvgTotalLoss:0.04578  AvgPostnetLoss:0.00098  AvgDecoderLoss:0.00104  AvgStopLoss:0.04377  EpochTime:43.18  AvgStepTime:0.63\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.08673   PostnetLoss: 0.00517   DecoderLoss:0.00551  StopLoss: 0.07605  \n",
      "   | > TotalLoss: 0.07031   PostnetLoss: 0.00820   DecoderLoss:0.00870  StopLoss: 0.05341  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00098   Validation Loss: 0.00756\n",
      "\n",
      " > Epoch 532/1000\n",
      "   | > Step:1/68  GlobalStep:105710  TotalLoss:0.00141  PostnetLoss:0.00068  DecoderLoss:0.00073  StopLoss:0.04793  GradNorm:0.00445  GradNormST:0.01569  AvgTextLen:3.7  AvgSpecLen:28.6  StepTime:0.36  LR:0.000100\n",
      "   | > Step:11/68  GlobalStep:105720  TotalLoss:0.00156  PostnetLoss:0.00076  DecoderLoss:0.00080  StopLoss:0.04249  GradNorm:0.00346  GradNormST:0.01347  AvgTextLen:10.3  AvgSpecLen:49.4  StepTime:0.31  LR:0.000100\n",
      "   | > Step:21/68  GlobalStep:105730  TotalLoss:0.00178  PostnetLoss:0.00087  DecoderLoss:0.00091  StopLoss:0.04855  GradNorm:0.00312  GradNormST:0.01532  AvgTextLen:16.6  AvgSpecLen:66.3  StepTime:0.34  LR:0.000100\n",
      "   | > Step:31/68  GlobalStep:105740  TotalLoss:0.00194  PostnetLoss:0.00094  DecoderLoss:0.00100  StopLoss:0.04349  GradNorm:0.00296  GradNormST:0.01280  AvgTextLen:24.1  AvgSpecLen:82.7  StepTime:0.54  LR:0.000100\n",
      "   | > Step:41/68  GlobalStep:105750  TotalLoss:0.00219  PostnetLoss:0.00107  DecoderLoss:0.00113  StopLoss:0.04135  GradNorm:0.00293  GradNormST:0.01557  AvgTextLen:32.3  AvgSpecLen:109.4  StepTime:0.66  LR:0.000100\n",
      "   | > Step:51/68  GlobalStep:105760  TotalLoss:0.00227  PostnetLoss:0.00110  DecoderLoss:0.00117  StopLoss:0.03259  GradNorm:0.00308  GradNormST:0.00803  AvgTextLen:39.2  AvgSpecLen:126.9  StepTime:0.61  LR:0.000100\n",
      "   | > Step:61/68  GlobalStep:105770  TotalLoss:0.00249  PostnetLoss:0.00121  DecoderLoss:0.00128  StopLoss:0.04539  GradNorm:0.00376  GradNormST:0.02209  AvgTextLen:54.6  AvgSpecLen:165.5  StepTime:0.90  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:105777  AvgTotalLoss:0.04579  AvgPostnetLoss:0.00098  AvgDecoderLoss:0.00104  AvgStopLoss:0.04377  EpochTime:42.87  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.08820   PostnetLoss: 0.00536   DecoderLoss:0.00571  StopLoss: 0.07713  \n",
      "   | > TotalLoss: 0.07113   PostnetLoss: 0.00827   DecoderLoss:0.00875  StopLoss: 0.05411  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00098   Validation Loss: 0.00765\n",
      "\n",
      " > Epoch 533/1000\n",
      "   | > Step:2/68  GlobalStep:105780  TotalLoss:0.00133  PostnetLoss:0.00065  DecoderLoss:0.00068  StopLoss:0.05047  GradNorm:0.00423  GradNormST:0.01717  AvgTextLen:4.0  AvgSpecLen:31.3  StepTime:0.50  LR:0.000100\n",
      "   | > Step:12/68  GlobalStep:105790  TotalLoss:0.00157  PostnetLoss:0.00077  DecoderLoss:0.00080  StopLoss:0.04791  GradNorm:0.00373  GradNormST:0.01310  AvgTextLen:11.0  AvgSpecLen:49.3  StepTime:0.34  LR:0.000100\n",
      "   | > Step:22/68  GlobalStep:105800  TotalLoss:0.00180  PostnetLoss:0.00088  DecoderLoss:0.00092  StopLoss:0.05764  GradNorm:0.00314  GradNormST:0.01478  AvgTextLen:18.0  AvgSpecLen:72.0  StepTime:0.48  LR:0.000100\n",
      "   | > Step:32/68  GlobalStep:105810  TotalLoss:0.00196  PostnetLoss:0.00095  DecoderLoss:0.00100  StopLoss:0.05422  GradNorm:0.00301  GradNormST:0.01637  AvgTextLen:24.4  AvgSpecLen:91.5  StepTime:0.57  LR:0.000100\n",
      "   | > Step:42/68  GlobalStep:105820  TotalLoss:0.00215  PostnetLoss:0.00104  DecoderLoss:0.00111  StopLoss:0.05253  GradNorm:0.00274  GradNormST:0.01485  AvgTextLen:33.1  AvgSpecLen:109.7  StepTime:0.58  LR:0.000100\n",
      "   | > Step:52/68  GlobalStep:105830  TotalLoss:0.00234  PostnetLoss:0.00114  DecoderLoss:0.00121  StopLoss:0.04916  GradNorm:0.00337  GradNormST:0.00876  AvgTextLen:42.1  AvgSpecLen:137.5  StepTime:0.80  LR:0.000100\n",
      "   | > Step:62/68  GlobalStep:105840  TotalLoss:0.00257  PostnetLoss:0.00124  DecoderLoss:0.00133  StopLoss:0.03413  GradNorm:0.00365  GradNormST:0.01324  AvgTextLen:54.0  AvgSpecLen:174.7  StepTime:0.76  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:105846  AvgTotalLoss:0.04579  AvgPostnetLoss:0.00098  AvgDecoderLoss:0.00104  AvgStopLoss:0.04377  EpochTime:43.28  AvgStepTime:0.63\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.08799   PostnetLoss: 0.00548   DecoderLoss:0.00580  StopLoss: 0.07671  \n",
      "   | > TotalLoss: 0.06817   PostnetLoss: 0.00791   DecoderLoss:0.00838  StopLoss: 0.05188  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00098   Validation Loss: 0.00751\n",
      "\n",
      " > Epoch 534/1000\n",
      "   | > Step:3/68  GlobalStep:105850  TotalLoss:0.00137  PostnetLoss:0.00067  DecoderLoss:0.00070  StopLoss:0.07294  GradNorm:0.00409  GradNormST:0.02683  AvgTextLen:4.7  AvgSpecLen:29.7  StepTime:0.34  LR:0.000100\n",
      "   | > Step:13/68  GlobalStep:105860  TotalLoss:0.00156  PostnetLoss:0.00076  DecoderLoss:0.00080  StopLoss:0.07987  GradNorm:0.00343  GradNormST:0.02486  AvgTextLen:11.5  AvgSpecLen:55.2  StepTime:0.43  LR:0.000100\n",
      "   | > Step:23/68  GlobalStep:105870  TotalLoss:0.00187  PostnetLoss:0.00091  DecoderLoss:0.00096  StopLoss:0.04877  GradNorm:0.00346  GradNormST:0.01297  AvgTextLen:18.3  AvgSpecLen:72.2  StepTime:0.45  LR:0.000100\n",
      "   | > Step:33/68  GlobalStep:105880  TotalLoss:0.00202  PostnetLoss:0.00098  DecoderLoss:0.00104  StopLoss:0.03891  GradNorm:0.00313  GradNormST:0.00889  AvgTextLen:25.9  AvgSpecLen:88.9  StepTime:0.43  LR:0.000100\n",
      "   | > Step:43/68  GlobalStep:105890  TotalLoss:0.00215  PostnetLoss:0.00105  DecoderLoss:0.00111  StopLoss:0.04402  GradNorm:0.00274  GradNormST:0.01235  AvgTextLen:33.4  AvgSpecLen:115.7  StepTime:0.62  LR:0.000100\n",
      "   | > Step:53/68  GlobalStep:105900  TotalLoss:0.00230  PostnetLoss:0.00112  DecoderLoss:0.00119  StopLoss:0.03292  GradNorm:0.00297  GradNormST:0.00713  AvgTextLen:42.4  AvgSpecLen:137.6  StepTime:0.74  LR:0.000100\n",
      "   | > Step:63/68  GlobalStep:105910  TotalLoss:0.00259  PostnetLoss:0.00126  DecoderLoss:0.00134  StopLoss:0.02503  GradNorm:0.00300  GradNormST:0.00618  AvgTextLen:58.6  AvgSpecLen:185.1  StepTime:0.91  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:105915  AvgTotalLoss:0.04558  AvgPostnetLoss:0.00098  AvgDecoderLoss:0.00104  AvgStopLoss:0.04356  EpochTime:41.96  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.08266   PostnetLoss: 0.00504   DecoderLoss:0.00535  StopLoss: 0.07226  \n",
      "   | > TotalLoss: 0.07168   PostnetLoss: 0.00823   DecoderLoss:0.00871  StopLoss: 0.05474  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00098   Validation Loss: 0.00760\n",
      "\n",
      " > Epoch 535/1000\n",
      "   | > Step:4/68  GlobalStep:105920  TotalLoss:0.00140  PostnetLoss:0.00068  DecoderLoss:0.00071  StopLoss:0.06551  GradNorm:0.00528  GradNormST:0.02392  AvgTextLen:4.8  AvgSpecLen:31.1  StepTime:0.25  LR:0.000100\n",
      "   | > Step:14/68  GlobalStep:105930  TotalLoss:0.00160  PostnetLoss:0.00078  DecoderLoss:0.00082  StopLoss:0.04418  GradNorm:0.00343  GradNormST:0.01564  AvgTextLen:12.2  AvgSpecLen:52.0  StepTime:0.33  LR:0.000100\n",
      "   | > Step:24/68  GlobalStep:105940  TotalLoss:0.00191  PostnetLoss:0.00093  DecoderLoss:0.00098  StopLoss:0.05019  GradNorm:0.00324  GradNormST:0.01349  AvgTextLen:19.2  AvgSpecLen:79.0  StepTime:0.43  LR:0.000100\n",
      "   | > Step:34/68  GlobalStep:105950  TotalLoss:0.00204  PostnetLoss:0.00099  DecoderLoss:0.00105  StopLoss:0.03589  GradNorm:0.00323  GradNormST:0.01212  AvgTextLen:26.2  AvgSpecLen:95.9  StepTime:0.52  LR:0.000100\n",
      "   | > Step:44/68  GlobalStep:105960  TotalLoss:0.00222  PostnetLoss:0.00108  DecoderLoss:0.00114  StopLoss:0.03159  GradNorm:0.00286  GradNormST:0.01068  AvgTextLen:33.8  AvgSpecLen:116.3  StepTime:0.54  LR:0.000100\n",
      "   | > Step:54/68  GlobalStep:105970  TotalLoss:0.00236  PostnetLoss:0.00114  DecoderLoss:0.00122  StopLoss:0.04224  GradNorm:0.00308  GradNormST:0.01303  AvgTextLen:42.2  AvgSpecLen:144.9  StepTime:0.82  LR:0.000100\n",
      "   | > Step:64/68  GlobalStep:105980  TotalLoss:0.00246  PostnetLoss:0.00119  DecoderLoss:0.00127  StopLoss:0.03208  GradNorm:0.00258  GradNormST:0.01086  AvgTextLen:60.6  AvgSpecLen:177.6  StepTime:0.80  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:105984  AvgTotalLoss:0.04531  AvgPostnetLoss:0.00098  AvgDecoderLoss:0.00104  AvgStopLoss:0.04328  EpochTime:42.50  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.08335   PostnetLoss: 0.00513   DecoderLoss:0.00545  StopLoss: 0.07277  \n",
      "   | > TotalLoss: 0.07116   PostnetLoss: 0.00840   DecoderLoss:0.00887  StopLoss: 0.05388  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00098   Validation Loss: 0.00777\n",
      "\n",
      " > Epoch 536/1000\n",
      "   | > Step:5/68  GlobalStep:105990  TotalLoss:0.00144  PostnetLoss:0.00070  DecoderLoss:0.00074  StopLoss:0.04390  GradNorm:0.00348  GradNormST:0.01376  AvgTextLen:5.9  AvgSpecLen:33.2  StepTime:0.34  LR:0.000100\n",
      "   | > Step:15/68  GlobalStep:106000  TotalLoss:0.00171  PostnetLoss:0.00083  DecoderLoss:0.00087  StopLoss:0.03951  GradNorm:0.00358  GradNormST:0.01268  AvgTextLen:12.8  AvgSpecLen:59.1  StepTime:0.42  LR:0.000100\n",
      " | | > Checkpoint saving : /vm/TTS/models/cartman_models/mozilla-no-loc-fattn-stopnet-sigmoid-loss_masking-August-15-2019_11+26AM-de6aeb9/checkpoint_106000.pth.tar\n",
      "warning: audio amplitude out of range, auto clipped.\n",
      "   | > Step:25/68  GlobalStep:106010  TotalLoss:0.00186  PostnetLoss:0.00091  DecoderLoss:0.00095  StopLoss:0.04352  GradNorm:0.00329  GradNormST:0.01091  AvgTextLen:19.8  AvgSpecLen:73.3  StepTime:0.46  LR:0.000100\n",
      "   | > Step:35/68  GlobalStep:106020  TotalLoss:0.00207  PostnetLoss:0.00101  DecoderLoss:0.00107  StopLoss:0.04458  GradNorm:0.00338  GradNormST:0.01805  AvgTextLen:27.1  AvgSpecLen:104.7  StepTime:0.78  LR:0.000100\n",
      "   | > Step:45/68  GlobalStep:106030  TotalLoss:0.00217  PostnetLoss:0.00106  DecoderLoss:0.00112  StopLoss:0.05044  GradNorm:0.00292  GradNormST:0.01760  AvgTextLen:34.4  AvgSpecLen:118.6  StepTime:0.65  LR:0.000100\n",
      "   | > Step:55/68  GlobalStep:106040  TotalLoss:0.00243  PostnetLoss:0.00118  DecoderLoss:0.00125  StopLoss:0.03231  GradNorm:0.00292  GradNormST:0.00821  AvgTextLen:43.9  AvgSpecLen:145.2  StepTime:0.83  LR:0.000100\n",
      "   | > Step:65/68  GlobalStep:106050  TotalLoss:0.00267  PostnetLoss:0.00129  DecoderLoss:0.00138  StopLoss:0.02982  GradNorm:0.00321  GradNormST:0.00911  AvgTextLen:64.2  AvgSpecLen:207.4  StepTime:1.19  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:106053  AvgTotalLoss:0.04569  AvgPostnetLoss:0.00098  AvgDecoderLoss:0.00104  AvgStopLoss:0.04367  EpochTime:41.99  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.08957   PostnetLoss: 0.00535   DecoderLoss:0.00569  StopLoss: 0.07853  \n",
      "   | > TotalLoss: 0.07010   PostnetLoss: 0.00813   DecoderLoss:0.00860  StopLoss: 0.05336  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00098   Validation Loss: 0.00754\n",
      "\n",
      " > Epoch 537/1000\n",
      "   | > Step:6/68  GlobalStep:106060  TotalLoss:0.00145  PostnetLoss:0.00071  DecoderLoss:0.00074  StopLoss:0.03121  GradNorm:0.00429  GradNormST:0.00923  AvgTextLen:6.0  AvgSpecLen:34.8  StepTime:0.37  LR:0.000100\n",
      "   | > Step:16/68  GlobalStep:106070  TotalLoss:0.00173  PostnetLoss:0.00084  DecoderLoss:0.00089  StopLoss:0.04494  GradNorm:0.00359  GradNormST:0.01434  AvgTextLen:13.8  AvgSpecLen:60.6  StepTime:0.29  LR:0.000100\n",
      "   | > Step:26/68  GlobalStep:106080  TotalLoss:0.00187  PostnetLoss:0.00091  DecoderLoss:0.00096  StopLoss:0.04334  GradNorm:0.00322  GradNormST:0.01487  AvgTextLen:20.3  AvgSpecLen:77.5  StepTime:0.48  LR:0.000100\n",
      "   | > Step:36/68  GlobalStep:106090  TotalLoss:0.00205  PostnetLoss:0.00099  DecoderLoss:0.00105  StopLoss:0.04801  GradNorm:0.00309  GradNormST:0.01059  AvgTextLen:28.5  AvgSpecLen:100.2  StepTime:0.50  LR:0.000100\n",
      "   | > Step:46/68  GlobalStep:106100  TotalLoss:0.00220  PostnetLoss:0.00107  DecoderLoss:0.00113  StopLoss:0.04886  GradNorm:0.00272  GradNormST:0.01469  AvgTextLen:35.4  AvgSpecLen:122.6  StepTime:0.94  LR:0.000100\n",
      "   | > Step:56/68  GlobalStep:106110  TotalLoss:0.00240  PostnetLoss:0.00116  DecoderLoss:0.00124  StopLoss:0.02788  GradNorm:0.00288  GradNormST:0.01407  AvgTextLen:45.8  AvgSpecLen:145.4  StepTime:0.96  LR:0.000100\n",
      "   | > Step:66/68  GlobalStep:106120  TotalLoss:0.00281  PostnetLoss:0.00136  DecoderLoss:0.00146  StopLoss:0.04377  GradNorm:0.00291  GradNormST:0.02773  AvgTextLen:71.2  AvgSpecLen:220.8  StepTime:1.05  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:106122  AvgTotalLoss:0.04546  AvgPostnetLoss:0.00098  AvgDecoderLoss:0.00104  AvgStopLoss:0.04344  EpochTime:41.88  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.08955   PostnetLoss: 0.00527   DecoderLoss:0.00560  StopLoss: 0.07868  \n",
      "   | > TotalLoss: 0.06993   PostnetLoss: 0.00820   DecoderLoss:0.00870  StopLoss: 0.05303  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00098   Validation Loss: 0.00764\n",
      "\n",
      " > Epoch 538/1000\n",
      "   | > Step:7/68  GlobalStep:106130  TotalLoss:0.00149  PostnetLoss:0.00073  DecoderLoss:0.00076  StopLoss:0.06402  GradNorm:0.00373  GradNormST:0.02430  AvgTextLen:7.4  AvgSpecLen:44.6  StepTime:0.38  LR:0.000100\n",
      "   | > Step:17/68  GlobalStep:106140  TotalLoss:0.00169  PostnetLoss:0.00082  DecoderLoss:0.00087  StopLoss:0.05589  GradNorm:0.00336  GradNormST:0.01193  AvgTextLen:14.7  AvgSpecLen:66.0  StepTime:0.46  LR:0.000100\n",
      "   | > Step:27/68  GlobalStep:106150  TotalLoss:0.00194  PostnetLoss:0.00094  DecoderLoss:0.00099  StopLoss:0.04506  GradNorm:0.00325  GradNormST:0.00881  AvgTextLen:21.0  AvgSpecLen:80.2  StepTime:0.53  LR:0.000100\n",
      "   | > Step:37/68  GlobalStep:106160  TotalLoss:0.00209  PostnetLoss:0.00102  DecoderLoss:0.00107  StopLoss:0.04219  GradNorm:0.00303  GradNormST:0.01063  AvgTextLen:29.3  AvgSpecLen:102.4  StepTime:0.58  LR:0.000100\n",
      "   | > Step:47/68  GlobalStep:106170  TotalLoss:0.00228  PostnetLoss:0.00111  DecoderLoss:0.00117  StopLoss:0.03566  GradNorm:0.00282  GradNormST:0.00837  AvgTextLen:36.7  AvgSpecLen:126.9  StepTime:0.74  LR:0.000100\n",
      "   | > Step:57/68  GlobalStep:106180  TotalLoss:0.00244  PostnetLoss:0.00118  DecoderLoss:0.00126  StopLoss:0.03407  GradNorm:0.00261  GradNormST:0.01210  AvgTextLen:47.1  AvgSpecLen:161.0  StepTime:0.84  LR:0.000100\n",
      "   | > Step:67/68  GlobalStep:106190  TotalLoss:0.00281  PostnetLoss:0.00136  DecoderLoss:0.00146  StopLoss:0.02973  GradNorm:0.00317  GradNormST:0.01551  AvgTextLen:80.6  AvgSpecLen:246.5  StepTime:1.23  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:106191  AvgTotalLoss:0.04436  AvgPostnetLoss:0.00098  AvgDecoderLoss:0.00104  AvgStopLoss:0.04235  EpochTime:41.88  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.08550   PostnetLoss: 0.00516   DecoderLoss:0.00549  StopLoss: 0.07485  \n",
      "   | > TotalLoss: 0.07436   PostnetLoss: 0.00848   DecoderLoss:0.00897  StopLoss: 0.05690  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00098   Validation Loss: 0.00778\n",
      "\n",
      " > Epoch 539/1000\n",
      "   | > Step:8/68  GlobalStep:106200  TotalLoss:0.00153  PostnetLoss:0.00075  DecoderLoss:0.00078  StopLoss:0.04771  GradNorm:0.00421  GradNormST:0.01708  AvgTextLen:8.3  AvgSpecLen:44.0  StepTime:0.33  LR:0.000100\n",
      "   | > Step:18/68  GlobalStep:106210  TotalLoss:0.00167  PostnetLoss:0.00081  DecoderLoss:0.00086  StopLoss:0.04200  GradNorm:0.00311  GradNormST:0.01120  AvgTextLen:15.2  AvgSpecLen:64.5  StepTime:0.42  LR:0.000100\n",
      "   | > Step:28/68  GlobalStep:106220  TotalLoss:0.00195  PostnetLoss:0.00095  DecoderLoss:0.00100  StopLoss:0.05103  GradNorm:0.00321  GradNormST:0.01145  AvgTextLen:22.1  AvgSpecLen:85.4  StepTime:0.53  LR:0.000100\n",
      "   | > Step:38/68  GlobalStep:106230  TotalLoss:0.00205  PostnetLoss:0.00100  DecoderLoss:0.00105  StopLoss:0.03722  GradNorm:0.00297  GradNormST:0.00928  AvgTextLen:29.4  AvgSpecLen:104.1  StepTime:0.63  LR:0.000100\n",
      "   | > Step:48/68  GlobalStep:106240  TotalLoss:0.00225  PostnetLoss:0.00109  DecoderLoss:0.00116  StopLoss:0.03194  GradNorm:0.00289  GradNormST:0.00883  AvgTextLen:37.5  AvgSpecLen:123.8  StepTime:0.72  LR:0.000100\n",
      "   | > Step:58/68  GlobalStep:106250  TotalLoss:0.00244  PostnetLoss:0.00118  DecoderLoss:0.00126  StopLoss:0.03865  GradNorm:0.00263  GradNormST:0.01217  AvgTextLen:47.8  AvgSpecLen:158.0  StepTime:0.78  LR:0.000100\n",
      "   | > Step:68/68  GlobalStep:106260  TotalLoss:0.00280  PostnetLoss:0.00135  DecoderLoss:0.00145  StopLoss:0.02202  GradNorm:0.00271  GradNormST:0.01067  AvgTextLen:103.0  AvgSpecLen:308.9  StepTime:1.87  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:106260  AvgTotalLoss:0.04487  AvgPostnetLoss:0.00098  AvgDecoderLoss:0.00104  AvgStopLoss:0.04286  EpochTime:41.90  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.08474   PostnetLoss: 0.00530   DecoderLoss:0.00564  StopLoss: 0.07380  \n",
      "   | > TotalLoss: 0.06714   PostnetLoss: 0.00812   DecoderLoss:0.00859  StopLoss: 0.05042  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00098   Validation Loss: 0.00754\n",
      "\n",
      " > Epoch 540/1000\n",
      "   | > Step:9/68  GlobalStep:106270  TotalLoss:0.00150  PostnetLoss:0.00073  DecoderLoss:0.00077  StopLoss:0.04224  GradNorm:0.00398  GradNormST:0.01775  AvgTextLen:9.0  AvgSpecLen:42.1  StepTime:0.36  LR:0.000100\n",
      "   | > Step:19/68  GlobalStep:106280  TotalLoss:0.00171  PostnetLoss:0.00083  DecoderLoss:0.00087  StopLoss:0.05235  GradNorm:0.00333  GradNormST:0.01788  AvgTextLen:15.5  AvgSpecLen:63.2  StepTime:0.33  LR:0.000100\n",
      "   | > Step:29/68  GlobalStep:106290  TotalLoss:0.00192  PostnetLoss:0.00093  DecoderLoss:0.00098  StopLoss:0.05113  GradNorm:0.00321  GradNormST:0.00873  AvgTextLen:21.8  AvgSpecLen:83.4  StepTime:0.53  LR:0.000100\n",
      "   | > Step:39/68  GlobalStep:106300  TotalLoss:0.00216  PostnetLoss:0.00105  DecoderLoss:0.00111  StopLoss:0.03395  GradNorm:0.00378  GradNormST:0.00973  AvgTextLen:30.2  AvgSpecLen:110.4  StepTime:0.68  LR:0.000100\n",
      "   | > Step:49/68  GlobalStep:106310  TotalLoss:0.00225  PostnetLoss:0.00110  DecoderLoss:0.00116  StopLoss:0.04007  GradNorm:0.00279  GradNormST:0.01100  AvgTextLen:38.2  AvgSpecLen:127.7  StepTime:0.70  LR:0.000100\n",
      "   | > Step:59/68  GlobalStep:106320  TotalLoss:0.00243  PostnetLoss:0.00118  DecoderLoss:0.00125  StopLoss:0.03952  GradNorm:0.00258  GradNormST:0.01025  AvgTextLen:50.5  AvgSpecLen:155.9  StepTime:0.94  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:106329  AvgTotalLoss:0.04308  AvgPostnetLoss:0.00098  AvgDecoderLoss:0.00104  AvgStopLoss:0.04106  EpochTime:42.86  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.08998   PostnetLoss: 0.00532   DecoderLoss:0.00565  StopLoss: 0.07902  \n",
      "   | > TotalLoss: 0.07414   PostnetLoss: 0.00847   DecoderLoss:0.00893  StopLoss: 0.05674  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00098   Validation Loss: 0.00784\n",
      "\n",
      " > Epoch 541/1000\n",
      "   | > Step:0/68  GlobalStep:106330  TotalLoss:0.00145  PostnetLoss:0.00070  DecoderLoss:0.00075  StopLoss:0.06865  GradNorm:0.00664  GradNormST:0.02544  AvgTextLen:3.0  AvgSpecLen:25.3  StepTime:0.59  LR:0.000100\n",
      "   | > Step:10/68  GlobalStep:106340  TotalLoss:0.00155  PostnetLoss:0.00076  DecoderLoss:0.00080  StopLoss:0.03919  GradNorm:0.00361  GradNormST:0.01159  AvgTextLen:9.8  AvgSpecLen:48.6  StepTime:0.51  LR:0.000100\n",
      "   | > Step:20/68  GlobalStep:106350  TotalLoss:0.00180  PostnetLoss:0.00088  DecoderLoss:0.00092  StopLoss:0.03567  GradNorm:0.00343  GradNormST:0.01505  AvgTextLen:16.4  AvgSpecLen:64.8  StepTime:0.39  LR:0.000100\n",
      "   | > Step:30/68  GlobalStep:106360  TotalLoss:0.00194  PostnetLoss:0.00094  DecoderLoss:0.00099  StopLoss:0.04583  GradNorm:0.00296  GradNormST:0.01087  AvgTextLen:22.9  AvgSpecLen:85.8  StepTime:0.60  LR:0.000100\n",
      "   | > Step:40/68  GlobalStep:106370  TotalLoss:0.00209  PostnetLoss:0.00102  DecoderLoss:0.00107  StopLoss:0.04136  GradNorm:0.00313  GradNormST:0.01018  AvgTextLen:30.9  AvgSpecLen:105.3  StepTime:0.60  LR:0.000100\n",
      "   | > Step:50/68  GlobalStep:106380  TotalLoss:0.00227  PostnetLoss:0.00110  DecoderLoss:0.00117  StopLoss:0.03817  GradNorm:0.00289  GradNormST:0.01192  AvgTextLen:38.8  AvgSpecLen:132.0  StepTime:0.83  LR:0.000100\n",
      "   | > Step:60/68  GlobalStep:106390  TotalLoss:0.00247  PostnetLoss:0.00120  DecoderLoss:0.00127  StopLoss:0.03644  GradNorm:0.00262  GradNormST:0.01056  AvgTextLen:52.0  AvgSpecLen:172.7  StepTime:1.01  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:106398  AvgTotalLoss:0.04341  AvgPostnetLoss:0.00098  AvgDecoderLoss:0.00103  AvgStopLoss:0.04140  EpochTime:42.91  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.09214   PostnetLoss: 0.00529   DecoderLoss:0.00562  StopLoss: 0.08123  \n",
      "   | > TotalLoss: 0.07684   PostnetLoss: 0.00872   DecoderLoss:0.00921  StopLoss: 0.05891  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00098   Validation Loss: 0.00802\n",
      "\n",
      " > Epoch 542/1000\n",
      "   | > Step:1/68  GlobalStep:106400  TotalLoss:0.00141  PostnetLoss:0.00068  DecoderLoss:0.00073  StopLoss:0.04241  GradNorm:0.00458  GradNormST:0.01944  AvgTextLen:3.7  AvgSpecLen:28.6  StepTime:0.47  LR:0.000100\n",
      "   | > Step:11/68  GlobalStep:106410  TotalLoss:0.00156  PostnetLoss:0.00076  DecoderLoss:0.00080  StopLoss:0.04151  GradNorm:0.00401  GradNormST:0.01315  AvgTextLen:10.3  AvgSpecLen:49.4  StepTime:0.38  LR:0.000100\n",
      "   | > Step:21/68  GlobalStep:106420  TotalLoss:0.00174  PostnetLoss:0.00085  DecoderLoss:0.00089  StopLoss:0.04665  GradNorm:0.00341  GradNormST:0.01659  AvgTextLen:16.6  AvgSpecLen:66.3  StepTime:0.43  LR:0.000100\n",
      "   | > Step:31/68  GlobalStep:106430  TotalLoss:0.00194  PostnetLoss:0.00094  DecoderLoss:0.00100  StopLoss:0.03156  GradNorm:0.00301  GradNormST:0.00885  AvgTextLen:24.1  AvgSpecLen:82.7  StepTime:0.57  LR:0.000100\n",
      "   | > Step:41/68  GlobalStep:106440  TotalLoss:0.00213  PostnetLoss:0.00104  DecoderLoss:0.00109  StopLoss:0.03476  GradNorm:0.00284  GradNormST:0.00946  AvgTextLen:32.3  AvgSpecLen:109.4  StepTime:0.64  LR:0.000100\n",
      "   | > Step:51/68  GlobalStep:106450  TotalLoss:0.00226  PostnetLoss:0.00110  DecoderLoss:0.00116  StopLoss:0.03026  GradNorm:0.00280  GradNormST:0.00625  AvgTextLen:39.2  AvgSpecLen:126.9  StepTime:0.63  LR:0.000100\n",
      "   | > Step:61/68  GlobalStep:106460  TotalLoss:0.00246  PostnetLoss:0.00119  DecoderLoss:0.00127  StopLoss:0.02720  GradNorm:0.00267  GradNormST:0.00949  AvgTextLen:54.6  AvgSpecLen:165.5  StepTime:0.93  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:106467  AvgTotalLoss:0.04403  AvgPostnetLoss:0.00098  AvgDecoderLoss:0.00103  AvgStopLoss:0.04202  EpochTime:42.32  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.08408   PostnetLoss: 0.00526   DecoderLoss:0.00559  StopLoss: 0.07324  \n",
      "   | > TotalLoss: 0.07417   PostnetLoss: 0.00855   DecoderLoss:0.00902  StopLoss: 0.05660  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00098   Validation Loss: 0.00789\n",
      "\n",
      " > Epoch 543/1000\n",
      "   | > Step:2/68  GlobalStep:106470  TotalLoss:0.00130  PostnetLoss:0.00063  DecoderLoss:0.00067  StopLoss:0.04580  GradNorm:0.00381  GradNormST:0.01714  AvgTextLen:4.0  AvgSpecLen:31.3  StepTime:0.43  LR:0.000100\n",
      "   | > Step:12/68  GlobalStep:106480  TotalLoss:0.00153  PostnetLoss:0.00075  DecoderLoss:0.00078  StopLoss:0.05265  GradNorm:0.00340  GradNormST:0.01488  AvgTextLen:11.0  AvgSpecLen:49.3  StepTime:0.42  LR:0.000100\n",
      "   | > Step:22/68  GlobalStep:106490  TotalLoss:0.00182  PostnetLoss:0.00089  DecoderLoss:0.00093  StopLoss:0.04295  GradNorm:0.00345  GradNormST:0.00971  AvgTextLen:18.0  AvgSpecLen:72.0  StepTime:0.49  LR:0.000100\n",
      "   | > Step:32/68  GlobalStep:106500  TotalLoss:0.00198  PostnetLoss:0.00096  DecoderLoss:0.00101  StopLoss:0.04833  GradNorm:0.00325  GradNormST:0.01317  AvgTextLen:24.4  AvgSpecLen:91.5  StepTime:0.55  LR:0.000100\n",
      "   | > Step:42/68  GlobalStep:106510  TotalLoss:0.00213  PostnetLoss:0.00103  DecoderLoss:0.00110  StopLoss:0.04771  GradNorm:0.00298  GradNormST:0.01436  AvgTextLen:33.1  AvgSpecLen:109.7  StepTime:0.60  LR:0.000100\n",
      "   | > Step:52/68  GlobalStep:106520  TotalLoss:0.00232  PostnetLoss:0.00113  DecoderLoss:0.00119  StopLoss:0.05353  GradNorm:0.00281  GradNormST:0.01414  AvgTextLen:42.1  AvgSpecLen:137.5  StepTime:0.83  LR:0.000100\n",
      "   | > Step:62/68  GlobalStep:106530  TotalLoss:0.00254  PostnetLoss:0.00123  DecoderLoss:0.00131  StopLoss:0.02986  GradNorm:0.00263  GradNormST:0.00690  AvgTextLen:54.0  AvgSpecLen:174.7  StepTime:0.76  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:106536  AvgTotalLoss:0.04512  AvgPostnetLoss:0.00098  AvgDecoderLoss:0.00103  AvgStopLoss:0.04311  EpochTime:42.35  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.09305   PostnetLoss: 0.00534   DecoderLoss:0.00567  StopLoss: 0.08204  \n",
      "   | > TotalLoss: 0.07835   PostnetLoss: 0.00870   DecoderLoss:0.00918  StopLoss: 0.06047  \n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00098   Validation Loss: 0.00793\n",
      "\n",
      " > Epoch 544/1000\n",
      "   | > Step:3/68  GlobalStep:106540  TotalLoss:0.00133  PostnetLoss:0.00065  DecoderLoss:0.00068  StopLoss:0.05708  GradNorm:0.00357  GradNormST:0.02131  AvgTextLen:4.7  AvgSpecLen:29.7  StepTime:0.38  LR:0.000100\n",
      "   | > Step:13/68  GlobalStep:106550  TotalLoss:0.00160  PostnetLoss:0.00078  DecoderLoss:0.00082  StopLoss:0.06875  GradNorm:0.00412  GradNormST:0.02182  AvgTextLen:11.5  AvgSpecLen:55.2  StepTime:0.44  LR:0.000100\n",
      "   | > Step:23/68  GlobalStep:106560  TotalLoss:0.00187  PostnetLoss:0.00091  DecoderLoss:0.00096  StopLoss:0.04927  GradNorm:0.00335  GradNormST:0.01409  AvgTextLen:18.3  AvgSpecLen:72.2  StepTime:0.34  LR:0.000100\n",
      "   | > Step:33/68  GlobalStep:106570  TotalLoss:0.00205  PostnetLoss:0.00100  DecoderLoss:0.00105  StopLoss:0.03976  GradNorm:0.00354  GradNormST:0.00964  AvgTextLen:25.9  AvgSpecLen:88.9  StepTime:0.55  LR:0.000100\n",
      "   | > Step:43/68  GlobalStep:106580  TotalLoss:0.00217  PostnetLoss:0.00105  DecoderLoss:0.00111  StopLoss:0.04162  GradNorm:0.00307  GradNormST:0.01384  AvgTextLen:33.4  AvgSpecLen:115.7  StepTime:0.60  LR:0.000100\n",
      "   | > Step:53/68  GlobalStep:106590  TotalLoss:0.00227  PostnetLoss:0.00110  DecoderLoss:0.00117  StopLoss:0.02940  GradNorm:0.00263  GradNormST:0.00612  AvgTextLen:42.4  AvgSpecLen:137.6  StepTime:0.63  LR:0.000100\n",
      "   | > Step:63/68  GlobalStep:106600  TotalLoss:0.00259  PostnetLoss:0.00125  DecoderLoss:0.00134  StopLoss:0.02413  GradNorm:0.00263  GradNormST:0.00780  AvgTextLen:58.6  AvgSpecLen:185.1  StepTime:0.90  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:106605  AvgTotalLoss:0.04305  AvgPostnetLoss:0.00098  AvgDecoderLoss:0.00103  AvgStopLoss:0.04104  EpochTime:42.61  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.09367   PostnetLoss: 0.00529   DecoderLoss:0.00562  StopLoss: 0.08277  \n",
      "   | > TotalLoss: 0.07600   PostnetLoss: 0.00877   DecoderLoss:0.00924  StopLoss: 0.05799  \n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00098   Validation Loss: 0.00795\n",
      "\n",
      " > Epoch 545/1000\n",
      "   | > Step:4/68  GlobalStep:106610  TotalLoss:0.00137  PostnetLoss:0.00066  DecoderLoss:0.00070  StopLoss:0.06588  GradNorm:0.00344  GradNormST:0.02306  AvgTextLen:4.8  AvgSpecLen:31.1  StepTime:0.29  LR:0.000100\n",
      "   | > Step:14/68  GlobalStep:106620  TotalLoss:0.00161  PostnetLoss:0.00078  DecoderLoss:0.00083  StopLoss:0.04808  GradNorm:0.00329  GradNormST:0.01411  AvgTextLen:12.2  AvgSpecLen:52.0  StepTime:0.31  LR:0.000100\n",
      "   | > Step:24/68  GlobalStep:106630  TotalLoss:0.00191  PostnetLoss:0.00093  DecoderLoss:0.00098  StopLoss:0.04533  GradNorm:0.00326  GradNormST:0.01006  AvgTextLen:19.2  AvgSpecLen:79.0  StepTime:0.40  LR:0.000100\n",
      "   | > Step:34/68  GlobalStep:106640  TotalLoss:0.00208  PostnetLoss:0.00101  DecoderLoss:0.00107  StopLoss:0.03705  GradNorm:0.00382  GradNormST:0.00943  AvgTextLen:26.2  AvgSpecLen:95.9  StepTime:0.48  LR:0.000100\n",
      "   | > Step:44/68  GlobalStep:106650  TotalLoss:0.00223  PostnetLoss:0.00108  DecoderLoss:0.00115  StopLoss:0.03652  GradNorm:0.00357  GradNormST:0.00914  AvgTextLen:33.8  AvgSpecLen:116.3  StepTime:0.58  LR:0.000100\n",
      "   | > Step:54/68  GlobalStep:106660  TotalLoss:0.00235  PostnetLoss:0.00114  DecoderLoss:0.00121  StopLoss:0.03519  GradNorm:0.00289  GradNormST:0.01130  AvgTextLen:42.2  AvgSpecLen:144.9  StepTime:0.82  LR:0.000100\n",
      "   | > Step:64/68  GlobalStep:106670  TotalLoss:0.00244  PostnetLoss:0.00118  DecoderLoss:0.00126  StopLoss:0.02600  GradNorm:0.00247  GradNormST:0.00825  AvgTextLen:60.6  AvgSpecLen:177.6  StepTime:0.80  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:106674  AvgTotalLoss:0.04378  AvgPostnetLoss:0.00097  AvgDecoderLoss:0.00103  AvgStopLoss:0.04178  EpochTime:41.93  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.09231   PostnetLoss: 0.00539   DecoderLoss:0.00571  StopLoss: 0.08121  \n",
      "   | > TotalLoss: 0.07496   PostnetLoss: 0.00857   DecoderLoss:0.00905  StopLoss: 0.05734  \n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00097   Validation Loss: 0.00787\n",
      "\n",
      " > Epoch 546/1000\n",
      "   | > Step:5/68  GlobalStep:106680  TotalLoss:0.00143  PostnetLoss:0.00069  DecoderLoss:0.00074  StopLoss:0.04408  GradNorm:0.00340  GradNormST:0.01214  AvgTextLen:5.9  AvgSpecLen:33.2  StepTime:0.37  LR:0.000100\n",
      "   | > Step:15/68  GlobalStep:106690  TotalLoss:0.00167  PostnetLoss:0.00081  DecoderLoss:0.00086  StopLoss:0.03442  GradNorm:0.00327  GradNormST:0.01155  AvgTextLen:12.8  AvgSpecLen:59.1  StepTime:0.50  LR:0.000100\n",
      "   | > Step:25/68  GlobalStep:106700  TotalLoss:0.00184  PostnetLoss:0.00089  DecoderLoss:0.00094  StopLoss:0.05029  GradNorm:0.00351  GradNormST:0.00951  AvgTextLen:19.8  AvgSpecLen:73.3  StepTime:0.39  LR:0.000100\n",
      "   | > Step:35/68  GlobalStep:106710  TotalLoss:0.00207  PostnetLoss:0.00101  DecoderLoss:0.00106  StopLoss:0.04102  GradNorm:0.00322  GradNormST:0.01377  AvgTextLen:27.1  AvgSpecLen:104.7  StepTime:0.63  LR:0.000100\n",
      "   | > Step:45/68  GlobalStep:106720  TotalLoss:0.00218  PostnetLoss:0.00106  DecoderLoss:0.00112  StopLoss:0.04279  GradNorm:0.00359  GradNormST:0.01420  AvgTextLen:34.4  AvgSpecLen:118.6  StepTime:0.65  LR:0.000100\n",
      "   | > Step:55/68  GlobalStep:106730  TotalLoss:0.00243  PostnetLoss:0.00118  DecoderLoss:0.00125  StopLoss:0.03176  GradNorm:0.00329  GradNormST:0.00995  AvgTextLen:43.9  AvgSpecLen:145.2  StepTime:0.77  LR:0.000100\n",
      "   | > Step:65/68  GlobalStep:106740  TotalLoss:0.00264  PostnetLoss:0.00128  DecoderLoss:0.00136  StopLoss:0.03237  GradNorm:0.00255  GradNormST:0.00902  AvgTextLen:64.2  AvgSpecLen:207.4  StepTime:1.20  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:106743  AvgTotalLoss:0.04241  AvgPostnetLoss:0.00098  AvgDecoderLoss:0.00103  AvgStopLoss:0.04040  EpochTime:42.55  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.09185   PostnetLoss: 0.00529   DecoderLoss:0.00561  StopLoss: 0.08094  \n",
      "   | > TotalLoss: 0.07560   PostnetLoss: 0.00849   DecoderLoss:0.00896  StopLoss: 0.05815  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00098   Validation Loss: 0.00779\n",
      "\n",
      " > Epoch 547/1000\n",
      "   | > Step:6/68  GlobalStep:106750  TotalLoss:0.00140  PostnetLoss:0.00068  DecoderLoss:0.00072  StopLoss:0.03557  GradNorm:0.00394  GradNormST:0.01302  AvgTextLen:6.0  AvgSpecLen:34.8  StepTime:0.35  LR:0.000100\n",
      "   | > Step:16/68  GlobalStep:106760  TotalLoss:0.00173  PostnetLoss:0.00084  DecoderLoss:0.00089  StopLoss:0.03774  GradNorm:0.00375  GradNormST:0.01015  AvgTextLen:13.8  AvgSpecLen:60.6  StepTime:0.38  LR:0.000100\n",
      "   | > Step:26/68  GlobalStep:106770  TotalLoss:0.00190  PostnetLoss:0.00092  DecoderLoss:0.00097  StopLoss:0.03959  GradNorm:0.00326  GradNormST:0.01116  AvgTextLen:20.3  AvgSpecLen:77.5  StepTime:0.48  LR:0.000100\n",
      "   | > Step:36/68  GlobalStep:106780  TotalLoss:0.00203  PostnetLoss:0.00099  DecoderLoss:0.00104  StopLoss:0.04918  GradNorm:0.00310  GradNormST:0.01173  AvgTextLen:28.5  AvgSpecLen:100.2  StepTime:0.69  LR:0.000100\n",
      "   | > Step:46/68  GlobalStep:106790  TotalLoss:0.00219  PostnetLoss:0.00106  DecoderLoss:0.00113  StopLoss:0.04170  GradNorm:0.00339  GradNormST:0.01290  AvgTextLen:35.4  AvgSpecLen:122.6  StepTime:0.78  LR:0.000100\n",
      "   | > Step:56/68  GlobalStep:106800  TotalLoss:0.00241  PostnetLoss:0.00117  DecoderLoss:0.00124  StopLoss:0.04008  GradNorm:0.00321  GradNormST:0.02146  AvgTextLen:45.8  AvgSpecLen:145.4  StepTime:1.04  LR:0.000100\n",
      "   | > Step:66/68  GlobalStep:106810  TotalLoss:0.00282  PostnetLoss:0.00136  DecoderLoss:0.00146  StopLoss:0.02755  GradNorm:0.00289  GradNormST:0.01423  AvgTextLen:71.2  AvgSpecLen:220.8  StepTime:1.05  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:106812  AvgTotalLoss:0.04398  AvgPostnetLoss:0.00098  AvgDecoderLoss:0.00103  AvgStopLoss:0.04198  EpochTime:42.64  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.09147   PostnetLoss: 0.00530   DecoderLoss:0.00562  StopLoss: 0.08055  \n",
      "   | > TotalLoss: 0.07224   PostnetLoss: 0.00846   DecoderLoss:0.00895  StopLoss: 0.05482  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00098   Validation Loss: 0.00776\n",
      "\n",
      " > Epoch 548/1000\n",
      "   | > Step:7/68  GlobalStep:106820  TotalLoss:0.00148  PostnetLoss:0.00072  DecoderLoss:0.00076  StopLoss:0.03969  GradNorm:0.00349  GradNormST:0.01512  AvgTextLen:7.4  AvgSpecLen:44.6  StepTime:0.35  LR:0.000100\n",
      "   | > Step:17/68  GlobalStep:106830  TotalLoss:0.00168  PostnetLoss:0.00082  DecoderLoss:0.00086  StopLoss:0.04508  GradNorm:0.00333  GradNormST:0.00951  AvgTextLen:14.7  AvgSpecLen:66.0  StepTime:0.41  LR:0.000100\n",
      "   | > Step:27/68  GlobalStep:106840  TotalLoss:0.00192  PostnetLoss:0.00094  DecoderLoss:0.00099  StopLoss:0.04442  GradNorm:0.00386  GradNormST:0.01487  AvgTextLen:21.0  AvgSpecLen:80.2  StepTime:0.55  LR:0.000100\n",
      "   | > Step:37/68  GlobalStep:106850  TotalLoss:0.00209  PostnetLoss:0.00102  DecoderLoss:0.00107  StopLoss:0.03922  GradNorm:0.00337  GradNormST:0.01239  AvgTextLen:29.3  AvgSpecLen:102.4  StepTime:0.48  LR:0.000100\n",
      "   | > Step:47/68  GlobalStep:106860  TotalLoss:0.00226  PostnetLoss:0.00110  DecoderLoss:0.00116  StopLoss:0.03167  GradNorm:0.00370  GradNormST:0.00690  AvgTextLen:36.7  AvgSpecLen:126.9  StepTime:0.74  LR:0.000100\n",
      "   | > Step:57/68  GlobalStep:106870  TotalLoss:0.00245  PostnetLoss:0.00119  DecoderLoss:0.00126  StopLoss:0.03311  GradNorm:0.00308  GradNormST:0.00672  AvgTextLen:47.1  AvgSpecLen:161.0  StepTime:0.84  LR:0.000100\n",
      "   | > Step:67/68  GlobalStep:106880  TotalLoss:0.00279  PostnetLoss:0.00135  DecoderLoss:0.00144  StopLoss:0.02748  GradNorm:0.00262  GradNormST:0.01420  AvgTextLen:80.6  AvgSpecLen:246.5  StepTime:1.23  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:106881  AvgTotalLoss:0.04251  AvgPostnetLoss:0.00097  AvgDecoderLoss:0.00103  AvgStopLoss:0.04051  EpochTime:42.18  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.08988   PostnetLoss: 0.00538   DecoderLoss:0.00570  StopLoss: 0.07880  \n",
      "   | > TotalLoss: 0.07150   PostnetLoss: 0.00841   DecoderLoss:0.00887  StopLoss: 0.05422  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00097   Validation Loss: 0.00773\n",
      "\n",
      " > Epoch 549/1000\n",
      "   | > Step:8/68  GlobalStep:106890  TotalLoss:0.00153  PostnetLoss:0.00075  DecoderLoss:0.00078  StopLoss:0.05088  GradNorm:0.00453  GradNormST:0.01276  AvgTextLen:8.3  AvgSpecLen:44.0  StepTime:0.32  LR:0.000100\n",
      "   | > Step:18/68  GlobalStep:106900  TotalLoss:0.00169  PostnetLoss:0.00082  DecoderLoss:0.00086  StopLoss:0.04806  GradNorm:0.00337  GradNormST:0.01096  AvgTextLen:15.2  AvgSpecLen:64.5  StepTime:0.42  LR:0.000100\n",
      "   | > Step:28/68  GlobalStep:106910  TotalLoss:0.00189  PostnetLoss:0.00092  DecoderLoss:0.00097  StopLoss:0.05126  GradNorm:0.00321  GradNormST:0.01239  AvgTextLen:22.1  AvgSpecLen:85.4  StepTime:0.45  LR:0.000100\n",
      "   | > Step:38/68  GlobalStep:106920  TotalLoss:0.00206  PostnetLoss:0.00100  DecoderLoss:0.00106  StopLoss:0.02760  GradNorm:0.00376  GradNormST:0.00755  AvgTextLen:29.4  AvgSpecLen:104.1  StepTime:0.64  LR:0.000100\n",
      "   | > Step:48/68  GlobalStep:106930  TotalLoss:0.00226  PostnetLoss:0.00110  DecoderLoss:0.00116  StopLoss:0.03523  GradNorm:0.00398  GradNormST:0.01090  AvgTextLen:37.5  AvgSpecLen:123.8  StepTime:0.71  LR:0.000100\n",
      "   | > Step:58/68  GlobalStep:106940  TotalLoss:0.00243  PostnetLoss:0.00118  DecoderLoss:0.00125  StopLoss:0.03701  GradNorm:0.00357  GradNormST:0.01261  AvgTextLen:47.8  AvgSpecLen:158.0  StepTime:0.93  LR:0.000100\n",
      "   | > Step:68/68  GlobalStep:106950  TotalLoss:0.00278  PostnetLoss:0.00134  DecoderLoss:0.00144  StopLoss:0.02386  GradNorm:0.00284  GradNormST:0.01037  AvgTextLen:103.0  AvgSpecLen:308.9  StepTime:1.86  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:106950  AvgTotalLoss:0.04336  AvgPostnetLoss:0.00098  AvgDecoderLoss:0.00103  AvgStopLoss:0.04135  EpochTime:43.85  AvgStepTime:0.64\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.08966   PostnetLoss: 0.00530   DecoderLoss:0.00561  StopLoss: 0.07875  \n",
      "   | > TotalLoss: 0.07387   PostnetLoss: 0.00847   DecoderLoss:0.00895  StopLoss: 0.05644  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00098   Validation Loss: 0.00779\n",
      "\n",
      " > Epoch 550/1000\n",
      "   | > Step:9/68  GlobalStep:106960  TotalLoss:0.00148  PostnetLoss:0.00072  DecoderLoss:0.00075  StopLoss:0.04981  GradNorm:0.00362  GradNormST:0.01896  AvgTextLen:9.0  AvgSpecLen:42.1  StepTime:0.41  LR:0.000100\n",
      "   | > Step:19/68  GlobalStep:106970  TotalLoss:0.00170  PostnetLoss:0.00083  DecoderLoss:0.00087  StopLoss:0.04153  GradNorm:0.00312  GradNormST:0.01335  AvgTextLen:15.5  AvgSpecLen:63.2  StepTime:0.42  LR:0.000100\n",
      "   | > Step:29/68  GlobalStep:106980  TotalLoss:0.00186  PostnetLoss:0.00091  DecoderLoss:0.00095  StopLoss:0.04806  GradNorm:0.00327  GradNormST:0.00989  AvgTextLen:21.8  AvgSpecLen:83.4  StepTime:0.55  LR:0.000100\n",
      "   | > Step:39/68  GlobalStep:106990  TotalLoss:0.00212  PostnetLoss:0.00103  DecoderLoss:0.00109  StopLoss:0.04073  GradNorm:0.00338  GradNormST:0.01236  AvgTextLen:30.2  AvgSpecLen:110.4  StepTime:0.64  LR:0.000100\n",
      "   | > Step:49/68  GlobalStep:107000  TotalLoss:0.00229  PostnetLoss:0.00111  DecoderLoss:0.00118  StopLoss:0.03758  GradNorm:0.00400  GradNormST:0.01167  AvgTextLen:38.2  AvgSpecLen:127.7  StepTime:0.59  LR:0.000100\n",
      " | | > Checkpoint saving : /vm/TTS/models/cartman_models/mozilla-no-loc-fattn-stopnet-sigmoid-loss_masking-August-15-2019_11+26AM-de6aeb9/checkpoint_107000.pth.tar\n",
      "warning: audio amplitude out of range, auto clipped.\n",
      "   | > Step:59/68  GlobalStep:107010  TotalLoss:0.00250  PostnetLoss:0.00122  DecoderLoss:0.00129  StopLoss:0.04220  GradNorm:0.00446  GradNormST:0.00908  AvgTextLen:50.5  AvgSpecLen:155.9  StepTime:0.85  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:107019  AvgTotalLoss:0.04345  AvgPostnetLoss:0.00098  AvgDecoderLoss:0.00103  AvgStopLoss:0.04144  EpochTime:42.17  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.08609   PostnetLoss: 0.00528   DecoderLoss:0.00560  StopLoss: 0.07521  \n",
      "   | > TotalLoss: 0.06770   PostnetLoss: 0.00817   DecoderLoss:0.00862  StopLoss: 0.05091  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00098   Validation Loss: 0.00764\n",
      "\n",
      " > Epoch 551/1000\n",
      "   | > Step:0/68  GlobalStep:107020  TotalLoss:0.00141  PostnetLoss:0.00068  DecoderLoss:0.00073  StopLoss:0.06759  GradNorm:0.00624  GradNormST:0.02063  AvgTextLen:3.0  AvgSpecLen:25.3  StepTime:0.36  LR:0.000100\n",
      "   | > Step:10/68  GlobalStep:107030  TotalLoss:0.00157  PostnetLoss:0.00076  DecoderLoss:0.00080  StopLoss:0.05244  GradNorm:0.00430  GradNormST:0.01774  AvgTextLen:9.8  AvgSpecLen:48.6  StepTime:0.51  LR:0.000100\n",
      "   | > Step:20/68  GlobalStep:107040  TotalLoss:0.00183  PostnetLoss:0.00089  DecoderLoss:0.00094  StopLoss:0.04234  GradNorm:0.00356  GradNormST:0.01735  AvgTextLen:16.4  AvgSpecLen:64.8  StepTime:0.50  LR:0.000100\n",
      "   | > Step:30/68  GlobalStep:107050  TotalLoss:0.00195  PostnetLoss:0.00095  DecoderLoss:0.00100  StopLoss:0.04088  GradNorm:0.00333  GradNormST:0.01285  AvgTextLen:22.9  AvgSpecLen:85.8  StepTime:0.58  LR:0.000100\n",
      "   | > Step:40/68  GlobalStep:107060  TotalLoss:0.00209  PostnetLoss:0.00102  DecoderLoss:0.00107  StopLoss:0.04037  GradNorm:0.00358  GradNormST:0.01017  AvgTextLen:30.9  AvgSpecLen:105.3  StepTime:0.49  LR:0.000100\n",
      "   | > Step:50/68  GlobalStep:107070  TotalLoss:0.00231  PostnetLoss:0.00112  DecoderLoss:0.00119  StopLoss:0.03783  GradNorm:0.00425  GradNormST:0.01277  AvgTextLen:38.8  AvgSpecLen:132.0  StepTime:0.81  LR:0.000100\n",
      "   | > Step:60/68  GlobalStep:107080  TotalLoss:0.00253  PostnetLoss:0.00123  DecoderLoss:0.00130  StopLoss:0.03249  GradNorm:0.00435  GradNormST:0.00830  AvgTextLen:52.0  AvgSpecLen:172.7  StepTime:0.93  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:107088  AvgTotalLoss:0.04300  AvgPostnetLoss:0.00098  AvgDecoderLoss:0.00103  AvgStopLoss:0.04099  EpochTime:42.90  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.09077   PostnetLoss: 0.00534   DecoderLoss:0.00567  StopLoss: 0.07976  \n",
      "   | > TotalLoss: 0.07138   PostnetLoss: 0.00818   DecoderLoss:0.00866  StopLoss: 0.05454  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00098   Validation Loss: 0.00768\n",
      "\n",
      " > Epoch 552/1000\n",
      "   | > Step:1/68  GlobalStep:107090  TotalLoss:0.00143  PostnetLoss:0.00070  DecoderLoss:0.00074  StopLoss:0.03982  GradNorm:0.00563  GradNormST:0.01673  AvgTextLen:3.7  AvgSpecLen:28.6  StepTime:0.24  LR:0.000100\n",
      "   | > Step:11/68  GlobalStep:107100  TotalLoss:0.00154  PostnetLoss:0.00075  DecoderLoss:0.00079  StopLoss:0.03468  GradNorm:0.00396  GradNormST:0.01493  AvgTextLen:10.3  AvgSpecLen:49.4  StepTime:0.40  LR:0.000100\n",
      "   | > Step:21/68  GlobalStep:107110  TotalLoss:0.00176  PostnetLoss:0.00086  DecoderLoss:0.00090  StopLoss:0.04539  GradNorm:0.00338  GradNormST:0.01512  AvgTextLen:16.6  AvgSpecLen:66.3  StepTime:0.42  LR:0.000100\n",
      "   | > Step:31/68  GlobalStep:107120  TotalLoss:0.00193  PostnetLoss:0.00094  DecoderLoss:0.00099  StopLoss:0.03175  GradNorm:0.00340  GradNormST:0.01125  AvgTextLen:24.1  AvgSpecLen:82.7  StepTime:0.46  LR:0.000100\n",
      "   | > Step:41/68  GlobalStep:107130  TotalLoss:0.00217  PostnetLoss:0.00106  DecoderLoss:0.00111  StopLoss:0.03960  GradNorm:0.00354  GradNormST:0.01313  AvgTextLen:32.3  AvgSpecLen:109.4  StepTime:0.52  LR:0.000100\n",
      "   | > Step:51/68  GlobalStep:107140  TotalLoss:0.00231  PostnetLoss:0.00112  DecoderLoss:0.00119  StopLoss:0.02837  GradNorm:0.00436  GradNormST:0.00708  AvgTextLen:39.2  AvgSpecLen:126.9  StepTime:0.66  LR:0.000100\n",
      "   | > Step:61/68  GlobalStep:107150  TotalLoss:0.00248  PostnetLoss:0.00120  DecoderLoss:0.00128  StopLoss:0.03332  GradNorm:0.00330  GradNormST:0.00957  AvgTextLen:54.6  AvgSpecLen:165.5  StepTime:0.90  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:107157  AvgTotalLoss:0.04427  AvgPostnetLoss:0.00098  AvgDecoderLoss:0.00103  AvgStopLoss:0.04226  EpochTime:42.00  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.09027   PostnetLoss: 0.00530   DecoderLoss:0.00562  StopLoss: 0.07935  \n",
      "   | > TotalLoss: 0.07394   PostnetLoss: 0.00838   DecoderLoss:0.00886  StopLoss: 0.05670  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00098   Validation Loss: 0.00778\n",
      "\n",
      " > Epoch 553/1000\n",
      "   | > Step:2/68  GlobalStep:107160  TotalLoss:0.00130  PostnetLoss:0.00063  DecoderLoss:0.00067  StopLoss:0.05279  GradNorm:0.00424  GradNormST:0.01560  AvgTextLen:4.0  AvgSpecLen:31.3  StepTime:0.32  LR:0.000100\n",
      "   | > Step:12/68  GlobalStep:107170  TotalLoss:0.00153  PostnetLoss:0.00074  DecoderLoss:0.00078  StopLoss:0.05715  GradNorm:0.00316  GradNormST:0.01786  AvgTextLen:11.0  AvgSpecLen:49.3  StepTime:0.42  LR:0.000100\n",
      "   | > Step:22/68  GlobalStep:107180  TotalLoss:0.00182  PostnetLoss:0.00089  DecoderLoss:0.00093  StopLoss:0.04377  GradNorm:0.00345  GradNormST:0.01146  AvgTextLen:18.0  AvgSpecLen:72.0  StepTime:0.40  LR:0.000100\n",
      "   | > Step:32/68  GlobalStep:107190  TotalLoss:0.00196  PostnetLoss:0.00095  DecoderLoss:0.00100  StopLoss:0.05253  GradNorm:0.00367  GradNormST:0.01045  AvgTextLen:24.4  AvgSpecLen:91.5  StepTime:0.56  LR:0.000100\n",
      "   | > Step:42/68  GlobalStep:107200  TotalLoss:0.00214  PostnetLoss:0.00104  DecoderLoss:0.00110  StopLoss:0.04369  GradNorm:0.00322  GradNormST:0.01487  AvgTextLen:33.1  AvgSpecLen:109.7  StepTime:0.53  LR:0.000100\n",
      "   | > Step:52/68  GlobalStep:107210  TotalLoss:0.00234  PostnetLoss:0.00114  DecoderLoss:0.00120  StopLoss:0.04484  GradNorm:0.00397  GradNormST:0.00989  AvgTextLen:42.1  AvgSpecLen:137.5  StepTime:0.80  LR:0.000100\n",
      "   | > Step:62/68  GlobalStep:107220  TotalLoss:0.00254  PostnetLoss:0.00123  DecoderLoss:0.00131  StopLoss:0.04025  GradNorm:0.00379  GradNormST:0.00912  AvgTextLen:54.0  AvgSpecLen:174.7  StepTime:0.74  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:107226  AvgTotalLoss:0.04322  AvgPostnetLoss:0.00098  AvgDecoderLoss:0.00103  AvgStopLoss:0.04121  EpochTime:42.28  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.09408   PostnetLoss: 0.00532   DecoderLoss:0.00565  StopLoss: 0.08311  \n",
      "   | > TotalLoss: 0.08126   PostnetLoss: 0.00853   DecoderLoss:0.00901  StopLoss: 0.06371  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00098   Validation Loss: 0.00793\n",
      "\n",
      " > Epoch 554/1000\n",
      "   | > Step:3/68  GlobalStep:107230  TotalLoss:0.00140  PostnetLoss:0.00068  DecoderLoss:0.00072  StopLoss:0.05666  GradNorm:0.00502  GradNormST:0.01800  AvgTextLen:4.7  AvgSpecLen:29.7  StepTime:0.31  LR:0.000100\n",
      "   | > Step:13/68  GlobalStep:107240  TotalLoss:0.00152  PostnetLoss:0.00074  DecoderLoss:0.00078  StopLoss:0.05811  GradNorm:0.00343  GradNormST:0.02494  AvgTextLen:11.5  AvgSpecLen:55.2  StepTime:0.41  LR:0.000100\n",
      "   | > Step:23/68  GlobalStep:107250  TotalLoss:0.00184  PostnetLoss:0.00090  DecoderLoss:0.00094  StopLoss:0.04957  GradNorm:0.00329  GradNormST:0.01249  AvgTextLen:18.3  AvgSpecLen:72.2  StepTime:0.43  LR:0.000100\n",
      "   | > Step:33/68  GlobalStep:107260  TotalLoss:0.00201  PostnetLoss:0.00098  DecoderLoss:0.00103  StopLoss:0.04023  GradNorm:0.00363  GradNormST:0.01174  AvgTextLen:25.9  AvgSpecLen:88.9  StepTime:0.56  LR:0.000100\n",
      "   | > Step:43/68  GlobalStep:107270  TotalLoss:0.00216  PostnetLoss:0.00105  DecoderLoss:0.00111  StopLoss:0.04720  GradNorm:0.00370  GradNormST:0.01091  AvgTextLen:33.4  AvgSpecLen:115.7  StepTime:0.61  LR:0.000100\n",
      "   | > Step:53/68  GlobalStep:107280  TotalLoss:0.00235  PostnetLoss:0.00114  DecoderLoss:0.00121  StopLoss:0.03745  GradNorm:0.00430  GradNormST:0.01195  AvgTextLen:42.4  AvgSpecLen:137.6  StepTime:0.81  LR:0.000100\n",
      "   | > Step:63/68  GlobalStep:107290  TotalLoss:0.00263  PostnetLoss:0.00127  DecoderLoss:0.00135  StopLoss:0.02690  GradNorm:0.00446  GradNormST:0.00902  AvgTextLen:58.6  AvgSpecLen:185.1  StepTime:0.90  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:107295  AvgTotalLoss:0.04451  AvgPostnetLoss:0.00098  AvgDecoderLoss:0.00104  AvgStopLoss:0.04250  EpochTime:43.10  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.09007   PostnetLoss: 0.00528   DecoderLoss:0.00561  StopLoss: 0.07918  \n",
      "   | > TotalLoss: 0.07331   PostnetLoss: 0.00822   DecoderLoss:0.00867  StopLoss: 0.05642  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00098   Validation Loss: 0.00772\n",
      "\n",
      " > Epoch 555/1000\n",
      "   | > Step:4/68  GlobalStep:107300  TotalLoss:0.00139  PostnetLoss:0.00067  DecoderLoss:0.00071  StopLoss:0.07358  GradNorm:0.00467  GradNormST:0.03436  AvgTextLen:4.8  AvgSpecLen:31.1  StepTime:0.20  LR:0.000100\n",
      "   | > Step:14/68  GlobalStep:107310  TotalLoss:0.00165  PostnetLoss:0.00080  DecoderLoss:0.00085  StopLoss:0.05522  GradNorm:0.00373  GradNormST:0.01441  AvgTextLen:12.2  AvgSpecLen:52.0  StepTime:0.34  LR:0.000100\n",
      "   | > Step:24/68  GlobalStep:107320  TotalLoss:0.00189  PostnetLoss:0.00092  DecoderLoss:0.00097  StopLoss:0.04538  GradNorm:0.00306  GradNormST:0.00954  AvgTextLen:19.2  AvgSpecLen:79.0  StepTime:0.48  LR:0.000100\n",
      "   | > Step:34/68  GlobalStep:107330  TotalLoss:0.00205  PostnetLoss:0.00100  DecoderLoss:0.00105  StopLoss:0.03230  GradNorm:0.00319  GradNormST:0.00751  AvgTextLen:26.2  AvgSpecLen:95.9  StepTime:0.62  LR:0.000100\n",
      "   | > Step:44/68  GlobalStep:107340  TotalLoss:0.00222  PostnetLoss:0.00108  DecoderLoss:0.00114  StopLoss:0.03842  GradNorm:0.00347  GradNormST:0.01402  AvgTextLen:33.8  AvgSpecLen:116.3  StepTime:0.61  LR:0.000100\n",
      "   | > Step:54/68  GlobalStep:107350  TotalLoss:0.00236  PostnetLoss:0.00115  DecoderLoss:0.00122  StopLoss:0.03758  GradNorm:0.00361  GradNormST:0.00833  AvgTextLen:42.2  AvgSpecLen:144.9  StepTime:0.79  LR:0.000100\n",
      "   | > Step:64/68  GlobalStep:107360  TotalLoss:0.00246  PostnetLoss:0.00119  DecoderLoss:0.00127  StopLoss:0.02761  GradNorm:0.00351  GradNormST:0.00771  AvgTextLen:60.6  AvgSpecLen:177.6  StepTime:0.80  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:107364  AvgTotalLoss:0.04431  AvgPostnetLoss:0.00098  AvgDecoderLoss:0.00104  AvgStopLoss:0.04229  EpochTime:42.43  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.09229   PostnetLoss: 0.00525   DecoderLoss:0.00557  StopLoss: 0.08148  \n",
      "   | > TotalLoss: 0.06757   PostnetLoss: 0.00807   DecoderLoss:0.00853  StopLoss: 0.05097  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00098   Validation Loss: 0.00753\n",
      "\n",
      " > Epoch 556/1000\n",
      "   | > Step:5/68  GlobalStep:107370  TotalLoss:0.00146  PostnetLoss:0.00071  DecoderLoss:0.00075  StopLoss:0.05420  GradNorm:0.00353  GradNormST:0.01933  AvgTextLen:5.9  AvgSpecLen:33.2  StepTime:0.32  LR:0.000100\n",
      "   | > Step:15/68  GlobalStep:107380  TotalLoss:0.00168  PostnetLoss:0.00082  DecoderLoss:0.00086  StopLoss:0.03844  GradNorm:0.00363  GradNormST:0.01632  AvgTextLen:12.8  AvgSpecLen:59.1  StepTime:0.52  LR:0.000100\n",
      "   | > Step:25/68  GlobalStep:107390  TotalLoss:0.00184  PostnetLoss:0.00089  DecoderLoss:0.00094  StopLoss:0.04722  GradNorm:0.00328  GradNormST:0.01120  AvgTextLen:19.8  AvgSpecLen:73.3  StepTime:0.36  LR:0.000100\n",
      "   | > Step:35/68  GlobalStep:107400  TotalLoss:0.00207  PostnetLoss:0.00101  DecoderLoss:0.00106  StopLoss:0.05694  GradNorm:0.00326  GradNormST:0.02701  AvgTextLen:27.1  AvgSpecLen:104.7  StepTime:0.75  LR:0.000100\n",
      "   | > Step:45/68  GlobalStep:107410  TotalLoss:0.00219  PostnetLoss:0.00107  DecoderLoss:0.00113  StopLoss:0.04851  GradNorm:0.00315  GradNormST:0.01270  AvgTextLen:34.4  AvgSpecLen:118.6  StepTime:0.72  LR:0.000100\n",
      "   | > Step:55/68  GlobalStep:107420  TotalLoss:0.00245  PostnetLoss:0.00119  DecoderLoss:0.00126  StopLoss:0.03325  GradNorm:0.00348  GradNormST:0.00813  AvgTextLen:43.9  AvgSpecLen:145.2  StepTime:0.86  LR:0.000100\n",
      "   | > Step:65/68  GlobalStep:107430  TotalLoss:0.00266  PostnetLoss:0.00129  DecoderLoss:0.00137  StopLoss:0.02574  GradNorm:0.00316  GradNormST:0.00598  AvgTextLen:64.2  AvgSpecLen:207.4  StepTime:1.20  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:107433  AvgTotalLoss:0.04602  AvgPostnetLoss:0.00099  AvgDecoderLoss:0.00104  AvgStopLoss:0.04399  EpochTime:43.23  AvgStepTime:0.63\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.09029   PostnetLoss: 0.00507   DecoderLoss:0.00539  StopLoss: 0.07984  \n",
      "   | > TotalLoss: 0.07805   PostnetLoss: 0.00843   DecoderLoss:0.00891  StopLoss: 0.06070  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00099   Validation Loss: 0.00769\n",
      "\n",
      " > Epoch 557/1000\n",
      "   | > Step:6/68  GlobalStep:107440  TotalLoss:0.00145  PostnetLoss:0.00071  DecoderLoss:0.00074  StopLoss:0.03735  GradNorm:0.00413  GradNormST:0.01728  AvgTextLen:6.0  AvgSpecLen:34.8  StepTime:0.29  LR:0.000100\n",
      "   | > Step:16/68  GlobalStep:107450  TotalLoss:0.00171  PostnetLoss:0.00084  DecoderLoss:0.00088  StopLoss:0.03538  GradNorm:0.00337  GradNormST:0.01135  AvgTextLen:13.8  AvgSpecLen:60.6  StepTime:0.37  LR:0.000100\n",
      "   | > Step:26/68  GlobalStep:107460  TotalLoss:0.00185  PostnetLoss:0.00090  DecoderLoss:0.00095  StopLoss:0.03290  GradNorm:0.00320  GradNormST:0.00881  AvgTextLen:20.3  AvgSpecLen:77.5  StepTime:0.48  LR:0.000100\n",
      "   | > Step:36/68  GlobalStep:107470  TotalLoss:0.00207  PostnetLoss:0.00100  DecoderLoss:0.00106  StopLoss:0.05144  GradNorm:0.00361  GradNormST:0.01038  AvgTextLen:28.5  AvgSpecLen:100.2  StepTime:0.47  LR:0.000100\n",
      "   | > Step:46/68  GlobalStep:107480  TotalLoss:0.00222  PostnetLoss:0.00108  DecoderLoss:0.00114  StopLoss:0.04503  GradNorm:0.00317  GradNormST:0.01432  AvgTextLen:35.4  AvgSpecLen:122.6  StepTime:0.69  LR:0.000100\n",
      "   | > Step:56/68  GlobalStep:107490  TotalLoss:0.00240  PostnetLoss:0.00116  DecoderLoss:0.00124  StopLoss:0.03811  GradNorm:0.00311  GradNormST:0.02258  AvgTextLen:45.8  AvgSpecLen:145.4  StepTime:0.96  LR:0.000100\n",
      "   | > Step:66/68  GlobalStep:107500  TotalLoss:0.00282  PostnetLoss:0.00136  DecoderLoss:0.00146  StopLoss:0.02495  GradNorm:0.00347  GradNormST:0.01227  AvgTextLen:71.2  AvgSpecLen:220.8  StepTime:1.04  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:107502  AvgTotalLoss:0.04556  AvgPostnetLoss:0.00099  AvgDecoderLoss:0.00104  AvgStopLoss:0.04352  EpochTime:42.44  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.08164   PostnetLoss: 0.00512   DecoderLoss:0.00543  StopLoss: 0.07110  \n",
      "   | > TotalLoss: 0.07447   PostnetLoss: 0.00820   DecoderLoss:0.00866  StopLoss: 0.05761  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00099   Validation Loss: 0.00760\n",
      "\n",
      " > Epoch 558/1000\n",
      "   | > Step:7/68  GlobalStep:107510  TotalLoss:0.00152  PostnetLoss:0.00074  DecoderLoss:0.00078  StopLoss:0.07046  GradNorm:0.00369  GradNormST:0.03573  AvgTextLen:7.4  AvgSpecLen:44.6  StepTime:0.43  LR:0.000100\n",
      "   | > Step:17/68  GlobalStep:107520  TotalLoss:0.00169  PostnetLoss:0.00082  DecoderLoss:0.00087  StopLoss:0.07778  GradNorm:0.00327  GradNormST:0.01944  AvgTextLen:14.7  AvgSpecLen:66.0  StepTime:0.42  LR:0.000100\n",
      "   | > Step:27/68  GlobalStep:107530  TotalLoss:0.00191  PostnetLoss:0.00093  DecoderLoss:0.00098  StopLoss:0.04622  GradNorm:0.00301  GradNormST:0.01222  AvgTextLen:21.0  AvgSpecLen:80.2  StepTime:0.68  LR:0.000100\n",
      "   | > Step:37/68  GlobalStep:107540  TotalLoss:0.00209  PostnetLoss:0.00102  DecoderLoss:0.00107  StopLoss:0.05218  GradNorm:0.00334  GradNormST:0.01684  AvgTextLen:29.3  AvgSpecLen:102.4  StepTime:0.61  LR:0.000100\n",
      "   | > Step:47/68  GlobalStep:107550  TotalLoss:0.00229  PostnetLoss:0.00111  DecoderLoss:0.00118  StopLoss:0.04625  GradNorm:0.00328  GradNormST:0.01473  AvgTextLen:36.7  AvgSpecLen:126.9  StepTime:0.74  LR:0.000100\n",
      "   | > Step:57/68  GlobalStep:107560  TotalLoss:0.00245  PostnetLoss:0.00119  DecoderLoss:0.00126  StopLoss:0.03957  GradNorm:0.00326  GradNormST:0.00800  AvgTextLen:47.1  AvgSpecLen:161.0  StepTime:0.89  LR:0.000100\n",
      "   | > Step:67/68  GlobalStep:107570  TotalLoss:0.00279  PostnetLoss:0.00135  DecoderLoss:0.00144  StopLoss:0.03549  GradNorm:0.00383  GradNormST:0.01758  AvgTextLen:80.6  AvgSpecLen:246.5  StepTime:1.23  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:107571  AvgTotalLoss:0.04951  AvgPostnetLoss:0.00099  AvgDecoderLoss:0.00104  AvgStopLoss:0.04748  EpochTime:43.00  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.08045   PostnetLoss: 0.00535   DecoderLoss:0.00566  StopLoss: 0.06943  \n",
      "   | > TotalLoss: 0.07321   PostnetLoss: 0.00841   DecoderLoss:0.00887  StopLoss: 0.05593  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00099   Validation Loss: 0.00777\n",
      "\n",
      " > Epoch 559/1000\n",
      "   | > Step:8/68  GlobalStep:107580  TotalLoss:0.00155  PostnetLoss:0.00076  DecoderLoss:0.00079  StopLoss:0.05642  GradNorm:0.00453  GradNormST:0.02206  AvgTextLen:8.3  AvgSpecLen:44.0  StepTime:0.33  LR:0.000100\n",
      "   | > Step:18/68  GlobalStep:107590  TotalLoss:0.00168  PostnetLoss:0.00082  DecoderLoss:0.00086  StopLoss:0.04615  GradNorm:0.00319  GradNormST:0.01142  AvgTextLen:15.2  AvgSpecLen:64.5  StepTime:0.40  LR:0.000100\n",
      "   | > Step:28/68  GlobalStep:107600  TotalLoss:0.00192  PostnetLoss:0.00094  DecoderLoss:0.00099  StopLoss:0.04642  GradNorm:0.00289  GradNormST:0.01077  AvgTextLen:22.1  AvgSpecLen:85.4  StepTime:0.48  LR:0.000100\n",
      "   | > Step:38/68  GlobalStep:107610  TotalLoss:0.00207  PostnetLoss:0.00101  DecoderLoss:0.00106  StopLoss:0.04121  GradNorm:0.00315  GradNormST:0.00991  AvgTextLen:29.4  AvgSpecLen:104.1  StepTime:0.61  LR:0.000100\n",
      "   | > Step:48/68  GlobalStep:107620  TotalLoss:0.00227  PostnetLoss:0.00111  DecoderLoss:0.00117  StopLoss:0.03869  GradNorm:0.00306  GradNormST:0.01065  AvgTextLen:37.5  AvgSpecLen:123.8  StepTime:0.66  LR:0.000100\n",
      "   | > Step:58/68  GlobalStep:107630  TotalLoss:0.00247  PostnetLoss:0.00120  DecoderLoss:0.00127  StopLoss:0.03853  GradNorm:0.00352  GradNormST:0.01169  AvgTextLen:47.8  AvgSpecLen:158.0  StepTime:0.86  LR:0.000100\n",
      "   | > Step:68/68  GlobalStep:107640  TotalLoss:0.00276  PostnetLoss:0.00133  DecoderLoss:0.00142  StopLoss:0.02284  GradNorm:0.00379  GradNormST:0.01045  AvgTextLen:103.0  AvgSpecLen:308.9  StepTime:1.87  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:107640  AvgTotalLoss:0.04332  AvgPostnetLoss:0.00099  AvgDecoderLoss:0.00104  AvgStopLoss:0.04129  EpochTime:42.78  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.08544   PostnetLoss: 0.00542   DecoderLoss:0.00572  StopLoss: 0.07431  \n",
      "   | > TotalLoss: 0.07801   PostnetLoss: 0.00850   DecoderLoss:0.00896  StopLoss: 0.06055  \n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00099   Validation Loss: 0.00780\n",
      "\n",
      " > Epoch 560/1000\n",
      "   | > Step:9/68  GlobalStep:107650  TotalLoss:0.00148  PostnetLoss:0.00072  DecoderLoss:0.00076  StopLoss:0.04746  GradNorm:0.00387  GradNormST:0.01372  AvgTextLen:9.0  AvgSpecLen:42.1  StepTime:0.39  LR:0.000100\n",
      "   | > Step:19/68  GlobalStep:107660  TotalLoss:0.00170  PostnetLoss:0.00083  DecoderLoss:0.00087  StopLoss:0.04975  GradNorm:0.00332  GradNormST:0.02357  AvgTextLen:15.5  AvgSpecLen:63.2  StepTime:0.43  LR:0.000100\n",
      "   | > Step:29/68  GlobalStep:107670  TotalLoss:0.00190  PostnetLoss:0.00093  DecoderLoss:0.00098  StopLoss:0.05333  GradNorm:0.00306  GradNormST:0.01279  AvgTextLen:21.8  AvgSpecLen:83.4  StepTime:0.45  LR:0.000100\n",
      "   | > Step:39/68  GlobalStep:107680  TotalLoss:0.00216  PostnetLoss:0.00105  DecoderLoss:0.00111  StopLoss:0.03436  GradNorm:0.00356  GradNormST:0.00816  AvgTextLen:30.2  AvgSpecLen:110.4  StepTime:0.75  LR:0.000100\n",
      "   | > Step:49/68  GlobalStep:107690  TotalLoss:0.00227  PostnetLoss:0.00110  DecoderLoss:0.00116  StopLoss:0.04480  GradNorm:0.00350  GradNormST:0.01182  AvgTextLen:38.2  AvgSpecLen:127.7  StepTime:0.65  LR:0.000100\n",
      "   | > Step:59/68  GlobalStep:107700  TotalLoss:0.00243  PostnetLoss:0.00118  DecoderLoss:0.00125  StopLoss:0.03079  GradNorm:0.00370  GradNormST:0.00867  AvgTextLen:50.5  AvgSpecLen:155.9  StepTime:0.83  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:107709  AvgTotalLoss:0.04445  AvgPostnetLoss:0.00099  AvgDecoderLoss:0.00104  AvgStopLoss:0.04242  EpochTime:41.98  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.09453   PostnetLoss: 0.00533   DecoderLoss:0.00564  StopLoss: 0.08357  \n",
      "   | > TotalLoss: 0.07880   PostnetLoss: 0.00861   DecoderLoss:0.00906  StopLoss: 0.06113  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00099   Validation Loss: 0.00794\n",
      "\n",
      " > Epoch 561/1000\n",
      "   | > Step:0/68  GlobalStep:107710  TotalLoss:0.00156  PostnetLoss:0.00076  DecoderLoss:0.00080  StopLoss:0.09426  GradNorm:0.00871  GradNormST:0.03489  AvgTextLen:3.0  AvgSpecLen:25.3  StepTime:0.51  LR:0.000100\n",
      "   | > Step:10/68  GlobalStep:107720  TotalLoss:0.00156  PostnetLoss:0.00076  DecoderLoss:0.00080  StopLoss:0.03668  GradNorm:0.00384  GradNormST:0.01193  AvgTextLen:9.8  AvgSpecLen:48.6  StepTime:0.62  LR:0.000100\n",
      "   | > Step:20/68  GlobalStep:107730  TotalLoss:0.00180  PostnetLoss:0.00088  DecoderLoss:0.00092  StopLoss:0.03127  GradNorm:0.00370  GradNormST:0.01194  AvgTextLen:16.4  AvgSpecLen:64.8  StepTime:0.55  LR:0.000100\n",
      "   | > Step:30/68  GlobalStep:107740  TotalLoss:0.00194  PostnetLoss:0.00095  DecoderLoss:0.00100  StopLoss:0.04027  GradNorm:0.00323  GradNormST:0.00852  AvgTextLen:22.9  AvgSpecLen:85.8  StepTime:0.57  LR:0.000100\n",
      "   | > Step:40/68  GlobalStep:107750  TotalLoss:0.00211  PostnetLoss:0.00103  DecoderLoss:0.00108  StopLoss:0.04013  GradNorm:0.00335  GradNormST:0.01053  AvgTextLen:30.9  AvgSpecLen:105.3  StepTime:0.49  LR:0.000100\n",
      "   | > Step:50/68  GlobalStep:107760  TotalLoss:0.00231  PostnetLoss:0.00112  DecoderLoss:0.00119  StopLoss:0.03378  GradNorm:0.00342  GradNormST:0.00963  AvgTextLen:38.8  AvgSpecLen:132.0  StepTime:0.78  LR:0.000100\n",
      "   | > Step:60/68  GlobalStep:107770  TotalLoss:0.00255  PostnetLoss:0.00124  DecoderLoss:0.00131  StopLoss:0.03396  GradNorm:0.00486  GradNormST:0.00718  AvgTextLen:52.0  AvgSpecLen:172.7  StepTime:0.96  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:107778  AvgTotalLoss:0.04468  AvgPostnetLoss:0.00100  AvgDecoderLoss:0.00105  AvgStopLoss:0.04263  EpochTime:42.68  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.09410   PostnetLoss: 0.00531   DecoderLoss:0.00561  StopLoss: 0.08318  \n",
      "   | > TotalLoss: 0.07694   PostnetLoss: 0.00839   DecoderLoss:0.00886  StopLoss: 0.05968  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00100   Validation Loss: 0.00782\n",
      "\n",
      " > Epoch 562/1000\n",
      "   | > Step:1/68  GlobalStep:107780  TotalLoss:0.00153  PostnetLoss:0.00074  DecoderLoss:0.00078  StopLoss:0.03612  GradNorm:0.00745  GradNormST:0.01767  AvgTextLen:3.7  AvgSpecLen:28.6  StepTime:0.34  LR:0.000100\n",
      "   | > Step:11/68  GlobalStep:107790  TotalLoss:0.00154  PostnetLoss:0.00075  DecoderLoss:0.00079  StopLoss:0.03366  GradNorm:0.00371  GradNormST:0.01329  AvgTextLen:10.3  AvgSpecLen:49.4  StepTime:0.38  LR:0.000100\n",
      "   | > Step:21/68  GlobalStep:107800  TotalLoss:0.00177  PostnetLoss:0.00087  DecoderLoss:0.00091  StopLoss:0.04321  GradNorm:0.00338  GradNormST:0.01750  AvgTextLen:16.6  AvgSpecLen:66.3  StepTime:0.42  LR:0.000100\n",
      "   | > Step:31/68  GlobalStep:107810  TotalLoss:0.00196  PostnetLoss:0.00095  DecoderLoss:0.00100  StopLoss:0.03213  GradNorm:0.00322  GradNormST:0.00883  AvgTextLen:24.1  AvgSpecLen:82.7  StepTime:0.51  LR:0.000100\n",
      "   | > Step:41/68  GlobalStep:107820  TotalLoss:0.00216  PostnetLoss:0.00105  DecoderLoss:0.00111  StopLoss:0.03810  GradNorm:0.00297  GradNormST:0.01433  AvgTextLen:32.3  AvgSpecLen:109.4  StepTime:0.53  LR:0.000100\n",
      "   | > Step:51/68  GlobalStep:107830  TotalLoss:0.00228  PostnetLoss:0.00111  DecoderLoss:0.00117  StopLoss:0.05280  GradNorm:0.00325  GradNormST:0.02350  AvgTextLen:39.2  AvgSpecLen:126.9  StepTime:0.73  LR:0.000100\n",
      "   | > Step:61/68  GlobalStep:107840  TotalLoss:0.00251  PostnetLoss:0.00122  DecoderLoss:0.00129  StopLoss:0.03493  GradNorm:0.00441  GradNormST:0.01098  AvgTextLen:54.6  AvgSpecLen:165.5  StepTime:0.90  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:107847  AvgTotalLoss:0.04605  AvgPostnetLoss:0.00099  AvgDecoderLoss:0.00104  AvgStopLoss:0.04402  EpochTime:41.87  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.09433   PostnetLoss: 0.00541   DecoderLoss:0.00573  StopLoss: 0.08319  \n",
      "   | > TotalLoss: 0.08412   PostnetLoss: 0.00914   DecoderLoss:0.00961  StopLoss: 0.06537  \n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00099   Validation Loss: 0.00834\n",
      "\n",
      " > Epoch 563/1000\n",
      "   | > Step:2/68  GlobalStep:107850  TotalLoss:0.00137  PostnetLoss:0.00067  DecoderLoss:0.00070  StopLoss:0.04829  GradNorm:0.00574  GradNormST:0.02259  AvgTextLen:4.0  AvgSpecLen:31.3  StepTime:0.34  LR:0.000100\n",
      "   | > Step:12/68  GlobalStep:107860  TotalLoss:0.00159  PostnetLoss:0.00077  DecoderLoss:0.00081  StopLoss:0.04370  GradNorm:0.00428  GradNormST:0.01394  AvgTextLen:11.0  AvgSpecLen:49.3  StepTime:0.43  LR:0.000100\n",
      "   | > Step:22/68  GlobalStep:107870  TotalLoss:0.00180  PostnetLoss:0.00088  DecoderLoss:0.00093  StopLoss:0.05016  GradNorm:0.00334  GradNormST:0.01155  AvgTextLen:18.0  AvgSpecLen:72.0  StepTime:0.48  LR:0.000100\n",
      "   | > Step:32/68  GlobalStep:107880  TotalLoss:0.00194  PostnetLoss:0.00094  DecoderLoss:0.00099  StopLoss:0.05284  GradNorm:0.00285  GradNormST:0.01264  AvgTextLen:24.4  AvgSpecLen:91.5  StepTime:0.46  LR:0.000100\n",
      "   | > Step:42/68  GlobalStep:107890  TotalLoss:0.00214  PostnetLoss:0.00104  DecoderLoss:0.00110  StopLoss:0.04178  GradNorm:0.00308  GradNormST:0.01661  AvgTextLen:33.1  AvgSpecLen:109.7  StepTime:0.60  LR:0.000100\n",
      "   | > Step:52/68  GlobalStep:107900  TotalLoss:0.00234  PostnetLoss:0.00114  DecoderLoss:0.00120  StopLoss:0.04304  GradNorm:0.00388  GradNormST:0.01233  AvgTextLen:42.1  AvgSpecLen:137.5  StepTime:0.71  LR:0.000100\n",
      "   | > Step:62/68  GlobalStep:107910  TotalLoss:0.00261  PostnetLoss:0.00127  DecoderLoss:0.00134  StopLoss:0.03865  GradNorm:0.00540  GradNormST:0.01345  AvgTextLen:54.0  AvgSpecLen:174.7  StepTime:0.75  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:107916  AvgTotalLoss:0.04612  AvgPostnetLoss:0.00100  AvgDecoderLoss:0.00105  AvgStopLoss:0.04407  EpochTime:42.18  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.09058   PostnetLoss: 0.00549   DecoderLoss:0.00580  StopLoss: 0.07929  \n",
      "   | > TotalLoss: 0.08081   PostnetLoss: 0.00881   DecoderLoss:0.00927  StopLoss: 0.06272  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00100   Validation Loss: 0.00811\n",
      "\n",
      " > Epoch 564/1000\n",
      "   | > Step:3/68  GlobalStep:107920  TotalLoss:0.00141  PostnetLoss:0.00069  DecoderLoss:0.00072  StopLoss:0.06890  GradNorm:0.00473  GradNormST:0.02783  AvgTextLen:4.7  AvgSpecLen:29.7  StepTime:0.30  LR:0.000100\n",
      "   | > Step:13/68  GlobalStep:107930  TotalLoss:0.00160  PostnetLoss:0.00078  DecoderLoss:0.00082  StopLoss:0.09071  GradNorm:0.00430  GradNormST:0.02882  AvgTextLen:11.5  AvgSpecLen:55.2  StepTime:0.42  LR:0.000100\n",
      "   | > Step:23/68  GlobalStep:107940  TotalLoss:0.00188  PostnetLoss:0.00092  DecoderLoss:0.00096  StopLoss:0.04800  GradNorm:0.00371  GradNormST:0.01153  AvgTextLen:18.3  AvgSpecLen:72.2  StepTime:0.45  LR:0.000100\n",
      "   | > Step:33/68  GlobalStep:107950  TotalLoss:0.00204  PostnetLoss:0.00099  DecoderLoss:0.00105  StopLoss:0.03892  GradNorm:0.00312  GradNormST:0.00963  AvgTextLen:25.9  AvgSpecLen:88.9  StepTime:0.44  LR:0.000100\n",
      "   | > Step:43/68  GlobalStep:107960  TotalLoss:0.00214  PostnetLoss:0.00104  DecoderLoss:0.00110  StopLoss:0.05835  GradNorm:0.00281  GradNormST:0.01448  AvgTextLen:33.4  AvgSpecLen:115.7  StepTime:0.61  LR:0.000100\n",
      "   | > Step:53/68  GlobalStep:107970  TotalLoss:0.00238  PostnetLoss:0.00115  DecoderLoss:0.00123  StopLoss:0.05467  GradNorm:0.00514  GradNormST:0.02120  AvgTextLen:42.4  AvgSpecLen:137.6  StepTime:0.76  LR:0.000100\n",
      "   | > Step:63/68  GlobalStep:107980  TotalLoss:0.00265  PostnetLoss:0.00129  DecoderLoss:0.00137  StopLoss:0.02964  GradNorm:0.00369  GradNormST:0.00909  AvgTextLen:58.6  AvgSpecLen:185.1  StepTime:0.94  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:107985  AvgTotalLoss:0.04692  AvgPostnetLoss:0.00099  AvgDecoderLoss:0.00105  AvgStopLoss:0.04488  EpochTime:42.45  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.08642   PostnetLoss: 0.00533   DecoderLoss:0.00564  StopLoss: 0.07546  \n",
      "   | > TotalLoss: 0.08689   PostnetLoss: 0.00973   DecoderLoss:0.01018  StopLoss: 0.06699  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00099   Validation Loss: 0.00879\n",
      "\n",
      " > Epoch 565/1000\n",
      "   | > Step:4/68  GlobalStep:107990  TotalLoss:0.00145  PostnetLoss:0.00071  DecoderLoss:0.00075  StopLoss:0.06511  GradNorm:0.00465  GradNormST:0.02314  AvgTextLen:4.8  AvgSpecLen:31.1  StepTime:0.26  LR:0.000100\n",
      "   | > Step:14/68  GlobalStep:108000  TotalLoss:0.00162  PostnetLoss:0.00079  DecoderLoss:0.00083  StopLoss:0.04389  GradNorm:0.00343  GradNormST:0.01184  AvgTextLen:12.2  AvgSpecLen:52.0  StepTime:0.26  LR:0.000100\n",
      " | | > Checkpoint saving : /vm/TTS/models/cartman_models/mozilla-no-loc-fattn-stopnet-sigmoid-loss_masking-August-15-2019_11+26AM-de6aeb9/checkpoint_108000.pth.tar\n",
      "   | > Step:24/68  GlobalStep:108010  TotalLoss:0.00194  PostnetLoss:0.00094  DecoderLoss:0.00099  StopLoss:0.04173  GradNorm:0.00316  GradNormST:0.01088  AvgTextLen:19.2  AvgSpecLen:79.0  StepTime:0.43  LR:0.000100\n",
      "   | > Step:34/68  GlobalStep:108020  TotalLoss:0.00202  PostnetLoss:0.00098  DecoderLoss:0.00104  StopLoss:0.03150  GradNorm:0.00284  GradNormST:0.00853  AvgTextLen:26.2  AvgSpecLen:95.9  StepTime:0.61  LR:0.000100\n",
      "   | > Step:44/68  GlobalStep:108030  TotalLoss:0.00219  PostnetLoss:0.00107  DecoderLoss:0.00113  StopLoss:0.03383  GradNorm:0.00304  GradNormST:0.00902  AvgTextLen:33.8  AvgSpecLen:116.3  StepTime:0.69  LR:0.000100\n",
      "   | > Step:54/68  GlobalStep:108040  TotalLoss:0.00238  PostnetLoss:0.00115  DecoderLoss:0.00122  StopLoss:0.04276  GradNorm:0.00463  GradNormST:0.01133  AvgTextLen:42.2  AvgSpecLen:144.9  StepTime:0.81  LR:0.000100\n",
      "   | > Step:64/68  GlobalStep:108050  TotalLoss:0.00258  PostnetLoss:0.00125  DecoderLoss:0.00133  StopLoss:0.03189  GradNorm:0.00670  GradNormST:0.00854  AvgTextLen:60.6  AvgSpecLen:177.6  StepTime:0.81  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:108054  AvgTotalLoss:0.04518  AvgPostnetLoss:0.00099  AvgDecoderLoss:0.00105  AvgStopLoss:0.04314  EpochTime:42.98  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.09119   PostnetLoss: 0.00548   DecoderLoss:0.00579  StopLoss: 0.07993  \n",
      "   | > TotalLoss: 0.07842   PostnetLoss: 0.00875   DecoderLoss:0.00921  StopLoss: 0.06046  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00099   Validation Loss: 0.00804\n",
      "\n",
      " > Epoch 566/1000\n",
      "   | > Step:5/68  GlobalStep:108060  TotalLoss:0.00150  PostnetLoss:0.00073  DecoderLoss:0.00077  StopLoss:0.04623  GradNorm:0.00651  GradNormST:0.02104  AvgTextLen:5.9  AvgSpecLen:33.2  StepTime:0.32  LR:0.000100\n",
      "   | > Step:15/68  GlobalStep:108070  TotalLoss:0.00170  PostnetLoss:0.00083  DecoderLoss:0.00087  StopLoss:0.06289  GradNorm:0.00372  GradNormST:0.03004  AvgTextLen:12.8  AvgSpecLen:59.1  StepTime:0.43  LR:0.000100\n",
      "   | > Step:25/68  GlobalStep:108080  TotalLoss:0.00184  PostnetLoss:0.00090  DecoderLoss:0.00094  StopLoss:0.05231  GradNorm:0.00315  GradNormST:0.01254  AvgTextLen:19.8  AvgSpecLen:73.3  StepTime:0.45  LR:0.000100\n",
      "   | > Step:35/68  GlobalStep:108090  TotalLoss:0.00206  PostnetLoss:0.00100  DecoderLoss:0.00106  StopLoss:0.06471  GradNorm:0.00338  GradNormST:0.02633  AvgTextLen:27.1  AvgSpecLen:104.7  StepTime:0.66  LR:0.000100\n",
      "   | > Step:45/68  GlobalStep:108100  TotalLoss:0.00217  PostnetLoss:0.00105  DecoderLoss:0.00112  StopLoss:0.04955  GradNorm:0.00340  GradNormST:0.01775  AvgTextLen:34.4  AvgSpecLen:118.6  StepTime:0.69  LR:0.000100\n",
      "   | > Step:55/68  GlobalStep:108110  TotalLoss:0.00242  PostnetLoss:0.00118  DecoderLoss:0.00125  StopLoss:0.03755  GradNorm:0.00437  GradNormST:0.00928  AvgTextLen:43.9  AvgSpecLen:145.2  StepTime:0.81  LR:0.000100\n",
      "   | > Step:65/68  GlobalStep:108120  TotalLoss:0.00275  PostnetLoss:0.00133  DecoderLoss:0.00142  StopLoss:0.02637  GradNorm:0.00497  GradNormST:0.00776  AvgTextLen:64.2  AvgSpecLen:207.4  StepTime:1.18  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:108123  AvgTotalLoss:0.04447  AvgPostnetLoss:0.00099  AvgDecoderLoss:0.00105  AvgStopLoss:0.04243  EpochTime:41.37  AvgStepTime:0.60\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.10028   PostnetLoss: 0.00536   DecoderLoss:0.00567  StopLoss: 0.08925  \n",
      "   | > TotalLoss: 0.09205   PostnetLoss: 0.00997   DecoderLoss:0.01042  StopLoss: 0.07166  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00099   Validation Loss: 0.00909\n",
      "\n",
      " > Epoch 567/1000\n",
      "   | > Step:6/68  GlobalStep:108130  TotalLoss:0.00143  PostnetLoss:0.00070  DecoderLoss:0.00073  StopLoss:0.04445  GradNorm:0.00418  GradNormST:0.02029  AvgTextLen:6.0  AvgSpecLen:34.8  StepTime:0.30  LR:0.000100\n",
      "   | > Step:16/68  GlobalStep:108140  TotalLoss:0.00176  PostnetLoss:0.00086  DecoderLoss:0.00090  StopLoss:0.03391  GradNorm:0.00422  GradNormST:0.01184  AvgTextLen:13.8  AvgSpecLen:60.6  StepTime:0.29  LR:0.000100\n",
      "   | > Step:26/68  GlobalStep:108150  TotalLoss:0.00189  PostnetLoss:0.00092  DecoderLoss:0.00097  StopLoss:0.03940  GradNorm:0.00335  GradNormST:0.00918  AvgTextLen:20.3  AvgSpecLen:77.5  StepTime:0.45  LR:0.000100\n",
      "   | > Step:36/68  GlobalStep:108160  TotalLoss:0.00204  PostnetLoss:0.00099  DecoderLoss:0.00105  StopLoss:0.04627  GradNorm:0.00300  GradNormST:0.00837  AvgTextLen:28.5  AvgSpecLen:100.2  StepTime:0.58  LR:0.000100\n",
      "   | > Step:46/68  GlobalStep:108170  TotalLoss:0.00219  PostnetLoss:0.00106  DecoderLoss:0.00113  StopLoss:0.04645  GradNorm:0.00339  GradNormST:0.01269  AvgTextLen:35.4  AvgSpecLen:122.6  StepTime:0.67  LR:0.000100\n",
      "   | > Step:56/68  GlobalStep:108180  TotalLoss:0.00240  PostnetLoss:0.00117  DecoderLoss:0.00123  StopLoss:0.03601  GradNorm:0.00421  GradNormST:0.01796  AvgTextLen:45.8  AvgSpecLen:145.4  StepTime:0.97  LR:0.000100\n",
      "   | > Step:66/68  GlobalStep:108190  TotalLoss:0.00296  PostnetLoss:0.00144  DecoderLoss:0.00152  StopLoss:0.03294  GradNorm:0.00819  GradNormST:0.01639  AvgTextLen:71.2  AvgSpecLen:220.8  StepTime:1.03  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:108192  AvgTotalLoss:0.04609  AvgPostnetLoss:0.00099  AvgDecoderLoss:0.00104  AvgStopLoss:0.04406  EpochTime:42.12  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.09156   PostnetLoss: 0.00524   DecoderLoss:0.00554  StopLoss: 0.08078  \n",
      "   | > TotalLoss: 0.08101   PostnetLoss: 0.00904   DecoderLoss:0.00951  StopLoss: 0.06246  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00099   Validation Loss: 0.00824\n",
      "\n",
      " > Epoch 568/1000\n",
      "   | > Step:7/68  GlobalStep:108200  TotalLoss:0.00154  PostnetLoss:0.00075  DecoderLoss:0.00079  StopLoss:0.04305  GradNorm:0.00513  GradNormST:0.01867  AvgTextLen:7.4  AvgSpecLen:44.6  StepTime:0.38  LR:0.000100\n",
      "   | > Step:17/68  GlobalStep:108210  TotalLoss:0.00169  PostnetLoss:0.00083  DecoderLoss:0.00086  StopLoss:0.05096  GradNorm:0.00371  GradNormST:0.01359  AvgTextLen:14.7  AvgSpecLen:66.0  StepTime:0.42  LR:0.000100\n",
      "   | > Step:27/68  GlobalStep:108220  TotalLoss:0.00193  PostnetLoss:0.00094  DecoderLoss:0.00099  StopLoss:0.04546  GradNorm:0.00307  GradNormST:0.01154  AvgTextLen:21.0  AvgSpecLen:80.2  StepTime:0.53  LR:0.000100\n",
      "   | > Step:37/68  GlobalStep:108230  TotalLoss:0.00206  PostnetLoss:0.00100  DecoderLoss:0.00106  StopLoss:0.05024  GradNorm:0.00291  GradNormST:0.01540  AvgTextLen:29.3  AvgSpecLen:102.4  StepTime:0.62  LR:0.000100\n",
      "   | > Step:47/68  GlobalStep:108240  TotalLoss:0.00226  PostnetLoss:0.00110  DecoderLoss:0.00116  StopLoss:0.03558  GradNorm:0.00328  GradNormST:0.01312  AvgTextLen:36.7  AvgSpecLen:126.9  StepTime:0.63  LR:0.000100\n",
      "   | > Step:57/68  GlobalStep:108250  TotalLoss:0.00254  PostnetLoss:0.00124  DecoderLoss:0.00131  StopLoss:0.03389  GradNorm:0.00757  GradNormST:0.00822  AvgTextLen:47.1  AvgSpecLen:161.0  StepTime:0.90  LR:0.000100\n",
      "   | > Step:67/68  GlobalStep:108260  TotalLoss:0.00289  PostnetLoss:0.00140  DecoderLoss:0.00149  StopLoss:0.02794  GradNorm:0.00431  GradNormST:0.00953  AvgTextLen:80.6  AvgSpecLen:246.5  StepTime:1.27  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:108261  AvgTotalLoss:0.04607  AvgPostnetLoss:0.00099  AvgDecoderLoss:0.00104  AvgStopLoss:0.04404  EpochTime:42.11  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.09517   PostnetLoss: 0.00522   DecoderLoss:0.00552  StopLoss: 0.08443  \n",
      "   | > TotalLoss: 0.09186   PostnetLoss: 0.01002   DecoderLoss:0.01046  StopLoss: 0.07138  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00099   Validation Loss: 0.00902\n",
      "\n",
      " > Epoch 569/1000\n",
      "   | > Step:8/68  GlobalStep:108270  TotalLoss:0.00158  PostnetLoss:0.00077  DecoderLoss:0.00081  StopLoss:0.07167  GradNorm:0.00426  GradNormST:0.02539  AvgTextLen:8.3  AvgSpecLen:44.0  StepTime:0.24  LR:0.000100\n",
      "   | > Step:18/68  GlobalStep:108280  TotalLoss:0.00172  PostnetLoss:0.00084  DecoderLoss:0.00088  StopLoss:0.06001  GradNorm:0.00426  GradNormST:0.02011  AvgTextLen:15.2  AvgSpecLen:64.5  StepTime:0.35  LR:0.000100\n",
      "   | > Step:28/68  GlobalStep:108290  TotalLoss:0.00193  PostnetLoss:0.00094  DecoderLoss:0.00099  StopLoss:0.04267  GradNorm:0.00363  GradNormST:0.01089  AvgTextLen:22.1  AvgSpecLen:85.4  StepTime:0.44  LR:0.000100\n",
      "   | > Step:38/68  GlobalStep:108300  TotalLoss:0.00203  PostnetLoss:0.00099  DecoderLoss:0.00104  StopLoss:0.04461  GradNorm:0.00311  GradNormST:0.01331  AvgTextLen:29.4  AvgSpecLen:104.1  StepTime:0.62  LR:0.000100\n",
      "   | > Step:48/68  GlobalStep:108310  TotalLoss:0.00222  PostnetLoss:0.00108  DecoderLoss:0.00114  StopLoss:0.03318  GradNorm:0.00284  GradNormST:0.00698  AvgTextLen:37.5  AvgSpecLen:123.8  StepTime:0.65  LR:0.000100\n",
      "   | > Step:58/68  GlobalStep:108320  TotalLoss:0.00249  PostnetLoss:0.00121  DecoderLoss:0.00128  StopLoss:0.03537  GradNorm:0.00492  GradNormST:0.00911  AvgTextLen:47.8  AvgSpecLen:158.0  StepTime:0.91  LR:0.000100\n",
      "   | > Step:68/68  GlobalStep:108330  TotalLoss:0.00284  PostnetLoss:0.00137  DecoderLoss:0.00147  StopLoss:0.03073  GradNorm:0.00574  GradNormST:0.01344  AvgTextLen:103.0  AvgSpecLen:308.9  StepTime:1.88  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:108330  AvgTotalLoss:0.04525  AvgPostnetLoss:0.00099  AvgDecoderLoss:0.00104  AvgStopLoss:0.04322  EpochTime:41.85  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.08824   PostnetLoss: 0.00532   DecoderLoss:0.00562  StopLoss: 0.07730  \n",
      "   | > TotalLoss: 0.08812   PostnetLoss: 0.00936   DecoderLoss:0.00984  StopLoss: 0.06892  \n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00099   Validation Loss: 0.00845\n",
      "\n",
      " > Epoch 570/1000\n",
      "   | > Step:9/68  GlobalStep:108340  TotalLoss:0.00150  PostnetLoss:0.00073  DecoderLoss:0.00077  StopLoss:0.04947  GradNorm:0.00416  GradNormST:0.01704  AvgTextLen:9.0  AvgSpecLen:42.1  StepTime:0.43  LR:0.000100\n",
      "   | > Step:19/68  GlobalStep:108350  TotalLoss:0.00174  PostnetLoss:0.00085  DecoderLoss:0.00089  StopLoss:0.05463  GradNorm:0.00352  GradNormST:0.01381  AvgTextLen:15.5  AvgSpecLen:63.2  StepTime:0.33  LR:0.000100\n",
      "   | > Step:29/68  GlobalStep:108360  TotalLoss:0.00187  PostnetLoss:0.00091  DecoderLoss:0.00096  StopLoss:0.06179  GradNorm:0.00299  GradNormST:0.01711  AvgTextLen:21.8  AvgSpecLen:83.4  StepTime:0.46  LR:0.000100\n",
      "   | > Step:39/68  GlobalStep:108370  TotalLoss:0.00210  PostnetLoss:0.00102  DecoderLoss:0.00108  StopLoss:0.03554  GradNorm:0.00338  GradNormST:0.01015  AvgTextLen:30.2  AvgSpecLen:110.4  StepTime:0.52  LR:0.000100\n",
      "   | > Step:49/68  GlobalStep:108380  TotalLoss:0.00226  PostnetLoss:0.00110  DecoderLoss:0.00116  StopLoss:0.04425  GradNorm:0.00360  GradNormST:0.00977  AvgTextLen:38.2  AvgSpecLen:127.7  StepTime:0.70  LR:0.000100\n",
      "   | > Step:59/68  GlobalStep:108390  TotalLoss:0.00245  PostnetLoss:0.00118  DecoderLoss:0.00126  StopLoss:0.03387  GradNorm:0.00448  GradNormST:0.00771  AvgTextLen:50.5  AvgSpecLen:155.9  StepTime:0.85  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:108399  AvgTotalLoss:0.04638  AvgPostnetLoss:0.00098  AvgDecoderLoss:0.00104  AvgStopLoss:0.04436  EpochTime:42.52  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.09406   PostnetLoss: 0.00530   DecoderLoss:0.00561  StopLoss: 0.08315  \n",
      "   | > TotalLoss: 0.09729   PostnetLoss: 0.01020   DecoderLoss:0.01065  StopLoss: 0.07643  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00098   Validation Loss: 0.00910\n",
      "\n",
      " > Epoch 571/1000\n",
      "   | > Step:0/68  GlobalStep:108400  TotalLoss:0.00149  PostnetLoss:0.00071  DecoderLoss:0.00077  StopLoss:0.06283  GradNorm:0.00587  GradNormST:0.01977  AvgTextLen:3.0  AvgSpecLen:25.3  StepTime:0.29  LR:0.000100\n",
      "   | > Step:10/68  GlobalStep:108410  TotalLoss:0.00158  PostnetLoss:0.00077  DecoderLoss:0.00081  StopLoss:0.03834  GradNorm:0.00412  GradNormST:0.00984  AvgTextLen:9.8  AvgSpecLen:48.6  StepTime:0.55  LR:0.000100\n",
      "   | > Step:20/68  GlobalStep:108420  TotalLoss:0.00184  PostnetLoss:0.00090  DecoderLoss:0.00094  StopLoss:0.03313  GradNorm:0.00411  GradNormST:0.01412  AvgTextLen:16.4  AvgSpecLen:64.8  StepTime:0.49  LR:0.000100\n",
      "   | > Step:30/68  GlobalStep:108430  TotalLoss:0.00193  PostnetLoss:0.00094  DecoderLoss:0.00099  StopLoss:0.04052  GradNorm:0.00337  GradNormST:0.00795  AvgTextLen:22.9  AvgSpecLen:85.8  StepTime:0.57  LR:0.000100\n",
      "   | > Step:40/68  GlobalStep:108440  TotalLoss:0.00210  PostnetLoss:0.00102  DecoderLoss:0.00108  StopLoss:0.04609  GradNorm:0.00358  GradNormST:0.01142  AvgTextLen:30.9  AvgSpecLen:105.3  StepTime:0.64  LR:0.000100\n",
      "   | > Step:50/68  GlobalStep:108450  TotalLoss:0.00227  PostnetLoss:0.00110  DecoderLoss:0.00117  StopLoss:0.03586  GradNorm:0.00344  GradNormST:0.00890  AvgTextLen:38.8  AvgSpecLen:132.0  StepTime:0.80  LR:0.000100\n",
      "   | > Step:60/68  GlobalStep:108460  TotalLoss:0.00254  PostnetLoss:0.00123  DecoderLoss:0.00131  StopLoss:0.03019  GradNorm:0.00519  GradNormST:0.00789  AvgTextLen:52.0  AvgSpecLen:172.7  StepTime:0.96  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:108468  AvgTotalLoss:0.04676  AvgPostnetLoss:0.00098  AvgDecoderLoss:0.00104  AvgStopLoss:0.04475  EpochTime:42.77  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.09300   PostnetLoss: 0.00528   DecoderLoss:0.00559  StopLoss: 0.08213  \n",
      "   | > TotalLoss: 0.08805   PostnetLoss: 0.00963   DecoderLoss:0.01008  StopLoss: 0.06834  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00098   Validation Loss: 0.00874\n",
      "\n",
      " > Epoch 572/1000\n",
      "   | > Step:1/68  GlobalStep:108470  TotalLoss:0.00143  PostnetLoss:0.00070  DecoderLoss:0.00073  StopLoss:0.04563  GradNorm:0.00466  GradNormST:0.01418  AvgTextLen:3.7  AvgSpecLen:28.6  StepTime:0.25  LR:0.000100\n",
      "   | > Step:11/68  GlobalStep:108480  TotalLoss:0.00153  PostnetLoss:0.00075  DecoderLoss:0.00078  StopLoss:0.04938  GradNorm:0.00344  GradNormST:0.01341  AvgTextLen:10.3  AvgSpecLen:49.4  StepTime:0.30  LR:0.000100\n",
      "   | > Step:21/68  GlobalStep:108490  TotalLoss:0.00174  PostnetLoss:0.00085  DecoderLoss:0.00089  StopLoss:0.04352  GradNorm:0.00345  GradNormST:0.01742  AvgTextLen:16.6  AvgSpecLen:66.3  StepTime:0.43  LR:0.000100\n",
      "   | > Step:31/68  GlobalStep:108500  TotalLoss:0.00196  PostnetLoss:0.00096  DecoderLoss:0.00100  StopLoss:0.03990  GradNorm:0.00380  GradNormST:0.01063  AvgTextLen:24.1  AvgSpecLen:82.7  StepTime:0.51  LR:0.000100\n",
      "   | > Step:41/68  GlobalStep:108510  TotalLoss:0.00215  PostnetLoss:0.00105  DecoderLoss:0.00110  StopLoss:0.03028  GradNorm:0.00325  GradNormST:0.01074  AvgTextLen:32.3  AvgSpecLen:109.4  StepTime:0.62  LR:0.000100\n",
      "   | > Step:51/68  GlobalStep:108520  TotalLoss:0.00227  PostnetLoss:0.00110  DecoderLoss:0.00117  StopLoss:0.03563  GradNorm:0.00352  GradNormST:0.01014  AvgTextLen:39.2  AvgSpecLen:126.9  StepTime:0.62  LR:0.000100\n",
      "   | > Step:61/68  GlobalStep:108530  TotalLoss:0.00246  PostnetLoss:0.00119  DecoderLoss:0.00127  StopLoss:0.04159  GradNorm:0.00420  GradNormST:0.01636  AvgTextLen:54.6  AvgSpecLen:165.5  StepTime:0.92  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:108537  AvgTotalLoss:0.04670  AvgPostnetLoss:0.00098  AvgDecoderLoss:0.00103  AvgStopLoss:0.04469  EpochTime:41.42  AvgStepTime:0.60\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.08878   PostnetLoss: 0.00526   DecoderLoss:0.00557  StopLoss: 0.07795  \n",
      "   | > TotalLoss: 0.08759   PostnetLoss: 0.00978   DecoderLoss:0.01020  StopLoss: 0.06761  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00098   Validation Loss: 0.00882\n",
      "\n",
      " > Epoch 573/1000\n",
      "   | > Step:2/68  GlobalStep:108540  TotalLoss:0.00128  PostnetLoss:0.00063  DecoderLoss:0.00066  StopLoss:0.03782  GradNorm:0.00371  GradNormST:0.01734  AvgTextLen:4.0  AvgSpecLen:31.3  StepTime:0.35  LR:0.000100\n",
      "   | > Step:12/68  GlobalStep:108550  TotalLoss:0.00156  PostnetLoss:0.00076  DecoderLoss:0.00080  StopLoss:0.03815  GradNorm:0.00377  GradNormST:0.01104  AvgTextLen:11.0  AvgSpecLen:49.3  StepTime:0.40  LR:0.000100\n",
      "   | > Step:22/68  GlobalStep:108560  TotalLoss:0.00179  PostnetLoss:0.00087  DecoderLoss:0.00091  StopLoss:0.05274  GradNorm:0.00343  GradNormST:0.01414  AvgTextLen:18.0  AvgSpecLen:72.0  StepTime:0.37  LR:0.000100\n",
      "   | > Step:32/68  GlobalStep:108570  TotalLoss:0.00197  PostnetLoss:0.00096  DecoderLoss:0.00101  StopLoss:0.06146  GradNorm:0.00362  GradNormST:0.01770  AvgTextLen:24.4  AvgSpecLen:91.5  StepTime:0.56  LR:0.000100\n",
      "   | > Step:42/68  GlobalStep:108580  TotalLoss:0.00215  PostnetLoss:0.00104  DecoderLoss:0.00111  StopLoss:0.04373  GradNorm:0.00307  GradNormST:0.01566  AvgTextLen:33.1  AvgSpecLen:109.7  StepTime:0.57  LR:0.000100\n",
      "   | > Step:52/68  GlobalStep:108590  TotalLoss:0.00230  PostnetLoss:0.00112  DecoderLoss:0.00118  StopLoss:0.04409  GradNorm:0.00346  GradNormST:0.01070  AvgTextLen:42.1  AvgSpecLen:137.5  StepTime:0.67  LR:0.000100\n",
      "   | > Step:62/68  GlobalStep:108600  TotalLoss:0.00255  PostnetLoss:0.00124  DecoderLoss:0.00131  StopLoss:0.03036  GradNorm:0.00378  GradNormST:0.00843  AvgTextLen:54.0  AvgSpecLen:174.7  StepTime:0.76  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:108606  AvgTotalLoss:0.04622  AvgPostnetLoss:0.00098  AvgDecoderLoss:0.00103  AvgStopLoss:0.04421  EpochTime:41.97  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.09217   PostnetLoss: 0.00521   DecoderLoss:0.00553  StopLoss: 0.08144  \n",
      "   | > TotalLoss: 0.08077   PostnetLoss: 0.00908   DecoderLoss:0.00953  StopLoss: 0.06216  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00098   Validation Loss: 0.00818\n",
      "\n",
      " > Epoch 574/1000\n",
      "   | > Step:3/68  GlobalStep:108610  TotalLoss:0.00137  PostnetLoss:0.00067  DecoderLoss:0.00070  StopLoss:0.06502  GradNorm:0.00359  GradNormST:0.01997  AvgTextLen:4.7  AvgSpecLen:29.7  StepTime:0.32  LR:0.000100\n",
      "   | > Step:13/68  GlobalStep:108620  TotalLoss:0.00158  PostnetLoss:0.00077  DecoderLoss:0.00081  StopLoss:0.10618  GradNorm:0.00365  GradNormST:0.03406  AvgTextLen:11.5  AvgSpecLen:55.2  StepTime:0.35  LR:0.000100\n",
      "   | > Step:23/68  GlobalStep:108630  TotalLoss:0.00184  PostnetLoss:0.00090  DecoderLoss:0.00094  StopLoss:0.05338  GradNorm:0.00341  GradNormST:0.01160  AvgTextLen:18.3  AvgSpecLen:72.2  StepTime:0.43  LR:0.000100\n",
      "   | > Step:33/68  GlobalStep:108640  TotalLoss:0.00203  PostnetLoss:0.00099  DecoderLoss:0.00104  StopLoss:0.05332  GradNorm:0.00341  GradNormST:0.01763  AvgTextLen:25.9  AvgSpecLen:88.9  StepTime:0.52  LR:0.000100\n",
      "   | > Step:43/68  GlobalStep:108650  TotalLoss:0.00213  PostnetLoss:0.00103  DecoderLoss:0.00109  StopLoss:0.04098  GradNorm:0.00304  GradNormST:0.00901  AvgTextLen:33.4  AvgSpecLen:115.7  StepTime:0.51  LR:0.000100\n",
      "   | > Step:53/68  GlobalStep:108660  TotalLoss:0.00227  PostnetLoss:0.00110  DecoderLoss:0.00117  StopLoss:0.03738  GradNorm:0.00312  GradNormST:0.00825  AvgTextLen:42.4  AvgSpecLen:137.6  StepTime:0.81  LR:0.000100\n",
      "   | > Step:63/68  GlobalStep:108670  TotalLoss:0.00259  PostnetLoss:0.00126  DecoderLoss:0.00134  StopLoss:0.02706  GradNorm:0.00278  GradNormST:0.00889  AvgTextLen:58.6  AvgSpecLen:185.1  StepTime:0.91  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:108675  AvgTotalLoss:0.04732  AvgPostnetLoss:0.00098  AvgDecoderLoss:0.00104  AvgStopLoss:0.04530  EpochTime:42.32  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.08711   PostnetLoss: 0.00543   DecoderLoss:0.00576  StopLoss: 0.07592  \n",
      "   | > TotalLoss: 0.07859   PostnetLoss: 0.00876   DecoderLoss:0.00919  StopLoss: 0.06064  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00098   Validation Loss: 0.00808\n",
      "\n",
      " > Epoch 575/1000\n",
      "   | > Step:4/68  GlobalStep:108680  TotalLoss:0.00139  PostnetLoss:0.00068  DecoderLoss:0.00071  StopLoss:0.08350  GradNorm:0.00366  GradNormST:0.04214  AvgTextLen:4.8  AvgSpecLen:31.1  StepTime:0.30  LR:0.000100\n",
      "   | > Step:14/68  GlobalStep:108690  TotalLoss:0.00166  PostnetLoss:0.00080  DecoderLoss:0.00085  StopLoss:0.04420  GradNorm:0.00370  GradNormST:0.01604  AvgTextLen:12.2  AvgSpecLen:52.0  StepTime:0.27  LR:0.000100\n",
      "   | > Step:24/68  GlobalStep:108700  TotalLoss:0.00189  PostnetLoss:0.00092  DecoderLoss:0.00097  StopLoss:0.04587  GradNorm:0.00322  GradNormST:0.01100  AvgTextLen:19.2  AvgSpecLen:79.0  StepTime:0.50  LR:0.000100\n",
      "   | > Step:34/68  GlobalStep:108710  TotalLoss:0.00205  PostnetLoss:0.00100  DecoderLoss:0.00105  StopLoss:0.03350  GradNorm:0.00330  GradNormST:0.00820  AvgTextLen:26.2  AvgSpecLen:95.9  StepTime:0.59  LR:0.000100\n",
      "   | > Step:44/68  GlobalStep:108720  TotalLoss:0.00218  PostnetLoss:0.00106  DecoderLoss:0.00112  StopLoss:0.04354  GradNorm:0.00289  GradNormST:0.01180  AvgTextLen:33.8  AvgSpecLen:116.3  StepTime:0.69  LR:0.000100\n",
      "   | > Step:54/68  GlobalStep:108730  TotalLoss:0.00234  PostnetLoss:0.00114  DecoderLoss:0.00120  StopLoss:0.03812  GradNorm:0.00363  GradNormST:0.00988  AvgTextLen:42.2  AvgSpecLen:144.9  StepTime:0.87  LR:0.000100\n",
      "   | > Step:64/68  GlobalStep:108740  TotalLoss:0.00245  PostnetLoss:0.00119  DecoderLoss:0.00127  StopLoss:0.03200  GradNorm:0.00282  GradNormST:0.00691  AvgTextLen:60.6  AvgSpecLen:177.6  StepTime:0.80  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:108744  AvgTotalLoss:0.04548  AvgPostnetLoss:0.00098  AvgDecoderLoss:0.00103  AvgStopLoss:0.04347  EpochTime:42.56  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.09009   PostnetLoss: 0.00527   DecoderLoss:0.00558  StopLoss: 0.07924  \n",
      "   | > TotalLoss: 0.08025   PostnetLoss: 0.00914   DecoderLoss:0.00962  StopLoss: 0.06148  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00098   Validation Loss: 0.00824\n",
      "\n",
      " > Epoch 576/1000\n",
      "   | > Step:5/68  GlobalStep:108750  TotalLoss:0.00147  PostnetLoss:0.00072  DecoderLoss:0.00075  StopLoss:0.06849  GradNorm:0.00677  GradNormST:0.02946  AvgTextLen:5.9  AvgSpecLen:33.2  StepTime:0.28  LR:0.000100\n",
      "   | > Step:15/68  GlobalStep:108760  TotalLoss:0.00175  PostnetLoss:0.00086  DecoderLoss:0.00090  StopLoss:0.04421  GradNorm:0.00491  GradNormST:0.01204  AvgTextLen:12.8  AvgSpecLen:59.1  StepTime:0.58  LR:0.000100\n",
      "   | > Step:25/68  GlobalStep:108770  TotalLoss:0.00184  PostnetLoss:0.00089  DecoderLoss:0.00094  StopLoss:0.04046  GradNorm:0.00341  GradNormST:0.00879  AvgTextLen:19.8  AvgSpecLen:73.3  StepTime:0.36  LR:0.000100\n",
      "   | > Step:35/68  GlobalStep:108780  TotalLoss:0.00208  PostnetLoss:0.00101  DecoderLoss:0.00107  StopLoss:0.05469  GradNorm:0.00313  GradNormST:0.02585  AvgTextLen:27.1  AvgSpecLen:104.7  StepTime:0.64  LR:0.000100\n",
      "   | > Step:45/68  GlobalStep:108790  TotalLoss:0.00211  PostnetLoss:0.00103  DecoderLoss:0.00109  StopLoss:0.05039  GradNorm:0.00289  GradNormST:0.01566  AvgTextLen:34.4  AvgSpecLen:118.6  StepTime:0.54  LR:0.000100\n",
      "   | > Step:55/68  GlobalStep:108800  TotalLoss:0.00237  PostnetLoss:0.00115  DecoderLoss:0.00122  StopLoss:0.03858  GradNorm:0.00312  GradNormST:0.01045  AvgTextLen:43.9  AvgSpecLen:145.2  StepTime:0.85  LR:0.000100\n",
      "   | > Step:65/68  GlobalStep:108810  TotalLoss:0.00262  PostnetLoss:0.00127  DecoderLoss:0.00136  StopLoss:0.02542  GradNorm:0.00269  GradNormST:0.00737  AvgTextLen:64.2  AvgSpecLen:207.4  StepTime:1.18  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:108813  AvgTotalLoss:0.04908  AvgPostnetLoss:0.00098  AvgDecoderLoss:0.00103  AvgStopLoss:0.04707  EpochTime:42.00  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.09087   PostnetLoss: 0.00524   DecoderLoss:0.00554  StopLoss: 0.08009  \n",
      "   | > TotalLoss: 0.08073   PostnetLoss: 0.00880   DecoderLoss:0.00922  StopLoss: 0.06271  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00098   Validation Loss: 0.00806\n",
      "\n",
      " > Epoch 577/1000\n",
      "   | > Step:6/68  GlobalStep:108820  TotalLoss:0.00140  PostnetLoss:0.00068  DecoderLoss:0.00072  StopLoss:0.03906  GradNorm:0.00368  GradNormST:0.01189  AvgTextLen:6.0  AvgSpecLen:34.8  StepTime:0.37  LR:0.000100\n",
      "   | > Step:16/68  GlobalStep:108830  TotalLoss:0.00174  PostnetLoss:0.00085  DecoderLoss:0.00089  StopLoss:0.04304  GradNorm:0.00513  GradNormST:0.01517  AvgTextLen:13.8  AvgSpecLen:60.6  StepTime:0.37  LR:0.000100\n",
      "   | > Step:26/68  GlobalStep:108840  TotalLoss:0.00190  PostnetLoss:0.00093  DecoderLoss:0.00097  StopLoss:0.05038  GradNorm:0.00452  GradNormST:0.02005  AvgTextLen:20.3  AvgSpecLen:77.5  StepTime:0.46  LR:0.000100\n",
      "   | > Step:36/68  GlobalStep:108850  TotalLoss:0.00205  PostnetLoss:0.00100  DecoderLoss:0.00105  StopLoss:0.05157  GradNorm:0.00354  GradNormST:0.00979  AvgTextLen:28.5  AvgSpecLen:100.2  StepTime:0.57  LR:0.000100\n",
      "   | > Step:46/68  GlobalStep:108860  TotalLoss:0.00219  PostnetLoss:0.00106  DecoderLoss:0.00112  StopLoss:0.04808  GradNorm:0.00323  GradNormST:0.01733  AvgTextLen:35.4  AvgSpecLen:122.6  StepTime:0.77  LR:0.000100\n",
      "   | > Step:56/68  GlobalStep:108870  TotalLoss:0.00235  PostnetLoss:0.00114  DecoderLoss:0.00121  StopLoss:0.03641  GradNorm:0.00281  GradNormST:0.01894  AvgTextLen:45.8  AvgSpecLen:145.4  StepTime:1.05  LR:0.000100\n",
      "   | > Step:66/68  GlobalStep:108880  TotalLoss:0.00276  PostnetLoss:0.00133  DecoderLoss:0.00143  StopLoss:0.03018  GradNorm:0.00282  GradNormST:0.01156  AvgTextLen:71.2  AvgSpecLen:220.8  StepTime:1.05  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:108882  AvgTotalLoss:0.04942  AvgPostnetLoss:0.00097  AvgDecoderLoss:0.00103  AvgStopLoss:0.04742  EpochTime:42.26  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.08687   PostnetLoss: 0.00516   DecoderLoss:0.00548  StopLoss: 0.07622  \n",
      "   | > TotalLoss: 0.06834   PostnetLoss: 0.00820   DecoderLoss:0.00864  StopLoss: 0.05150  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00097   Validation Loss: 0.00763\n",
      "\n",
      " > Epoch 578/1000\n",
      "   | > Step:7/68  GlobalStep:108890  TotalLoss:0.00149  PostnetLoss:0.00073  DecoderLoss:0.00076  StopLoss:0.05384  GradNorm:0.00361  GradNormST:0.02348  AvgTextLen:7.4  AvgSpecLen:44.6  StepTime:0.33  LR:0.000100\n",
      "   | > Step:17/68  GlobalStep:108900  TotalLoss:0.00170  PostnetLoss:0.00083  DecoderLoss:0.00087  StopLoss:0.05465  GradNorm:0.00395  GradNormST:0.01237  AvgTextLen:14.7  AvgSpecLen:66.0  StepTime:0.42  LR:0.000100\n",
      "   | > Step:27/68  GlobalStep:108910  TotalLoss:0.00195  PostnetLoss:0.00095  DecoderLoss:0.00100  StopLoss:0.05738  GradNorm:0.00379  GradNormST:0.02374  AvgTextLen:21.0  AvgSpecLen:80.2  StepTime:0.67  LR:0.000100\n",
      "   | > Step:37/68  GlobalStep:108920  TotalLoss:0.00207  PostnetLoss:0.00101  DecoderLoss:0.00106  StopLoss:0.04996  GradNorm:0.00366  GradNormST:0.01528  AvgTextLen:29.3  AvgSpecLen:102.4  StepTime:0.57  LR:0.000100\n",
      "   | > Step:47/68  GlobalStep:108930  TotalLoss:0.00224  PostnetLoss:0.00109  DecoderLoss:0.00115  StopLoss:0.02788  GradNorm:0.00309  GradNormST:0.00986  AvgTextLen:36.7  AvgSpecLen:126.9  StepTime:0.70  LR:0.000100\n",
      "   | > Step:57/68  GlobalStep:108940  TotalLoss:0.00240  PostnetLoss:0.00117  DecoderLoss:0.00124  StopLoss:0.02544  GradNorm:0.00290  GradNormST:0.00542  AvgTextLen:47.1  AvgSpecLen:161.0  StepTime:0.69  LR:0.000100\n",
      "   | > Step:67/68  GlobalStep:108950  TotalLoss:0.00273  PostnetLoss:0.00132  DecoderLoss:0.00141  StopLoss:0.02514  GradNorm:0.00252  GradNormST:0.01156  AvgTextLen:80.6  AvgSpecLen:246.5  StepTime:1.22  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:108951  AvgTotalLoss:0.04756  AvgPostnetLoss:0.00097  AvgDecoderLoss:0.00103  AvgStopLoss:0.04556  EpochTime:42.42  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.08985   PostnetLoss: 0.00520   DecoderLoss:0.00551  StopLoss: 0.07914  \n",
      "   | > TotalLoss: 0.07511   PostnetLoss: 0.00852   DecoderLoss:0.00897  StopLoss: 0.05761  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00097   Validation Loss: 0.00787\n",
      "\n",
      " > Epoch 579/1000\n",
      "   | > Step:8/68  GlobalStep:108960  TotalLoss:0.00151  PostnetLoss:0.00074  DecoderLoss:0.00077  StopLoss:0.04036  GradNorm:0.00426  GradNormST:0.01344  AvgTextLen:8.3  AvgSpecLen:44.0  StepTime:0.31  LR:0.000100\n",
      "   | > Step:18/68  GlobalStep:108970  TotalLoss:0.00171  PostnetLoss:0.00083  DecoderLoss:0.00087  StopLoss:0.04308  GradNorm:0.00384  GradNormST:0.00986  AvgTextLen:15.2  AvgSpecLen:64.5  StepTime:0.44  LR:0.000100\n",
      "   | > Step:28/68  GlobalStep:108980  TotalLoss:0.00191  PostnetLoss:0.00093  DecoderLoss:0.00098  StopLoss:0.04585  GradNorm:0.00330  GradNormST:0.00990  AvgTextLen:22.1  AvgSpecLen:85.4  StepTime:0.55  LR:0.000100\n",
      "   | > Step:38/68  GlobalStep:108990  TotalLoss:0.00206  PostnetLoss:0.00100  DecoderLoss:0.00106  StopLoss:0.02800  GradNorm:0.00347  GradNormST:0.00589  AvgTextLen:29.4  AvgSpecLen:104.1  StepTime:0.49  LR:0.000100\n",
      "   | > Step:48/68  GlobalStep:109000  TotalLoss:0.00221  PostnetLoss:0.00108  DecoderLoss:0.00113  StopLoss:0.03499  GradNorm:0.00300  GradNormST:0.00830  AvgTextLen:37.5  AvgSpecLen:123.8  StepTime:0.61  LR:0.000100\n",
      " | | > Checkpoint saving : /vm/TTS/models/cartman_models/mozilla-no-loc-fattn-stopnet-sigmoid-loss_masking-August-15-2019_11+26AM-de6aeb9/checkpoint_109000.pth.tar\n",
      "warning: audio amplitude out of range, auto clipped.\n",
      "   | > Step:58/68  GlobalStep:109010  TotalLoss:0.00243  PostnetLoss:0.00118  DecoderLoss:0.00125  StopLoss:0.03357  GradNorm:0.00290  GradNormST:0.01438  AvgTextLen:47.8  AvgSpecLen:158.0  StepTime:0.85  LR:0.000100\n",
      "   | > Step:68/68  GlobalStep:109020  TotalLoss:0.00272  PostnetLoss:0.00132  DecoderLoss:0.00141  StopLoss:0.02607  GradNorm:0.00341  GradNormST:0.01062  AvgTextLen:103.0  AvgSpecLen:308.9  StepTime:1.92  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:109020  AvgTotalLoss:0.04621  AvgPostnetLoss:0.00097  AvgDecoderLoss:0.00102  AvgStopLoss:0.04422  EpochTime:41.51  AvgStepTime:0.60\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.08617   PostnetLoss: 0.00511   DecoderLoss:0.00543  StopLoss: 0.07562  \n",
      "   | > TotalLoss: 0.07319   PostnetLoss: 0.00837   DecoderLoss:0.00882  StopLoss: 0.05600  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00097   Validation Loss: 0.00770\n",
      "\n",
      " > Epoch 580/1000\n",
      "   | > Step:9/68  GlobalStep:109030  TotalLoss:0.00150  PostnetLoss:0.00073  DecoderLoss:0.00077  StopLoss:0.04052  GradNorm:0.00448  GradNormST:0.01112  AvgTextLen:9.0  AvgSpecLen:42.1  StepTime:0.42  LR:0.000100\n",
      "   | > Step:19/68  GlobalStep:109040  TotalLoss:0.00173  PostnetLoss:0.00085  DecoderLoss:0.00089  StopLoss:0.06576  GradNorm:0.00412  GradNormST:0.02765  AvgTextLen:15.5  AvgSpecLen:63.2  StepTime:0.32  LR:0.000100\n",
      "   | > Step:29/68  GlobalStep:109050  TotalLoss:0.00187  PostnetLoss:0.00091  DecoderLoss:0.00096  StopLoss:0.05567  GradNorm:0.00373  GradNormST:0.01442  AvgTextLen:21.8  AvgSpecLen:83.4  StepTime:0.55  LR:0.000100\n",
      "   | > Step:39/68  GlobalStep:109060  TotalLoss:0.00211  PostnetLoss:0.00103  DecoderLoss:0.00108  StopLoss:0.04034  GradNorm:0.00366  GradNormST:0.01263  AvgTextLen:30.2  AvgSpecLen:110.4  StepTime:0.52  LR:0.000100\n",
      "   | > Step:49/68  GlobalStep:109070  TotalLoss:0.00224  PostnetLoss:0.00109  DecoderLoss:0.00115  StopLoss:0.03559  GradNorm:0.00305  GradNormST:0.00947  AvgTextLen:38.2  AvgSpecLen:127.7  StepTime:0.76  LR:0.000100\n",
      "   | > Step:59/68  GlobalStep:109080  TotalLoss:0.00238  PostnetLoss:0.00116  DecoderLoss:0.00123  StopLoss:0.03417  GradNorm:0.00274  GradNormST:0.00740  AvgTextLen:50.5  AvgSpecLen:155.9  StepTime:0.76  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:109089  AvgTotalLoss:0.04717  AvgPostnetLoss:0.00097  AvgDecoderLoss:0.00103  AvgStopLoss:0.04517  EpochTime:42.03  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.08956   PostnetLoss: 0.00502   DecoderLoss:0.00534  StopLoss: 0.07920  \n",
      "   | > TotalLoss: 0.06972   PostnetLoss: 0.00835   DecoderLoss:0.00879  StopLoss: 0.05258  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00097   Validation Loss: 0.00766\n",
      "\n",
      " > Epoch 581/1000\n",
      "   | > Step:0/68  GlobalStep:109090  TotalLoss:0.00145  PostnetLoss:0.00070  DecoderLoss:0.00075  StopLoss:0.06387  GradNorm:0.00578  GradNormST:0.02772  AvgTextLen:3.0  AvgSpecLen:25.3  StepTime:0.54  LR:0.000100\n",
      "   | > Step:10/68  GlobalStep:109100  TotalLoss:0.00153  PostnetLoss:0.00075  DecoderLoss:0.00079  StopLoss:0.04378  GradNorm:0.00408  GradNormST:0.02037  AvgTextLen:9.8  AvgSpecLen:48.6  StepTime:0.52  LR:0.000100\n",
      "   | > Step:20/68  GlobalStep:109110  TotalLoss:0.00181  PostnetLoss:0.00089  DecoderLoss:0.00092  StopLoss:0.03404  GradNorm:0.00543  GradNormST:0.01108  AvgTextLen:16.4  AvgSpecLen:64.8  StepTime:0.46  LR:0.000100\n",
      "   | > Step:30/68  GlobalStep:109120  TotalLoss:0.00196  PostnetLoss:0.00095  DecoderLoss:0.00100  StopLoss:0.04493  GradNorm:0.00373  GradNormST:0.01374  AvgTextLen:22.9  AvgSpecLen:85.8  StepTime:0.50  LR:0.000100\n",
      "   | > Step:40/68  GlobalStep:109130  TotalLoss:0.00208  PostnetLoss:0.00102  DecoderLoss:0.00107  StopLoss:0.04186  GradNorm:0.00362  GradNormST:0.01094  AvgTextLen:30.9  AvgSpecLen:105.3  StepTime:0.64  LR:0.000100\n",
      "   | > Step:50/68  GlobalStep:109140  TotalLoss:0.00225  PostnetLoss:0.00109  DecoderLoss:0.00116  StopLoss:0.03342  GradNorm:0.00324  GradNormST:0.00811  AvgTextLen:38.8  AvgSpecLen:132.0  StepTime:0.71  LR:0.000100\n",
      "   | > Step:60/68  GlobalStep:109150  TotalLoss:0.00248  PostnetLoss:0.00120  DecoderLoss:0.00128  StopLoss:0.02974  GradNorm:0.00295  GradNormST:0.00793  AvgTextLen:52.0  AvgSpecLen:172.7  StepTime:0.92  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:109158  AvgTotalLoss:0.04943  AvgPostnetLoss:0.00097  AvgDecoderLoss:0.00103  AvgStopLoss:0.04743  EpochTime:42.96  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.08951   PostnetLoss: 0.00510   DecoderLoss:0.00542  StopLoss: 0.07899  \n",
      "   | > TotalLoss: 0.07049   PostnetLoss: 0.00821   DecoderLoss:0.00867  StopLoss: 0.05361  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00097   Validation Loss: 0.00776\n",
      "\n",
      " > Epoch 582/1000\n",
      "   | > Step:1/68  GlobalStep:109160  TotalLoss:0.00140  PostnetLoss:0.00068  DecoderLoss:0.00072  StopLoss:0.03600  GradNorm:0.00454  GradNormST:0.01319  AvgTextLen:3.7  AvgSpecLen:28.6  StepTime:0.26  LR:0.000100\n",
      "   | > Step:11/68  GlobalStep:109170  TotalLoss:0.00155  PostnetLoss:0.00076  DecoderLoss:0.00079  StopLoss:0.03564  GradNorm:0.00440  GradNormST:0.00954  AvgTextLen:10.3  AvgSpecLen:49.4  StepTime:0.31  LR:0.000100\n",
      "   | > Step:21/68  GlobalStep:109180  TotalLoss:0.00179  PostnetLoss:0.00088  DecoderLoss:0.00091  StopLoss:0.04186  GradNorm:0.00498  GradNormST:0.01477  AvgTextLen:16.6  AvgSpecLen:66.3  StepTime:0.44  LR:0.000100\n",
      "   | > Step:31/68  GlobalStep:109190  TotalLoss:0.00195  PostnetLoss:0.00095  DecoderLoss:0.00100  StopLoss:0.05891  GradNorm:0.00451  GradNormST:0.02768  AvgTextLen:24.1  AvgSpecLen:82.7  StepTime:0.54  LR:0.000100\n",
      "   | > Step:41/68  GlobalStep:109200  TotalLoss:0.00215  PostnetLoss:0.00105  DecoderLoss:0.00110  StopLoss:0.03784  GradNorm:0.00396  GradNormST:0.01280  AvgTextLen:32.3  AvgSpecLen:109.4  StepTime:0.68  LR:0.000100\n",
      "   | > Step:51/68  GlobalStep:109210  TotalLoss:0.00223  PostnetLoss:0.00108  DecoderLoss:0.00115  StopLoss:0.03450  GradNorm:0.00303  GradNormST:0.00715  AvgTextLen:39.2  AvgSpecLen:126.9  StepTime:0.80  LR:0.000100\n",
      "   | > Step:61/68  GlobalStep:109220  TotalLoss:0.00241  PostnetLoss:0.00117  DecoderLoss:0.00124  StopLoss:0.05731  GradNorm:0.00267  GradNormST:0.01273  AvgTextLen:54.6  AvgSpecLen:165.5  StepTime:0.89  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:109227  AvgTotalLoss:0.04894  AvgPostnetLoss:0.00098  AvgDecoderLoss:0.00103  AvgStopLoss:0.04694  EpochTime:42.41  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.09385   PostnetLoss: 0.00514   DecoderLoss:0.00546  StopLoss: 0.08325  \n",
      "   | > TotalLoss: 0.07234   PostnetLoss: 0.00860   DecoderLoss:0.00905  StopLoss: 0.05469  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00098   Validation Loss: 0.00802\n",
      "\n",
      " > Epoch 583/1000\n",
      "   | > Step:2/68  GlobalStep:109230  TotalLoss:0.00133  PostnetLoss:0.00065  DecoderLoss:0.00068  StopLoss:0.05493  GradNorm:0.00433  GradNormST:0.02312  AvgTextLen:4.0  AvgSpecLen:31.3  StepTime:0.51  LR:0.000100\n",
      "   | > Step:12/68  GlobalStep:109240  TotalLoss:0.00153  PostnetLoss:0.00074  DecoderLoss:0.00078  StopLoss:0.05417  GradNorm:0.00339  GradNormST:0.01916  AvgTextLen:11.0  AvgSpecLen:49.3  StepTime:0.36  LR:0.000100\n",
      "   | > Step:22/68  GlobalStep:109250  TotalLoss:0.00178  PostnetLoss:0.00087  DecoderLoss:0.00091  StopLoss:0.04913  GradNorm:0.00388  GradNormST:0.01353  AvgTextLen:18.0  AvgSpecLen:72.0  StepTime:0.46  LR:0.000100\n",
      "   | > Step:32/68  GlobalStep:109260  TotalLoss:0.00196  PostnetLoss:0.00096  DecoderLoss:0.00100  StopLoss:0.06344  GradNorm:0.00435  GradNormST:0.01712  AvgTextLen:24.4  AvgSpecLen:91.5  StepTime:0.46  LR:0.000100\n",
      "   | > Step:42/68  GlobalStep:109270  TotalLoss:0.00213  PostnetLoss:0.00104  DecoderLoss:0.00109  StopLoss:0.04499  GradNorm:0.00391  GradNormST:0.01869  AvgTextLen:33.1  AvgSpecLen:109.7  StepTime:0.48  LR:0.000100\n",
      "   | > Step:52/68  GlobalStep:109280  TotalLoss:0.00231  PostnetLoss:0.00112  DecoderLoss:0.00119  StopLoss:0.05467  GradNorm:0.00351  GradNormST:0.01241  AvgTextLen:42.1  AvgSpecLen:137.5  StepTime:0.69  LR:0.000100\n",
      "   | > Step:62/68  GlobalStep:109290  TotalLoss:0.00250  PostnetLoss:0.00121  DecoderLoss:0.00129  StopLoss:0.04208  GradNorm:0.00279  GradNormST:0.01071  AvgTextLen:54.0  AvgSpecLen:174.7  StepTime:0.75  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:109296  AvgTotalLoss:0.04972  AvgPostnetLoss:0.00098  AvgDecoderLoss:0.00103  AvgStopLoss:0.04772  EpochTime:41.89  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.09482   PostnetLoss: 0.00539   DecoderLoss:0.00571  StopLoss: 0.08372  \n",
      "   | > TotalLoss: 0.08914   PostnetLoss: 0.00983   DecoderLoss:0.01031  StopLoss: 0.06900  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00098   Validation Loss: 0.00909\n",
      "\n",
      " > Epoch 584/1000\n",
      "   | > Step:3/68  GlobalStep:109300  TotalLoss:0.00150  PostnetLoss:0.00073  DecoderLoss:0.00077  StopLoss:0.07555  GradNorm:0.00497  GradNormST:0.03387  AvgTextLen:4.7  AvgSpecLen:29.7  StepTime:0.27  LR:0.000100\n",
      "   | > Step:13/68  GlobalStep:109310  TotalLoss:0.00185  PostnetLoss:0.00090  DecoderLoss:0.00094  StopLoss:0.12157  GradNorm:0.00494  GradNormST:0.05390  AvgTextLen:11.5  AvgSpecLen:55.2  StepTime:0.53  LR:0.000100\n",
      "   | > Step:23/68  GlobalStep:109320  TotalLoss:0.00228  PostnetLoss:0.00111  DecoderLoss:0.00117  StopLoss:0.13651  GradNorm:0.00529  GradNormST:0.07130  AvgTextLen:18.3  AvgSpecLen:72.2  StepTime:0.42  LR:0.000100\n",
      "   | > Step:33/68  GlobalStep:109330  TotalLoss:0.00240  PostnetLoss:0.00117  DecoderLoss:0.00123  StopLoss:0.15015  GradNorm:0.00498  GradNormST:0.10279  AvgTextLen:25.9  AvgSpecLen:88.9  StepTime:0.43  LR:0.000100\n",
      "   | > Step:43/68  GlobalStep:109340  TotalLoss:0.00263  PostnetLoss:0.00127  DecoderLoss:0.00136  StopLoss:0.10819  GradNorm:0.00501  GradNormST:0.05865  AvgTextLen:33.4  AvgSpecLen:115.7  StepTime:0.66  LR:0.000100\n",
      "   | > Step:53/68  GlobalStep:109350  TotalLoss:0.00258  PostnetLoss:0.00125  DecoderLoss:0.00133  StopLoss:0.13381  GradNorm:0.00405  GradNormST:0.09679  AvgTextLen:42.4  AvgSpecLen:137.6  StepTime:0.64  LR:0.000100\n",
      "   | > Step:63/68  GlobalStep:109360  TotalLoss:0.00298  PostnetLoss:0.00144  DecoderLoss:0.00154  StopLoss:0.11932  GradNorm:0.00410  GradNormST:0.09734  AvgTextLen:58.6  AvgSpecLen:185.1  StepTime:0.92  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:109365  AvgTotalLoss:0.13402  AvgPostnetLoss:0.00116  AvgDecoderLoss:0.00123  AvgStopLoss:0.13163  EpochTime:42.17  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.08902   PostnetLoss: 0.00520   DecoderLoss:0.00555  StopLoss: 0.07828  \n",
      "   | > TotalLoss: 0.09559   PostnetLoss: 0.00862   DecoderLoss:0.00911  StopLoss: 0.07786  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00116   Validation Loss: 0.00780\n",
      "\n",
      " > Epoch 585/1000\n",
      "   | > Step:4/68  GlobalStep:109370  TotalLoss:0.00151  PostnetLoss:0.00073  DecoderLoss:0.00078  StopLoss:0.10314  GradNorm:0.00451  GradNormST:0.03924  AvgTextLen:4.8  AvgSpecLen:31.1  StepTime:0.33  LR:0.000100\n",
      "   | > Step:14/68  GlobalStep:109380  TotalLoss:0.00175  PostnetLoss:0.00086  DecoderLoss:0.00089  StopLoss:0.11238  GradNorm:0.00455  GradNormST:0.03954  AvgTextLen:12.2  AvgSpecLen:52.0  StepTime:0.26  LR:0.000100\n",
      "   | > Step:24/68  GlobalStep:109390  TotalLoss:0.00205  PostnetLoss:0.00100  DecoderLoss:0.00105  StopLoss:0.12028  GradNorm:0.00470  GradNormST:0.04872  AvgTextLen:19.2  AvgSpecLen:79.0  StepTime:0.53  LR:0.000100\n",
      "   | > Step:34/68  GlobalStep:109400  TotalLoss:0.00224  PostnetLoss:0.00109  DecoderLoss:0.00115  StopLoss:0.13063  GradNorm:0.00433  GradNormST:0.06978  AvgTextLen:26.2  AvgSpecLen:95.9  StepTime:0.57  LR:0.000100\n",
      "   | > Step:44/68  GlobalStep:109410  TotalLoss:0.00236  PostnetLoss:0.00115  DecoderLoss:0.00122  StopLoss:0.11555  GradNorm:0.00394  GradNormST:0.07307  AvgTextLen:33.8  AvgSpecLen:116.3  StepTime:0.69  LR:0.000100\n",
      "   | > Step:54/68  GlobalStep:109420  TotalLoss:0.00252  PostnetLoss:0.00122  DecoderLoss:0.00130  StopLoss:0.11736  GradNorm:0.00371  GradNormST:0.04517  AvgTextLen:42.2  AvgSpecLen:144.9  StepTime:0.68  LR:0.000100\n",
      "   | > Step:64/68  GlobalStep:109430  TotalLoss:0.00261  PostnetLoss:0.00126  DecoderLoss:0.00135  StopLoss:0.09446  GradNorm:0.00340  GradNormST:0.06142  AvgTextLen:60.6  AvgSpecLen:177.6  StepTime:0.83  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:109434  AvgTotalLoss:0.11846  AvgPostnetLoss:0.00105  AvgDecoderLoss:0.00111  AvgStopLoss:0.11630  EpochTime:42.38  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.10064   PostnetLoss: 0.00526   DecoderLoss:0.00558  StopLoss: 0.08980  \n",
      "   | > TotalLoss: 0.10270   PostnetLoss: 0.00912   DecoderLoss:0.00957  StopLoss: 0.08401  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00105   Validation Loss: 0.00812\n",
      "\n",
      " > Epoch 586/1000\n",
      "   | > Step:5/68  GlobalStep:109440  TotalLoss:0.00145  PostnetLoss:0.00070  DecoderLoss:0.00075  StopLoss:0.07058  GradNorm:0.00370  GradNormST:0.03559  AvgTextLen:5.9  AvgSpecLen:33.2  StepTime:0.35  LR:0.000100\n",
      "   | > Step:15/68  GlobalStep:109450  TotalLoss:0.00177  PostnetLoss:0.00087  DecoderLoss:0.00091  StopLoss:0.11142  GradNorm:0.00409  GradNormST:0.06759  AvgTextLen:12.8  AvgSpecLen:59.1  StepTime:0.51  LR:0.000100\n",
      "   | > Step:25/68  GlobalStep:109460  TotalLoss:0.00192  PostnetLoss:0.00093  DecoderLoss:0.00098  StopLoss:0.08250  GradNorm:0.00436  GradNormST:0.02499  AvgTextLen:19.8  AvgSpecLen:73.3  StepTime:0.46  LR:0.000100\n",
      "   | > Step:35/68  GlobalStep:109470  TotalLoss:0.00220  PostnetLoss:0.00107  DecoderLoss:0.00113  StopLoss:0.15381  GradNorm:0.00580  GradNormST:0.10539  AvgTextLen:27.1  AvgSpecLen:104.7  StepTime:0.75  LR:0.000100\n",
      "   | > Step:45/68  GlobalStep:109480  TotalLoss:0.00221  PostnetLoss:0.00107  DecoderLoss:0.00114  StopLoss:0.11042  GradNorm:0.00306  GradNormST:0.03962  AvgTextLen:34.4  AvgSpecLen:118.6  StepTime:0.65  LR:0.000100\n",
      "   | > Step:55/68  GlobalStep:109490  TotalLoss:0.00248  PostnetLoss:0.00120  DecoderLoss:0.00127  StopLoss:0.06838  GradNorm:0.00354  GradNormST:0.02214  AvgTextLen:43.9  AvgSpecLen:145.2  StepTime:0.66  LR:0.000100\n",
      "   | > Step:65/68  GlobalStep:109500  TotalLoss:0.00281  PostnetLoss:0.00136  DecoderLoss:0.00145  StopLoss:0.08544  GradNorm:0.00444  GradNormST:0.06421  AvgTextLen:64.2  AvgSpecLen:207.4  StepTime:1.19  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:109503  AvgTotalLoss:0.10136  AvgPostnetLoss:0.00102  AvgDecoderLoss:0.00108  AvgStopLoss:0.09926  EpochTime:42.92  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.10591   PostnetLoss: 0.00522   DecoderLoss:0.00553  StopLoss: 0.09516  \n",
      "   | > TotalLoss: 0.10934   PostnetLoss: 0.00952   DecoderLoss:0.00996  StopLoss: 0.08986  \n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00102   Validation Loss: 0.00833\n",
      "\n",
      " > Epoch 587/1000\n",
      "   | > Step:6/68  GlobalStep:109510  TotalLoss:0.00145  PostnetLoss:0.00071  DecoderLoss:0.00074  StopLoss:0.08040  GradNorm:0.00404  GradNormST:0.05245  AvgTextLen:6.0  AvgSpecLen:34.8  StepTime:0.36  LR:0.000100\n",
      "   | > Step:16/68  GlobalStep:109520  TotalLoss:0.00173  PostnetLoss:0.00084  DecoderLoss:0.00088  StopLoss:0.07660  GradNorm:0.00343  GradNormST:0.03143  AvgTextLen:13.8  AvgSpecLen:60.6  StepTime:0.39  LR:0.000100\n",
      "   | > Step:26/68  GlobalStep:109530  TotalLoss:0.00198  PostnetLoss:0.00096  DecoderLoss:0.00101  StopLoss:0.09498  GradNorm:0.00381  GradNormST:0.06684  AvgTextLen:20.3  AvgSpecLen:77.5  StepTime:0.48  LR:0.000100\n",
      "   | > Step:36/68  GlobalStep:109540  TotalLoss:0.00210  PostnetLoss:0.00102  DecoderLoss:0.00108  StopLoss:0.14782  GradNorm:0.00410  GradNormST:0.10013  AvgTextLen:28.5  AvgSpecLen:100.2  StepTime:0.56  LR:0.000100\n",
      "   | > Step:46/68  GlobalStep:109550  TotalLoss:0.00228  PostnetLoss:0.00111  DecoderLoss:0.00117  StopLoss:0.11470  GradNorm:0.00417  GradNormST:0.09198  AvgTextLen:35.4  AvgSpecLen:122.6  StepTime:0.66  LR:0.000100\n",
      "   | > Step:56/68  GlobalStep:109560  TotalLoss:0.00248  PostnetLoss:0.00120  DecoderLoss:0.00127  StopLoss:0.11347  GradNorm:0.00363  GradNormST:0.11233  AvgTextLen:45.8  AvgSpecLen:145.4  StepTime:0.83  LR:0.000100\n",
      "   | > Step:66/68  GlobalStep:109570  TotalLoss:0.00286  PostnetLoss:0.00139  DecoderLoss:0.00148  StopLoss:0.08682  GradNorm:0.00357  GradNormST:0.06650  AvgTextLen:71.2  AvgSpecLen:220.8  StepTime:1.05  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:109572  AvgTotalLoss:0.11696  AvgPostnetLoss:0.00101  AvgDecoderLoss:0.00106  AvgStopLoss:0.11489  EpochTime:42.21  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.10350   PostnetLoss: 0.00526   DecoderLoss:0.00558  StopLoss: 0.09266  \n",
      "   | > TotalLoss: 0.10589   PostnetLoss: 0.00900   DecoderLoss:0.00944  StopLoss: 0.08745  \n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00101   Validation Loss: 0.00801\n",
      "\n",
      " > Epoch 588/1000\n",
      "   | > Step:7/68  GlobalStep:109580  TotalLoss:0.00154  PostnetLoss:0.00075  DecoderLoss:0.00078  StopLoss:0.05477  GradNorm:0.00441  GradNormST:0.02412  AvgTextLen:7.4  AvgSpecLen:44.6  StepTime:0.41  LR:0.000100\n",
      "   | > Step:17/68  GlobalStep:109590  TotalLoss:0.00169  PostnetLoss:0.00082  DecoderLoss:0.00086  StopLoss:0.10280  GradNorm:0.00352  GradNormST:0.03564  AvgTextLen:14.7  AvgSpecLen:66.0  StepTime:0.45  LR:0.000100\n",
      "   | > Step:27/68  GlobalStep:109600  TotalLoss:0.00200  PostnetLoss:0.00098  DecoderLoss:0.00103  StopLoss:0.14433  GradNorm:0.00408  GradNormST:0.09107  AvgTextLen:21.0  AvgSpecLen:80.2  StepTime:0.53  LR:0.000100\n",
      "   | > Step:37/68  GlobalStep:109610  TotalLoss:0.00212  PostnetLoss:0.00103  DecoderLoss:0.00109  StopLoss:0.10738  GradNorm:0.00407  GradNormST:0.06584  AvgTextLen:29.3  AvgSpecLen:102.4  StepTime:0.59  LR:0.000100\n",
      "   | > Step:47/68  GlobalStep:109620  TotalLoss:0.00237  PostnetLoss:0.00115  DecoderLoss:0.00122  StopLoss:0.07492  GradNorm:0.00489  GradNormST:0.03534  AvgTextLen:36.7  AvgSpecLen:126.9  StepTime:0.75  LR:0.000100\n",
      "   | > Step:57/68  GlobalStep:109630  TotalLoss:0.00250  PostnetLoss:0.00122  DecoderLoss:0.00129  StopLoss:0.06499  GradNorm:0.00420  GradNormST:0.02699  AvgTextLen:47.1  AvgSpecLen:161.0  StepTime:0.81  LR:0.000100\n",
      "   | > Step:67/68  GlobalStep:109640  TotalLoss:0.00285  PostnetLoss:0.00138  DecoderLoss:0.00147  StopLoss:0.05734  GradNorm:0.00408  GradNormST:0.03547  AvgTextLen:80.6  AvgSpecLen:246.5  StepTime:1.22  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:109641  AvgTotalLoss:0.10121  AvgPostnetLoss:0.00100  AvgDecoderLoss:0.00105  AvgStopLoss:0.09916  EpochTime:42.37  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.10445   PostnetLoss: 0.00520   DecoderLoss:0.00549  StopLoss: 0.09376  \n",
      "   | > TotalLoss: 0.10456   PostnetLoss: 0.00899   DecoderLoss:0.00939  StopLoss: 0.08618  \n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00100   Validation Loss: 0.00809\n",
      "\n",
      " > Epoch 589/1000\n",
      "   | > Step:8/68  GlobalStep:109650  TotalLoss:0.00153  PostnetLoss:0.00075  DecoderLoss:0.00078  StopLoss:0.07102  GradNorm:0.00408  GradNormST:0.02963  AvgTextLen:8.3  AvgSpecLen:44.0  StepTime:0.33  LR:0.000100\n",
      "   | > Step:18/68  GlobalStep:109660  TotalLoss:0.00170  PostnetLoss:0.00083  DecoderLoss:0.00087  StopLoss:0.11088  GradNorm:0.00360  GradNormST:0.05178  AvgTextLen:15.2  AvgSpecLen:64.5  StepTime:0.37  LR:0.000100\n",
      "   | > Step:28/68  GlobalStep:109670  TotalLoss:0.00196  PostnetLoss:0.00095  DecoderLoss:0.00100  StopLoss:0.07372  GradNorm:0.00394  GradNormST:0.02554  AvgTextLen:22.1  AvgSpecLen:85.4  StepTime:0.55  LR:0.000100\n",
      "   | > Step:38/68  GlobalStep:109680  TotalLoss:0.00210  PostnetLoss:0.00102  DecoderLoss:0.00108  StopLoss:0.08406  GradNorm:0.00415  GradNormST:0.04023  AvgTextLen:29.4  AvgSpecLen:104.1  StepTime:0.49  LR:0.000100\n",
      "   | > Step:48/68  GlobalStep:109690  TotalLoss:0.00227  PostnetLoss:0.00110  DecoderLoss:0.00116  StopLoss:0.07236  GradNorm:0.00433  GradNormST:0.02585  AvgTextLen:37.5  AvgSpecLen:123.8  StepTime:0.78  LR:0.000100\n",
      "   | > Step:58/68  GlobalStep:109700  TotalLoss:0.00254  PostnetLoss:0.00124  DecoderLoss:0.00131  StopLoss:0.08655  GradNorm:0.00484  GradNormST:0.03534  AvgTextLen:47.8  AvgSpecLen:158.0  StepTime:0.85  LR:0.000100\n",
      "   | > Step:68/68  GlobalStep:109710  TotalLoss:0.00280  PostnetLoss:0.00135  DecoderLoss:0.00145  StopLoss:0.06360  GradNorm:0.00328  GradNormST:0.04393  AvgTextLen:103.0  AvgSpecLen:308.9  StepTime:1.86  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:109710  AvgTotalLoss:0.09503  AvgPostnetLoss:0.00099  AvgDecoderLoss:0.00105  AvgStopLoss:0.09299  EpochTime:42.20  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.10964   PostnetLoss: 0.00525   DecoderLoss:0.00556  StopLoss: 0.09883  \n",
      "   | > TotalLoss: 0.10986   PostnetLoss: 0.00913   DecoderLoss:0.00956  StopLoss: 0.09118  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00099   Validation Loss: 0.00826\n",
      "\n",
      " > Epoch 590/1000\n",
      "   | > Step:9/68  GlobalStep:109720  TotalLoss:0.00147  PostnetLoss:0.00072  DecoderLoss:0.00075  StopLoss:0.05767  GradNorm:0.00367  GradNormST:0.02015  AvgTextLen:9.0  AvgSpecLen:42.1  StepTime:0.41  LR:0.000100\n",
      "   | > Step:19/68  GlobalStep:109730  TotalLoss:0.00171  PostnetLoss:0.00083  DecoderLoss:0.00088  StopLoss:0.09034  GradNorm:0.00330  GradNormST:0.02521  AvgTextLen:15.5  AvgSpecLen:63.2  StepTime:0.43  LR:0.000100\n",
      "   | > Step:29/68  GlobalStep:109740  TotalLoss:0.00190  PostnetLoss:0.00092  DecoderLoss:0.00097  StopLoss:0.11967  GradNorm:0.00338  GradNormST:0.03160  AvgTextLen:21.8  AvgSpecLen:83.4  StepTime:0.56  LR:0.000100\n",
      "   | > Step:39/68  GlobalStep:109750  TotalLoss:0.00217  PostnetLoss:0.00106  DecoderLoss:0.00111  StopLoss:0.08150  GradNorm:0.00386  GradNormST:0.02790  AvgTextLen:30.2  AvgSpecLen:110.4  StepTime:0.64  LR:0.000100\n",
      "   | > Step:49/68  GlobalStep:109760  TotalLoss:0.00230  PostnetLoss:0.00112  DecoderLoss:0.00118  StopLoss:0.09769  GradNorm:0.00429  GradNormST:0.03381  AvgTextLen:38.2  AvgSpecLen:127.7  StepTime:0.77  LR:0.000100\n",
      "   | > Step:59/68  GlobalStep:109770  TotalLoss:0.00248  PostnetLoss:0.00121  DecoderLoss:0.00128  StopLoss:0.08341  GradNorm:0.00457  GradNormST:0.03504  AvgTextLen:50.5  AvgSpecLen:155.9  StepTime:0.92  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:109779  AvgTotalLoss:0.09032  AvgPostnetLoss:0.00099  AvgDecoderLoss:0.00104  AvgStopLoss:0.08829  EpochTime:43.11  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.10337   PostnetLoss: 0.00525   DecoderLoss:0.00556  StopLoss: 0.09256  \n",
      "   | > TotalLoss: 0.10112   PostnetLoss: 0.00883   DecoderLoss:0.00926  StopLoss: 0.08302  \n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00099   Validation Loss: 0.00808\n",
      "\n",
      " > Epoch 591/1000\n",
      "   | > Step:0/68  GlobalStep:109780  TotalLoss:0.00148  PostnetLoss:0.00071  DecoderLoss:0.00077  StopLoss:0.09763  GradNorm:0.00591  GradNormST:0.04131  AvgTextLen:3.0  AvgSpecLen:25.3  StepTime:0.54  LR:0.000100\n",
      "   | > Step:10/68  GlobalStep:109790  TotalLoss:0.00154  PostnetLoss:0.00075  DecoderLoss:0.00079  StopLoss:0.06570  GradNorm:0.00354  GradNormST:0.02526  AvgTextLen:9.8  AvgSpecLen:48.6  StepTime:0.43  LR:0.000100\n",
      "   | > Step:20/68  GlobalStep:109800  TotalLoss:0.00179  PostnetLoss:0.00087  DecoderLoss:0.00092  StopLoss:0.08938  GradNorm:0.00337  GradNormST:0.03820  AvgTextLen:16.4  AvgSpecLen:64.8  StepTime:0.48  LR:0.000100\n",
      "   | > Step:30/68  GlobalStep:109810  TotalLoss:0.00192  PostnetLoss:0.00094  DecoderLoss:0.00099  StopLoss:0.10045  GradNorm:0.00295  GradNormST:0.03155  AvgTextLen:22.9  AvgSpecLen:85.8  StepTime:0.58  LR:0.000100\n",
      "   | > Step:40/68  GlobalStep:109820  TotalLoss:0.00215  PostnetLoss:0.00104  DecoderLoss:0.00110  StopLoss:0.07787  GradNorm:0.00406  GradNormST:0.02209  AvgTextLen:30.9  AvgSpecLen:105.3  StepTime:0.65  LR:0.000100\n",
      "   | > Step:50/68  GlobalStep:109830  TotalLoss:0.00228  PostnetLoss:0.00111  DecoderLoss:0.00118  StopLoss:0.07974  GradNorm:0.01280  GradNormST:0.03373  AvgTextLen:38.8  AvgSpecLen:132.0  StepTime:0.79  LR:0.000100\n",
      "   | > Step:60/68  GlobalStep:109840  TotalLoss:0.00255  PostnetLoss:0.00124  DecoderLoss:0.00131  StopLoss:0.08760  GradNorm:0.00421  GradNormST:0.04662  AvgTextLen:52.0  AvgSpecLen:172.7  StepTime:0.90  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:109848  AvgTotalLoss:0.08745  AvgPostnetLoss:0.00098  AvgDecoderLoss:0.00104  AvgStopLoss:0.08543  EpochTime:42.70  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.10249   PostnetLoss: 0.00518   DecoderLoss:0.00548  StopLoss: 0.09183  \n",
      "   | > TotalLoss: 0.10427   PostnetLoss: 0.00876   DecoderLoss:0.00918  StopLoss: 0.08633  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00098   Validation Loss: 0.00796\n",
      "\n",
      " > Epoch 592/1000\n",
      "   | > Step:1/68  GlobalStep:109850  TotalLoss:0.00136  PostnetLoss:0.00066  DecoderLoss:0.00070  StopLoss:0.05988  GradNorm:0.00407  GradNormST:0.02992  AvgTextLen:3.7  AvgSpecLen:28.6  StepTime:0.31  LR:0.000100\n",
      "   | > Step:11/68  GlobalStep:109860  TotalLoss:0.00156  PostnetLoss:0.00076  DecoderLoss:0.00080  StopLoss:0.08504  GradNorm:0.00376  GradNormST:0.04119  AvgTextLen:10.3  AvgSpecLen:49.4  StepTime:0.31  LR:0.000100\n",
      "   | > Step:21/68  GlobalStep:109870  TotalLoss:0.00172  PostnetLoss:0.00084  DecoderLoss:0.00088  StopLoss:0.07205  GradNorm:0.00321  GradNormST:0.02263  AvgTextLen:16.6  AvgSpecLen:66.3  StepTime:0.45  LR:0.000100\n",
      "   | > Step:31/68  GlobalStep:109880  TotalLoss:0.00194  PostnetLoss:0.00095  DecoderLoss:0.00100  StopLoss:0.09913  GradNorm:0.00373  GradNormST:0.04637  AvgTextLen:24.1  AvgSpecLen:82.7  StepTime:0.53  LR:0.000100\n",
      "   | > Step:41/68  GlobalStep:109890  TotalLoss:0.00214  PostnetLoss:0.00104  DecoderLoss:0.00110  StopLoss:0.08823  GradNorm:0.00323  GradNormST:0.04923  AvgTextLen:32.3  AvgSpecLen:109.4  StepTime:0.57  LR:0.000100\n",
      "   | > Step:51/68  GlobalStep:109900  TotalLoss:0.00226  PostnetLoss:0.00110  DecoderLoss:0.00116  StopLoss:0.05865  GradNorm:0.00363  GradNormST:0.01990  AvgTextLen:39.2  AvgSpecLen:126.9  StepTime:0.72  LR:0.000100\n",
      "   | > Step:61/68  GlobalStep:109910  TotalLoss:0.00249  PostnetLoss:0.00121  DecoderLoss:0.00128  StopLoss:0.10544  GradNorm:0.00416  GradNormST:0.06209  AvgTextLen:54.6  AvgSpecLen:165.5  StepTime:0.89  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:109917  AvgTotalLoss:0.08699  AvgPostnetLoss:0.00098  AvgDecoderLoss:0.00103  AvgStopLoss:0.08498  EpochTime:41.93  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.10402   PostnetLoss: 0.00517   DecoderLoss:0.00547  StopLoss: 0.09337  \n",
      "   | > TotalLoss: 0.10333   PostnetLoss: 0.00884   DecoderLoss:0.00927  StopLoss: 0.08522  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00098   Validation Loss: 0.00812\n",
      "\n",
      " > Epoch 593/1000\n",
      "   | > Step:2/68  GlobalStep:109920  TotalLoss:0.00128  PostnetLoss:0.00062  DecoderLoss:0.00066  StopLoss:0.07173  GradNorm:0.00360  GradNormST:0.02508  AvgTextLen:4.0  AvgSpecLen:31.3  StepTime:0.36  LR:0.000100\n",
      "   | > Step:12/68  GlobalStep:109930  TotalLoss:0.00152  PostnetLoss:0.00074  DecoderLoss:0.00078  StopLoss:0.08821  GradNorm:0.00344  GradNormST:0.02900  AvgTextLen:11.0  AvgSpecLen:49.3  StepTime:0.36  LR:0.000100\n",
      "   | > Step:22/68  GlobalStep:109940  TotalLoss:0.00180  PostnetLoss:0.00088  DecoderLoss:0.00092  StopLoss:0.08251  GradNorm:0.00339  GradNormST:0.02150  AvgTextLen:18.0  AvgSpecLen:72.0  StepTime:0.48  LR:0.000100\n",
      "   | > Step:32/68  GlobalStep:109950  TotalLoss:0.00196  PostnetLoss:0.00096  DecoderLoss:0.00101  StopLoss:0.09232  GradNorm:0.00290  GradNormST:0.02473  AvgTextLen:24.4  AvgSpecLen:91.5  StepTime:0.47  LR:0.000100\n",
      "   | > Step:42/68  GlobalStep:109960  TotalLoss:0.00217  PostnetLoss:0.00105  DecoderLoss:0.00112  StopLoss:0.07090  GradNorm:0.00319  GradNormST:0.02780  AvgTextLen:33.1  AvgSpecLen:109.7  StepTime:0.63  LR:0.000100\n",
      "   | > Step:52/68  GlobalStep:109970  TotalLoss:0.00234  PostnetLoss:0.00114  DecoderLoss:0.00120  StopLoss:0.07455  GradNorm:0.00321  GradNormST:0.01664  AvgTextLen:42.1  AvgSpecLen:137.5  StepTime:0.85  LR:0.000100\n",
      "   | > Step:62/68  GlobalStep:109980  TotalLoss:0.00257  PostnetLoss:0.00125  DecoderLoss:0.00132  StopLoss:0.05825  GradNorm:0.00398  GradNormST:0.01886  AvgTextLen:54.0  AvgSpecLen:174.7  StepTime:0.75  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:109986  AvgTotalLoss:0.07442  AvgPostnetLoss:0.00098  AvgDecoderLoss:0.00103  AvgStopLoss:0.07240  EpochTime:41.86  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.09612   PostnetLoss: 0.00514   DecoderLoss:0.00544  StopLoss: 0.08554  \n",
      "   | > TotalLoss: 0.10368   PostnetLoss: 0.00884   DecoderLoss:0.00927  StopLoss: 0.08557  \n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00098   Validation Loss: 0.00807\n",
      "\n",
      " > Epoch 594/1000\n",
      "   | > Step:3/68  GlobalStep:109990  TotalLoss:0.00136  PostnetLoss:0.00066  DecoderLoss:0.00070  StopLoss:0.09299  GradNorm:0.00417  GradNormST:0.03860  AvgTextLen:4.7  AvgSpecLen:29.7  StepTime:0.34  LR:0.000100\n",
      "   | > Step:13/68  GlobalStep:110000  TotalLoss:0.00150  PostnetLoss:0.00073  DecoderLoss:0.00077  StopLoss:0.09642  GradNorm:0.00325  GradNormST:0.03190  AvgTextLen:11.5  AvgSpecLen:55.2  StepTime:0.36  LR:0.000100\n",
      " | | > Checkpoint saving : /vm/TTS/models/cartman_models/mozilla-no-loc-fattn-stopnet-sigmoid-loss_masking-August-15-2019_11+26AM-de6aeb9/checkpoint_110000.pth.tar\n",
      "warning: audio amplitude out of range, auto clipped.\n",
      "   | > Step:23/68  GlobalStep:110010  TotalLoss:0.00186  PostnetLoss:0.00091  DecoderLoss:0.00095  StopLoss:0.06342  GradNorm:0.00372  GradNormST:0.01877  AvgTextLen:18.3  AvgSpecLen:72.2  StepTime:0.45  LR:0.000100\n",
      "   | > Step:33/68  GlobalStep:110020  TotalLoss:0.00201  PostnetLoss:0.00098  DecoderLoss:0.00103  StopLoss:0.07351  GradNorm:0.00310  GradNormST:0.03012  AvgTextLen:25.9  AvgSpecLen:88.9  StepTime:0.53  LR:0.000100\n",
      "   | > Step:43/68  GlobalStep:110030  TotalLoss:0.00215  PostnetLoss:0.00105  DecoderLoss:0.00111  StopLoss:0.07770  GradNorm:0.00287  GradNormST:0.02024  AvgTextLen:33.4  AvgSpecLen:115.7  StepTime:0.59  LR:0.000100\n",
      "   | > Step:53/68  GlobalStep:110040  TotalLoss:0.00229  PostnetLoss:0.00111  DecoderLoss:0.00118  StopLoss:0.06258  GradNorm:0.00339  GradNormST:0.01803  AvgTextLen:42.4  AvgSpecLen:137.6  StepTime:0.67  LR:0.000100\n",
      "   | > Step:63/68  GlobalStep:110050  TotalLoss:0.00265  PostnetLoss:0.00129  DecoderLoss:0.00137  StopLoss:0.04984  GradNorm:0.00328  GradNormST:0.02377  AvgTextLen:58.6  AvgSpecLen:185.1  StepTime:0.91  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:110055  AvgTotalLoss:0.07222  AvgPostnetLoss:0.00097  AvgDecoderLoss:0.00103  AvgStopLoss:0.07022  EpochTime:42.46  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.10264   PostnetLoss: 0.00521   DecoderLoss:0.00553  StopLoss: 0.09190  \n",
      "   | > TotalLoss: 0.09641   PostnetLoss: 0.00859   DecoderLoss:0.00903  StopLoss: 0.07879  \n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00097   Validation Loss: 0.00796\n",
      "\n",
      " > Epoch 595/1000\n",
      "   | > Step:4/68  GlobalStep:110060  TotalLoss:0.00137  PostnetLoss:0.00067  DecoderLoss:0.00071  StopLoss:0.10168  GradNorm:0.00347  GradNormST:0.04859  AvgTextLen:4.8  AvgSpecLen:31.1  StepTime:0.29  LR:0.000100\n",
      "   | > Step:14/68  GlobalStep:110070  TotalLoss:0.00159  PostnetLoss:0.00078  DecoderLoss:0.00082  StopLoss:0.07660  GradNorm:0.00361  GradNormST:0.02856  AvgTextLen:12.2  AvgSpecLen:52.0  StepTime:0.34  LR:0.000100\n",
      "   | > Step:24/68  GlobalStep:110080  TotalLoss:0.00190  PostnetLoss:0.00093  DecoderLoss:0.00097  StopLoss:0.07426  GradNorm:0.00380  GradNormST:0.02152  AvgTextLen:19.2  AvgSpecLen:79.0  StepTime:0.50  LR:0.000100\n",
      "   | > Step:34/68  GlobalStep:110090  TotalLoss:0.00201  PostnetLoss:0.00098  DecoderLoss:0.00103  StopLoss:0.07949  GradNorm:0.00297  GradNormST:0.02950  AvgTextLen:26.2  AvgSpecLen:95.9  StepTime:0.56  LR:0.000100\n",
      "   | > Step:44/68  GlobalStep:110100  TotalLoss:0.00217  PostnetLoss:0.00106  DecoderLoss:0.00111  StopLoss:0.04869  GradNorm:0.00280  GradNormST:0.01808  AvgTextLen:33.8  AvgSpecLen:116.3  StepTime:0.65  LR:0.000100\n",
      "   | > Step:54/68  GlobalStep:110110  TotalLoss:0.00235  PostnetLoss:0.00114  DecoderLoss:0.00121  StopLoss:0.06009  GradNorm:0.00322  GradNormST:0.01962  AvgTextLen:42.2  AvgSpecLen:144.9  StepTime:0.79  LR:0.000100\n",
      "   | > Step:64/68  GlobalStep:110120  TotalLoss:0.00243  PostnetLoss:0.00118  DecoderLoss:0.00126  StopLoss:0.05512  GradNorm:0.00264  GradNormST:0.01932  AvgTextLen:60.6  AvgSpecLen:177.6  StepTime:0.83  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:110124  AvgTotalLoss:0.07238  AvgPostnetLoss:0.00097  AvgDecoderLoss:0.00103  AvgStopLoss:0.07039  EpochTime:42.77  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.09954   PostnetLoss: 0.00525   DecoderLoss:0.00557  StopLoss: 0.08872  \n",
      "   | > TotalLoss: 0.10005   PostnetLoss: 0.00867   DecoderLoss:0.00912  StopLoss: 0.08225  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00097   Validation Loss: 0.00804\n",
      "\n",
      " > Epoch 596/1000\n",
      "   | > Step:5/68  GlobalStep:110130  TotalLoss:0.00141  PostnetLoss:0.00068  DecoderLoss:0.00072  StopLoss:0.07588  GradNorm:0.00333  GradNormST:0.02087  AvgTextLen:5.9  AvgSpecLen:33.2  StepTime:0.35  LR:0.000100\n",
      "   | > Step:15/68  GlobalStep:110140  TotalLoss:0.00167  PostnetLoss:0.00082  DecoderLoss:0.00086  StopLoss:0.07503  GradNorm:0.00331  GradNormST:0.02546  AvgTextLen:12.8  AvgSpecLen:59.1  StepTime:0.53  LR:0.000100\n",
      "   | > Step:25/68  GlobalStep:110150  TotalLoss:0.00182  PostnetLoss:0.00089  DecoderLoss:0.00093  StopLoss:0.09037  GradNorm:0.00325  GradNormST:0.02792  AvgTextLen:19.8  AvgSpecLen:73.3  StepTime:0.39  LR:0.000100\n",
      "   | > Step:35/68  GlobalStep:110160  TotalLoss:0.00205  PostnetLoss:0.00100  DecoderLoss:0.00105  StopLoss:0.07386  GradNorm:0.00309  GradNormST:0.04374  AvgTextLen:27.1  AvgSpecLen:104.7  StepTime:0.74  LR:0.000100\n",
      "   | > Step:45/68  GlobalStep:110170  TotalLoss:0.00211  PostnetLoss:0.00103  DecoderLoss:0.00109  StopLoss:0.07891  GradNorm:0.00268  GradNormST:0.02148  AvgTextLen:34.4  AvgSpecLen:118.6  StepTime:0.64  LR:0.000100\n",
      "   | > Step:55/68  GlobalStep:110180  TotalLoss:0.00237  PostnetLoss:0.00115  DecoderLoss:0.00122  StopLoss:0.06354  GradNorm:0.00298  GradNormST:0.02011  AvgTextLen:43.9  AvgSpecLen:145.2  StepTime:0.80  LR:0.000100\n",
      "   | > Step:65/68  GlobalStep:110190  TotalLoss:0.00263  PostnetLoss:0.00127  DecoderLoss:0.00136  StopLoss:0.06255  GradNorm:0.00283  GradNormST:0.01994  AvgTextLen:64.2  AvgSpecLen:207.4  StepTime:1.19  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:110193  AvgTotalLoss:0.07259  AvgPostnetLoss:0.00097  AvgDecoderLoss:0.00102  AvgStopLoss:0.07061  EpochTime:42.72  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.10136   PostnetLoss: 0.00516   DecoderLoss:0.00548  StopLoss: 0.09072  \n",
      "   | > TotalLoss: 0.09816   PostnetLoss: 0.00882   DecoderLoss:0.00925  StopLoss: 0.08009  \n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00097   Validation Loss: 0.00813\n",
      "\n",
      " > Epoch 597/1000\n",
      "   | > Step:6/68  GlobalStep:110200  TotalLoss:0.00136  PostnetLoss:0.00067  DecoderLoss:0.00070  StopLoss:0.08851  GradNorm:0.00373  GradNormST:0.04170  AvgTextLen:6.0  AvgSpecLen:34.8  StepTime:0.39  LR:0.000100\n",
      "   | > Step:16/68  GlobalStep:110210  TotalLoss:0.00167  PostnetLoss:0.00081  DecoderLoss:0.00085  StopLoss:0.07176  GradNorm:0.00328  GradNormST:0.03010  AvgTextLen:13.8  AvgSpecLen:60.6  StepTime:0.36  LR:0.000100\n",
      "   | > Step:26/68  GlobalStep:110220  TotalLoss:0.00186  PostnetLoss:0.00091  DecoderLoss:0.00095  StopLoss:0.06823  GradNorm:0.00357  GradNormST:0.02013  AvgTextLen:20.3  AvgSpecLen:77.5  StepTime:0.45  LR:0.000100\n",
      "   | > Step:36/68  GlobalStep:110230  TotalLoss:0.00202  PostnetLoss:0.00098  DecoderLoss:0.00104  StopLoss:0.07539  GradNorm:0.00323  GradNormST:0.01954  AvgTextLen:28.5  AvgSpecLen:100.2  StepTime:0.47  LR:0.000100\n",
      "   | > Step:46/68  GlobalStep:110240  TotalLoss:0.00216  PostnetLoss:0.00105  DecoderLoss:0.00111  StopLoss:0.07479  GradNorm:0.00286  GradNormST:0.02598  AvgTextLen:35.4  AvgSpecLen:122.6  StepTime:0.77  LR:0.000100\n",
      "   | > Step:56/68  GlobalStep:110250  TotalLoss:0.00236  PostnetLoss:0.00114  DecoderLoss:0.00121  StopLoss:0.06688  GradNorm:0.00273  GradNormST:0.02763  AvgTextLen:45.8  AvgSpecLen:145.4  StepTime:0.96  LR:0.000100\n",
      "   | > Step:66/68  GlobalStep:110260  TotalLoss:0.00281  PostnetLoss:0.00136  DecoderLoss:0.00145  StopLoss:0.05699  GradNorm:0.00262  GradNormST:0.01711  AvgTextLen:71.2  AvgSpecLen:220.8  StepTime:1.05  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:110262  AvgTotalLoss:0.07209  AvgPostnetLoss:0.00097  AvgDecoderLoss:0.00102  AvgStopLoss:0.07010  EpochTime:42.07  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.10233   PostnetLoss: 0.00524   DecoderLoss:0.00556  StopLoss: 0.09153  \n",
      "   | > TotalLoss: 0.10000   PostnetLoss: 0.00870   DecoderLoss:0.00913  StopLoss: 0.08217  \n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00097   Validation Loss: 0.00807\n",
      "\n",
      " > Epoch 598/1000\n",
      "   | > Step:7/68  GlobalStep:110270  TotalLoss:0.00145  PostnetLoss:0.00070  DecoderLoss:0.00074  StopLoss:0.06502  GradNorm:0.00343  GradNormST:0.03460  AvgTextLen:7.4  AvgSpecLen:44.6  StepTime:0.38  LR:0.000100\n",
      "   | > Step:17/68  GlobalStep:110280  TotalLoss:0.00167  PostnetLoss:0.00082  DecoderLoss:0.00086  StopLoss:0.07402  GradNorm:0.00346  GradNormST:0.01876  AvgTextLen:14.7  AvgSpecLen:66.0  StepTime:0.36  LR:0.000100\n",
      "   | > Step:27/68  GlobalStep:110290  TotalLoss:0.00190  PostnetLoss:0.00093  DecoderLoss:0.00097  StopLoss:0.08619  GradNorm:0.00342  GradNormST:0.02824  AvgTextLen:21.0  AvgSpecLen:80.2  StepTime:0.67  LR:0.000100\n",
      "   | > Step:37/68  GlobalStep:110300  TotalLoss:0.00208  PostnetLoss:0.00101  DecoderLoss:0.00107  StopLoss:0.07852  GradNorm:0.00322  GradNormST:0.02099  AvgTextLen:29.3  AvgSpecLen:102.4  StepTime:0.60  LR:0.000100\n",
      "   | > Step:47/68  GlobalStep:110310  TotalLoss:0.00226  PostnetLoss:0.00110  DecoderLoss:0.00116  StopLoss:0.05822  GradNorm:0.00313  GradNormST:0.01840  AvgTextLen:36.7  AvgSpecLen:126.9  StepTime:0.62  LR:0.000100\n",
      "   | > Step:57/68  GlobalStep:110320  TotalLoss:0.00242  PostnetLoss:0.00117  DecoderLoss:0.00125  StopLoss:0.04194  GradNorm:0.00271  GradNormST:0.01242  AvgTextLen:47.1  AvgSpecLen:161.0  StepTime:0.84  LR:0.000100\n",
      "   | > Step:67/68  GlobalStep:110330  TotalLoss:0.00275  PostnetLoss:0.00133  DecoderLoss:0.00142  StopLoss:0.04372  GradNorm:0.00285  GradNormST:0.01686  AvgTextLen:80.6  AvgSpecLen:246.5  StepTime:1.22  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:110331  AvgTotalLoss:0.07219  AvgPostnetLoss:0.00097  AvgDecoderLoss:0.00102  AvgStopLoss:0.07020  EpochTime:42.65  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.09888   PostnetLoss: 0.00523   DecoderLoss:0.00555  StopLoss: 0.08810  \n",
      "   | > TotalLoss: 0.10246   PostnetLoss: 0.00865   DecoderLoss:0.00908  StopLoss: 0.08473  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00097   Validation Loss: 0.00802\n",
      "\n",
      " > Epoch 599/1000\n",
      "   | > Step:8/68  GlobalStep:110340  TotalLoss:0.00153  PostnetLoss:0.00075  DecoderLoss:0.00078  StopLoss:0.06785  GradNorm:0.00399  GradNormST:0.03303  AvgTextLen:8.3  AvgSpecLen:44.0  StepTime:0.28  LR:0.000100\n",
      "   | > Step:18/68  GlobalStep:110350  TotalLoss:0.00166  PostnetLoss:0.00081  DecoderLoss:0.00085  StopLoss:0.06387  GradNorm:0.00321  GradNormST:0.01678  AvgTextLen:15.2  AvgSpecLen:64.5  StepTime:0.32  LR:0.000100\n",
      "   | > Step:28/68  GlobalStep:110360  TotalLoss:0.00190  PostnetLoss:0.00093  DecoderLoss:0.00097  StopLoss:0.07064  GradNorm:0.00390  GradNormST:0.01888  AvgTextLen:22.1  AvgSpecLen:85.4  StepTime:0.59  LR:0.000100\n",
      "   | > Step:38/68  GlobalStep:110370  TotalLoss:0.00203  PostnetLoss:0.00099  DecoderLoss:0.00104  StopLoss:0.06545  GradNorm:0.00349  GradNormST:0.01742  AvgTextLen:29.4  AvgSpecLen:104.1  StepTime:0.61  LR:0.000100\n",
      "   | > Step:48/68  GlobalStep:110380  TotalLoss:0.00222  PostnetLoss:0.00108  DecoderLoss:0.00114  StopLoss:0.05430  GradNorm:0.00303  GradNormST:0.01408  AvgTextLen:37.5  AvgSpecLen:123.8  StepTime:0.55  LR:0.000100\n",
      "   | > Step:58/68  GlobalStep:110390  TotalLoss:0.00241  PostnetLoss:0.00117  DecoderLoss:0.00124  StopLoss:0.06920  GradNorm:0.00274  GradNormST:0.01728  AvgTextLen:47.8  AvgSpecLen:158.0  StepTime:0.85  LR:0.000100\n",
      "   | > Step:68/68  GlobalStep:110400  TotalLoss:0.00274  PostnetLoss:0.00132  DecoderLoss:0.00141  StopLoss:0.05371  GradNorm:0.00273  GradNormST:0.01818  AvgTextLen:103.0  AvgSpecLen:308.9  StepTime:1.86  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:110400  AvgTotalLoss:0.07127  AvgPostnetLoss:0.00096  AvgDecoderLoss:0.00102  AvgStopLoss:0.06929  EpochTime:42.78  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.09914   PostnetLoss: 0.00517   DecoderLoss:0.00548  StopLoss: 0.08849  \n",
      "   | > TotalLoss: 0.10196   PostnetLoss: 0.00866   DecoderLoss:0.00908  StopLoss: 0.08422  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00096   Validation Loss: 0.00804\n",
      "\n",
      " > Epoch 600/1000\n",
      "   | > Step:9/68  GlobalStep:110410  TotalLoss:0.00143  PostnetLoss:0.00070  DecoderLoss:0.00073  StopLoss:0.05562  GradNorm:0.00353  GradNormST:0.02433  AvgTextLen:9.0  AvgSpecLen:42.1  StepTime:0.42  LR:0.000100\n",
      "   | > Step:19/68  GlobalStep:110420  TotalLoss:0.00169  PostnetLoss:0.00082  DecoderLoss:0.00087  StopLoss:0.08732  GradNorm:0.00321  GradNormST:0.02536  AvgTextLen:15.5  AvgSpecLen:63.2  StepTime:0.41  LR:0.000100\n",
      "   | > Step:29/68  GlobalStep:110430  TotalLoss:0.00186  PostnetLoss:0.00091  DecoderLoss:0.00096  StopLoss:0.07898  GradNorm:0.00355  GradNormST:0.02057  AvgTextLen:21.8  AvgSpecLen:83.4  StepTime:0.56  LR:0.000100\n",
      "   | > Step:39/68  GlobalStep:110440  TotalLoss:0.00210  PostnetLoss:0.00102  DecoderLoss:0.00108  StopLoss:0.05876  GradNorm:0.00349  GradNormST:0.02034  AvgTextLen:30.2  AvgSpecLen:110.4  StepTime:0.71  LR:0.000100\n",
      "   | > Step:49/68  GlobalStep:110450  TotalLoss:0.00226  PostnetLoss:0.00110  DecoderLoss:0.00116  StopLoss:0.06986  GradNorm:0.00368  GradNormST:0.01864  AvgTextLen:38.2  AvgSpecLen:127.7  StepTime:0.71  LR:0.000100\n",
      "   | > Step:59/68  GlobalStep:110460  TotalLoss:0.00240  PostnetLoss:0.00116  DecoderLoss:0.00124  StopLoss:0.05190  GradNorm:0.00299  GradNormST:0.01565  AvgTextLen:50.5  AvgSpecLen:155.9  StepTime:0.87  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:110469  AvgTotalLoss:0.07141  AvgPostnetLoss:0.00096  AvgDecoderLoss:0.00102  AvgStopLoss:0.06943  EpochTime:41.61  AvgStepTime:0.60\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.10816   PostnetLoss: 0.00520   DecoderLoss:0.00552  StopLoss: 0.09745  \n",
      "   | > TotalLoss: 0.09865   PostnetLoss: 0.00871   DecoderLoss:0.00914  StopLoss: 0.08079  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00096   Validation Loss: 0.00808\n",
      "\n",
      " > Epoch 601/1000\n",
      "   | > Step:0/68  GlobalStep:110470  TotalLoss:0.00142  PostnetLoss:0.00069  DecoderLoss:0.00074  StopLoss:0.07737  GradNorm:0.00447  GradNormST:0.04149  AvgTextLen:3.0  AvgSpecLen:25.3  StepTime:0.54  LR:0.000100\n",
      "   | > Step:10/68  GlobalStep:110480  TotalLoss:0.00152  PostnetLoss:0.00074  DecoderLoss:0.00078  StopLoss:0.05313  GradNorm:0.00332  GradNormST:0.02056  AvgTextLen:9.8  AvgSpecLen:48.6  StepTime:0.61  LR:0.000100\n",
      "   | > Step:20/68  GlobalStep:110490  TotalLoss:0.00175  PostnetLoss:0.00085  DecoderLoss:0.00090  StopLoss:0.04629  GradNorm:0.00317  GradNormST:0.01704  AvgTextLen:16.4  AvgSpecLen:64.8  StepTime:0.48  LR:0.000100\n",
      "   | > Step:30/68  GlobalStep:110500  TotalLoss:0.00187  PostnetLoss:0.00091  DecoderLoss:0.00096  StopLoss:0.07390  GradNorm:0.00307  GradNormST:0.01741  AvgTextLen:22.9  AvgSpecLen:85.8  StepTime:0.56  LR:0.000100\n",
      "   | > Step:40/68  GlobalStep:110510  TotalLoss:0.00208  PostnetLoss:0.00101  DecoderLoss:0.00107  StopLoss:0.06017  GradNorm:0.00381  GradNormST:0.01721  AvgTextLen:30.9  AvgSpecLen:105.3  StepTime:0.63  LR:0.000100\n",
      "   | > Step:50/68  GlobalStep:110520  TotalLoss:0.00226  PostnetLoss:0.00110  DecoderLoss:0.00116  StopLoss:0.06606  GradNorm:0.00377  GradNormST:0.01648  AvgTextLen:38.8  AvgSpecLen:132.0  StepTime:0.79  LR:0.000100\n",
      "   | > Step:60/68  GlobalStep:110530  TotalLoss:0.00246  PostnetLoss:0.00119  DecoderLoss:0.00127  StopLoss:0.05066  GradNorm:0.00335  GradNormST:0.01561  AvgTextLen:52.0  AvgSpecLen:172.7  StepTime:0.93  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:110538  AvgTotalLoss:0.06790  AvgPostnetLoss:0.00096  AvgDecoderLoss:0.00101  AvgStopLoss:0.06593  EpochTime:42.76  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.12034   PostnetLoss: 0.00526   DecoderLoss:0.00557  StopLoss: 0.10950  \n",
      "   | > TotalLoss: 0.10280   PostnetLoss: 0.00871   DecoderLoss:0.00914  StopLoss: 0.08495  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00096   Validation Loss: 0.00805\n",
      "\n",
      " > Epoch 602/1000\n",
      "   | > Step:1/68  GlobalStep:110540  TotalLoss:0.00136  PostnetLoss:0.00066  DecoderLoss:0.00070  StopLoss:0.04956  GradNorm:0.00442  GradNormST:0.02336  AvgTextLen:3.7  AvgSpecLen:28.6  StepTime:0.45  LR:0.000100\n",
      "   | > Step:11/68  GlobalStep:110550  TotalLoss:0.00152  PostnetLoss:0.00075  DecoderLoss:0.00078  StopLoss:0.06066  GradNorm:0.00337  GradNormST:0.02064  AvgTextLen:10.3  AvgSpecLen:49.4  StepTime:0.39  LR:0.000100\n",
      "   | > Step:21/68  GlobalStep:110560  TotalLoss:0.00172  PostnetLoss:0.00084  DecoderLoss:0.00088  StopLoss:0.07288  GradNorm:0.00315  GradNormST:0.02203  AvgTextLen:16.6  AvgSpecLen:66.3  StepTime:0.42  LR:0.000100\n",
      "   | > Step:31/68  GlobalStep:110570  TotalLoss:0.00188  PostnetLoss:0.00092  DecoderLoss:0.00097  StopLoss:0.07444  GradNorm:0.00310  GradNormST:0.02117  AvgTextLen:24.1  AvgSpecLen:82.7  StepTime:0.43  LR:0.000100\n",
      "   | > Step:41/68  GlobalStep:110580  TotalLoss:0.00213  PostnetLoss:0.00104  DecoderLoss:0.00109  StopLoss:0.04194  GradNorm:0.00354  GradNormST:0.01792  AvgTextLen:32.3  AvgSpecLen:109.4  StepTime:0.64  LR:0.000100\n",
      "   | > Step:51/68  GlobalStep:110590  TotalLoss:0.00222  PostnetLoss:0.00108  DecoderLoss:0.00114  StopLoss:0.06518  GradNorm:0.00364  GradNormST:0.01450  AvgTextLen:39.2  AvgSpecLen:126.9  StepTime:0.61  LR:0.000100\n",
      "   | > Step:61/68  GlobalStep:110600  TotalLoss:0.00245  PostnetLoss:0.00119  DecoderLoss:0.00126  StopLoss:0.06251  GradNorm:0.00362  GradNormST:0.01887  AvgTextLen:54.6  AvgSpecLen:165.5  StepTime:0.91  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:110607  AvgTotalLoss:0.06728  AvgPostnetLoss:0.00096  AvgDecoderLoss:0.00101  AvgStopLoss:0.06531  EpochTime:42.00  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.10702   PostnetLoss: 0.00520   DecoderLoss:0.00551  StopLoss: 0.09631  \n",
      "   | > TotalLoss: 0.10506   PostnetLoss: 0.00883   DecoderLoss:0.00928  StopLoss: 0.08695  \n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00096   Validation Loss: 0.00811\n",
      "\n",
      " > Epoch 603/1000\n",
      "   | > Step:2/68  GlobalStep:110610  TotalLoss:0.00132  PostnetLoss:0.00064  DecoderLoss:0.00068  StopLoss:0.06451  GradNorm:0.00417  GradNormST:0.02368  AvgTextLen:4.0  AvgSpecLen:31.3  StepTime:0.52  LR:0.000100\n",
      "   | > Step:12/68  GlobalStep:110620  TotalLoss:0.00149  PostnetLoss:0.00072  DecoderLoss:0.00076  StopLoss:0.08153  GradNorm:0.00307  GradNormST:0.03546  AvgTextLen:11.0  AvgSpecLen:49.3  StepTime:0.39  LR:0.000100\n",
      "   | > Step:22/68  GlobalStep:110630  TotalLoss:0.00177  PostnetLoss:0.00086  DecoderLoss:0.00091  StopLoss:0.08243  GradNorm:0.00317  GradNormST:0.01749  AvgTextLen:18.0  AvgSpecLen:72.0  StepTime:0.46  LR:0.000100\n",
      "   | > Step:32/68  GlobalStep:110640  TotalLoss:0.00192  PostnetLoss:0.00094  DecoderLoss:0.00099  StopLoss:0.08434  GradNorm:0.00311  GradNormST:0.02126  AvgTextLen:24.4  AvgSpecLen:91.5  StepTime:0.59  LR:0.000100\n",
      "   | > Step:42/68  GlobalStep:110650  TotalLoss:0.00211  PostnetLoss:0.00103  DecoderLoss:0.00109  StopLoss:0.07133  GradNorm:0.00349  GradNormST:0.02641  AvgTextLen:33.1  AvgSpecLen:109.7  StepTime:0.57  LR:0.000100\n",
      "   | > Step:52/68  GlobalStep:110660  TotalLoss:0.00231  PostnetLoss:0.00113  DecoderLoss:0.00119  StopLoss:0.07150  GradNorm:0.00399  GradNormST:0.01562  AvgTextLen:42.1  AvgSpecLen:137.5  StepTime:0.85  LR:0.000100\n",
      "   | > Step:62/68  GlobalStep:110670  TotalLoss:0.00249  PostnetLoss:0.00121  DecoderLoss:0.00128  StopLoss:0.06985  GradNorm:0.00336  GradNormST:0.01956  AvgTextLen:54.0  AvgSpecLen:174.7  StepTime:0.75  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:110676  AvgTotalLoss:0.07129  AvgPostnetLoss:0.00096  AvgDecoderLoss:0.00101  AvgStopLoss:0.06932  EpochTime:42.72  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.09580   PostnetLoss: 0.00530   DecoderLoss:0.00561  StopLoss: 0.08488  \n",
      "   | > TotalLoss: 0.10572   PostnetLoss: 0.00886   DecoderLoss:0.00930  StopLoss: 0.08756  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00096   Validation Loss: 0.00820\n",
      "\n",
      " > Epoch 604/1000\n",
      "   | > Step:3/68  GlobalStep:110680  TotalLoss:0.00134  PostnetLoss:0.00065  DecoderLoss:0.00069  StopLoss:0.10282  GradNorm:0.00378  GradNormST:0.03898  AvgTextLen:4.7  AvgSpecLen:29.7  StepTime:0.31  LR:0.000100\n",
      "   | > Step:13/68  GlobalStep:110690  TotalLoss:0.00153  PostnetLoss:0.00074  DecoderLoss:0.00078  StopLoss:0.09338  GradNorm:0.00333  GradNormST:0.02927  AvgTextLen:11.5  AvgSpecLen:55.2  StepTime:0.43  LR:0.000100\n",
      "   | > Step:23/68  GlobalStep:110700  TotalLoss:0.00178  PostnetLoss:0.00087  DecoderLoss:0.00091  StopLoss:0.06390  GradNorm:0.00313  GradNormST:0.02225  AvgTextLen:18.3  AvgSpecLen:72.2  StepTime:0.42  LR:0.000100\n",
      "   | > Step:33/68  GlobalStep:110710  TotalLoss:0.00197  PostnetLoss:0.00096  DecoderLoss:0.00101  StopLoss:0.07567  GradNorm:0.00346  GradNormST:0.02222  AvgTextLen:25.9  AvgSpecLen:88.9  StepTime:0.50  LR:0.000100\n",
      "   | > Step:43/68  GlobalStep:110720  TotalLoss:0.00215  PostnetLoss:0.00104  DecoderLoss:0.00111  StopLoss:0.06840  GradNorm:0.00345  GradNormST:0.01820  AvgTextLen:33.4  AvgSpecLen:115.7  StepTime:0.51  LR:0.000100\n",
      "   | > Step:53/68  GlobalStep:110730  TotalLoss:0.00230  PostnetLoss:0.00112  DecoderLoss:0.00118  StopLoss:0.05889  GradNorm:0.00395  GradNormST:0.01538  AvgTextLen:42.4  AvgSpecLen:137.6  StepTime:0.80  LR:0.000100\n",
      "   | > Step:63/68  GlobalStep:110740  TotalLoss:0.00257  PostnetLoss:0.00125  DecoderLoss:0.00132  StopLoss:0.05620  GradNorm:0.00344  GradNormST:0.02446  AvgTextLen:58.6  AvgSpecLen:185.1  StepTime:0.93  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:110745  AvgTotalLoss:0.06897  AvgPostnetLoss:0.00096  AvgDecoderLoss:0.00101  AvgStopLoss:0.06700  EpochTime:42.91  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.10249   PostnetLoss: 0.00532   DecoderLoss:0.00565  StopLoss: 0.09152  \n",
      "   | > TotalLoss: 0.10325   PostnetLoss: 0.00889   DecoderLoss:0.00934  StopLoss: 0.08503  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00096   Validation Loss: 0.00818\n",
      "\n",
      " > Epoch 605/1000\n",
      "   | > Step:4/68  GlobalStep:110750  TotalLoss:0.00135  PostnetLoss:0.00065  DecoderLoss:0.00069  StopLoss:0.10342  GradNorm:0.00361  GradNormST:0.04345  AvgTextLen:4.8  AvgSpecLen:31.1  StepTime:0.26  LR:0.000100\n",
      "   | > Step:14/68  GlobalStep:110760  TotalLoss:0.00157  PostnetLoss:0.00076  DecoderLoss:0.00080  StopLoss:0.06449  GradNorm:0.00333  GradNormST:0.03105  AvgTextLen:12.2  AvgSpecLen:52.0  StepTime:0.31  LR:0.000100\n",
      "   | > Step:24/68  GlobalStep:110770  TotalLoss:0.00187  PostnetLoss:0.00091  DecoderLoss:0.00096  StopLoss:0.05644  GradNorm:0.00319  GradNormST:0.02369  AvgTextLen:19.2  AvgSpecLen:79.0  StepTime:0.52  LR:0.000100\n",
      "   | > Step:34/68  GlobalStep:110780  TotalLoss:0.00199  PostnetLoss:0.00097  DecoderLoss:0.00102  StopLoss:0.06100  GradNorm:0.00296  GradNormST:0.02144  AvgTextLen:26.2  AvgSpecLen:95.9  StepTime:0.64  LR:0.000100\n",
      "   | > Step:44/68  GlobalStep:110790  TotalLoss:0.00216  PostnetLoss:0.00105  DecoderLoss:0.00111  StopLoss:0.06107  GradNorm:0.00309  GradNormST:0.02428  AvgTextLen:33.8  AvgSpecLen:116.3  StepTime:0.65  LR:0.000100\n",
      "   | > Step:54/68  GlobalStep:110800  TotalLoss:0.00233  PostnetLoss:0.00113  DecoderLoss:0.00120  StopLoss:0.08279  GradNorm:0.00381  GradNormST:0.01868  AvgTextLen:42.2  AvgSpecLen:144.9  StepTime:0.85  LR:0.000100\n",
      "   | > Step:64/68  GlobalStep:110810  TotalLoss:0.00243  PostnetLoss:0.00118  DecoderLoss:0.00125  StopLoss:0.05487  GradNorm:0.00324  GradNormST:0.01537  AvgTextLen:60.6  AvgSpecLen:177.6  StepTime:0.80  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:110814  AvgTotalLoss:0.07007  AvgPostnetLoss:0.00096  AvgDecoderLoss:0.00101  AvgStopLoss:0.06811  EpochTime:42.58  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.09858   PostnetLoss: 0.00535   DecoderLoss:0.00567  StopLoss: 0.08756  \n",
      "   | > TotalLoss: 0.10252   PostnetLoss: 0.00891   DecoderLoss:0.00936  StopLoss: 0.08425  \n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00096   Validation Loss: 0.00817\n",
      "\n",
      " > Epoch 606/1000\n",
      "   | > Step:5/68  GlobalStep:110820  TotalLoss:0.00142  PostnetLoss:0.00069  DecoderLoss:0.00073  StopLoss:0.07476  GradNorm:0.00339  GradNormST:0.02339  AvgTextLen:5.9  AvgSpecLen:33.2  StepTime:0.33  LR:0.000100\n",
      "   | > Step:15/68  GlobalStep:110830  TotalLoss:0.00164  PostnetLoss:0.00080  DecoderLoss:0.00084  StopLoss:0.07474  GradNorm:0.00338  GradNormST:0.02792  AvgTextLen:12.8  AvgSpecLen:59.1  StepTime:0.55  LR:0.000100\n",
      "   | > Step:25/68  GlobalStep:110840  TotalLoss:0.00181  PostnetLoss:0.00088  DecoderLoss:0.00093  StopLoss:0.07246  GradNorm:0.00316  GradNormST:0.02407  AvgTextLen:19.8  AvgSpecLen:73.3  StepTime:0.44  LR:0.000100\n",
      "   | > Step:35/68  GlobalStep:110850  TotalLoss:0.00201  PostnetLoss:0.00098  DecoderLoss:0.00103  StopLoss:0.07202  GradNorm:0.00283  GradNormST:0.03442  AvgTextLen:27.1  AvgSpecLen:104.7  StepTime:0.80  LR:0.000100\n",
      "   | > Step:45/68  GlobalStep:110860  TotalLoss:0.00211  PostnetLoss:0.00102  DecoderLoss:0.00108  StopLoss:0.07186  GradNorm:0.00297  GradNormST:0.01995  AvgTextLen:34.4  AvgSpecLen:118.6  StepTime:0.65  LR:0.000100\n",
      "   | > Step:55/68  GlobalStep:110870  TotalLoss:0.00237  PostnetLoss:0.00115  DecoderLoss:0.00122  StopLoss:0.05772  GradNorm:0.00345  GradNormST:0.01634  AvgTextLen:43.9  AvgSpecLen:145.2  StepTime:0.80  LR:0.000100\n",
      "   | > Step:65/68  GlobalStep:110880  TotalLoss:0.00260  PostnetLoss:0.00126  DecoderLoss:0.00134  StopLoss:0.05281  GradNorm:0.00257  GradNormST:0.01363  AvgTextLen:64.2  AvgSpecLen:207.4  StepTime:1.19  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:110883  AvgTotalLoss:0.06698  AvgPostnetLoss:0.00095  AvgDecoderLoss:0.00101  AvgStopLoss:0.06502  EpochTime:42.64  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.09907   PostnetLoss: 0.00527   DecoderLoss:0.00559  StopLoss: 0.08821  \n",
      "   | > TotalLoss: 0.09967   PostnetLoss: 0.00869   DecoderLoss:0.00912  StopLoss: 0.08187  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00095   Validation Loss: 0.00798\n",
      "\n",
      " > Epoch 607/1000\n",
      "   | > Step:6/68  GlobalStep:110890  TotalLoss:0.00136  PostnetLoss:0.00066  DecoderLoss:0.00070  StopLoss:0.05766  GradNorm:0.00358  GradNormST:0.02841  AvgTextLen:6.0  AvgSpecLen:34.8  StepTime:0.38  LR:0.000100\n",
      "   | > Step:16/68  GlobalStep:110900  TotalLoss:0.00167  PostnetLoss:0.00082  DecoderLoss:0.00086  StopLoss:0.04003  GradNorm:0.00343  GradNormST:0.02075  AvgTextLen:13.8  AvgSpecLen:60.6  StepTime:0.39  LR:0.000100\n",
      "   | > Step:26/68  GlobalStep:110910  TotalLoss:0.00186  PostnetLoss:0.00091  DecoderLoss:0.00095  StopLoss:0.05698  GradNorm:0.00345  GradNormST:0.01598  AvgTextLen:20.3  AvgSpecLen:77.5  StepTime:0.50  LR:0.000100\n",
      "   | > Step:36/68  GlobalStep:110920  TotalLoss:0.00199  PostnetLoss:0.00097  DecoderLoss:0.00102  StopLoss:0.06300  GradNorm:0.00277  GradNormST:0.01657  AvgTextLen:28.5  AvgSpecLen:100.2  StepTime:0.58  LR:0.000100\n",
      "   | > Step:46/68  GlobalStep:110930  TotalLoss:0.00213  PostnetLoss:0.00104  DecoderLoss:0.00110  StopLoss:0.06803  GradNorm:0.00302  GradNormST:0.02600  AvgTextLen:35.4  AvgSpecLen:122.6  StepTime:0.77  LR:0.000100\n",
      "   | > Step:56/68  GlobalStep:110940  TotalLoss:0.00234  PostnetLoss:0.00114  DecoderLoss:0.00121  StopLoss:0.05719  GradNorm:0.00329  GradNormST:0.01924  AvgTextLen:45.8  AvgSpecLen:145.4  StepTime:1.00  LR:0.000100\n",
      "   | > Step:66/68  GlobalStep:110950  TotalLoss:0.00273  PostnetLoss:0.00132  DecoderLoss:0.00141  StopLoss:0.05753  GradNorm:0.00294  GradNormST:0.01578  AvgTextLen:71.2  AvgSpecLen:220.8  StepTime:1.06  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:110952  AvgTotalLoss:0.06585  AvgPostnetLoss:0.00096  AvgDecoderLoss:0.00101  AvgStopLoss:0.06389  EpochTime:42.13  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.09583   PostnetLoss: 0.00529   DecoderLoss:0.00559  StopLoss: 0.08494  \n",
      "   | > TotalLoss: 0.10013   PostnetLoss: 0.00888   DecoderLoss:0.00931  StopLoss: 0.08193  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00096   Validation Loss: 0.00813\n",
      "\n",
      " > Epoch 608/1000\n",
      "   | > Step:7/68  GlobalStep:110960  TotalLoss:0.00146  PostnetLoss:0.00071  DecoderLoss:0.00075  StopLoss:0.06110  GradNorm:0.00368  GradNormST:0.02782  AvgTextLen:7.4  AvgSpecLen:44.6  StepTime:0.33  LR:0.000100\n",
      "   | > Step:17/68  GlobalStep:110970  TotalLoss:0.00164  PostnetLoss:0.00080  DecoderLoss:0.00084  StopLoss:0.08138  GradNorm:0.00347  GradNormST:0.01908  AvgTextLen:14.7  AvgSpecLen:66.0  StepTime:0.39  LR:0.000100\n",
      "   | > Step:27/68  GlobalStep:110980  TotalLoss:0.00189  PostnetLoss:0.00092  DecoderLoss:0.00097  StopLoss:0.07216  GradNorm:0.00349  GradNormST:0.01631  AvgTextLen:21.0  AvgSpecLen:80.2  StepTime:0.64  LR:0.000100\n",
      "   | > Step:37/68  GlobalStep:110990  TotalLoss:0.00204  PostnetLoss:0.00099  DecoderLoss:0.00104  StopLoss:0.06870  GradNorm:0.00300  GradNormST:0.01488  AvgTextLen:29.3  AvgSpecLen:102.4  StepTime:0.48  LR:0.000100\n",
      "   | > Step:47/68  GlobalStep:111000  TotalLoss:0.00220  PostnetLoss:0.00107  DecoderLoss:0.00113  StopLoss:0.04949  GradNorm:0.00278  GradNormST:0.01432  AvgTextLen:36.7  AvgSpecLen:126.9  StepTime:0.73  LR:0.000100\n",
      " | | > Checkpoint saving : /vm/TTS/models/cartman_models/mozilla-no-loc-fattn-stopnet-sigmoid-loss_masking-August-15-2019_11+26AM-de6aeb9/checkpoint_111000.pth.tar\n",
      "warning: audio amplitude out of range, auto clipped.\n",
      "   | > Step:57/68  GlobalStep:111010  TotalLoss:0.00240  PostnetLoss:0.00117  DecoderLoss:0.00124  StopLoss:0.05030  GradNorm:0.00307  GradNormST:0.01149  AvgTextLen:47.1  AvgSpecLen:161.0  StepTime:0.81  LR:0.000100\n",
      "   | > Step:67/68  GlobalStep:111020  TotalLoss:0.00274  PostnetLoss:0.00132  DecoderLoss:0.00141  StopLoss:0.05142  GradNorm:0.00259  GradNormST:0.01467  AvgTextLen:80.6  AvgSpecLen:246.5  StepTime:1.23  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:111021  AvgTotalLoss:0.06450  AvgPostnetLoss:0.00096  AvgDecoderLoss:0.00101  AvgStopLoss:0.06253  EpochTime:42.92  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.09539   PostnetLoss: 0.00539   DecoderLoss:0.00570  StopLoss: 0.08429  \n",
      "   | > TotalLoss: 0.09784   PostnetLoss: 0.00864   DecoderLoss:0.00907  StopLoss: 0.08013  \n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00096   Validation Loss: 0.00802\n",
      "\n",
      " > Epoch 609/1000\n",
      "   | > Step:8/68  GlobalStep:111030  TotalLoss:0.00147  PostnetLoss:0.00072  DecoderLoss:0.00075  StopLoss:0.06733  GradNorm:0.00344  GradNormST:0.02554  AvgTextLen:8.3  AvgSpecLen:44.0  StepTime:0.24  LR:0.000100\n",
      "   | > Step:18/68  GlobalStep:111040  TotalLoss:0.00166  PostnetLoss:0.00081  DecoderLoss:0.00085  StopLoss:0.06902  GradNorm:0.00324  GradNormST:0.01805  AvgTextLen:15.2  AvgSpecLen:64.5  StepTime:0.40  LR:0.000100\n",
      "   | > Step:28/68  GlobalStep:111050  TotalLoss:0.00185  PostnetLoss:0.00090  DecoderLoss:0.00095  StopLoss:0.04963  GradNorm:0.00302  GradNormST:0.01513  AvgTextLen:22.1  AvgSpecLen:85.4  StepTime:0.57  LR:0.000100\n",
      "   | > Step:38/68  GlobalStep:111060  TotalLoss:0.00200  PostnetLoss:0.00098  DecoderLoss:0.00103  StopLoss:0.06786  GradNorm:0.00317  GradNormST:0.02053  AvgTextLen:29.4  AvgSpecLen:104.1  StepTime:0.64  LR:0.000100\n",
      "   | > Step:48/68  GlobalStep:111070  TotalLoss:0.00220  PostnetLoss:0.00107  DecoderLoss:0.00113  StopLoss:0.05650  GradNorm:0.00284  GradNormST:0.01606  AvgTextLen:37.5  AvgSpecLen:123.8  StepTime:0.55  LR:0.000100\n",
      "   | > Step:58/68  GlobalStep:111080  TotalLoss:0.00241  PostnetLoss:0.00117  DecoderLoss:0.00124  StopLoss:0.06033  GradNorm:0.00290  GradNormST:0.01772  AvgTextLen:47.8  AvgSpecLen:158.0  StepTime:0.82  LR:0.000100\n",
      "   | > Step:68/68  GlobalStep:111090  TotalLoss:0.00272  PostnetLoss:0.00131  DecoderLoss:0.00141  StopLoss:0.05109  GradNorm:0.00258  GradNormST:0.02115  AvgTextLen:103.0  AvgSpecLen:308.9  StepTime:1.87  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:111090  AvgTotalLoss:0.06393  AvgPostnetLoss:0.00095  AvgDecoderLoss:0.00101  AvgStopLoss:0.06197  EpochTime:42.18  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.09606   PostnetLoss: 0.00542   DecoderLoss:0.00573  StopLoss: 0.08491  \n",
      "   | > TotalLoss: 0.10033   PostnetLoss: 0.00889   DecoderLoss:0.00934  StopLoss: 0.08210  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00095   Validation Loss: 0.00810\n",
      "\n",
      " > Epoch 610/1000\n",
      "   | > Step:9/68  GlobalStep:111100  TotalLoss:0.00142  PostnetLoss:0.00069  DecoderLoss:0.00072  StopLoss:0.05734  GradNorm:0.00322  GradNormST:0.02246  AvgTextLen:9.0  AvgSpecLen:42.1  StepTime:0.41  LR:0.000100\n",
      "   | > Step:19/68  GlobalStep:111110  TotalLoss:0.00165  PostnetLoss:0.00081  DecoderLoss:0.00085  StopLoss:0.07087  GradNorm:0.00307  GradNormST:0.02611  AvgTextLen:15.5  AvgSpecLen:63.2  StepTime:0.34  LR:0.000100\n",
      "   | > Step:29/68  GlobalStep:111120  TotalLoss:0.00186  PostnetLoss:0.00091  DecoderLoss:0.00096  StopLoss:0.08019  GradNorm:0.00341  GradNormST:0.01713  AvgTextLen:21.8  AvgSpecLen:83.4  StepTime:0.58  LR:0.000100\n",
      "   | > Step:39/68  GlobalStep:111130  TotalLoss:0.00204  PostnetLoss:0.00099  DecoderLoss:0.00105  StopLoss:0.05105  GradNorm:0.00309  GradNormST:0.01604  AvgTextLen:30.2  AvgSpecLen:110.4  StepTime:0.68  LR:0.000100\n",
      "   | > Step:49/68  GlobalStep:111140  TotalLoss:0.00220  PostnetLoss:0.00107  DecoderLoss:0.00113  StopLoss:0.06807  GradNorm:0.00278  GradNormST:0.01862  AvgTextLen:38.2  AvgSpecLen:127.7  StepTime:0.76  LR:0.000100\n",
      "   | > Step:59/68  GlobalStep:111150  TotalLoss:0.00238  PostnetLoss:0.00115  DecoderLoss:0.00122  StopLoss:0.06042  GradNorm:0.00259  GradNormST:0.01541  AvgTextLen:50.5  AvgSpecLen:155.9  StepTime:0.71  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:111159  AvgTotalLoss:0.06462  AvgPostnetLoss:0.00095  AvgDecoderLoss:0.00100  AvgStopLoss:0.06267  EpochTime:42.18  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.09250   PostnetLoss: 0.00529   DecoderLoss:0.00561  StopLoss: 0.08160  \n",
      "   | > TotalLoss: 0.09651   PostnetLoss: 0.00875   DecoderLoss:0.00919  StopLoss: 0.07857  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00095   Validation Loss: 0.00805\n",
      "\n",
      " > Epoch 611/1000\n",
      "   | > Step:0/68  GlobalStep:111160  TotalLoss:0.00140  PostnetLoss:0.00067  DecoderLoss:0.00073  StopLoss:0.08249  GradNorm:0.00495  GradNormST:0.04085  AvgTextLen:3.0  AvgSpecLen:25.3  StepTime:0.56  LR:0.000100\n",
      "   | > Step:10/68  GlobalStep:111170  TotalLoss:0.00151  PostnetLoss:0.00074  DecoderLoss:0.00078  StopLoss:0.05405  GradNorm:0.00328  GradNormST:0.02392  AvgTextLen:9.8  AvgSpecLen:48.6  StepTime:0.67  LR:0.000100\n",
      "   | > Step:20/68  GlobalStep:111180  TotalLoss:0.00173  PostnetLoss:0.00085  DecoderLoss:0.00089  StopLoss:0.05426  GradNorm:0.00310  GradNormST:0.02205  AvgTextLen:16.4  AvgSpecLen:64.8  StepTime:0.51  LR:0.000100\n",
      "   | > Step:30/68  GlobalStep:111190  TotalLoss:0.00189  PostnetLoss:0.00092  DecoderLoss:0.00097  StopLoss:0.07202  GradNorm:0.00321  GradNormST:0.01986  AvgTextLen:22.9  AvgSpecLen:85.8  StepTime:0.60  LR:0.000100\n",
      "   | > Step:40/68  GlobalStep:111200  TotalLoss:0.00206  PostnetLoss:0.00100  DecoderLoss:0.00106  StopLoss:0.07165  GradNorm:0.00307  GradNormST:0.02531  AvgTextLen:30.9  AvgSpecLen:105.3  StepTime:0.60  LR:0.000100\n",
      "   | > Step:50/68  GlobalStep:111210  TotalLoss:0.00221  PostnetLoss:0.00107  DecoderLoss:0.00114  StopLoss:0.06234  GradNorm:0.00285  GradNormST:0.01519  AvgTextLen:38.8  AvgSpecLen:132.0  StepTime:0.67  LR:0.000100\n",
      "   | > Step:60/68  GlobalStep:111220  TotalLoss:0.00245  PostnetLoss:0.00119  DecoderLoss:0.00126  StopLoss:0.04572  GradNorm:0.00273  GradNormST:0.01383  AvgTextLen:52.0  AvgSpecLen:172.7  StepTime:0.78  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:111228  AvgTotalLoss:0.06518  AvgPostnetLoss:0.00095  AvgDecoderLoss:0.00100  AvgStopLoss:0.06323  EpochTime:43.10  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.09104   PostnetLoss: 0.00530   DecoderLoss:0.00561  StopLoss: 0.08013  \n",
      "   | > TotalLoss: 0.09478   PostnetLoss: 0.00870   DecoderLoss:0.00915  StopLoss: 0.07693  \n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00095   Validation Loss: 0.00806\n",
      "\n",
      " > Epoch 612/1000\n",
      "   | > Step:1/68  GlobalStep:111230  TotalLoss:0.00139  PostnetLoss:0.00067  DecoderLoss:0.00071  StopLoss:0.05351  GradNorm:0.00444  GradNormST:0.02945  AvgTextLen:3.7  AvgSpecLen:28.6  StepTime:0.48  LR:0.000100\n",
      "   | > Step:11/68  GlobalStep:111240  TotalLoss:0.00150  PostnetLoss:0.00073  DecoderLoss:0.00077  StopLoss:0.04708  GradNorm:0.00374  GradNormST:0.01952  AvgTextLen:10.3  AvgSpecLen:49.4  StepTime:0.39  LR:0.000100\n",
      "   | > Step:21/68  GlobalStep:111250  TotalLoss:0.00169  PostnetLoss:0.00082  DecoderLoss:0.00086  StopLoss:0.06153  GradNorm:0.00308  GradNormST:0.01984  AvgTextLen:16.6  AvgSpecLen:66.3  StepTime:0.45  LR:0.000100\n",
      "   | > Step:31/68  GlobalStep:111260  TotalLoss:0.00188  PostnetLoss:0.00092  DecoderLoss:0.00096  StopLoss:0.07092  GradNorm:0.00310  GradNormST:0.01913  AvgTextLen:24.1  AvgSpecLen:82.7  StepTime:0.54  LR:0.000100\n",
      "   | > Step:41/68  GlobalStep:111270  TotalLoss:0.00210  PostnetLoss:0.00102  DecoderLoss:0.00108  StopLoss:0.05476  GradNorm:0.00319  GradNormST:0.02444  AvgTextLen:32.3  AvgSpecLen:109.4  StepTime:0.52  LR:0.000100\n",
      "   | > Step:51/68  GlobalStep:111280  TotalLoss:0.00222  PostnetLoss:0.00108  DecoderLoss:0.00114  StopLoss:0.05160  GradNorm:0.00329  GradNormST:0.01127  AvgTextLen:39.2  AvgSpecLen:126.9  StepTime:0.74  LR:0.000100\n",
      "   | > Step:61/68  GlobalStep:111290  TotalLoss:0.00241  PostnetLoss:0.00117  DecoderLoss:0.00124  StopLoss:0.07671  GradNorm:0.00278  GradNormST:0.02761  AvgTextLen:54.6  AvgSpecLen:165.5  StepTime:0.90  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:111297  AvgTotalLoss:0.06588  AvgPostnetLoss:0.00095  AvgDecoderLoss:0.00101  AvgStopLoss:0.06393  EpochTime:41.70  AvgStepTime:0.60\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.09274   PostnetLoss: 0.00529   DecoderLoss:0.00560  StopLoss: 0.08185  \n",
      "   | > TotalLoss: 0.09558   PostnetLoss: 0.00874   DecoderLoss:0.00918  StopLoss: 0.07766  \n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00095   Validation Loss: 0.00805\n",
      "\n",
      " > Epoch 613/1000\n",
      "   | > Step:2/68  GlobalStep:111300  TotalLoss:0.00132  PostnetLoss:0.00064  DecoderLoss:0.00068  StopLoss:0.05712  GradNorm:0.00396  GradNormST:0.02281  AvgTextLen:4.0  AvgSpecLen:31.3  StepTime:0.50  LR:0.000100\n",
      "   | > Step:12/68  GlobalStep:111310  TotalLoss:0.00149  PostnetLoss:0.00073  DecoderLoss:0.00076  StopLoss:0.07207  GradNorm:0.00315  GradNormST:0.02934  AvgTextLen:11.0  AvgSpecLen:49.3  StepTime:0.33  LR:0.000100\n",
      "   | > Step:22/68  GlobalStep:111320  TotalLoss:0.00176  PostnetLoss:0.00086  DecoderLoss:0.00090  StopLoss:0.06965  GradNorm:0.00318  GradNormST:0.01606  AvgTextLen:18.0  AvgSpecLen:72.0  StepTime:0.42  LR:0.000100\n",
      "   | > Step:32/68  GlobalStep:111330  TotalLoss:0.00192  PostnetLoss:0.00094  DecoderLoss:0.00098  StopLoss:0.14292  GradNorm:0.00307  GradNormST:0.04284  AvgTextLen:24.4  AvgSpecLen:91.5  StepTime:0.46  LR:0.000100\n",
      "   | > Step:42/68  GlobalStep:111340  TotalLoss:0.00210  PostnetLoss:0.00102  DecoderLoss:0.00108  StopLoss:0.05731  GradNorm:0.00333  GradNormST:0.02627  AvgTextLen:33.1  AvgSpecLen:109.7  StepTime:0.53  LR:0.000100\n",
      "   | > Step:52/68  GlobalStep:111350  TotalLoss:0.00226  PostnetLoss:0.00110  DecoderLoss:0.00116  StopLoss:0.07133  GradNorm:0.00322  GradNormST:0.01649  AvgTextLen:42.1  AvgSpecLen:137.5  StepTime:0.80  LR:0.000100\n",
      "   | > Step:62/68  GlobalStep:111360  TotalLoss:0.00250  PostnetLoss:0.00121  DecoderLoss:0.00129  StopLoss:0.06666  GradNorm:0.00299  GradNormST:0.01892  AvgTextLen:54.0  AvgSpecLen:174.7  StepTime:0.75  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:111366  AvgTotalLoss:0.06609  AvgPostnetLoss:0.00095  AvgDecoderLoss:0.00100  AvgStopLoss:0.06413  EpochTime:41.37  AvgStepTime:0.60\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.09245   PostnetLoss: 0.00535   DecoderLoss:0.00566  StopLoss: 0.08144  \n",
      "   | > TotalLoss: 0.09557   PostnetLoss: 0.00865   DecoderLoss:0.00910  StopLoss: 0.07782  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00095   Validation Loss: 0.00798\n",
      "\n",
      " > Epoch 614/1000\n",
      "   | > Step:3/68  GlobalStep:111370  TotalLoss:0.00130  PostnetLoss:0.00063  DecoderLoss:0.00067  StopLoss:0.11838  GradNorm:0.00396  GradNormST:0.03827  AvgTextLen:4.7  AvgSpecLen:29.7  StepTime:0.30  LR:0.000100\n",
      "   | > Step:13/68  GlobalStep:111380  TotalLoss:0.00149  PostnetLoss:0.00073  DecoderLoss:0.00076  StopLoss:0.08125  GradNorm:0.00340  GradNormST:0.03027  AvgTextLen:11.5  AvgSpecLen:55.2  StepTime:0.41  LR:0.000100\n",
      "   | > Step:23/68  GlobalStep:111390  TotalLoss:0.00178  PostnetLoss:0.00087  DecoderLoss:0.00091  StopLoss:0.06601  GradNorm:0.00317  GradNormST:0.02249  AvgTextLen:18.3  AvgSpecLen:72.2  StepTime:0.45  LR:0.000100\n",
      "   | > Step:33/68  GlobalStep:111400  TotalLoss:0.00197  PostnetLoss:0.00096  DecoderLoss:0.00101  StopLoss:0.06245  GradNorm:0.00342  GradNormST:0.01565  AvgTextLen:25.9  AvgSpecLen:88.9  StepTime:0.52  LR:0.000100\n",
      "   | > Step:43/68  GlobalStep:111410  TotalLoss:0.00210  PostnetLoss:0.00102  DecoderLoss:0.00108  StopLoss:0.07145  GradNorm:0.00320  GradNormST:0.01796  AvgTextLen:33.4  AvgSpecLen:115.7  StepTime:0.54  LR:0.000100\n",
      "   | > Step:53/68  GlobalStep:111420  TotalLoss:0.00225  PostnetLoss:0.00110  DecoderLoss:0.00116  StopLoss:0.04627  GradNorm:0.00313  GradNormST:0.01140  AvgTextLen:42.4  AvgSpecLen:137.6  StepTime:0.76  LR:0.000100\n",
      "   | > Step:63/68  GlobalStep:111430  TotalLoss:0.00252  PostnetLoss:0.00122  DecoderLoss:0.00130  StopLoss:0.04863  GradNorm:0.00278  GradNormST:0.02258  AvgTextLen:58.6  AvgSpecLen:185.1  StepTime:0.94  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:111435  AvgTotalLoss:0.06385  AvgPostnetLoss:0.00095  AvgDecoderLoss:0.00100  AvgStopLoss:0.06190  EpochTime:43.01  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.09273   PostnetLoss: 0.00526   DecoderLoss:0.00558  StopLoss: 0.08189  \n",
      "   | > TotalLoss: 0.09784   PostnetLoss: 0.00874   DecoderLoss:0.00919  StopLoss: 0.07991  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00095   Validation Loss: 0.00805\n",
      "\n",
      " > Epoch 615/1000\n",
      "   | > Step:4/68  GlobalStep:111440  TotalLoss:0.00135  PostnetLoss:0.00066  DecoderLoss:0.00069  StopLoss:0.09437  GradNorm:0.00365  GradNormST:0.04348  AvgTextLen:4.8  AvgSpecLen:31.1  StepTime:0.22  LR:0.000100\n",
      "   | > Step:14/68  GlobalStep:111450  TotalLoss:0.00156  PostnetLoss:0.00076  DecoderLoss:0.00080  StopLoss:0.05652  GradNorm:0.00325  GradNormST:0.02430  AvgTextLen:12.2  AvgSpecLen:52.0  StepTime:0.26  LR:0.000100\n",
      "   | > Step:24/68  GlobalStep:111460  TotalLoss:0.00186  PostnetLoss:0.00091  DecoderLoss:0.00095  StopLoss:0.06684  GradNorm:0.00320  GradNormST:0.01706  AvgTextLen:19.2  AvgSpecLen:79.0  StepTime:0.42  LR:0.000100\n",
      "   | > Step:34/68  GlobalStep:111470  TotalLoss:0.00201  PostnetLoss:0.00098  DecoderLoss:0.00103  StopLoss:0.04326  GradNorm:0.00310  GradNormST:0.01361  AvgTextLen:26.2  AvgSpecLen:95.9  StepTime:0.58  LR:0.000100\n",
      "   | > Step:44/68  GlobalStep:111480  TotalLoss:0.00215  PostnetLoss:0.00105  DecoderLoss:0.00110  StopLoss:0.04382  GradNorm:0.00308  GradNormST:0.01727  AvgTextLen:33.8  AvgSpecLen:116.3  StepTime:0.69  LR:0.000100\n",
      "   | > Step:54/68  GlobalStep:111490  TotalLoss:0.00229  PostnetLoss:0.00111  DecoderLoss:0.00118  StopLoss:0.05530  GradNorm:0.00323  GradNormST:0.01633  AvgTextLen:42.2  AvgSpecLen:144.9  StepTime:0.80  LR:0.000100\n",
      "   | > Step:64/68  GlobalStep:111500  TotalLoss:0.00239  PostnetLoss:0.00116  DecoderLoss:0.00123  StopLoss:0.04109  GradNorm:0.00278  GradNormST:0.01175  AvgTextLen:60.6  AvgSpecLen:177.6  StepTime:0.80  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:111504  AvgTotalLoss:0.06068  AvgPostnetLoss:0.00095  AvgDecoderLoss:0.00100  AvgStopLoss:0.05873  EpochTime:41.49  AvgStepTime:0.60\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.08888   PostnetLoss: 0.00532   DecoderLoss:0.00563  StopLoss: 0.07793  \n",
      "   | > TotalLoss: 0.09179   PostnetLoss: 0.00864   DecoderLoss:0.00909  StopLoss: 0.07405  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00095   Validation Loss: 0.00796\n",
      "\n",
      " > Epoch 616/1000\n",
      "   | > Step:5/68  GlobalStep:111510  TotalLoss:0.00140  PostnetLoss:0.00068  DecoderLoss:0.00072  StopLoss:0.06853  GradNorm:0.00364  GradNormST:0.02099  AvgTextLen:5.9  AvgSpecLen:33.2  StepTime:0.30  LR:0.000100\n",
      "   | > Step:15/68  GlobalStep:111520  TotalLoss:0.00165  PostnetLoss:0.00080  DecoderLoss:0.00085  StopLoss:0.07897  GradNorm:0.00386  GradNormST:0.03842  AvgTextLen:12.8  AvgSpecLen:59.1  StepTime:0.42  LR:0.000100\n",
      "   | > Step:25/68  GlobalStep:111530  TotalLoss:0.00179  PostnetLoss:0.00087  DecoderLoss:0.00092  StopLoss:0.05703  GradNorm:0.00307  GradNormST:0.02340  AvgTextLen:19.8  AvgSpecLen:73.3  StepTime:0.46  LR:0.000100\n",
      "   | > Step:35/68  GlobalStep:111540  TotalLoss:0.00201  PostnetLoss:0.00098  DecoderLoss:0.00103  StopLoss:0.07541  GradNorm:0.00284  GradNormST:0.03621  AvgTextLen:27.1  AvgSpecLen:104.7  StepTime:0.62  LR:0.000100\n",
      "   | > Step:45/68  GlobalStep:111550  TotalLoss:0.00209  PostnetLoss:0.00101  DecoderLoss:0.00107  StopLoss:0.06473  GradNorm:0.00284  GradNormST:0.01754  AvgTextLen:34.4  AvgSpecLen:118.6  StepTime:0.64  LR:0.000100\n",
      "   | > Step:55/68  GlobalStep:111560  TotalLoss:0.00235  PostnetLoss:0.00114  DecoderLoss:0.00121  StopLoss:0.04912  GradNorm:0.00302  GradNormST:0.01715  AvgTextLen:43.9  AvgSpecLen:145.2  StepTime:0.85  LR:0.000100\n",
      "   | > Step:65/68  GlobalStep:111570  TotalLoss:0.00259  PostnetLoss:0.00125  DecoderLoss:0.00133  StopLoss:0.04626  GradNorm:0.00285  GradNormST:0.01368  AvgTextLen:64.2  AvgSpecLen:207.4  StepTime:1.18  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:111573  AvgTotalLoss:0.05987  AvgPostnetLoss:0.00095  AvgDecoderLoss:0.00100  AvgStopLoss:0.05792  EpochTime:41.92  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.09072   PostnetLoss: 0.00544   DecoderLoss:0.00575  StopLoss: 0.07952  \n",
      "   | > TotalLoss: 0.09573   PostnetLoss: 0.00872   DecoderLoss:0.00915  StopLoss: 0.07785  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00095   Validation Loss: 0.00802\n",
      "\n",
      " > Epoch 617/1000\n",
      "   | > Step:6/68  GlobalStep:111580  TotalLoss:0.00136  PostnetLoss:0.00067  DecoderLoss:0.00070  StopLoss:0.05893  GradNorm:0.00340  GradNormST:0.02715  AvgTextLen:6.0  AvgSpecLen:34.8  StepTime:0.39  LR:0.000100\n",
      "   | > Step:16/68  GlobalStep:111590  TotalLoss:0.00165  PostnetLoss:0.00080  DecoderLoss:0.00084  StopLoss:0.04509  GradNorm:0.00346  GradNormST:0.01611  AvgTextLen:13.8  AvgSpecLen:60.6  StepTime:0.36  LR:0.000100\n",
      "   | > Step:26/68  GlobalStep:111600  TotalLoss:0.00182  PostnetLoss:0.00089  DecoderLoss:0.00093  StopLoss:0.05453  GradNorm:0.00340  GradNormST:0.01525  AvgTextLen:20.3  AvgSpecLen:77.5  StepTime:0.49  LR:0.000100\n",
      "   | > Step:36/68  GlobalStep:111610  TotalLoss:0.00195  PostnetLoss:0.00095  DecoderLoss:0.00100  StopLoss:0.06322  GradNorm:0.00295  GradNormST:0.01320  AvgTextLen:28.5  AvgSpecLen:100.2  StepTime:0.58  LR:0.000100\n",
      "   | > Step:46/68  GlobalStep:111620  TotalLoss:0.00211  PostnetLoss:0.00103  DecoderLoss:0.00108  StopLoss:0.05973  GradNorm:0.00293  GradNormST:0.02711  AvgTextLen:35.4  AvgSpecLen:122.6  StepTime:0.66  LR:0.000100\n",
      "   | > Step:56/68  GlobalStep:111630  TotalLoss:0.00234  PostnetLoss:0.00114  DecoderLoss:0.00121  StopLoss:0.04763  GradNorm:0.00313  GradNormST:0.01684  AvgTextLen:45.8  AvgSpecLen:145.4  StepTime:0.95  LR:0.000100\n",
      "   | > Step:66/68  GlobalStep:111640  TotalLoss:0.00273  PostnetLoss:0.00132  DecoderLoss:0.00141  StopLoss:0.04444  GradNorm:0.00258  GradNormST:0.01324  AvgTextLen:71.2  AvgSpecLen:220.8  StepTime:1.04  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:111642  AvgTotalLoss:0.05995  AvgPostnetLoss:0.00095  AvgDecoderLoss:0.00100  AvgStopLoss:0.05800  EpochTime:42.52  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.09229   PostnetLoss: 0.00540   DecoderLoss:0.00571  StopLoss: 0.08118  \n",
      "   | > TotalLoss: 0.09586   PostnetLoss: 0.00875   DecoderLoss:0.00919  StopLoss: 0.07791  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00095   Validation Loss: 0.00806\n",
      "\n",
      " > Epoch 618/1000\n",
      "   | > Step:7/68  GlobalStep:111650  TotalLoss:0.00142  PostnetLoss:0.00069  DecoderLoss:0.00072  StopLoss:0.05385  GradNorm:0.00344  GradNormST:0.02410  AvgTextLen:7.4  AvgSpecLen:44.6  StepTime:0.39  LR:0.000100\n",
      "   | > Step:17/68  GlobalStep:111660  TotalLoss:0.00165  PostnetLoss:0.00081  DecoderLoss:0.00084  StopLoss:0.06895  GradNorm:0.00365  GradNormST:0.01641  AvgTextLen:14.7  AvgSpecLen:66.0  StepTime:0.43  LR:0.000100\n",
      "   | > Step:27/68  GlobalStep:111670  TotalLoss:0.00190  PostnetLoss:0.00093  DecoderLoss:0.00097  StopLoss:0.07381  GradNorm:0.00334  GradNormST:0.02170  AvgTextLen:21.0  AvgSpecLen:80.2  StepTime:0.68  LR:0.000100\n",
      "   | > Step:37/68  GlobalStep:111680  TotalLoss:0.00201  PostnetLoss:0.00098  DecoderLoss:0.00103  StopLoss:0.05888  GradNorm:0.00301  GradNormST:0.01404  AvgTextLen:29.3  AvgSpecLen:102.4  StepTime:0.48  LR:0.000100\n",
      "   | > Step:47/68  GlobalStep:111690  TotalLoss:0.00219  PostnetLoss:0.00107  DecoderLoss:0.00113  StopLoss:0.04578  GradNorm:0.00301  GradNormST:0.01528  AvgTextLen:36.7  AvgSpecLen:126.9  StepTime:0.62  LR:0.000100\n",
      "   | > Step:57/68  GlobalStep:111700  TotalLoss:0.00236  PostnetLoss:0.00115  DecoderLoss:0.00121  StopLoss:0.04381  GradNorm:0.00297  GradNormST:0.01067  AvgTextLen:47.1  AvgSpecLen:161.0  StepTime:0.81  LR:0.000100\n",
      "   | > Step:67/68  GlobalStep:111710  TotalLoss:0.00272  PostnetLoss:0.00131  DecoderLoss:0.00140  StopLoss:0.04218  GradNorm:0.00256  GradNormST:0.01696  AvgTextLen:80.6  AvgSpecLen:246.5  StepTime:1.21  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:111711  AvgTotalLoss:0.06222  AvgPostnetLoss:0.00095  AvgDecoderLoss:0.00100  AvgStopLoss:0.06027  EpochTime:42.59  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.08969   PostnetLoss: 0.00534   DecoderLoss:0.00566  StopLoss: 0.07869  \n",
      "   | > TotalLoss: 0.09678   PostnetLoss: 0.00876   DecoderLoss:0.00920  StopLoss: 0.07882  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00095   Validation Loss: 0.00806\n",
      "\n",
      " > Epoch 619/1000\n",
      "   | > Step:8/68  GlobalStep:111720  TotalLoss:0.00147  PostnetLoss:0.00072  DecoderLoss:0.00075  StopLoss:0.06746  GradNorm:0.00335  GradNormST:0.02804  AvgTextLen:8.3  AvgSpecLen:44.0  StepTime:0.31  LR:0.000100\n",
      "   | > Step:18/68  GlobalStep:111730  TotalLoss:0.00162  PostnetLoss:0.00079  DecoderLoss:0.00083  StopLoss:0.05068  GradNorm:0.00299  GradNormST:0.01221  AvgTextLen:15.2  AvgSpecLen:64.5  StepTime:0.41  LR:0.000100\n",
      "   | > Step:28/68  GlobalStep:111740  TotalLoss:0.00186  PostnetLoss:0.00091  DecoderLoss:0.00095  StopLoss:0.04477  GradNorm:0.00308  GradNormST:0.01331  AvgTextLen:22.1  AvgSpecLen:85.4  StepTime:0.48  LR:0.000100\n",
      "   | > Step:38/68  GlobalStep:111750  TotalLoss:0.00200  PostnetLoss:0.00098  DecoderLoss:0.00103  StopLoss:0.05228  GradNorm:0.00282  GradNormST:0.01492  AvgTextLen:29.4  AvgSpecLen:104.1  StepTime:0.61  LR:0.000100\n",
      "   | > Step:48/68  GlobalStep:111760  TotalLoss:0.00219  PostnetLoss:0.00107  DecoderLoss:0.00112  StopLoss:0.05841  GradNorm:0.00294  GradNormST:0.01978  AvgTextLen:37.5  AvgSpecLen:123.8  StepTime:0.55  LR:0.000100\n",
      "   | > Step:58/68  GlobalStep:111770  TotalLoss:0.00237  PostnetLoss:0.00115  DecoderLoss:0.00122  StopLoss:0.04885  GradNorm:0.00323  GradNormST:0.01550  AvgTextLen:47.8  AvgSpecLen:158.0  StepTime:0.91  LR:0.000100\n",
      "   | > Step:68/68  GlobalStep:111780  TotalLoss:0.00269  PostnetLoss:0.00130  DecoderLoss:0.00139  StopLoss:0.03939  GradNorm:0.00261  GradNormST:0.01099  AvgTextLen:103.0  AvgSpecLen:308.9  StepTime:1.87  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:111780  AvgTotalLoss:0.06179  AvgPostnetLoss:0.00095  AvgDecoderLoss:0.00100  AvgStopLoss:0.05985  EpochTime:42.08  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.08945   PostnetLoss: 0.00533   DecoderLoss:0.00565  StopLoss: 0.07847  \n",
      "   | > TotalLoss: 0.09529   PostnetLoss: 0.00885   DecoderLoss:0.00931  StopLoss: 0.07713  \n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00095   Validation Loss: 0.00812\n",
      "\n",
      " > Epoch 620/1000\n",
      "   | > Step:9/68  GlobalStep:111790  TotalLoss:0.00144  PostnetLoss:0.00070  DecoderLoss:0.00074  StopLoss:0.05011  GradNorm:0.00366  GradNormST:0.02043  AvgTextLen:9.0  AvgSpecLen:42.1  StepTime:0.35  LR:0.000100\n",
      "   | > Step:19/68  GlobalStep:111800  TotalLoss:0.00162  PostnetLoss:0.00079  DecoderLoss:0.00083  StopLoss:0.07224  GradNorm:0.00326  GradNormST:0.01864  AvgTextLen:15.5  AvgSpecLen:63.2  StepTime:0.41  LR:0.000100\n",
      "   | > Step:29/68  GlobalStep:111810  TotalLoss:0.00182  PostnetLoss:0.00089  DecoderLoss:0.00093  StopLoss:0.07684  GradNorm:0.00306  GradNormST:0.02060  AvgTextLen:21.8  AvgSpecLen:83.4  StepTime:0.49  LR:0.000100\n",
      "   | > Step:39/68  GlobalStep:111820  TotalLoss:0.00202  PostnetLoss:0.00099  DecoderLoss:0.00104  StopLoss:0.06731  GradNorm:0.00280  GradNormST:0.02697  AvgTextLen:30.2  AvgSpecLen:110.4  StepTime:0.62  LR:0.000100\n",
      "   | > Step:49/68  GlobalStep:111830  TotalLoss:0.00217  PostnetLoss:0.00106  DecoderLoss:0.00111  StopLoss:0.05668  GradNorm:0.00275  GradNormST:0.01461  AvgTextLen:38.2  AvgSpecLen:127.7  StepTime:0.69  LR:0.000100\n",
      "   | > Step:59/68  GlobalStep:111840  TotalLoss:0.00236  PostnetLoss:0.00114  DecoderLoss:0.00121  StopLoss:0.05535  GradNorm:0.00275  GradNormST:0.01446  AvgTextLen:50.5  AvgSpecLen:155.9  StepTime:0.93  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:111849  AvgTotalLoss:0.06082  AvgPostnetLoss:0.00095  AvgDecoderLoss:0.00100  AvgStopLoss:0.05887  EpochTime:43.51  AvgStepTime:0.63\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.08788   PostnetLoss: 0.00541   DecoderLoss:0.00572  StopLoss: 0.07675  \n",
      "   | > TotalLoss: 0.09512   PostnetLoss: 0.00871   DecoderLoss:0.00914  StopLoss: 0.07727  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00095   Validation Loss: 0.00804\n",
      "\n",
      " > Epoch 621/1000\n",
      "   | > Step:0/68  GlobalStep:111850  TotalLoss:0.00142  PostnetLoss:0.00068  DecoderLoss:0.00074  StopLoss:0.06508  GradNorm:0.00610  GradNormST:0.03158  AvgTextLen:3.0  AvgSpecLen:25.3  StepTime:0.50  LR:0.000100\n",
      "   | > Step:10/68  GlobalStep:111860  TotalLoss:0.00152  PostnetLoss:0.00074  DecoderLoss:0.00078  StopLoss:0.05143  GradNorm:0.00337  GradNormST:0.02007  AvgTextLen:9.8  AvgSpecLen:48.6  StepTime:0.52  LR:0.000100\n",
      "   | > Step:20/68  GlobalStep:111870  TotalLoss:0.00173  PostnetLoss:0.00085  DecoderLoss:0.00089  StopLoss:0.04676  GradNorm:0.00338  GradNormST:0.01790  AvgTextLen:16.4  AvgSpecLen:64.8  StepTime:0.40  LR:0.000100\n",
      "   | > Step:30/68  GlobalStep:111880  TotalLoss:0.00188  PostnetLoss:0.00092  DecoderLoss:0.00096  StopLoss:0.07706  GradNorm:0.00299  GradNormST:0.01713  AvgTextLen:22.9  AvgSpecLen:85.8  StepTime:0.56  LR:0.000100\n",
      "   | > Step:40/68  GlobalStep:111890  TotalLoss:0.00204  PostnetLoss:0.00100  DecoderLoss:0.00105  StopLoss:0.04948  GradNorm:0.00294  GradNormST:0.01365  AvgTextLen:30.9  AvgSpecLen:105.3  StepTime:0.65  LR:0.000100\n",
      "   | > Step:50/68  GlobalStep:111900  TotalLoss:0.00221  PostnetLoss:0.00108  DecoderLoss:0.00114  StopLoss:0.06678  GradNorm:0.00296  GradNormST:0.02116  AvgTextLen:38.8  AvgSpecLen:132.0  StepTime:0.80  LR:0.000100\n",
      "   | > Step:60/68  GlobalStep:111910  TotalLoss:0.00240  PostnetLoss:0.00116  DecoderLoss:0.00123  StopLoss:0.04395  GradNorm:0.00281  GradNormST:0.01328  AvgTextLen:52.0  AvgSpecLen:172.7  StepTime:1.00  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:111918  AvgTotalLoss:0.05885  AvgPostnetLoss:0.00095  AvgDecoderLoss:0.00100  AvgStopLoss:0.05690  EpochTime:42.61  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.08719   PostnetLoss: 0.00534   DecoderLoss:0.00564  StopLoss: 0.07622  \n",
      "   | > TotalLoss: 0.09503   PostnetLoss: 0.00878   DecoderLoss:0.00923  StopLoss: 0.07702  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00095   Validation Loss: 0.00805\n",
      "\n",
      " > Epoch 622/1000\n",
      "   | > Step:1/68  GlobalStep:111920  TotalLoss:0.00138  PostnetLoss:0.00067  DecoderLoss:0.00071  StopLoss:0.05902  GradNorm:0.00495  GradNormST:0.02389  AvgTextLen:3.7  AvgSpecLen:28.6  StepTime:0.48  LR:0.000100\n",
      "   | > Step:11/68  GlobalStep:111930  TotalLoss:0.00153  PostnetLoss:0.00075  DecoderLoss:0.00078  StopLoss:0.03845  GradNorm:0.00376  GradNormST:0.01628  AvgTextLen:10.3  AvgSpecLen:49.4  StepTime:0.36  LR:0.000100\n",
      "   | > Step:21/68  GlobalStep:111940  TotalLoss:0.00168  PostnetLoss:0.00082  DecoderLoss:0.00086  StopLoss:0.05348  GradNorm:0.00324  GradNormST:0.01529  AvgTextLen:16.6  AvgSpecLen:66.3  StepTime:0.43  LR:0.000100\n",
      "   | > Step:31/68  GlobalStep:111950  TotalLoss:0.00189  PostnetLoss:0.00092  DecoderLoss:0.00097  StopLoss:0.07342  GradNorm:0.00313  GradNormST:0.02077  AvgTextLen:24.1  AvgSpecLen:82.7  StepTime:0.56  LR:0.000100\n",
      "   | > Step:41/68  GlobalStep:111960  TotalLoss:0.00210  PostnetLoss:0.00102  DecoderLoss:0.00108  StopLoss:0.05240  GradNorm:0.00316  GradNormST:0.02520  AvgTextLen:32.3  AvgSpecLen:109.4  StepTime:0.63  LR:0.000100\n",
      "   | > Step:51/68  GlobalStep:111970  TotalLoss:0.00220  PostnetLoss:0.00107  DecoderLoss:0.00113  StopLoss:0.04717  GradNorm:0.00270  GradNormST:0.01173  AvgTextLen:39.2  AvgSpecLen:126.9  StepTime:0.78  LR:0.000100\n",
      "   | > Step:61/68  GlobalStep:111980  TotalLoss:0.00240  PostnetLoss:0.00117  DecoderLoss:0.00124  StopLoss:0.05380  GradNorm:0.00262  GradNormST:0.01425  AvgTextLen:54.6  AvgSpecLen:165.5  StepTime:0.93  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:111987  AvgTotalLoss:0.06149  AvgPostnetLoss:0.00095  AvgDecoderLoss:0.00100  AvgStopLoss:0.05955  EpochTime:42.45  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.08805   PostnetLoss: 0.00541   DecoderLoss:0.00571  StopLoss: 0.07693  \n",
      "   | > TotalLoss: 0.09448   PostnetLoss: 0.00874   DecoderLoss:0.00916  StopLoss: 0.07658  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00095   Validation Loss: 0.00799\n",
      "\n",
      " > Epoch 623/1000\n",
      "   | > Step:2/68  GlobalStep:111990  TotalLoss:0.00131  PostnetLoss:0.00064  DecoderLoss:0.00067  StopLoss:0.06050  GradNorm:0.00415  GradNormST:0.02025  AvgTextLen:4.0  AvgSpecLen:31.3  StepTime:0.39  LR:0.000100\n",
      "   | > Step:12/68  GlobalStep:112000  TotalLoss:0.00151  PostnetLoss:0.00073  DecoderLoss:0.00077  StopLoss:0.08316  GradNorm:0.00343  GradNormST:0.03915  AvgTextLen:11.0  AvgSpecLen:49.3  StepTime:0.36  LR:0.000100\n",
      " | | > Checkpoint saving : /vm/TTS/models/cartman_models/mozilla-no-loc-fattn-stopnet-sigmoid-loss_masking-August-15-2019_11+26AM-de6aeb9/checkpoint_112000.pth.tar\n",
      "warning: audio amplitude out of range, auto clipped.\n",
      "   | > Step:22/68  GlobalStep:112010  TotalLoss:0.00175  PostnetLoss:0.00085  DecoderLoss:0.00089  StopLoss:0.05655  GradNorm:0.00348  GradNormST:0.01401  AvgTextLen:18.0  AvgSpecLen:72.0  StepTime:0.46  LR:0.000100\n",
      "   | > Step:32/68  GlobalStep:112020  TotalLoss:0.00191  PostnetLoss:0.00093  DecoderLoss:0.00098  StopLoss:0.11036  GradNorm:0.00306  GradNormST:0.03864  AvgTextLen:24.4  AvgSpecLen:91.5  StepTime:0.56  LR:0.000100\n",
      "   | > Step:42/68  GlobalStep:112030  TotalLoss:0.00208  PostnetLoss:0.00101  DecoderLoss:0.00107  StopLoss:0.06483  GradNorm:0.00364  GradNormST:0.02203  AvgTextLen:33.1  AvgSpecLen:109.7  StepTime:0.64  LR:0.000100\n",
      "   | > Step:52/68  GlobalStep:112040  TotalLoss:0.00226  PostnetLoss:0.00110  DecoderLoss:0.00116  StopLoss:0.05915  GradNorm:0.00286  GradNormST:0.01207  AvgTextLen:42.1  AvgSpecLen:137.5  StepTime:0.84  LR:0.000100\n",
      "   | > Step:62/68  GlobalStep:112050  TotalLoss:0.00247  PostnetLoss:0.00120  DecoderLoss:0.00127  StopLoss:0.04938  GradNorm:0.00268  GradNormST:0.01276  AvgTextLen:54.0  AvgSpecLen:174.7  StepTime:0.75  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:112056  AvgTotalLoss:0.06090  AvgPostnetLoss:0.00095  AvgDecoderLoss:0.00100  AvgStopLoss:0.05896  EpochTime:43.02  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.08882   PostnetLoss: 0.00540   DecoderLoss:0.00571  StopLoss: 0.07771  \n",
      "   | > TotalLoss: 0.09406   PostnetLoss: 0.00860   DecoderLoss:0.00903  StopLoss: 0.07643  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00095   Validation Loss: 0.00801\n",
      "\n",
      " > Epoch 624/1000\n",
      "   | > Step:3/68  GlobalStep:112060  TotalLoss:0.00131  PostnetLoss:0.00064  DecoderLoss:0.00067  StopLoss:0.10025  GradNorm:0.00397  GradNormST:0.04768  AvgTextLen:4.7  AvgSpecLen:29.7  StepTime:0.35  LR:0.000100\n",
      "   | > Step:13/68  GlobalStep:112070  TotalLoss:0.00151  PostnetLoss:0.00074  DecoderLoss:0.00077  StopLoss:0.08572  GradNorm:0.00307  GradNormST:0.02514  AvgTextLen:11.5  AvgSpecLen:55.2  StepTime:0.43  LR:0.000100\n",
      "   | > Step:23/68  GlobalStep:112080  TotalLoss:0.00179  PostnetLoss:0.00087  DecoderLoss:0.00092  StopLoss:0.05872  GradNorm:0.00360  GradNormST:0.01636  AvgTextLen:18.3  AvgSpecLen:72.2  StepTime:0.46  LR:0.000100\n",
      "   | > Step:33/68  GlobalStep:112090  TotalLoss:0.00195  PostnetLoss:0.00095  DecoderLoss:0.00100  StopLoss:0.05455  GradNorm:0.00355  GradNormST:0.01329  AvgTextLen:25.9  AvgSpecLen:88.9  StepTime:0.57  LR:0.000100\n",
      "   | > Step:43/68  GlobalStep:112100  TotalLoss:0.00208  PostnetLoss:0.00101  DecoderLoss:0.00107  StopLoss:0.07595  GradNorm:0.00335  GradNormST:0.01916  AvgTextLen:33.4  AvgSpecLen:115.7  StepTime:0.64  LR:0.000100\n",
      "   | > Step:53/68  GlobalStep:112110  TotalLoss:0.00223  PostnetLoss:0.00108  DecoderLoss:0.00115  StopLoss:0.08639  GradNorm:0.00279  GradNormST:0.02980  AvgTextLen:42.4  AvgSpecLen:137.6  StepTime:0.64  LR:0.000100\n",
      "   | > Step:63/68  GlobalStep:112120  TotalLoss:0.00250  PostnetLoss:0.00121  DecoderLoss:0.00129  StopLoss:0.06088  GradNorm:0.00265  GradNormST:0.03223  AvgTextLen:58.6  AvgSpecLen:185.1  StepTime:0.91  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:112125  AvgTotalLoss:0.06790  AvgPostnetLoss:0.00094  AvgDecoderLoss:0.00100  AvgStopLoss:0.06596  EpochTime:41.93  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.09287   PostnetLoss: 0.00546   DecoderLoss:0.00576  StopLoss: 0.08166  \n",
      "   | > TotalLoss: 0.09542   PostnetLoss: 0.00867   DecoderLoss:0.00910  StopLoss: 0.07765  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00094   Validation Loss: 0.00806\n",
      "\n",
      " > Epoch 625/1000\n",
      "   | > Step:4/68  GlobalStep:112130  TotalLoss:0.00137  PostnetLoss:0.00067  DecoderLoss:0.00070  StopLoss:0.09125  GradNorm:0.00418  GradNormST:0.03285  AvgTextLen:4.8  AvgSpecLen:31.1  StepTime:0.21  LR:0.000100\n",
      "   | > Step:14/68  GlobalStep:112140  TotalLoss:0.00157  PostnetLoss:0.00077  DecoderLoss:0.00080  StopLoss:0.05865  GradNorm:0.00341  GradNormST:0.03073  AvgTextLen:12.2  AvgSpecLen:52.0  StepTime:0.26  LR:0.000100\n",
      "   | > Step:24/68  GlobalStep:112150  TotalLoss:0.00184  PostnetLoss:0.00090  DecoderLoss:0.00094  StopLoss:0.10655  GradNorm:0.00297  GradNormST:0.02889  AvgTextLen:19.2  AvgSpecLen:79.0  StepTime:0.49  LR:0.000100\n",
      "   | > Step:34/68  GlobalStep:112160  TotalLoss:0.00202  PostnetLoss:0.00098  DecoderLoss:0.00104  StopLoss:0.04650  GradNorm:0.00315  GradNormST:0.01209  AvgTextLen:26.2  AvgSpecLen:95.9  StepTime:0.60  LR:0.000100\n",
      "   | > Step:44/68  GlobalStep:112170  TotalLoss:0.00214  PostnetLoss:0.00104  DecoderLoss:0.00110  StopLoss:0.05867  GradNorm:0.00334  GradNormST:0.02770  AvgTextLen:33.8  AvgSpecLen:116.3  StepTime:0.74  LR:0.000100\n",
      "   | > Step:54/68  GlobalStep:112180  TotalLoss:0.00229  PostnetLoss:0.00111  DecoderLoss:0.00118  StopLoss:0.05167  GradNorm:0.00308  GradNormST:0.01286  AvgTextLen:42.2  AvgSpecLen:144.9  StepTime:0.87  LR:0.000100\n",
      "   | > Step:64/68  GlobalStep:112190  TotalLoss:0.00239  PostnetLoss:0.00116  DecoderLoss:0.00123  StopLoss:0.05084  GradNorm:0.00247  GradNormST:0.01487  AvgTextLen:60.6  AvgSpecLen:177.6  StepTime:0.81  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:112194  AvgTotalLoss:0.06375  AvgPostnetLoss:0.00094  AvgDecoderLoss:0.00100  AvgStopLoss:0.06181  EpochTime:42.19  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.08811   PostnetLoss: 0.00542   DecoderLoss:0.00573  StopLoss: 0.07696  \n",
      "   | > TotalLoss: 0.09674   PostnetLoss: 0.00879   DecoderLoss:0.00922  StopLoss: 0.07874  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00094   Validation Loss: 0.00808\n",
      "\n",
      " > Epoch 626/1000\n",
      "   | > Step:5/68  GlobalStep:112200  TotalLoss:0.00137  PostnetLoss:0.00067  DecoderLoss:0.00070  StopLoss:0.05143  GradNorm:0.00354  GradNormST:0.01950  AvgTextLen:5.9  AvgSpecLen:33.2  StepTime:0.38  LR:0.000100\n",
      "   | > Step:15/68  GlobalStep:112210  TotalLoss:0.00162  PostnetLoss:0.00079  DecoderLoss:0.00083  StopLoss:0.06808  GradNorm:0.00312  GradNormST:0.02983  AvgTextLen:12.8  AvgSpecLen:59.1  StepTime:0.55  LR:0.000100\n",
      "   | > Step:25/68  GlobalStep:112220  TotalLoss:0.00178  PostnetLoss:0.00087  DecoderLoss:0.00091  StopLoss:0.07504  GradNorm:0.00331  GradNormST:0.02380  AvgTextLen:19.8  AvgSpecLen:73.3  StepTime:0.36  LR:0.000100\n",
      "   | > Step:35/68  GlobalStep:112230  TotalLoss:0.00199  PostnetLoss:0.00097  DecoderLoss:0.00102  StopLoss:0.06815  GradNorm:0.00297  GradNormST:0.03148  AvgTextLen:27.1  AvgSpecLen:104.7  StepTime:0.74  LR:0.000100\n",
      "   | > Step:45/68  GlobalStep:112240  TotalLoss:0.00208  PostnetLoss:0.00101  DecoderLoss:0.00107  StopLoss:0.09189  GradNorm:0.00346  GradNormST:0.02590  AvgTextLen:34.4  AvgSpecLen:118.6  StepTime:0.59  LR:0.000100\n",
      "   | > Step:55/68  GlobalStep:112250  TotalLoss:0.00232  PostnetLoss:0.00113  DecoderLoss:0.00119  StopLoss:0.04543  GradNorm:0.00280  GradNormST:0.01270  AvgTextLen:43.9  AvgSpecLen:145.2  StepTime:0.72  LR:0.000100\n",
      "   | > Step:65/68  GlobalStep:112260  TotalLoss:0.00258  PostnetLoss:0.00125  DecoderLoss:0.00133  StopLoss:0.04565  GradNorm:0.00270  GradNormST:0.01116  AvgTextLen:64.2  AvgSpecLen:207.4  StepTime:1.19  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:112263  AvgTotalLoss:0.06503  AvgPostnetLoss:0.00094  AvgDecoderLoss:0.00099  AvgStopLoss:0.06309  EpochTime:42.23  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.09121   PostnetLoss: 0.00563   DecoderLoss:0.00594  StopLoss: 0.07964  \n",
      "   | > TotalLoss: 0.09721   PostnetLoss: 0.00878   DecoderLoss:0.00921  StopLoss: 0.07922  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00094   Validation Loss: 0.00814\n",
      "\n",
      " > Epoch 627/1000\n",
      "   | > Step:6/68  GlobalStep:112270  TotalLoss:0.00137  PostnetLoss:0.00067  DecoderLoss:0.00070  StopLoss:0.05062  GradNorm:0.00376  GradNormST:0.02052  AvgTextLen:6.0  AvgSpecLen:34.8  StepTime:0.29  LR:0.000100\n",
      "   | > Step:16/68  GlobalStep:112280  TotalLoss:0.00166  PostnetLoss:0.00081  DecoderLoss:0.00085  StopLoss:0.05318  GradNorm:0.00359  GradNormST:0.01983  AvgTextLen:13.8  AvgSpecLen:60.6  StepTime:0.36  LR:0.000100\n",
      "   | > Step:26/68  GlobalStep:112290  TotalLoss:0.00182  PostnetLoss:0.00089  DecoderLoss:0.00093  StopLoss:0.06397  GradNorm:0.00303  GradNormST:0.01827  AvgTextLen:20.3  AvgSpecLen:77.5  StepTime:0.46  LR:0.000100\n",
      "   | > Step:36/68  GlobalStep:112300  TotalLoss:0.00197  PostnetLoss:0.00096  DecoderLoss:0.00101  StopLoss:0.07061  GradNorm:0.00302  GradNormST:0.01630  AvgTextLen:28.5  AvgSpecLen:100.2  StepTime:0.58  LR:0.000100\n",
      "   | > Step:46/68  GlobalStep:112310  TotalLoss:0.00211  PostnetLoss:0.00103  DecoderLoss:0.00108  StopLoss:0.09678  GradNorm:0.00326  GradNormST:0.05223  AvgTextLen:35.4  AvgSpecLen:122.6  StepTime:0.66  LR:0.000100\n",
      "   | > Step:56/68  GlobalStep:112320  TotalLoss:0.00230  PostnetLoss:0.00112  DecoderLoss:0.00118  StopLoss:0.06879  GradNorm:0.00278  GradNormST:0.02986  AvgTextLen:45.8  AvgSpecLen:145.4  StepTime:0.86  LR:0.000100\n",
      "   | > Step:66/68  GlobalStep:112330  TotalLoss:0.00269  PostnetLoss:0.00130  DecoderLoss:0.00139  StopLoss:0.04020  GradNorm:0.00265  GradNormST:0.01498  AvgTextLen:71.2  AvgSpecLen:220.8  StepTime:1.03  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:112332  AvgTotalLoss:0.06341  AvgPostnetLoss:0.00095  AvgDecoderLoss:0.00100  AvgStopLoss:0.06147  EpochTime:42.10  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.08812   PostnetLoss: 0.00538   DecoderLoss:0.00569  StopLoss: 0.07705  \n",
      "   | > TotalLoss: 0.09697   PostnetLoss: 0.00918   DecoderLoss:0.00963  StopLoss: 0.07815  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00095   Validation Loss: 0.00821\n",
      "\n",
      " > Epoch 628/1000\n",
      "   | > Step:7/68  GlobalStep:112340  TotalLoss:0.00143  PostnetLoss:0.00070  DecoderLoss:0.00073  StopLoss:0.06014  GradNorm:0.00360  GradNormST:0.02593  AvgTextLen:7.4  AvgSpecLen:44.6  StepTime:0.37  LR:0.000100\n",
      "   | > Step:17/68  GlobalStep:112350  TotalLoss:0.00162  PostnetLoss:0.00079  DecoderLoss:0.00083  StopLoss:0.07328  GradNorm:0.00312  GradNormST:0.01882  AvgTextLen:14.7  AvgSpecLen:66.0  StepTime:0.45  LR:0.000100\n",
      "   | > Step:27/68  GlobalStep:112360  TotalLoss:0.00188  PostnetLoss:0.00092  DecoderLoss:0.00096  StopLoss:0.07082  GradNorm:0.00309  GradNormST:0.01803  AvgTextLen:21.0  AvgSpecLen:80.2  StepTime:0.63  LR:0.000100\n",
      "   | > Step:37/68  GlobalStep:112370  TotalLoss:0.00200  PostnetLoss:0.00098  DecoderLoss:0.00102  StopLoss:0.07412  GradNorm:0.00318  GradNormST:0.01657  AvgTextLen:29.3  AvgSpecLen:102.4  StepTime:0.63  LR:0.000100\n",
      "   | > Step:47/68  GlobalStep:112380  TotalLoss:0.00219  PostnetLoss:0.00107  DecoderLoss:0.00113  StopLoss:0.05039  GradNorm:0.00340  GradNormST:0.01345  AvgTextLen:36.7  AvgSpecLen:126.9  StepTime:0.72  LR:0.000100\n",
      "   | > Step:57/68  GlobalStep:112390  TotalLoss:0.00238  PostnetLoss:0.00116  DecoderLoss:0.00122  StopLoss:0.03648  GradNorm:0.00352  GradNormST:0.00945  AvgTextLen:47.1  AvgSpecLen:161.0  StepTime:0.83  LR:0.000100\n",
      "   | > Step:67/68  GlobalStep:112400  TotalLoss:0.00270  PostnetLoss:0.00131  DecoderLoss:0.00140  StopLoss:0.03577  GradNorm:0.00260  GradNormST:0.01533  AvgTextLen:80.6  AvgSpecLen:246.5  StepTime:1.22  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:112401  AvgTotalLoss:0.06261  AvgPostnetLoss:0.00094  AvgDecoderLoss:0.00100  AvgStopLoss:0.06067  EpochTime:43.39  AvgStepTime:0.63\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.08840   PostnetLoss: 0.00547   DecoderLoss:0.00577  StopLoss: 0.07716  \n",
      "   | > TotalLoss: 0.09763   PostnetLoss: 0.00905   DecoderLoss:0.00949  StopLoss: 0.07910  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00094   Validation Loss: 0.00818\n",
      "\n",
      " > Epoch 629/1000\n",
      "   | > Step:8/68  GlobalStep:112410  TotalLoss:0.00145  PostnetLoss:0.00071  DecoderLoss:0.00074  StopLoss:0.05519  GradNorm:0.00356  GradNormST:0.02078  AvgTextLen:8.3  AvgSpecLen:44.0  StepTime:0.33  LR:0.000100\n",
      "   | > Step:18/68  GlobalStep:112420  TotalLoss:0.00161  PostnetLoss:0.00079  DecoderLoss:0.00083  StopLoss:0.06418  GradNorm:0.00305  GradNormST:0.01784  AvgTextLen:15.2  AvgSpecLen:64.5  StepTime:0.43  LR:0.000100\n",
      "   | > Step:28/68  GlobalStep:112430  TotalLoss:0.00184  PostnetLoss:0.00090  DecoderLoss:0.00094  StopLoss:0.05829  GradNorm:0.00293  GradNormST:0.01362  AvgTextLen:22.1  AvgSpecLen:85.4  StepTime:0.55  LR:0.000100\n",
      "   | > Step:38/68  GlobalStep:112440  TotalLoss:0.00200  PostnetLoss:0.00098  DecoderLoss:0.00103  StopLoss:0.05260  GradNorm:0.00296  GradNormST:0.01285  AvgTextLen:29.4  AvgSpecLen:104.1  StepTime:0.62  LR:0.000100\n",
      "   | > Step:48/68  GlobalStep:112450  TotalLoss:0.00217  PostnetLoss:0.00106  DecoderLoss:0.00112  StopLoss:0.04864  GradNorm:0.00287  GradNormST:0.01718  AvgTextLen:37.5  AvgSpecLen:123.8  StepTime:0.68  LR:0.000100\n",
      "   | > Step:58/68  GlobalStep:112460  TotalLoss:0.00237  PostnetLoss:0.00115  DecoderLoss:0.00122  StopLoss:0.05340  GradNorm:0.00321  GradNormST:0.01312  AvgTextLen:47.8  AvgSpecLen:158.0  StepTime:0.69  LR:0.000100\n",
      "   | > Step:68/68  GlobalStep:112470  TotalLoss:0.00269  PostnetLoss:0.00130  DecoderLoss:0.00139  StopLoss:0.04406  GradNorm:0.00285  GradNormST:0.02491  AvgTextLen:103.0  AvgSpecLen:308.9  StepTime:1.86  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:112470  AvgTotalLoss:0.06023  AvgPostnetLoss:0.00094  AvgDecoderLoss:0.00099  AvgStopLoss:0.05829  EpochTime:42.04  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.08256   PostnetLoss: 0.00537   DecoderLoss:0.00568  StopLoss: 0.07150  \n",
      "   | > TotalLoss: 0.09713   PostnetLoss: 0.00890   DecoderLoss:0.00933  StopLoss: 0.07889  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00094   Validation Loss: 0.00811\n",
      "\n",
      " > Epoch 630/1000\n",
      "   | > Step:9/68  GlobalStep:112480  TotalLoss:0.00144  PostnetLoss:0.00071  DecoderLoss:0.00074  StopLoss:0.05218  GradNorm:0.00375  GradNormST:0.02032  AvgTextLen:9.0  AvgSpecLen:42.1  StepTime:0.32  LR:0.000100\n",
      "   | > Step:19/68  GlobalStep:112490  TotalLoss:0.00165  PostnetLoss:0.00081  DecoderLoss:0.00085  StopLoss:0.06848  GradNorm:0.00328  GradNormST:0.01707  AvgTextLen:15.5  AvgSpecLen:63.2  StepTime:0.37  LR:0.000100\n",
      "   | > Step:29/68  GlobalStep:112500  TotalLoss:0.00180  PostnetLoss:0.00088  DecoderLoss:0.00092  StopLoss:0.07477  GradNorm:0.00294  GradNormST:0.02100  AvgTextLen:21.8  AvgSpecLen:83.4  StepTime:0.56  LR:0.000100\n",
      "   | > Step:39/68  GlobalStep:112510  TotalLoss:0.00202  PostnetLoss:0.00098  DecoderLoss:0.00104  StopLoss:0.04804  GradNorm:0.00276  GradNormST:0.01434  AvgTextLen:30.2  AvgSpecLen:110.4  StepTime:0.65  LR:0.000100\n",
      "   | > Step:49/68  GlobalStep:112520  TotalLoss:0.00217  PostnetLoss:0.00106  DecoderLoss:0.00112  StopLoss:0.05214  GradNorm:0.00302  GradNormST:0.01081  AvgTextLen:38.2  AvgSpecLen:127.7  StepTime:0.72  LR:0.000100\n",
      "   | > Step:59/68  GlobalStep:112530  TotalLoss:0.00234  PostnetLoss:0.00114  DecoderLoss:0.00121  StopLoss:0.05274  GradNorm:0.00295  GradNormST:0.01608  AvgTextLen:50.5  AvgSpecLen:155.9  StepTime:0.72  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:112539  AvgTotalLoss:0.05775  AvgPostnetLoss:0.00094  AvgDecoderLoss:0.00100  AvgStopLoss:0.05582  EpochTime:41.63  AvgStepTime:0.60\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.08513   PostnetLoss: 0.00533   DecoderLoss:0.00564  StopLoss: 0.07416  \n",
      "   | > TotalLoss: 0.09756   PostnetLoss: 0.00893   DecoderLoss:0.00938  StopLoss: 0.07924  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00094   Validation Loss: 0.00813\n",
      "\n",
      " > Epoch 631/1000\n",
      "   | > Step:0/68  GlobalStep:112540  TotalLoss:0.00142  PostnetLoss:0.00068  DecoderLoss:0.00074  StopLoss:0.07998  GradNorm:0.00532  GradNormST:0.03065  AvgTextLen:3.0  AvgSpecLen:25.3  StepTime:0.51  LR:0.000100\n",
      "   | > Step:10/68  GlobalStep:112550  TotalLoss:0.00147  PostnetLoss:0.00072  DecoderLoss:0.00076  StopLoss:0.04536  GradNorm:0.00338  GradNormST:0.01698  AvgTextLen:9.8  AvgSpecLen:48.6  StepTime:0.62  LR:0.000100\n",
      "   | > Step:20/68  GlobalStep:112560  TotalLoss:0.00170  PostnetLoss:0.00083  DecoderLoss:0.00087  StopLoss:0.03805  GradNorm:0.00316  GradNormST:0.01669  AvgTextLen:16.4  AvgSpecLen:64.8  StepTime:0.46  LR:0.000100\n",
      "   | > Step:30/68  GlobalStep:112570  TotalLoss:0.00188  PostnetLoss:0.00092  DecoderLoss:0.00096  StopLoss:0.05781  GradNorm:0.00310  GradNormST:0.01362  AvgTextLen:22.9  AvgSpecLen:85.8  StepTime:0.61  LR:0.000100\n",
      "   | > Step:40/68  GlobalStep:112580  TotalLoss:0.00203  PostnetLoss:0.00099  DecoderLoss:0.00104  StopLoss:0.05824  GradNorm:0.00282  GradNormST:0.02008  AvgTextLen:30.9  AvgSpecLen:105.3  StepTime:0.61  LR:0.000100\n",
      "   | > Step:50/68  GlobalStep:112590  TotalLoss:0.00219  PostnetLoss:0.00106  DecoderLoss:0.00113  StopLoss:0.06864  GradNorm:0.00292  GradNormST:0.03158  AvgTextLen:38.8  AvgSpecLen:132.0  StepTime:0.80  LR:0.000100\n",
      "   | > Step:60/68  GlobalStep:112600  TotalLoss:0.00242  PostnetLoss:0.00118  DecoderLoss:0.00125  StopLoss:0.04601  GradNorm:0.00338  GradNormST:0.01122  AvgTextLen:52.0  AvgSpecLen:172.7  StepTime:1.00  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:112608  AvgTotalLoss:0.05690  AvgPostnetLoss:0.00094  AvgDecoderLoss:0.00100  AvgStopLoss:0.05496  EpochTime:43.00  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.08514   PostnetLoss: 0.00540   DecoderLoss:0.00570  StopLoss: 0.07404  \n",
      "   | > TotalLoss: 0.09720   PostnetLoss: 0.00903   DecoderLoss:0.00946  StopLoss: 0.07872  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00094   Validation Loss: 0.00828\n",
      "\n",
      " > Epoch 632/1000\n",
      "   | > Step:1/68  GlobalStep:112610  TotalLoss:0.00135  PostnetLoss:0.00065  DecoderLoss:0.00069  StopLoss:0.04306  GradNorm:0.00484  GradNormST:0.01900  AvgTextLen:3.7  AvgSpecLen:28.6  StepTime:0.33  LR:0.000100\n",
      "   | > Step:11/68  GlobalStep:112620  TotalLoss:0.00148  PostnetLoss:0.00072  DecoderLoss:0.00075  StopLoss:0.04086  GradNorm:0.00329  GradNormST:0.01325  AvgTextLen:10.3  AvgSpecLen:49.4  StepTime:0.38  LR:0.000100\n",
      "   | > Step:21/68  GlobalStep:112630  TotalLoss:0.00169  PostnetLoss:0.00083  DecoderLoss:0.00087  StopLoss:0.06115  GradNorm:0.00312  GradNormST:0.01653  AvgTextLen:16.6  AvgSpecLen:66.3  StepTime:0.38  LR:0.000100\n",
      "   | > Step:31/68  GlobalStep:112640  TotalLoss:0.00187  PostnetLoss:0.00091  DecoderLoss:0.00096  StopLoss:0.04536  GradNorm:0.00297  GradNormST:0.01218  AvgTextLen:24.1  AvgSpecLen:82.7  StepTime:0.56  LR:0.000100\n",
      "   | > Step:41/68  GlobalStep:112650  TotalLoss:0.00209  PostnetLoss:0.00102  DecoderLoss:0.00107  StopLoss:0.04447  GradNorm:0.00297  GradNormST:0.01605  AvgTextLen:32.3  AvgSpecLen:109.4  StepTime:0.62  LR:0.000100\n",
      "   | > Step:51/68  GlobalStep:112660  TotalLoss:0.00219  PostnetLoss:0.00106  DecoderLoss:0.00112  StopLoss:0.04147  GradNorm:0.00283  GradNormST:0.01035  AvgTextLen:39.2  AvgSpecLen:126.9  StepTime:0.62  LR:0.000100\n",
      "   | > Step:61/68  GlobalStep:112670  TotalLoss:0.00239  PostnetLoss:0.00116  DecoderLoss:0.00123  StopLoss:0.04951  GradNorm:0.00293  GradNormST:0.01273  AvgTextLen:54.6  AvgSpecLen:165.5  StepTime:0.90  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:112677  AvgTotalLoss:0.05373  AvgPostnetLoss:0.00094  AvgDecoderLoss:0.00100  AvgStopLoss:0.05178  EpochTime:41.53  AvgStepTime:0.60\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.08749   PostnetLoss: 0.00536   DecoderLoss:0.00566  StopLoss: 0.07646  \n",
      "   | > TotalLoss: 0.09397   PostnetLoss: 0.00894   DecoderLoss:0.00939  StopLoss: 0.07563  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00094   Validation Loss: 0.00810\n",
      "\n",
      " > Epoch 633/1000\n",
      "   | > Step:2/68  GlobalStep:112680  TotalLoss:0.00128  PostnetLoss:0.00062  DecoderLoss:0.00066  StopLoss:0.05767  GradNorm:0.00416  GradNormST:0.02111  AvgTextLen:4.0  AvgSpecLen:31.3  StepTime:0.40  LR:0.000100\n",
      "   | > Step:12/68  GlobalStep:112690  TotalLoss:0.00150  PostnetLoss:0.00073  DecoderLoss:0.00077  StopLoss:0.06795  GradNorm:0.00332  GradNormST:0.03133  AvgTextLen:11.0  AvgSpecLen:49.3  StepTime:0.36  LR:0.000100\n",
      "   | > Step:22/68  GlobalStep:112700  TotalLoss:0.00172  PostnetLoss:0.00084  DecoderLoss:0.00088  StopLoss:0.06949  GradNorm:0.00294  GradNormST:0.01513  AvgTextLen:18.0  AvgSpecLen:72.0  StepTime:0.37  LR:0.000100\n",
      "   | > Step:32/68  GlobalStep:112710  TotalLoss:0.00186  PostnetLoss:0.00091  DecoderLoss:0.00096  StopLoss:0.08806  GradNorm:0.00282  GradNormST:0.02664  AvgTextLen:24.4  AvgSpecLen:91.5  StepTime:0.55  LR:0.000100\n",
      "   | > Step:42/68  GlobalStep:112720  TotalLoss:0.00205  PostnetLoss:0.00100  DecoderLoss:0.00105  StopLoss:0.05401  GradNorm:0.00282  GradNormST:0.02160  AvgTextLen:33.1  AvgSpecLen:109.7  StepTime:0.60  LR:0.000100\n",
      "   | > Step:52/68  GlobalStep:112730  TotalLoss:0.00223  PostnetLoss:0.00109  DecoderLoss:0.00115  StopLoss:0.07860  GradNorm:0.00299  GradNormST:0.01916  AvgTextLen:42.1  AvgSpecLen:137.5  StepTime:0.66  LR:0.000100\n",
      "   | > Step:62/68  GlobalStep:112740  TotalLoss:0.00244  PostnetLoss:0.00119  DecoderLoss:0.00126  StopLoss:0.05152  GradNorm:0.00276  GradNormST:0.01518  AvgTextLen:54.0  AvgSpecLen:174.7  StepTime:0.77  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:112746  AvgTotalLoss:0.05965  AvgPostnetLoss:0.00094  AvgDecoderLoss:0.00100  AvgStopLoss:0.05771  EpochTime:42.69  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.08701   PostnetLoss: 0.00548   DecoderLoss:0.00578  StopLoss: 0.07575  \n",
      "   | > TotalLoss: 0.09537   PostnetLoss: 0.00890   DecoderLoss:0.00934  StopLoss: 0.07713  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00094   Validation Loss: 0.00803\n",
      "\n",
      " > Epoch 634/1000\n",
      "   | > Step:3/68  GlobalStep:112750  TotalLoss:0.00132  PostnetLoss:0.00064  DecoderLoss:0.00068  StopLoss:0.07622  GradNorm:0.00378  GradNormST:0.02410  AvgTextLen:4.7  AvgSpecLen:29.7  StepTime:0.24  LR:0.000100\n",
      "   | > Step:13/68  GlobalStep:112760  TotalLoss:0.00148  PostnetLoss:0.00072  DecoderLoss:0.00076  StopLoss:0.06407  GradNorm:0.00306  GradNormST:0.02791  AvgTextLen:11.5  AvgSpecLen:55.2  StepTime:0.36  LR:0.000100\n",
      "   | > Step:23/68  GlobalStep:112770  TotalLoss:0.00177  PostnetLoss:0.00086  DecoderLoss:0.00090  StopLoss:0.04739  GradNorm:0.00318  GradNormST:0.01498  AvgTextLen:18.3  AvgSpecLen:72.2  StepTime:0.38  LR:0.000100\n",
      "   | > Step:33/68  GlobalStep:112780  TotalLoss:0.00194  PostnetLoss:0.00095  DecoderLoss:0.00100  StopLoss:0.04929  GradNorm:0.00313  GradNormST:0.01151  AvgTextLen:25.9  AvgSpecLen:88.9  StepTime:0.57  LR:0.000100\n",
      "   | > Step:43/68  GlobalStep:112790  TotalLoss:0.00210  PostnetLoss:0.00102  DecoderLoss:0.00108  StopLoss:0.06235  GradNorm:0.00291  GradNormST:0.01503  AvgTextLen:33.4  AvgSpecLen:115.7  StepTime:0.60  LR:0.000100\n",
      "   | > Step:53/68  GlobalStep:112800  TotalLoss:0.00222  PostnetLoss:0.00108  DecoderLoss:0.00114  StopLoss:0.03827  GradNorm:0.00289  GradNormST:0.00860  AvgTextLen:42.4  AvgSpecLen:137.6  StepTime:0.81  LR:0.000100\n",
      "   | > Step:63/68  GlobalStep:112810  TotalLoss:0.00250  PostnetLoss:0.00121  DecoderLoss:0.00129  StopLoss:0.03956  GradNorm:0.00270  GradNormST:0.01369  AvgTextLen:58.6  AvgSpecLen:185.1  StepTime:0.95  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:112815  AvgTotalLoss:0.05597  AvgPostnetLoss:0.00094  AvgDecoderLoss:0.00099  AvgStopLoss:0.05403  EpochTime:42.54  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.08388   PostnetLoss: 0.00541   DecoderLoss:0.00570  StopLoss: 0.07277  \n",
      "   | > TotalLoss: 0.09289   PostnetLoss: 0.00883   DecoderLoss:0.00927  StopLoss: 0.07480  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00094   Validation Loss: 0.00802\n",
      "\n",
      " > Epoch 635/1000\n",
      "   | > Step:4/68  GlobalStep:112820  TotalLoss:0.00132  PostnetLoss:0.00064  DecoderLoss:0.00068  StopLoss:0.08091  GradNorm:0.00409  GradNormST:0.02575  AvgTextLen:4.8  AvgSpecLen:31.1  StepTime:0.25  LR:0.000100\n",
      "   | > Step:14/68  GlobalStep:112830  TotalLoss:0.00160  PostnetLoss:0.00078  DecoderLoss:0.00082  StopLoss:0.08748  GradNorm:0.00359  GradNormST:0.04552  AvgTextLen:12.2  AvgSpecLen:52.0  StepTime:0.33  LR:0.000100\n",
      "   | > Step:24/68  GlobalStep:112840  TotalLoss:0.00192  PostnetLoss:0.00094  DecoderLoss:0.00098  StopLoss:0.09939  GradNorm:0.00339  GradNormST:0.07248  AvgTextLen:19.2  AvgSpecLen:79.0  StepTime:0.49  LR:0.000100\n",
      "   | > Step:34/68  GlobalStep:112850  TotalLoss:0.00208  PostnetLoss:0.00101  DecoderLoss:0.00106  StopLoss:0.10715  GradNorm:0.00340  GradNormST:0.07749  AvgTextLen:26.2  AvgSpecLen:95.9  StepTime:0.58  LR:0.000100\n",
      "   | > Step:44/68  GlobalStep:112860  TotalLoss:0.00224  PostnetLoss:0.00109  DecoderLoss:0.00115  StopLoss:0.06890  GradNorm:0.00327  GradNormST:0.04415  AvgTextLen:33.8  AvgSpecLen:116.3  StepTime:0.66  LR:0.000100\n",
      "   | > Step:54/68  GlobalStep:112870  TotalLoss:0.00241  PostnetLoss:0.00117  DecoderLoss:0.00124  StopLoss:0.04808  GradNorm:0.00316  GradNormST:0.01928  AvgTextLen:42.2  AvgSpecLen:144.9  StepTime:0.82  LR:0.000100\n",
      "   | > Step:64/68  GlobalStep:112880  TotalLoss:0.00245  PostnetLoss:0.00119  DecoderLoss:0.00126  StopLoss:0.04468  GradNorm:0.00268  GradNormST:0.01887  AvgTextLen:60.6  AvgSpecLen:177.6  StepTime:0.81  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:112884  AvgTotalLoss:0.07887  AvgPostnetLoss:0.00099  AvgDecoderLoss:0.00104  AvgStopLoss:0.07685  EpochTime:42.76  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.09426   PostnetLoss: 0.00545   DecoderLoss:0.00575  StopLoss: 0.08307  \n",
      "   | > TotalLoss: 0.10066   PostnetLoss: 0.00886   DecoderLoss:0.00929  StopLoss: 0.08251  \n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00099   Validation Loss: 0.00795\n",
      "\n",
      " > Epoch 636/1000\n",
      "   | > Step:5/68  GlobalStep:112890  TotalLoss:0.00144  PostnetLoss:0.00070  DecoderLoss:0.00075  StopLoss:0.05236  GradNorm:0.00426  GradNormST:0.01772  AvgTextLen:5.9  AvgSpecLen:33.2  StepTime:0.27  LR:0.000100\n",
      "   | > Step:15/68  GlobalStep:112900  TotalLoss:0.00170  PostnetLoss:0.00083  DecoderLoss:0.00087  StopLoss:0.05550  GradNorm:0.00407  GradNormST:0.02974  AvgTextLen:12.8  AvgSpecLen:59.1  StepTime:0.54  LR:0.000100\n",
      "   | > Step:25/68  GlobalStep:112910  TotalLoss:0.00183  PostnetLoss:0.00089  DecoderLoss:0.00094  StopLoss:0.05516  GradNorm:0.00340  GradNormST:0.02172  AvgTextLen:19.8  AvgSpecLen:73.3  StepTime:0.40  LR:0.000100\n",
      "   | > Step:35/68  GlobalStep:112920  TotalLoss:0.00202  PostnetLoss:0.00098  DecoderLoss:0.00104  StopLoss:0.05878  GradNorm:0.00313  GradNormST:0.03923  AvgTextLen:27.1  AvgSpecLen:104.7  StepTime:0.63  LR:0.000100\n",
      "   | > Step:45/68  GlobalStep:112930  TotalLoss:0.00211  PostnetLoss:0.00103  DecoderLoss:0.00108  StopLoss:0.06818  GradNorm:0.00284  GradNormST:0.02561  AvgTextLen:34.4  AvgSpecLen:118.6  StepTime:0.63  LR:0.000100\n",
      "   | > Step:55/68  GlobalStep:112940  TotalLoss:0.00239  PostnetLoss:0.00116  DecoderLoss:0.00123  StopLoss:0.05025  GradNorm:0.00316  GradNormST:0.01990  AvgTextLen:43.9  AvgSpecLen:145.2  StepTime:0.86  LR:0.000100\n",
      "   | > Step:65/68  GlobalStep:112950  TotalLoss:0.00262  PostnetLoss:0.00127  DecoderLoss:0.00135  StopLoss:0.04378  GradNorm:0.00275  GradNormST:0.02150  AvgTextLen:64.2  AvgSpecLen:207.4  StepTime:1.18  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:112953  AvgTotalLoss:0.06172  AvgPostnetLoss:0.00097  AvgDecoderLoss:0.00102  AvgStopLoss:0.05973  EpochTime:42.29  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.09268   PostnetLoss: 0.00541   DecoderLoss:0.00571  StopLoss: 0.08156  \n",
      "   | > TotalLoss: 0.09884   PostnetLoss: 0.00886   DecoderLoss:0.00925  StopLoss: 0.08072  \n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00097   Validation Loss: 0.00796\n",
      "\n",
      " > Epoch 637/1000\n",
      "   | > Step:6/68  GlobalStep:112960  TotalLoss:0.00140  PostnetLoss:0.00068  DecoderLoss:0.00071  StopLoss:0.05479  GradNorm:0.00405  GradNormST:0.02899  AvgTextLen:6.0  AvgSpecLen:34.8  StepTime:0.32  LR:0.000100\n",
      "   | > Step:16/68  GlobalStep:112970  TotalLoss:0.00170  PostnetLoss:0.00083  DecoderLoss:0.00087  StopLoss:0.04504  GradNorm:0.00342  GradNormST:0.01497  AvgTextLen:13.8  AvgSpecLen:60.6  StepTime:0.38  LR:0.000100\n",
      "   | > Step:26/68  GlobalStep:112980  TotalLoss:0.00183  PostnetLoss:0.00089  DecoderLoss:0.00093  StopLoss:0.05209  GradNorm:0.00319  GradNormST:0.02266  AvgTextLen:20.3  AvgSpecLen:77.5  StepTime:0.50  LR:0.000100\n",
      "   | > Step:36/68  GlobalStep:112990  TotalLoss:0.00198  PostnetLoss:0.00096  DecoderLoss:0.00102  StopLoss:0.06694  GradNorm:0.00295  GradNormST:0.01964  AvgTextLen:28.5  AvgSpecLen:100.2  StepTime:0.58  LR:0.000100\n",
      "   | > Step:46/68  GlobalStep:113000  TotalLoss:0.00213  PostnetLoss:0.00104  DecoderLoss:0.00109  StopLoss:0.04781  GradNorm:0.00298  GradNormST:0.02148  AvgTextLen:35.4  AvgSpecLen:122.6  StepTime:0.76  LR:0.000100\n",
      " | | > Checkpoint saving : /vm/TTS/models/cartman_models/mozilla-no-loc-fattn-stopnet-sigmoid-loss_masking-August-15-2019_11+26AM-de6aeb9/checkpoint_113000.pth.tar\n",
      "   | > Step:56/68  GlobalStep:113010  TotalLoss:0.00241  PostnetLoss:0.00117  DecoderLoss:0.00124  StopLoss:0.04461  GradNorm:0.00331  GradNormST:0.01870  AvgTextLen:45.8  AvgSpecLen:145.4  StepTime:0.83  LR:0.000100\n",
      "   | > Step:66/68  GlobalStep:113020  TotalLoss:0.00276  PostnetLoss:0.00134  DecoderLoss:0.00142  StopLoss:0.03926  GradNorm:0.00275  GradNormST:0.01600  AvgTextLen:71.2  AvgSpecLen:220.8  StepTime:1.03  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:113022  AvgTotalLoss:0.05666  AvgPostnetLoss:0.00096  AvgDecoderLoss:0.00101  AvgStopLoss:0.05469  EpochTime:42.90  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.08970   PostnetLoss: 0.00555   DecoderLoss:0.00584  StopLoss: 0.07831  \n",
      "   | > TotalLoss: 0.09045   PostnetLoss: 0.00861   DecoderLoss:0.00901  StopLoss: 0.07283  \n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00096   Validation Loss: 0.00793\n",
      "\n",
      " > Epoch 638/1000\n",
      "   | > Step:7/68  GlobalStep:113030  TotalLoss:0.00148  PostnetLoss:0.00072  DecoderLoss:0.00075  StopLoss:0.06473  GradNorm:0.00420  GradNormST:0.04367  AvgTextLen:7.4  AvgSpecLen:44.6  StepTime:0.30  LR:0.000100\n",
      "   | > Step:17/68  GlobalStep:113040  TotalLoss:0.00166  PostnetLoss:0.00081  DecoderLoss:0.00085  StopLoss:0.07245  GradNorm:0.00324  GradNormST:0.02111  AvgTextLen:14.7  AvgSpecLen:66.0  StepTime:0.42  LR:0.000100\n",
      "   | > Step:27/68  GlobalStep:113050  TotalLoss:0.00185  PostnetLoss:0.00090  DecoderLoss:0.00095  StopLoss:0.06748  GradNorm:0.00311  GradNormST:0.02519  AvgTextLen:21.0  AvgSpecLen:80.2  StepTime:0.56  LR:0.000100\n",
      "   | > Step:37/68  GlobalStep:113060  TotalLoss:0.00202  PostnetLoss:0.00099  DecoderLoss:0.00104  StopLoss:0.05569  GradNorm:0.00304  GradNormST:0.01856  AvgTextLen:29.3  AvgSpecLen:102.4  StepTime:0.51  LR:0.000100\n",
      "   | > Step:47/68  GlobalStep:113070  TotalLoss:0.00222  PostnetLoss:0.00108  DecoderLoss:0.00114  StopLoss:0.04123  GradNorm:0.00323  GradNormST:0.01565  AvgTextLen:36.7  AvgSpecLen:126.9  StepTime:0.71  LR:0.000100\n",
      "   | > Step:57/68  GlobalStep:113080  TotalLoss:0.00241  PostnetLoss:0.00117  DecoderLoss:0.00124  StopLoss:0.03625  GradNorm:0.00305  GradNormST:0.01463  AvgTextLen:47.1  AvgSpecLen:161.0  StepTime:0.88  LR:0.000100\n",
      "   | > Step:67/68  GlobalStep:113090  TotalLoss:0.00272  PostnetLoss:0.00132  DecoderLoss:0.00141  StopLoss:0.03539  GradNorm:0.00266  GradNormST:0.01818  AvgTextLen:80.6  AvgSpecLen:246.5  StepTime:1.21  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:113091  AvgTotalLoss:0.05454  AvgPostnetLoss:0.00095  AvgDecoderLoss:0.00101  AvgStopLoss:0.05258  EpochTime:42.54  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.09460   PostnetLoss: 0.00531   DecoderLoss:0.00560  StopLoss: 0.08370  \n",
      "   | > TotalLoss: 0.09793   PostnetLoss: 0.00889   DecoderLoss:0.00932  StopLoss: 0.07971  \n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00095   Validation Loss: 0.00798\n",
      "\n",
      " > Epoch 639/1000\n",
      "   | > Step:8/68  GlobalStep:113100  TotalLoss:0.00149  PostnetLoss:0.00073  DecoderLoss:0.00076  StopLoss:0.05424  GradNorm:0.00385  GradNormST:0.03278  AvgTextLen:8.3  AvgSpecLen:44.0  StepTime:0.27  LR:0.000100\n",
      "   | > Step:18/68  GlobalStep:113110  TotalLoss:0.00163  PostnetLoss:0.00080  DecoderLoss:0.00084  StopLoss:0.05077  GradNorm:0.00331  GradNormST:0.01633  AvgTextLen:15.2  AvgSpecLen:64.5  StepTime:0.43  LR:0.000100\n",
      "   | > Step:28/68  GlobalStep:113120  TotalLoss:0.00187  PostnetLoss:0.00091  DecoderLoss:0.00096  StopLoss:0.07415  GradNorm:0.00306  GradNormST:0.02626  AvgTextLen:22.1  AvgSpecLen:85.4  StepTime:0.58  LR:0.000100\n",
      "   | > Step:38/68  GlobalStep:113130  TotalLoss:0.00199  PostnetLoss:0.00097  DecoderLoss:0.00102  StopLoss:0.04396  GradNorm:0.00305  GradNormST:0.01570  AvgTextLen:29.4  AvgSpecLen:104.1  StepTime:0.49  LR:0.000100\n",
      "   | > Step:48/68  GlobalStep:113140  TotalLoss:0.00219  PostnetLoss:0.00106  DecoderLoss:0.00112  StopLoss:0.03969  GradNorm:0.00313  GradNormST:0.01115  AvgTextLen:37.5  AvgSpecLen:123.8  StepTime:0.72  LR:0.000100\n",
      "   | > Step:58/68  GlobalStep:113150  TotalLoss:0.00237  PostnetLoss:0.00116  DecoderLoss:0.00122  StopLoss:0.04223  GradNorm:0.00311  GradNormST:0.01924  AvgTextLen:47.8  AvgSpecLen:158.0  StepTime:0.82  LR:0.000100\n",
      "   | > Step:68/68  GlobalStep:113160  TotalLoss:0.00278  PostnetLoss:0.00134  DecoderLoss:0.00143  StopLoss:0.03078  GradNorm:0.00278  GradNormST:0.01342  AvgTextLen:103.0  AvgSpecLen:308.9  StepTime:1.87  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:113160  AvgTotalLoss:0.05205  AvgPostnetLoss:0.00095  AvgDecoderLoss:0.00100  AvgStopLoss:0.05010  EpochTime:42.32  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.09107   PostnetLoss: 0.00541   DecoderLoss:0.00569  StopLoss: 0.07996  \n",
      "   | > TotalLoss: 0.09504   PostnetLoss: 0.00902   DecoderLoss:0.00941  StopLoss: 0.07660  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00095   Validation Loss: 0.00815\n",
      "\n",
      " > Epoch 640/1000\n",
      "   | > Step:9/68  GlobalStep:113170  TotalLoss:0.00145  PostnetLoss:0.00071  DecoderLoss:0.00074  StopLoss:0.04424  GradNorm:0.00339  GradNormST:0.01314  AvgTextLen:9.0  AvgSpecLen:42.1  StepTime:0.34  LR:0.000100\n",
      "   | > Step:19/68  GlobalStep:113180  TotalLoss:0.00164  PostnetLoss:0.00080  DecoderLoss:0.00084  StopLoss:0.05938  GradNorm:0.00309  GradNormST:0.02249  AvgTextLen:15.5  AvgSpecLen:63.2  StepTime:0.44  LR:0.000100\n",
      "   | > Step:29/68  GlobalStep:113190  TotalLoss:0.00181  PostnetLoss:0.00089  DecoderLoss:0.00093  StopLoss:0.05490  GradNorm:0.00292  GradNormST:0.01742  AvgTextLen:21.8  AvgSpecLen:83.4  StepTime:0.45  LR:0.000100\n",
      "   | > Step:39/68  GlobalStep:113200  TotalLoss:0.00204  PostnetLoss:0.00099  DecoderLoss:0.00105  StopLoss:0.04153  GradNorm:0.00303  GradNormST:0.01624  AvgTextLen:30.2  AvgSpecLen:110.4  StepTime:0.68  LR:0.000100\n",
      "   | > Step:49/68  GlobalStep:113210  TotalLoss:0.00222  PostnetLoss:0.00108  DecoderLoss:0.00114  StopLoss:0.03826  GradNorm:0.00347  GradNormST:0.01251  AvgTextLen:38.2  AvgSpecLen:127.7  StepTime:0.60  LR:0.000100\n",
      "   | > Step:59/68  GlobalStep:113220  TotalLoss:0.00236  PostnetLoss:0.00115  DecoderLoss:0.00122  StopLoss:0.04504  GradNorm:0.00354  GradNormST:0.01292  AvgTextLen:50.5  AvgSpecLen:155.9  StepTime:0.87  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:113229  AvgTotalLoss:0.04998  AvgPostnetLoss:0.00095  AvgDecoderLoss:0.00100  AvgStopLoss:0.04803  EpochTime:42.06  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.08967   PostnetLoss: 0.00558   DecoderLoss:0.00587  StopLoss: 0.07821  \n",
      "   | > TotalLoss: 0.09462   PostnetLoss: 0.00881   DecoderLoss:0.00922  StopLoss: 0.07659  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00095   Validation Loss: 0.00805\n",
      "\n",
      " > Epoch 641/1000\n",
      "   | > Step:0/68  GlobalStep:113230  TotalLoss:0.00151  PostnetLoss:0.00073  DecoderLoss:0.00079  StopLoss:0.06204  GradNorm:0.00913  GradNormST:0.02529  AvgTextLen:3.0  AvgSpecLen:25.3  StepTime:0.43  LR:0.000100\n",
      "   | > Step:10/68  GlobalStep:113240  TotalLoss:0.00152  PostnetLoss:0.00074  DecoderLoss:0.00077  StopLoss:0.04148  GradNorm:0.00378  GradNormST:0.01377  AvgTextLen:9.8  AvgSpecLen:48.6  StepTime:0.54  LR:0.000100\n",
      "   | > Step:20/68  GlobalStep:113250  TotalLoss:0.00173  PostnetLoss:0.00084  DecoderLoss:0.00089  StopLoss:0.03699  GradNorm:0.00317  GradNormST:0.01197  AvgTextLen:16.4  AvgSpecLen:64.8  StepTime:0.49  LR:0.000100\n",
      "   | > Step:30/68  GlobalStep:113260  TotalLoss:0.00186  PostnetLoss:0.00091  DecoderLoss:0.00096  StopLoss:0.05241  GradNorm:0.00313  GradNormST:0.01461  AvgTextLen:22.9  AvgSpecLen:85.8  StepTime:0.60  LR:0.000100\n",
      "   | > Step:40/68  GlobalStep:113270  TotalLoss:0.00201  PostnetLoss:0.00098  DecoderLoss:0.00103  StopLoss:0.04158  GradNorm:0.00306  GradNormST:0.01069  AvgTextLen:30.9  AvgSpecLen:105.3  StepTime:0.60  LR:0.000100\n",
      "   | > Step:50/68  GlobalStep:113280  TotalLoss:0.00222  PostnetLoss:0.00108  DecoderLoss:0.00114  StopLoss:0.03491  GradNorm:0.00330  GradNormST:0.01205  AvgTextLen:38.8  AvgSpecLen:132.0  StepTime:0.67  LR:0.000100\n",
      "   | > Step:60/68  GlobalStep:113290  TotalLoss:0.00242  PostnetLoss:0.00118  DecoderLoss:0.00124  StopLoss:0.03324  GradNorm:0.00308  GradNormST:0.01101  AvgTextLen:52.0  AvgSpecLen:172.7  StepTime:0.98  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:113298  AvgTotalLoss:0.04815  AvgPostnetLoss:0.00095  AvgDecoderLoss:0.00100  AvgStopLoss:0.04619  EpochTime:42.67  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.08735   PostnetLoss: 0.00543   DecoderLoss:0.00571  StopLoss: 0.07622  \n",
      "   | > TotalLoss: 0.09811   PostnetLoss: 0.00926   DecoderLoss:0.00966  StopLoss: 0.07919  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00095   Validation Loss: 0.00827\n",
      "\n",
      " > Epoch 642/1000\n",
      "   | > Step:1/68  GlobalStep:113300  TotalLoss:0.00138  PostnetLoss:0.00067  DecoderLoss:0.00071  StopLoss:0.04394  GradNorm:0.00525  GradNormST:0.02204  AvgTextLen:3.7  AvgSpecLen:28.6  StepTime:0.28  LR:0.000100\n",
      "   | > Step:11/68  GlobalStep:113310  TotalLoss:0.00148  PostnetLoss:0.00072  DecoderLoss:0.00076  StopLoss:0.04563  GradNorm:0.00360  GradNormST:0.01431  AvgTextLen:10.3  AvgSpecLen:49.4  StepTime:0.30  LR:0.000100\n",
      "   | > Step:21/68  GlobalStep:113320  TotalLoss:0.00169  PostnetLoss:0.00083  DecoderLoss:0.00087  StopLoss:0.04093  GradNorm:0.00306  GradNormST:0.01223  AvgTextLen:16.6  AvgSpecLen:66.3  StepTime:0.45  LR:0.000100\n",
      "   | > Step:31/68  GlobalStep:113330  TotalLoss:0.00185  PostnetLoss:0.00090  DecoderLoss:0.00095  StopLoss:0.03842  GradNorm:0.00310  GradNormST:0.01171  AvgTextLen:24.1  AvgSpecLen:82.7  StepTime:0.54  LR:0.000100\n",
      "   | > Step:41/68  GlobalStep:113340  TotalLoss:0.00211  PostnetLoss:0.00103  DecoderLoss:0.00108  StopLoss:0.03349  GradNorm:0.00309  GradNormST:0.00923  AvgTextLen:32.3  AvgSpecLen:109.4  StepTime:0.68  LR:0.000100\n",
      "   | > Step:51/68  GlobalStep:113350  TotalLoss:0.00219  PostnetLoss:0.00107  DecoderLoss:0.00113  StopLoss:0.05005  GradNorm:0.00319  GradNormST:0.01778  AvgTextLen:39.2  AvgSpecLen:126.9  StepTime:0.61  LR:0.000100\n",
      "   | > Step:61/68  GlobalStep:113360  TotalLoss:0.00240  PostnetLoss:0.00117  DecoderLoss:0.00124  StopLoss:0.04730  GradNorm:0.00346  GradNormST:0.02028  AvgTextLen:54.6  AvgSpecLen:165.5  StepTime:0.90  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:113367  AvgTotalLoss:0.04719  AvgPostnetLoss:0.00095  AvgDecoderLoss:0.00100  AvgStopLoss:0.04525  EpochTime:41.64  AvgStepTime:0.60\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.08323   PostnetLoss: 0.00542   DecoderLoss:0.00571  StopLoss: 0.07211  \n",
      "   | > TotalLoss: 0.09435   PostnetLoss: 0.00895   DecoderLoss:0.00937  StopLoss: 0.07604  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00095   Validation Loss: 0.00813\n",
      "\n",
      " > Epoch 643/1000\n",
      "   | > Step:2/68  GlobalStep:113370  TotalLoss:0.00127  PostnetLoss:0.00062  DecoderLoss:0.00065  StopLoss:0.04881  GradNorm:0.00367  GradNormST:0.01882  AvgTextLen:4.0  AvgSpecLen:31.3  StepTime:0.41  LR:0.000100\n",
      "   | > Step:12/68  GlobalStep:113380  TotalLoss:0.00147  PostnetLoss:0.00072  DecoderLoss:0.00076  StopLoss:0.05916  GradNorm:0.00319  GradNormST:0.01676  AvgTextLen:11.0  AvgSpecLen:49.3  StepTime:0.42  LR:0.000100\n",
      "   | > Step:22/68  GlobalStep:113390  TotalLoss:0.00171  PostnetLoss:0.00084  DecoderLoss:0.00088  StopLoss:0.04888  GradNorm:0.00307  GradNormST:0.01626  AvgTextLen:18.0  AvgSpecLen:72.0  StepTime:0.48  LR:0.000100\n",
      "   | > Step:32/68  GlobalStep:113400  TotalLoss:0.00187  PostnetLoss:0.00091  DecoderLoss:0.00096  StopLoss:0.06286  GradNorm:0.00288  GradNormST:0.01992  AvgTextLen:24.4  AvgSpecLen:91.5  StepTime:0.59  LR:0.000100\n",
      "   | > Step:42/68  GlobalStep:113410  TotalLoss:0.00203  PostnetLoss:0.00098  DecoderLoss:0.00104  StopLoss:0.05703  GradNorm:0.00285  GradNormST:0.02229  AvgTextLen:33.1  AvgSpecLen:109.7  StepTime:0.49  LR:0.000100\n",
      "   | > Step:52/68  GlobalStep:113420  TotalLoss:0.00224  PostnetLoss:0.00109  DecoderLoss:0.00115  StopLoss:0.04489  GradNorm:0.00344  GradNormST:0.01368  AvgTextLen:42.1  AvgSpecLen:137.5  StepTime:0.78  LR:0.000100\n",
      "   | > Step:62/68  GlobalStep:113430  TotalLoss:0.00248  PostnetLoss:0.00120  DecoderLoss:0.00127  StopLoss:0.03691  GradNorm:0.00346  GradNormST:0.02095  AvgTextLen:54.0  AvgSpecLen:174.7  StepTime:0.75  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:113436  AvgTotalLoss:0.04759  AvgPostnetLoss:0.00094  AvgDecoderLoss:0.00099  AvgStopLoss:0.04566  EpochTime:41.84  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.08479   PostnetLoss: 0.00555   DecoderLoss:0.00584  StopLoss: 0.07339  \n",
      "   | > TotalLoss: 0.09512   PostnetLoss: 0.00904   DecoderLoss:0.00945  StopLoss: 0.07662  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00094   Validation Loss: 0.00816\n",
      "\n",
      " > Epoch 644/1000\n",
      "   | > Step:3/68  GlobalStep:113440  TotalLoss:0.00127  PostnetLoss:0.00062  DecoderLoss:0.00065  StopLoss:0.08597  GradNorm:0.00378  GradNormST:0.02594  AvgTextLen:4.7  AvgSpecLen:29.7  StepTime:0.33  LR:0.000100\n",
      "   | > Step:13/68  GlobalStep:113450  TotalLoss:0.00149  PostnetLoss:0.00073  DecoderLoss:0.00076  StopLoss:0.06484  GradNorm:0.00339  GradNormST:0.02578  AvgTextLen:11.5  AvgSpecLen:55.2  StepTime:0.41  LR:0.000100\n",
      "   | > Step:23/68  GlobalStep:113460  TotalLoss:0.00178  PostnetLoss:0.00087  DecoderLoss:0.00091  StopLoss:0.05310  GradNorm:0.00303  GradNormST:0.01224  AvgTextLen:18.3  AvgSpecLen:72.2  StepTime:0.38  LR:0.000100\n",
      "   | > Step:33/68  GlobalStep:113470  TotalLoss:0.00193  PostnetLoss:0.00094  DecoderLoss:0.00099  StopLoss:0.04359  GradNorm:0.00286  GradNormST:0.01070  AvgTextLen:25.9  AvgSpecLen:88.9  StepTime:0.44  LR:0.000100\n",
      "   | > Step:43/68  GlobalStep:113480  TotalLoss:0.00204  PostnetLoss:0.00099  DecoderLoss:0.00105  StopLoss:0.04514  GradNorm:0.00288  GradNormST:0.01663  AvgTextLen:33.4  AvgSpecLen:115.7  StepTime:0.52  LR:0.000100\n",
      "   | > Step:53/68  GlobalStep:113490  TotalLoss:0.00223  PostnetLoss:0.00108  DecoderLoss:0.00115  StopLoss:0.03721  GradNorm:0.00340  GradNormST:0.00950  AvgTextLen:42.4  AvgSpecLen:137.6  StepTime:0.76  LR:0.000100\n",
      "   | > Step:63/68  GlobalStep:113500  TotalLoss:0.00250  PostnetLoss:0.00121  DecoderLoss:0.00129  StopLoss:0.02424  GradNorm:0.00292  GradNormST:0.00705  AvgTextLen:58.6  AvgSpecLen:185.1  StepTime:0.90  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:113505  AvgTotalLoss:0.04808  AvgPostnetLoss:0.00094  AvgDecoderLoss:0.00099  AvgStopLoss:0.04615  EpochTime:41.48  AvgStepTime:0.60\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.08375   PostnetLoss: 0.00547   DecoderLoss:0.00575  StopLoss: 0.07253  \n",
      "   | > TotalLoss: 0.09401   PostnetLoss: 0.00919   DecoderLoss:0.00960  StopLoss: 0.07521  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00094   Validation Loss: 0.00825\n",
      "\n",
      " > Epoch 645/1000\n",
      "   | > Step:4/68  GlobalStep:113510  TotalLoss:0.00131  PostnetLoss:0.00064  DecoderLoss:0.00067  StopLoss:0.08503  GradNorm:0.00433  GradNormST:0.04046  AvgTextLen:4.8  AvgSpecLen:31.1  StepTime:0.28  LR:0.000100\n",
      "   | > Step:14/68  GlobalStep:113520  TotalLoss:0.00156  PostnetLoss:0.00076  DecoderLoss:0.00080  StopLoss:0.04041  GradNorm:0.00320  GradNormST:0.01772  AvgTextLen:12.2  AvgSpecLen:52.0  StepTime:0.32  LR:0.000100\n",
      "   | > Step:24/68  GlobalStep:113530  TotalLoss:0.00183  PostnetLoss:0.00089  DecoderLoss:0.00094  StopLoss:0.05225  GradNorm:0.00302  GradNormST:0.01403  AvgTextLen:19.2  AvgSpecLen:79.0  StepTime:0.42  LR:0.000100\n",
      "   | > Step:34/68  GlobalStep:113540  TotalLoss:0.00196  PostnetLoss:0.00095  DecoderLoss:0.00100  StopLoss:0.04067  GradNorm:0.00278  GradNormST:0.01275  AvgTextLen:26.2  AvgSpecLen:95.9  StepTime:0.61  LR:0.000100\n",
      "   | > Step:44/68  GlobalStep:113550  TotalLoss:0.00214  PostnetLoss:0.00104  DecoderLoss:0.00110  StopLoss:0.05654  GradNorm:0.00297  GradNormST:0.02160  AvgTextLen:33.8  AvgSpecLen:116.3  StepTime:0.66  LR:0.000100\n",
      "   | > Step:54/68  GlobalStep:113560  TotalLoss:0.00226  PostnetLoss:0.00110  DecoderLoss:0.00116  StopLoss:0.03816  GradNorm:0.00302  GradNormST:0.01079  AvgTextLen:42.2  AvgSpecLen:144.9  StepTime:0.81  LR:0.000100\n",
      "   | > Step:64/68  GlobalStep:113570  TotalLoss:0.00239  PostnetLoss:0.00116  DecoderLoss:0.00123  StopLoss:0.02794  GradNorm:0.00296  GradNormST:0.01132  AvgTextLen:60.6  AvgSpecLen:177.6  StepTime:0.81  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:113574  AvgTotalLoss:0.04607  AvgPostnetLoss:0.00094  AvgDecoderLoss:0.00099  AvgStopLoss:0.04413  EpochTime:41.20  AvgStepTime:0.60\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.08817   PostnetLoss: 0.00540   DecoderLoss:0.00568  StopLoss: 0.07709  \n",
      "   | > TotalLoss: 0.09304   PostnetLoss: 0.00907   DecoderLoss:0.00946  StopLoss: 0.07451  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00094   Validation Loss: 0.00815\n",
      "\n",
      " > Epoch 646/1000\n",
      "   | > Step:5/68  GlobalStep:113580  TotalLoss:0.00135  PostnetLoss:0.00065  DecoderLoss:0.00070  StopLoss:0.05299  GradNorm:0.00325  GradNormST:0.01704  AvgTextLen:5.9  AvgSpecLen:33.2  StepTime:0.33  LR:0.000100\n",
      "   | > Step:15/68  GlobalStep:113590  TotalLoss:0.00165  PostnetLoss:0.00080  DecoderLoss:0.00084  StopLoss:0.05719  GradNorm:0.00317  GradNormST:0.01916  AvgTextLen:12.8  AvgSpecLen:59.1  StepTime:0.42  LR:0.000100\n",
      "   | > Step:25/68  GlobalStep:113600  TotalLoss:0.00176  PostnetLoss:0.00086  DecoderLoss:0.00090  StopLoss:0.05366  GradNorm:0.00309  GradNormST:0.01392  AvgTextLen:19.8  AvgSpecLen:73.3  StepTime:0.40  LR:0.000100\n",
      "   | > Step:35/68  GlobalStep:113610  TotalLoss:0.00197  PostnetLoss:0.00096  DecoderLoss:0.00101  StopLoss:0.04577  GradNorm:0.00278  GradNormST:0.01590  AvgTextLen:27.1  AvgSpecLen:104.7  StepTime:0.75  LR:0.000100\n",
      "   | > Step:45/68  GlobalStep:113620  TotalLoss:0.00205  PostnetLoss:0.00100  DecoderLoss:0.00105  StopLoss:0.04613  GradNorm:0.00272  GradNormST:0.01761  AvgTextLen:34.4  AvgSpecLen:118.6  StepTime:0.70  LR:0.000100\n",
      "   | > Step:55/68  GlobalStep:113630  TotalLoss:0.00232  PostnetLoss:0.00113  DecoderLoss:0.00119  StopLoss:0.04796  GradNorm:0.00312  GradNormST:0.01474  AvgTextLen:43.9  AvgSpecLen:145.2  StepTime:0.80  LR:0.000100\n",
      "   | > Step:65/68  GlobalStep:113640  TotalLoss:0.00257  PostnetLoss:0.00124  DecoderLoss:0.00132  StopLoss:0.03544  GradNorm:0.00289  GradNormST:0.01232  AvgTextLen:64.2  AvgSpecLen:207.4  StepTime:1.19  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:113643  AvgTotalLoss:0.04832  AvgPostnetLoss:0.00094  AvgDecoderLoss:0.00099  AvgStopLoss:0.04639  EpochTime:43.01  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.08611   PostnetLoss: 0.00546   DecoderLoss:0.00575  StopLoss: 0.07490  \n",
      "   | > TotalLoss: 0.08835   PostnetLoss: 0.00892   DecoderLoss:0.00933  StopLoss: 0.07010  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00094   Validation Loss: 0.00812\n",
      "\n",
      " > Epoch 647/1000\n",
      "   | > Step:6/68  GlobalStep:113650  TotalLoss:0.00135  PostnetLoss:0.00066  DecoderLoss:0.00069  StopLoss:0.04681  GradNorm:0.00404  GradNormST:0.01792  AvgTextLen:6.0  AvgSpecLen:34.8  StepTime:0.39  LR:0.000100\n",
      "   | > Step:16/68  GlobalStep:113660  TotalLoss:0.00162  PostnetLoss:0.00079  DecoderLoss:0.00083  StopLoss:0.03713  GradNorm:0.00325  GradNormST:0.01256  AvgTextLen:13.8  AvgSpecLen:60.6  StepTime:0.37  LR:0.000100\n",
      "   | > Step:26/68  GlobalStep:113670  TotalLoss:0.00183  PostnetLoss:0.00089  DecoderLoss:0.00094  StopLoss:0.04370  GradNorm:0.00339  GradNormST:0.01018  AvgTextLen:20.3  AvgSpecLen:77.5  StepTime:0.38  LR:0.000100\n",
      "   | > Step:36/68  GlobalStep:113680  TotalLoss:0.00198  PostnetLoss:0.00096  DecoderLoss:0.00102  StopLoss:0.05221  GradNorm:0.00290  GradNormST:0.01314  AvgTextLen:28.5  AvgSpecLen:100.2  StepTime:0.57  LR:0.000100\n",
      "   | > Step:46/68  GlobalStep:113690  TotalLoss:0.00211  PostnetLoss:0.00103  DecoderLoss:0.00108  StopLoss:0.04241  GradNorm:0.00271  GradNormST:0.01950  AvgTextLen:35.4  AvgSpecLen:122.6  StepTime:0.66  LR:0.000100\n",
      "   | > Step:56/68  GlobalStep:113700  TotalLoss:0.00230  PostnetLoss:0.00112  DecoderLoss:0.00118  StopLoss:0.03432  GradNorm:0.00273  GradNormST:0.01501  AvgTextLen:45.8  AvgSpecLen:145.4  StepTime:0.84  LR:0.000100\n",
      "   | > Step:66/68  GlobalStep:113710  TotalLoss:0.00269  PostnetLoss:0.00130  DecoderLoss:0.00139  StopLoss:0.03115  GradNorm:0.00314  GradNormST:0.01637  AvgTextLen:71.2  AvgSpecLen:220.8  StepTime:1.06  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:113712  AvgTotalLoss:0.04774  AvgPostnetLoss:0.00094  AvgDecoderLoss:0.00099  AvgStopLoss:0.04581  EpochTime:41.84  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.08748   PostnetLoss: 0.00534   DecoderLoss:0.00562  StopLoss: 0.07652  \n",
      "   | > TotalLoss: 0.09377   PostnetLoss: 0.00935   DecoderLoss:0.00975  StopLoss: 0.07467  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00094   Validation Loss: 0.00831\n",
      "\n",
      " > Epoch 648/1000\n",
      "   | > Step:7/68  GlobalStep:113720  TotalLoss:0.00147  PostnetLoss:0.00072  DecoderLoss:0.00075  StopLoss:0.04357  GradNorm:0.00433  GradNormST:0.01923  AvgTextLen:7.4  AvgSpecLen:44.6  StepTime:0.33  LR:0.000100\n",
      "   | > Step:17/68  GlobalStep:113730  TotalLoss:0.00163  PostnetLoss:0.00080  DecoderLoss:0.00084  StopLoss:0.06463  GradNorm:0.00350  GradNormST:0.01596  AvgTextLen:14.7  AvgSpecLen:66.0  StepTime:0.42  LR:0.000100\n",
      "   | > Step:27/68  GlobalStep:113740  TotalLoss:0.00187  PostnetLoss:0.00091  DecoderLoss:0.00096  StopLoss:0.05079  GradNorm:0.00292  GradNormST:0.01685  AvgTextLen:21.0  AvgSpecLen:80.2  StepTime:0.53  LR:0.000100\n",
      "   | > Step:37/68  GlobalStep:113750  TotalLoss:0.00198  PostnetLoss:0.00096  DecoderLoss:0.00101  StopLoss:0.05465  GradNorm:0.00283  GradNormST:0.01216  AvgTextLen:29.3  AvgSpecLen:102.4  StepTime:0.60  LR:0.000100\n",
      "   | > Step:47/68  GlobalStep:113760  TotalLoss:0.00217  PostnetLoss:0.00106  DecoderLoss:0.00111  StopLoss:0.04021  GradNorm:0.00306  GradNormST:0.01014  AvgTextLen:36.7  AvgSpecLen:126.9  StepTime:0.74  LR:0.000100\n",
      "   | > Step:57/68  GlobalStep:113770  TotalLoss:0.00237  PostnetLoss:0.00115  DecoderLoss:0.00122  StopLoss:0.03010  GradNorm:0.00261  GradNormST:0.00764  AvgTextLen:47.1  AvgSpecLen:161.0  StepTime:0.69  LR:0.000100\n",
      "   | > Step:67/68  GlobalStep:113780  TotalLoss:0.00271  PostnetLoss:0.00131  DecoderLoss:0.00140  StopLoss:0.02791  GradNorm:0.00317  GradNormST:0.01595  AvgTextLen:80.6  AvgSpecLen:246.5  StepTime:1.21  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:113781  AvgTotalLoss:0.04702  AvgPostnetLoss:0.00094  AvgDecoderLoss:0.00099  AvgStopLoss:0.04508  EpochTime:42.48  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.08919   PostnetLoss: 0.00554   DecoderLoss:0.00583  StopLoss: 0.07783  \n",
      "   | > TotalLoss: 0.09249   PostnetLoss: 0.00928   DecoderLoss:0.00969  StopLoss: 0.07352  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00094   Validation Loss: 0.00834\n",
      "\n",
      " > Epoch 649/1000\n",
      "   | > Step:8/68  GlobalStep:113790  TotalLoss:0.00148  PostnetLoss:0.00072  DecoderLoss:0.00075  StopLoss:0.04879  GradNorm:0.00376  GradNormST:0.02347  AvgTextLen:8.3  AvgSpecLen:44.0  StepTime:0.24  LR:0.000100\n",
      "   | > Step:18/68  GlobalStep:113800  TotalLoss:0.00159  PostnetLoss:0.00078  DecoderLoss:0.00082  StopLoss:0.05045  GradNorm:0.00303  GradNormST:0.01153  AvgTextLen:15.2  AvgSpecLen:64.5  StepTime:0.37  LR:0.000100\n",
      "   | > Step:28/68  GlobalStep:113810  TotalLoss:0.00185  PostnetLoss:0.00090  DecoderLoss:0.00095  StopLoss:0.05255  GradNorm:0.00304  GradNormST:0.01198  AvgTextLen:22.1  AvgSpecLen:85.4  StepTime:0.57  LR:0.000100\n",
      "   | > Step:38/68  GlobalStep:113820  TotalLoss:0.00200  PostnetLoss:0.00097  DecoderLoss:0.00103  StopLoss:0.03879  GradNorm:0.00296  GradNormST:0.01197  AvgTextLen:29.4  AvgSpecLen:104.1  StepTime:0.59  LR:0.000100\n",
      "   | > Step:48/68  GlobalStep:113830  TotalLoss:0.00214  PostnetLoss:0.00104  DecoderLoss:0.00110  StopLoss:0.03919  GradNorm:0.00281  GradNormST:0.01098  AvgTextLen:37.5  AvgSpecLen:123.8  StepTime:0.65  LR:0.000100\n",
      "   | > Step:58/68  GlobalStep:113840  TotalLoss:0.00233  PostnetLoss:0.00114  DecoderLoss:0.00120  StopLoss:0.03962  GradNorm:0.00293  GradNormST:0.01277  AvgTextLen:47.8  AvgSpecLen:158.0  StepTime:0.70  LR:0.000100\n",
      "   | > Step:68/68  GlobalStep:113850  TotalLoss:0.00268  PostnetLoss:0.00130  DecoderLoss:0.00138  StopLoss:0.02959  GradNorm:0.00324  GradNormST:0.01394  AvgTextLen:103.0  AvgSpecLen:308.9  StepTime:1.88  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:113850  AvgTotalLoss:0.04681  AvgPostnetLoss:0.00094  AvgDecoderLoss:0.00099  AvgStopLoss:0.04488  EpochTime:41.88  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.08949   PostnetLoss: 0.00546   DecoderLoss:0.00576  StopLoss: 0.07827  \n",
      "   | > TotalLoss: 0.09430   PostnetLoss: 0.00933   DecoderLoss:0.00975  StopLoss: 0.07522  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00094   Validation Loss: 0.00831\n",
      "\n",
      " > Epoch 650/1000\n",
      "   | > Step:9/68  GlobalStep:113860  TotalLoss:0.00144  PostnetLoss:0.00070  DecoderLoss:0.00073  StopLoss:0.04354  GradNorm:0.00387  GradNormST:0.01635  AvgTextLen:9.0  AvgSpecLen:42.1  StepTime:0.45  LR:0.000100\n",
      "   | > Step:19/68  GlobalStep:113870  TotalLoss:0.00162  PostnetLoss:0.00079  DecoderLoss:0.00083  StopLoss:0.05597  GradNorm:0.00308  GradNormST:0.01526  AvgTextLen:15.5  AvgSpecLen:63.2  StepTime:0.32  LR:0.000100\n",
      "   | > Step:29/68  GlobalStep:113880  TotalLoss:0.00181  PostnetLoss:0.00088  DecoderLoss:0.00093  StopLoss:0.05102  GradNorm:0.00303  GradNormST:0.01267  AvgTextLen:21.8  AvgSpecLen:83.4  StepTime:0.46  LR:0.000100\n",
      "   | > Step:39/68  GlobalStep:113890  TotalLoss:0.00202  PostnetLoss:0.00099  DecoderLoss:0.00104  StopLoss:0.04697  GradNorm:0.00299  GradNormST:0.01494  AvgTextLen:30.2  AvgSpecLen:110.4  StepTime:0.66  LR:0.000100\n",
      "   | > Step:49/68  GlobalStep:113900  TotalLoss:0.00216  PostnetLoss:0.00105  DecoderLoss:0.00111  StopLoss:0.03596  GradNorm:0.00304  GradNormST:0.01131  AvgTextLen:38.2  AvgSpecLen:127.7  StepTime:0.70  LR:0.000100\n",
      "   | > Step:59/68  GlobalStep:113910  TotalLoss:0.00234  PostnetLoss:0.00114  DecoderLoss:0.00120  StopLoss:0.04658  GradNorm:0.00270  GradNormST:0.02375  AvgTextLen:50.5  AvgSpecLen:155.9  StepTime:0.93  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:113919  AvgTotalLoss:0.04755  AvgPostnetLoss:0.00094  AvgDecoderLoss:0.00099  AvgStopLoss:0.04562  EpochTime:42.09  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.08991   PostnetLoss: 0.00558   DecoderLoss:0.00587  StopLoss: 0.07846  \n",
      "   | > TotalLoss: 0.09789   PostnetLoss: 0.00958   DecoderLoss:0.01001  StopLoss: 0.07830  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00094   Validation Loss: 0.00856\n",
      "\n",
      " > Epoch 651/1000\n",
      "   | > Step:0/68  GlobalStep:113920  TotalLoss:0.00143  PostnetLoss:0.00069  DecoderLoss:0.00074  StopLoss:0.05993  GradNorm:0.00750  GradNormST:0.02650  AvgTextLen:3.0  AvgSpecLen:25.3  StepTime:0.43  LR:0.000100\n",
      "   | > Step:10/68  GlobalStep:113930  TotalLoss:0.00149  PostnetLoss:0.00072  DecoderLoss:0.00076  StopLoss:0.03719  GradNorm:0.00360  GradNormST:0.01022  AvgTextLen:9.8  AvgSpecLen:48.6  StepTime:0.47  LR:0.000100\n",
      "   | > Step:20/68  GlobalStep:113940  TotalLoss:0.00170  PostnetLoss:0.00083  DecoderLoss:0.00087  StopLoss:0.04160  GradNorm:0.00319  GradNormST:0.02214  AvgTextLen:16.4  AvgSpecLen:64.8  StepTime:0.39  LR:0.000100\n",
      "   | > Step:30/68  GlobalStep:113950  TotalLoss:0.00185  PostnetLoss:0.00090  DecoderLoss:0.00095  StopLoss:0.04493  GradNorm:0.00302  GradNormST:0.01036  AvgTextLen:22.9  AvgSpecLen:85.8  StepTime:0.56  LR:0.000100\n",
      "   | > Step:40/68  GlobalStep:113960  TotalLoss:0.00201  PostnetLoss:0.00098  DecoderLoss:0.00103  StopLoss:0.03839  GradNorm:0.00291  GradNormST:0.00987  AvgTextLen:30.9  AvgSpecLen:105.3  StepTime:0.64  LR:0.000100\n",
      "   | > Step:50/68  GlobalStep:113970  TotalLoss:0.00219  PostnetLoss:0.00106  DecoderLoss:0.00112  StopLoss:0.03038  GradNorm:0.00294  GradNormST:0.00916  AvgTextLen:38.8  AvgSpecLen:132.0  StepTime:0.68  LR:0.000100\n",
      "   | > Step:60/68  GlobalStep:113980  TotalLoss:0.00239  PostnetLoss:0.00116  DecoderLoss:0.00123  StopLoss:0.02834  GradNorm:0.00311  GradNormST:0.00823  AvgTextLen:52.0  AvgSpecLen:172.7  StepTime:0.94  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:113988  AvgTotalLoss:0.04657  AvgPostnetLoss:0.00094  AvgDecoderLoss:0.00099  AvgStopLoss:0.04464  EpochTime:42.23  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.08743   PostnetLoss: 0.00557   DecoderLoss:0.00587  StopLoss: 0.07598  \n",
      "   | > TotalLoss: 0.09591   PostnetLoss: 0.00939   DecoderLoss:0.00981  StopLoss: 0.07671  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00094   Validation Loss: 0.00841\n",
      "\n",
      " > Epoch 652/1000\n",
      "   | > Step:1/68  GlobalStep:113990  TotalLoss:0.00136  PostnetLoss:0.00066  DecoderLoss:0.00070  StopLoss:0.04135  GradNorm:0.00544  GradNormST:0.01506  AvgTextLen:3.7  AvgSpecLen:28.6  StepTime:0.31  LR:0.000100\n",
      "   | > Step:11/68  GlobalStep:114000  TotalLoss:0.00147  PostnetLoss:0.00072  DecoderLoss:0.00075  StopLoss:0.03191  GradNorm:0.00359  GradNormST:0.01488  AvgTextLen:10.3  AvgSpecLen:49.4  StepTime:0.39  LR:0.000100\n",
      " | | > Checkpoint saving : /vm/TTS/models/cartman_models/mozilla-no-loc-fattn-stopnet-sigmoid-loss_masking-August-15-2019_11+26AM-de6aeb9/checkpoint_114000.pth.tar\n",
      "warning: audio amplitude out of range, auto clipped.\n",
      "   | > Step:21/68  GlobalStep:114010  TotalLoss:0.00170  PostnetLoss:0.00083  DecoderLoss:0.00087  StopLoss:0.04911  GradNorm:0.00335  GradNormST:0.01306  AvgTextLen:16.6  AvgSpecLen:66.3  StepTime:0.45  LR:0.000100\n",
      "   | > Step:31/68  GlobalStep:114020  TotalLoss:0.00186  PostnetLoss:0.00091  DecoderLoss:0.00095  StopLoss:0.03492  GradNorm:0.00306  GradNormST:0.00927  AvgTextLen:24.1  AvgSpecLen:82.7  StepTime:0.54  LR:0.000100\n",
      "   | > Step:41/68  GlobalStep:114030  TotalLoss:0.00209  PostnetLoss:0.00102  DecoderLoss:0.00107  StopLoss:0.02718  GradNorm:0.00330  GradNormST:0.00904  AvgTextLen:32.3  AvgSpecLen:109.4  StepTime:0.66  LR:0.000100\n",
      "   | > Step:51/68  GlobalStep:114040  TotalLoss:0.00217  PostnetLoss:0.00105  DecoderLoss:0.00111  StopLoss:0.03720  GradNorm:0.00353  GradNormST:0.00936  AvgTextLen:39.2  AvgSpecLen:126.9  StepTime:0.81  LR:0.000100\n",
      "   | > Step:61/68  GlobalStep:114050  TotalLoss:0.00238  PostnetLoss:0.00115  DecoderLoss:0.00122  StopLoss:0.05423  GradNorm:0.00322  GradNormST:0.02341  AvgTextLen:54.6  AvgSpecLen:165.5  StepTime:0.90  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:114057  AvgTotalLoss:0.04487  AvgPostnetLoss:0.00094  AvgDecoderLoss:0.00099  AvgStopLoss:0.04294  EpochTime:43.41  AvgStepTime:0.63\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.08717   PostnetLoss: 0.00551   DecoderLoss:0.00580  StopLoss: 0.07586  \n",
      "   | > TotalLoss: 0.09652   PostnetLoss: 0.00977   DecoderLoss:0.01020  StopLoss: 0.07654  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00094   Validation Loss: 0.00870\n",
      "\n",
      " > Epoch 653/1000\n",
      "   | > Step:2/68  GlobalStep:114060  TotalLoss:0.00127  PostnetLoss:0.00062  DecoderLoss:0.00065  StopLoss:0.04623  GradNorm:0.00426  GradNormST:0.01578  AvgTextLen:4.0  AvgSpecLen:31.3  StepTime:0.37  LR:0.000100\n",
      "   | > Step:12/68  GlobalStep:114070  TotalLoss:0.00147  PostnetLoss:0.00072  DecoderLoss:0.00075  StopLoss:0.05752  GradNorm:0.00323  GradNormST:0.02631  AvgTextLen:11.0  AvgSpecLen:49.3  StepTime:0.34  LR:0.000100\n",
      "   | > Step:22/68  GlobalStep:114080  TotalLoss:0.00171  PostnetLoss:0.00083  DecoderLoss:0.00087  StopLoss:0.04371  GradNorm:0.00308  GradNormST:0.01059  AvgTextLen:18.0  AvgSpecLen:72.0  StepTime:0.49  LR:0.000100\n",
      "   | > Step:32/68  GlobalStep:114090  TotalLoss:0.00187  PostnetLoss:0.00091  DecoderLoss:0.00096  StopLoss:0.05920  GradNorm:0.00289  GradNormST:0.01900  AvgTextLen:24.4  AvgSpecLen:91.5  StepTime:0.46  LR:0.000100\n",
      "   | > Step:42/68  GlobalStep:114100  TotalLoss:0.00202  PostnetLoss:0.00098  DecoderLoss:0.00104  StopLoss:0.04853  GradNorm:0.00289  GradNormST:0.02712  AvgTextLen:33.1  AvgSpecLen:109.7  StepTime:0.62  LR:0.000100\n",
      "   | > Step:52/68  GlobalStep:114110  TotalLoss:0.00224  PostnetLoss:0.00109  DecoderLoss:0.00115  StopLoss:0.04883  GradNorm:0.00344  GradNormST:0.01214  AvgTextLen:42.1  AvgSpecLen:137.5  StepTime:0.80  LR:0.000100\n",
      "   | > Step:62/68  GlobalStep:114120  TotalLoss:0.00243  PostnetLoss:0.00118  DecoderLoss:0.00125  StopLoss:0.02934  GradNorm:0.00341  GradNormST:0.01285  AvgTextLen:54.0  AvgSpecLen:174.7  StepTime:0.76  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:114126  AvgTotalLoss:0.04499  AvgPostnetLoss:0.00094  AvgDecoderLoss:0.00099  AvgStopLoss:0.04306  EpochTime:42.96  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.08815   PostnetLoss: 0.00548   DecoderLoss:0.00579  StopLoss: 0.07687  \n",
      "   | > TotalLoss: 0.08792   PostnetLoss: 0.00916   DecoderLoss:0.00957  StopLoss: 0.06919  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00094   Validation Loss: 0.00830\n",
      "\n",
      " > Epoch 654/1000\n",
      "   | > Step:3/68  GlobalStep:114130  TotalLoss:0.00132  PostnetLoss:0.00065  DecoderLoss:0.00067  StopLoss:0.07866  GradNorm:0.00459  GradNormST:0.03043  AvgTextLen:4.7  AvgSpecLen:29.7  StepTime:0.33  LR:0.000100\n",
      "   | > Step:13/68  GlobalStep:114140  TotalLoss:0.00148  PostnetLoss:0.00072  DecoderLoss:0.00075  StopLoss:0.07996  GradNorm:0.00320  GradNormST:0.02169  AvgTextLen:11.5  AvgSpecLen:55.2  StepTime:0.33  LR:0.000100\n",
      "   | > Step:23/68  GlobalStep:114150  TotalLoss:0.00179  PostnetLoss:0.00087  DecoderLoss:0.00091  StopLoss:0.06505  GradNorm:0.00339  GradNormST:0.02472  AvgTextLen:18.3  AvgSpecLen:72.2  StepTime:0.42  LR:0.000100\n",
      "   | > Step:33/68  GlobalStep:114160  TotalLoss:0.00194  PostnetLoss:0.00094  DecoderLoss:0.00099  StopLoss:0.09170  GradNorm:0.00332  GradNormST:0.05295  AvgTextLen:25.9  AvgSpecLen:88.9  StepTime:0.51  LR:0.000100\n",
      "   | > Step:43/68  GlobalStep:114170  TotalLoss:0.00209  PostnetLoss:0.00102  DecoderLoss:0.00107  StopLoss:0.09262  GradNorm:0.00310  GradNormST:0.04727  AvgTextLen:33.4  AvgSpecLen:115.7  StepTime:0.52  LR:0.000100\n",
      "   | > Step:53/68  GlobalStep:114180  TotalLoss:0.00220  PostnetLoss:0.00107  DecoderLoss:0.00113  StopLoss:0.07841  GradNorm:0.00306  GradNormST:0.06569  AvgTextLen:42.4  AvgSpecLen:137.6  StepTime:0.76  LR:0.000100\n",
      "   | > Step:63/68  GlobalStep:114190  TotalLoss:0.00251  PostnetLoss:0.00122  DecoderLoss:0.00129  StopLoss:0.07040  GradNorm:0.00334  GradNormST:0.06599  AvgTextLen:58.6  AvgSpecLen:185.1  StepTime:0.91  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:114195  AvgTotalLoss:0.08075  AvgPostnetLoss:0.00094  AvgDecoderLoss:0.00099  AvgStopLoss:0.07881  EpochTime:42.71  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.09733   PostnetLoss: 0.00555   DecoderLoss:0.00582  StopLoss: 0.08596  \n",
      "   | > TotalLoss: 0.10249   PostnetLoss: 0.00946   DecoderLoss:0.00987  StopLoss: 0.08316  \n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00094   Validation Loss: 0.00846\n",
      "\n",
      " > Epoch 655/1000\n",
      "   | > Step:4/68  GlobalStep:114200  TotalLoss:0.00130  PostnetLoss:0.00064  DecoderLoss:0.00067  StopLoss:0.10101  GradNorm:0.00379  GradNormST:0.02963  AvgTextLen:4.8  AvgSpecLen:31.1  StepTime:0.26  LR:0.000100\n",
      "   | > Step:14/68  GlobalStep:114210  TotalLoss:0.00155  PostnetLoss:0.00076  DecoderLoss:0.00079  StopLoss:0.05310  GradNorm:0.00349  GradNormST:0.01866  AvgTextLen:12.2  AvgSpecLen:52.0  StepTime:0.26  LR:0.000100\n",
      "   | > Step:24/68  GlobalStep:114220  TotalLoss:0.00181  PostnetLoss:0.00088  DecoderLoss:0.00093  StopLoss:0.05791  GradNorm:0.00306  GradNormST:0.01495  AvgTextLen:19.2  AvgSpecLen:79.0  StepTime:0.56  LR:0.000100\n",
      "   | > Step:34/68  GlobalStep:114230  TotalLoss:0.00195  PostnetLoss:0.00095  DecoderLoss:0.00100  StopLoss:0.05766  GradNorm:0.00295  GradNormST:0.02744  AvgTextLen:26.2  AvgSpecLen:95.9  StepTime:0.62  LR:0.000100\n",
      "   | > Step:44/68  GlobalStep:114240  TotalLoss:0.00213  PostnetLoss:0.00103  DecoderLoss:0.00109  StopLoss:0.08989  GradNorm:0.00295  GradNormST:0.06050  AvgTextLen:33.8  AvgSpecLen:116.3  StepTime:0.69  LR:0.000100\n",
      "   | > Step:54/68  GlobalStep:114250  TotalLoss:0.00227  PostnetLoss:0.00110  DecoderLoss:0.00117  StopLoss:0.06118  GradNorm:0.00354  GradNormST:0.03185  AvgTextLen:42.2  AvgSpecLen:144.9  StepTime:0.81  LR:0.000100\n",
      "   | > Step:64/68  GlobalStep:114260  TotalLoss:0.00237  PostnetLoss:0.00115  DecoderLoss:0.00122  StopLoss:0.05526  GradNorm:0.00323  GradNormST:0.03477  AvgTextLen:60.6  AvgSpecLen:177.6  StepTime:0.80  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:114264  AvgTotalLoss:0.07008  AvgPostnetLoss:0.00094  AvgDecoderLoss:0.00099  AvgStopLoss:0.06815  EpochTime:41.92  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.10076   PostnetLoss: 0.00541   DecoderLoss:0.00568  StopLoss: 0.08966  \n",
      "   | > TotalLoss: 0.10914   PostnetLoss: 0.00970   DecoderLoss:0.01010  StopLoss: 0.08934  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00094   Validation Loss: 0.00861\n",
      "\n",
      " > Epoch 656/1000\n",
      "   | > Step:5/68  GlobalStep:114270  TotalLoss:0.00139  PostnetLoss:0.00067  DecoderLoss:0.00072  StopLoss:0.05916  GradNorm:0.00400  GradNormST:0.02202  AvgTextLen:5.9  AvgSpecLen:33.2  StepTime:0.37  LR:0.000100\n",
      "   | > Step:15/68  GlobalStep:114280  TotalLoss:0.00166  PostnetLoss:0.00081  DecoderLoss:0.00085  StopLoss:0.07241  GradNorm:0.00362  GradNormST:0.03233  AvgTextLen:12.8  AvgSpecLen:59.1  StepTime:0.51  LR:0.000100\n",
      "   | > Step:25/68  GlobalStep:114290  TotalLoss:0.00176  PostnetLoss:0.00086  DecoderLoss:0.00090  StopLoss:0.05090  GradNorm:0.00304  GradNormST:0.01422  AvgTextLen:19.8  AvgSpecLen:73.3  StepTime:0.46  LR:0.000100\n",
      "   | > Step:35/68  GlobalStep:114300  TotalLoss:0.00196  PostnetLoss:0.00096  DecoderLoss:0.00101  StopLoss:0.07078  GradNorm:0.00278  GradNormST:0.03150  AvgTextLen:27.1  AvgSpecLen:104.7  StepTime:0.74  LR:0.000100\n",
      "   | > Step:45/68  GlobalStep:114310  TotalLoss:0.00208  PostnetLoss:0.00101  DecoderLoss:0.00107  StopLoss:0.06861  GradNorm:0.00281  GradNormST:0.02018  AvgTextLen:34.4  AvgSpecLen:118.6  StepTime:0.65  LR:0.000100\n",
      "   | > Step:55/68  GlobalStep:114320  TotalLoss:0.00230  PostnetLoss:0.00112  DecoderLoss:0.00118  StopLoss:0.04502  GradNorm:0.00305  GradNormST:0.01485  AvgTextLen:43.9  AvgSpecLen:145.2  StepTime:0.78  LR:0.000100\n",
      "   | > Step:65/68  GlobalStep:114330  TotalLoss:0.00256  PostnetLoss:0.00124  DecoderLoss:0.00132  StopLoss:0.05524  GradNorm:0.00436  GradNormST:0.02744  AvgTextLen:64.2  AvgSpecLen:207.4  StepTime:1.18  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:114333  AvgTotalLoss:0.06332  AvgPostnetLoss:0.00094  AvgDecoderLoss:0.00099  AvgStopLoss:0.06139  EpochTime:41.29  AvgStepTime:0.60\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.09743   PostnetLoss: 0.00544   DecoderLoss:0.00572  StopLoss: 0.08626  \n",
      "   | > TotalLoss: 0.10532   PostnetLoss: 0.00942   DecoderLoss:0.00981  StopLoss: 0.08609  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00094   Validation Loss: 0.00846\n",
      "\n",
      " > Epoch 657/1000\n",
      "   | > Step:6/68  GlobalStep:114340  TotalLoss:0.00133  PostnetLoss:0.00065  DecoderLoss:0.00068  StopLoss:0.04694  GradNorm:0.00345  GradNormST:0.01902  AvgTextLen:6.0  AvgSpecLen:34.8  StepTime:0.29  LR:0.000100\n",
      "   | > Step:16/68  GlobalStep:114350  TotalLoss:0.00164  PostnetLoss:0.00080  DecoderLoss:0.00084  StopLoss:0.04894  GradNorm:0.00371  GradNormST:0.01874  AvgTextLen:13.8  AvgSpecLen:60.6  StepTime:0.37  LR:0.000100\n",
      "   | > Step:26/68  GlobalStep:114360  TotalLoss:0.00181  PostnetLoss:0.00089  DecoderLoss:0.00093  StopLoss:0.06131  GradNorm:0.00355  GradNormST:0.02457  AvgTextLen:20.3  AvgSpecLen:77.5  StepTime:0.47  LR:0.000100\n",
      "   | > Step:36/68  GlobalStep:114370  TotalLoss:0.00194  PostnetLoss:0.00094  DecoderLoss:0.00099  StopLoss:0.06294  GradNorm:0.00276  GradNormST:0.01546  AvgTextLen:28.5  AvgSpecLen:100.2  StepTime:0.57  LR:0.000100\n",
      "   | > Step:46/68  GlobalStep:114380  TotalLoss:0.00210  PostnetLoss:0.00102  DecoderLoss:0.00108  StopLoss:0.07552  GradNorm:0.00272  GradNormST:0.04652  AvgTextLen:35.4  AvgSpecLen:122.6  StepTime:0.69  LR:0.000100\n",
      "   | > Step:56/68  GlobalStep:114390  TotalLoss:0.00228  PostnetLoss:0.00111  DecoderLoss:0.00117  StopLoss:0.05838  GradNorm:0.00331  GradNormST:0.03837  AvgTextLen:45.8  AvgSpecLen:145.4  StepTime:1.04  LR:0.000100\n",
      "   | > Step:66/68  GlobalStep:114400  TotalLoss:0.00268  PostnetLoss:0.00130  DecoderLoss:0.00138  StopLoss:0.05245  GradNorm:0.00297  GradNormST:0.01742  AvgTextLen:71.2  AvgSpecLen:220.8  StepTime:1.04  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:114402  AvgTotalLoss:0.06116  AvgPostnetLoss:0.00094  AvgDecoderLoss:0.00099  AvgStopLoss:0.05923  EpochTime:42.05  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.10540   PostnetLoss: 0.00549   DecoderLoss:0.00577  StopLoss: 0.09414  \n",
      "   | > TotalLoss: 0.11380   PostnetLoss: 0.00969   DecoderLoss:0.01011  StopLoss: 0.09400  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00094   Validation Loss: 0.00864\n",
      "\n",
      " > Epoch 658/1000\n",
      "   | > Step:7/68  GlobalStep:114410  TotalLoss:0.00142  PostnetLoss:0.00069  DecoderLoss:0.00073  StopLoss:0.05240  GradNorm:0.00338  GradNormST:0.02341  AvgTextLen:7.4  AvgSpecLen:44.6  StepTime:0.40  LR:0.000100\n",
      "   | > Step:17/68  GlobalStep:114420  TotalLoss:0.00162  PostnetLoss:0.00079  DecoderLoss:0.00083  StopLoss:0.06477  GradNorm:0.00337  GradNormST:0.02069  AvgTextLen:14.7  AvgSpecLen:66.0  StepTime:0.46  LR:0.000100\n",
      "   | > Step:27/68  GlobalStep:114430  TotalLoss:0.00186  PostnetLoss:0.00091  DecoderLoss:0.00095  StopLoss:0.07120  GradNorm:0.00333  GradNormST:0.03321  AvgTextLen:21.0  AvgSpecLen:80.2  StepTime:0.64  LR:0.000100\n",
      "   | > Step:37/68  GlobalStep:114440  TotalLoss:0.00200  PostnetLoss:0.00097  DecoderLoss:0.00103  StopLoss:0.05273  GradNorm:0.00303  GradNormST:0.02047  AvgTextLen:29.3  AvgSpecLen:102.4  StepTime:0.49  LR:0.000100\n",
      "   | > Step:47/68  GlobalStep:114450  TotalLoss:0.00220  PostnetLoss:0.00107  DecoderLoss:0.00113  StopLoss:0.05444  GradNorm:0.00284  GradNormST:0.01612  AvgTextLen:36.7  AvgSpecLen:126.9  StepTime:0.68  LR:0.000100\n",
      "   | > Step:57/68  GlobalStep:114460  TotalLoss:0.00235  PostnetLoss:0.00114  DecoderLoss:0.00121  StopLoss:0.05555  GradNorm:0.00301  GradNormST:0.02278  AvgTextLen:47.1  AvgSpecLen:161.0  StepTime:0.69  LR:0.000100\n",
      "   | > Step:67/68  GlobalStep:114470  TotalLoss:0.00269  PostnetLoss:0.00130  DecoderLoss:0.00139  StopLoss:0.04124  GradNorm:0.00257  GradNormST:0.01817  AvgTextLen:80.6  AvgSpecLen:246.5  StepTime:1.22  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:114471  AvgTotalLoss:0.06057  AvgPostnetLoss:0.00093  AvgDecoderLoss:0.00099  AvgStopLoss:0.05865  EpochTime:42.31  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.10651   PostnetLoss: 0.00543   DecoderLoss:0.00571  StopLoss: 0.09536  \n",
      "   | > TotalLoss: 0.11886   PostnetLoss: 0.01001   DecoderLoss:0.01042  StopLoss: 0.09843  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00093   Validation Loss: 0.00881\n",
      "\n",
      " > Epoch 659/1000\n",
      "   | > Step:8/68  GlobalStep:114480  TotalLoss:0.00150  PostnetLoss:0.00073  DecoderLoss:0.00077  StopLoss:0.07065  GradNorm:0.00431  GradNormST:0.02258  AvgTextLen:8.3  AvgSpecLen:44.0  StepTime:0.30  LR:0.000100\n",
      "   | > Step:18/68  GlobalStep:114490  TotalLoss:0.00160  PostnetLoss:0.00078  DecoderLoss:0.00082  StopLoss:0.05502  GradNorm:0.00332  GradNormST:0.01691  AvgTextLen:15.2  AvgSpecLen:64.5  StepTime:0.44  LR:0.000100\n",
      "   | > Step:28/68  GlobalStep:114500  TotalLoss:0.00187  PostnetLoss:0.00091  DecoderLoss:0.00096  StopLoss:0.06317  GradNorm:0.00318  GradNormST:0.02215  AvgTextLen:22.1  AvgSpecLen:85.4  StepTime:0.53  LR:0.000100\n",
      "   | > Step:38/68  GlobalStep:114510  TotalLoss:0.00196  PostnetLoss:0.00096  DecoderLoss:0.00101  StopLoss:0.05488  GradNorm:0.00310  GradNormST:0.01931  AvgTextLen:29.4  AvgSpecLen:104.1  StepTime:0.50  LR:0.000100\n",
      "   | > Step:48/68  GlobalStep:114520  TotalLoss:0.00215  PostnetLoss:0.00105  DecoderLoss:0.00110  StopLoss:0.05414  GradNorm:0.00292  GradNormST:0.02940  AvgTextLen:37.5  AvgSpecLen:123.8  StepTime:0.71  LR:0.000100\n",
      "   | > Step:58/68  GlobalStep:114530  TotalLoss:0.00232  PostnetLoss:0.00113  DecoderLoss:0.00119  StopLoss:0.05782  GradNorm:0.00280  GradNormST:0.02585  AvgTextLen:47.8  AvgSpecLen:158.0  StepTime:0.91  LR:0.000100\n",
      "   | > Step:68/68  GlobalStep:114540  TotalLoss:0.00264  PostnetLoss:0.00128  DecoderLoss:0.00136  StopLoss:0.07528  GradNorm:0.00257  GradNormST:0.05030  AvgTextLen:103.0  AvgSpecLen:308.9  StepTime:1.87  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:114540  AvgTotalLoss:0.06036  AvgPostnetLoss:0.00094  AvgDecoderLoss:0.00099  AvgStopLoss:0.05844  EpochTime:41.89  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.09933   PostnetLoss: 0.00547   DecoderLoss:0.00575  StopLoss: 0.08811  \n",
      "   | > TotalLoss: 0.11244   PostnetLoss: 0.00993   DecoderLoss:0.01033  StopLoss: 0.09218  \n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00094   Validation Loss: 0.00878\n",
      "\n",
      " > Epoch 660/1000\n",
      "   | > Step:9/68  GlobalStep:114550  TotalLoss:0.00142  PostnetLoss:0.00069  DecoderLoss:0.00072  StopLoss:0.04467  GradNorm:0.00377  GradNormST:0.01468  AvgTextLen:9.0  AvgSpecLen:42.1  StepTime:0.41  LR:0.000100\n",
      "   | > Step:19/68  GlobalStep:114560  TotalLoss:0.00166  PostnetLoss:0.00081  DecoderLoss:0.00085  StopLoss:0.06690  GradNorm:0.00356  GradNormST:0.01931  AvgTextLen:15.5  AvgSpecLen:63.2  StepTime:0.37  LR:0.000100\n",
      "   | > Step:29/68  GlobalStep:114570  TotalLoss:0.00182  PostnetLoss:0.00089  DecoderLoss:0.00093  StopLoss:0.06586  GradNorm:0.00306  GradNormST:0.01334  AvgTextLen:21.8  AvgSpecLen:83.4  StepTime:0.56  LR:0.000100\n",
      "   | > Step:39/68  GlobalStep:114580  TotalLoss:0.00202  PostnetLoss:0.00098  DecoderLoss:0.00104  StopLoss:0.04395  GradNorm:0.00300  GradNormST:0.01281  AvgTextLen:30.2  AvgSpecLen:110.4  StepTime:0.75  LR:0.000100\n",
      "   | > Step:49/68  GlobalStep:114590  TotalLoss:0.00215  PostnetLoss:0.00105  DecoderLoss:0.00111  StopLoss:0.05477  GradNorm:0.00268  GradNormST:0.01876  AvgTextLen:38.2  AvgSpecLen:127.7  StepTime:0.72  LR:0.000100\n",
      "   | > Step:59/68  GlobalStep:114600  TotalLoss:0.00233  PostnetLoss:0.00113  DecoderLoss:0.00120  StopLoss:0.06145  GradNorm:0.00257  GradNormST:0.02475  AvgTextLen:50.5  AvgSpecLen:155.9  StepTime:0.93  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:114609  AvgTotalLoss:0.05773  AvgPostnetLoss:0.00093  AvgDecoderLoss:0.00098  AvgStopLoss:0.05581  EpochTime:42.64  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.09518   PostnetLoss: 0.00554   DecoderLoss:0.00583  StopLoss: 0.08380  \n",
      "   | > TotalLoss: 0.10724   PostnetLoss: 0.00964   DecoderLoss:0.01005  StopLoss: 0.08755  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00093   Validation Loss: 0.00863\n",
      "\n",
      " > Epoch 661/1000\n",
      "   | > Step:0/68  GlobalStep:114610  TotalLoss:0.00142  PostnetLoss:0.00068  DecoderLoss:0.00073  StopLoss:0.06614  GradNorm:0.00607  GradNormST:0.03345  AvgTextLen:3.0  AvgSpecLen:25.3  StepTime:0.51  LR:0.000100\n",
      "   | > Step:10/68  GlobalStep:114620  TotalLoss:0.00149  PostnetLoss:0.00073  DecoderLoss:0.00076  StopLoss:0.03618  GradNorm:0.00388  GradNormST:0.01787  AvgTextLen:9.8  AvgSpecLen:48.6  StepTime:0.54  LR:0.000100\n",
      "   | > Step:20/68  GlobalStep:114630  TotalLoss:0.00174  PostnetLoss:0.00085  DecoderLoss:0.00089  StopLoss:0.03555  GradNorm:0.00394  GradNormST:0.01273  AvgTextLen:16.4  AvgSpecLen:64.8  StepTime:0.52  LR:0.000100\n",
      "   | > Step:30/68  GlobalStep:114640  TotalLoss:0.00187  PostnetLoss:0.00091  DecoderLoss:0.00096  StopLoss:0.06906  GradNorm:0.00355  GradNormST:0.01570  AvgTextLen:22.9  AvgSpecLen:85.8  StepTime:0.58  LR:0.000100\n",
      "   | > Step:40/68  GlobalStep:114650  TotalLoss:0.00202  PostnetLoss:0.00099  DecoderLoss:0.00103  StopLoss:0.05473  GradNorm:0.00322  GradNormST:0.01407  AvgTextLen:30.9  AvgSpecLen:105.3  StepTime:0.61  LR:0.000100\n",
      "   | > Step:50/68  GlobalStep:114660  TotalLoss:0.00215  PostnetLoss:0.00105  DecoderLoss:0.00111  StopLoss:0.06170  GradNorm:0.00280  GradNormST:0.02715  AvgTextLen:38.8  AvgSpecLen:132.0  StepTime:0.83  LR:0.000100\n",
      "   | > Step:60/68  GlobalStep:114670  TotalLoss:0.00238  PostnetLoss:0.00116  DecoderLoss:0.00123  StopLoss:0.03739  GradNorm:0.00276  GradNormST:0.01459  AvgTextLen:52.0  AvgSpecLen:172.7  StepTime:1.01  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:114678  AvgTotalLoss:0.05650  AvgPostnetLoss:0.00094  AvgDecoderLoss:0.00099  AvgStopLoss:0.05458  EpochTime:42.54  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.09624   PostnetLoss: 0.00559   DecoderLoss:0.00588  StopLoss: 0.08477  \n",
      "   | > TotalLoss: 0.11225   PostnetLoss: 0.00996   DecoderLoss:0.01039  StopLoss: 0.09190  \n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00094   Validation Loss: 0.00870\n",
      "\n",
      " > Epoch 662/1000\n",
      "   | > Step:1/68  GlobalStep:114680  TotalLoss:0.00134  PostnetLoss:0.00065  DecoderLoss:0.00069  StopLoss:0.04417  GradNorm:0.00414  GradNormST:0.02735  AvgTextLen:3.7  AvgSpecLen:28.6  StepTime:0.27  LR:0.000100\n",
      "   | > Step:11/68  GlobalStep:114690  TotalLoss:0.00147  PostnetLoss:0.00072  DecoderLoss:0.00075  StopLoss:0.04840  GradNorm:0.00346  GradNormST:0.01377  AvgTextLen:10.3  AvgSpecLen:49.4  StepTime:0.41  LR:0.000100\n",
      "   | > Step:21/68  GlobalStep:114700  TotalLoss:0.00169  PostnetLoss:0.00082  DecoderLoss:0.00086  StopLoss:0.03635  GradNorm:0.00324  GradNormST:0.01206  AvgTextLen:16.6  AvgSpecLen:66.3  StepTime:0.43  LR:0.000100\n",
      "   | > Step:31/68  GlobalStep:114710  TotalLoss:0.00188  PostnetLoss:0.00092  DecoderLoss:0.00096  StopLoss:0.03736  GradNorm:0.00354  GradNormST:0.01077  AvgTextLen:24.1  AvgSpecLen:82.7  StepTime:0.56  LR:0.000100\n",
      "   | > Step:41/68  GlobalStep:114720  TotalLoss:0.00206  PostnetLoss:0.00101  DecoderLoss:0.00106  StopLoss:0.04692  GradNorm:0.00350  GradNormST:0.01672  AvgTextLen:32.3  AvgSpecLen:109.4  StepTime:0.64  LR:0.000100\n",
      "   | > Step:51/68  GlobalStep:114730  TotalLoss:0.00219  PostnetLoss:0.00107  DecoderLoss:0.00112  StopLoss:0.04420  GradNorm:0.00322  GradNormST:0.01132  AvgTextLen:39.2  AvgSpecLen:126.9  StepTime:0.75  LR:0.000100\n",
      "   | > Step:61/68  GlobalStep:114740  TotalLoss:0.00236  PostnetLoss:0.00114  DecoderLoss:0.00121  StopLoss:0.05475  GradNorm:0.00254  GradNormST:0.02696  AvgTextLen:54.6  AvgSpecLen:165.5  StepTime:0.90  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:114747  AvgTotalLoss:0.05417  AvgPostnetLoss:0.00094  AvgDecoderLoss:0.00099  AvgStopLoss:0.05225  EpochTime:42.21  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.09424   PostnetLoss: 0.00546   DecoderLoss:0.00575  StopLoss: 0.08303  \n",
      "   | > TotalLoss: 0.10778   PostnetLoss: 0.00985   DecoderLoss:0.01027  StopLoss: 0.08767  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00094   Validation Loss: 0.00872\n",
      "\n",
      " > Epoch 663/1000\n",
      "   | > Step:2/68  GlobalStep:114750  TotalLoss:0.00128  PostnetLoss:0.00062  DecoderLoss:0.00066  StopLoss:0.04129  GradNorm:0.00386  GradNormST:0.01416  AvgTextLen:4.0  AvgSpecLen:31.3  StepTime:0.34  LR:0.000100\n",
      "   | > Step:12/68  GlobalStep:114760  TotalLoss:0.00146  PostnetLoss:0.00071  DecoderLoss:0.00075  StopLoss:0.05865  GradNorm:0.00319  GradNormST:0.01859  AvgTextLen:11.0  AvgSpecLen:49.3  StepTime:0.45  LR:0.000100\n",
      "   | > Step:22/68  GlobalStep:114770  TotalLoss:0.00173  PostnetLoss:0.00084  DecoderLoss:0.00088  StopLoss:0.05761  GradNorm:0.00314  GradNormST:0.01469  AvgTextLen:18.0  AvgSpecLen:72.0  StepTime:0.37  LR:0.000100\n",
      "   | > Step:32/68  GlobalStep:114780  TotalLoss:0.00189  PostnetLoss:0.00092  DecoderLoss:0.00097  StopLoss:0.11180  GradNorm:0.00303  GradNormST:0.03993  AvgTextLen:24.4  AvgSpecLen:91.5  StepTime:0.60  LR:0.000100\n",
      "   | > Step:42/68  GlobalStep:114790  TotalLoss:0.00204  PostnetLoss:0.00099  DecoderLoss:0.00105  StopLoss:0.06593  GradNorm:0.00394  GradNormST:0.03182  AvgTextLen:33.1  AvgSpecLen:109.7  StepTime:0.60  LR:0.000100\n",
      "   | > Step:52/68  GlobalStep:114800  TotalLoss:0.00224  PostnetLoss:0.00109  DecoderLoss:0.00115  StopLoss:0.05282  GradNorm:0.00399  GradNormST:0.01140  AvgTextLen:42.1  AvgSpecLen:137.5  StepTime:0.84  LR:0.000100\n",
      "   | > Step:62/68  GlobalStep:114810  TotalLoss:0.00242  PostnetLoss:0.00118  DecoderLoss:0.00125  StopLoss:0.04302  GradNorm:0.00307  GradNormST:0.01532  AvgTextLen:54.0  AvgSpecLen:174.7  StepTime:0.75  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:114816  AvgTotalLoss:0.05339  AvgPostnetLoss:0.00093  AvgDecoderLoss:0.00098  AvgStopLoss:0.05148  EpochTime:42.25  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.08551   PostnetLoss: 0.00540   DecoderLoss:0.00569  StopLoss: 0.07443  \n",
      "   | > TotalLoss: 0.10452   PostnetLoss: 0.00950   DecoderLoss:0.00991  StopLoss: 0.08510  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00093   Validation Loss: 0.00842\n",
      "\n",
      " > Epoch 664/1000\n",
      "   | > Step:3/68  GlobalStep:114820  TotalLoss:0.00134  PostnetLoss:0.00065  DecoderLoss:0.00069  StopLoss:0.07462  GradNorm:0.00406  GradNormST:0.02662  AvgTextLen:4.7  AvgSpecLen:29.7  StepTime:0.33  LR:0.000100\n",
      "   | > Step:13/68  GlobalStep:114830  TotalLoss:0.00150  PostnetLoss:0.00073  DecoderLoss:0.00076  StopLoss:0.06913  GradNorm:0.00325  GradNormST:0.02841  AvgTextLen:11.5  AvgSpecLen:55.2  StepTime:0.41  LR:0.000100\n",
      "   | > Step:23/68  GlobalStep:114840  TotalLoss:0.00175  PostnetLoss:0.00085  DecoderLoss:0.00089  StopLoss:0.05247  GradNorm:0.00309  GradNormST:0.01431  AvgTextLen:18.3  AvgSpecLen:72.2  StepTime:0.38  LR:0.000100\n",
      "   | > Step:33/68  GlobalStep:114850  TotalLoss:0.00191  PostnetLoss:0.00093  DecoderLoss:0.00098  StopLoss:0.05499  GradNorm:0.00313  GradNormST:0.01121  AvgTextLen:25.9  AvgSpecLen:88.9  StepTime:0.43  LR:0.000100\n",
      "   | > Step:43/68  GlobalStep:114860  TotalLoss:0.00208  PostnetLoss:0.00101  DecoderLoss:0.00107  StopLoss:0.06970  GradNorm:0.00342  GradNormST:0.01756  AvgTextLen:33.4  AvgSpecLen:115.7  StepTime:0.62  LR:0.000100\n",
      "   | > Step:53/68  GlobalStep:114870  TotalLoss:0.00222  PostnetLoss:0.00108  DecoderLoss:0.00114  StopLoss:0.04969  GradNorm:0.00369  GradNormST:0.01479  AvgTextLen:42.4  AvgSpecLen:137.6  StepTime:0.64  LR:0.000100\n",
      "   | > Step:63/68  GlobalStep:114880  TotalLoss:0.00252  PostnetLoss:0.00122  DecoderLoss:0.00130  StopLoss:0.04278  GradNorm:0.00334  GradNormST:0.01747  AvgTextLen:58.6  AvgSpecLen:185.1  StepTime:0.91  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:114885  AvgTotalLoss:0.05550  AvgPostnetLoss:0.00093  AvgDecoderLoss:0.00099  AvgStopLoss:0.05358  EpochTime:41.79  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.09162   PostnetLoss: 0.00552   DecoderLoss:0.00581  StopLoss: 0.08030  \n",
      "   | > TotalLoss: 0.10488   PostnetLoss: 0.00970   DecoderLoss:0.01012  StopLoss: 0.08506  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00093   Validation Loss: 0.00864\n",
      "\n",
      " > Epoch 665/1000\n",
      "   | > Step:4/68  GlobalStep:114890  TotalLoss:0.00130  PostnetLoss:0.00063  DecoderLoss:0.00067  StopLoss:0.08401  GradNorm:0.00386  GradNormST:0.02882  AvgTextLen:4.8  AvgSpecLen:31.1  StepTime:0.26  LR:0.000100\n",
      "   | > Step:14/68  GlobalStep:114900  TotalLoss:0.00158  PostnetLoss:0.00077  DecoderLoss:0.00081  StopLoss:0.04778  GradNorm:0.00354  GradNormST:0.02096  AvgTextLen:12.2  AvgSpecLen:52.0  StepTime:0.33  LR:0.000100\n",
      "   | > Step:24/68  GlobalStep:114910  TotalLoss:0.00183  PostnetLoss:0.00089  DecoderLoss:0.00094  StopLoss:0.05514  GradNorm:0.00304  GradNormST:0.01291  AvgTextLen:19.2  AvgSpecLen:79.0  StepTime:0.42  LR:0.000100\n",
      "   | > Step:34/68  GlobalStep:114920  TotalLoss:0.00196  PostnetLoss:0.00095  DecoderLoss:0.00101  StopLoss:0.04215  GradNorm:0.00317  GradNormST:0.01266  AvgTextLen:26.2  AvgSpecLen:95.9  StepTime:0.62  LR:0.000100\n",
      "   | > Step:44/68  GlobalStep:114930  TotalLoss:0.00212  PostnetLoss:0.00103  DecoderLoss:0.00109  StopLoss:0.06357  GradNorm:0.00316  GradNormST:0.02125  AvgTextLen:33.8  AvgSpecLen:116.3  StepTime:0.54  LR:0.000100\n",
      "   | > Step:54/68  GlobalStep:114940  TotalLoss:0.00225  PostnetLoss:0.00109  DecoderLoss:0.00116  StopLoss:0.03614  GradNorm:0.00329  GradNormST:0.00993  AvgTextLen:42.2  AvgSpecLen:144.9  StepTime:0.81  LR:0.000100\n",
      "   | > Step:64/68  GlobalStep:114950  TotalLoss:0.00238  PostnetLoss:0.00115  DecoderLoss:0.00123  StopLoss:0.03691  GradNorm:0.00311  GradNormST:0.01018  AvgTextLen:60.6  AvgSpecLen:177.6  StepTime:0.80  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:114954  AvgTotalLoss:0.05262  AvgPostnetLoss:0.00093  AvgDecoderLoss:0.00098  AvgStopLoss:0.05070  EpochTime:42.29  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.09436   PostnetLoss: 0.00551   DecoderLoss:0.00580  StopLoss: 0.08304  \n",
      "   | > TotalLoss: 0.10266   PostnetLoss: 0.00986   DecoderLoss:0.01026  StopLoss: 0.08255  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00093   Validation Loss: 0.00869\n",
      "\n",
      " > Epoch 666/1000\n",
      "   | > Step:5/68  GlobalStep:114960  TotalLoss:0.00139  PostnetLoss:0.00067  DecoderLoss:0.00072  StopLoss:0.05028  GradNorm:0.00386  GradNormST:0.01692  AvgTextLen:5.9  AvgSpecLen:33.2  StepTime:0.34  LR:0.000100\n",
      "   | > Step:15/68  GlobalStep:114970  TotalLoss:0.00164  PostnetLoss:0.00080  DecoderLoss:0.00083  StopLoss:0.05311  GradNorm:0.00386  GradNormST:0.01632  AvgTextLen:12.8  AvgSpecLen:59.1  StepTime:0.51  LR:0.000100\n",
      "   | > Step:25/68  GlobalStep:114980  TotalLoss:0.00174  PostnetLoss:0.00085  DecoderLoss:0.00089  StopLoss:0.05250  GradNorm:0.00310  GradNormST:0.01688  AvgTextLen:19.8  AvgSpecLen:73.3  StepTime:0.48  LR:0.000100\n",
      "   | > Step:35/68  GlobalStep:114990  TotalLoss:0.00195  PostnetLoss:0.00095  DecoderLoss:0.00100  StopLoss:0.06485  GradNorm:0.00282  GradNormST:0.03084  AvgTextLen:27.1  AvgSpecLen:104.7  StepTime:0.74  LR:0.000100\n",
      "   | > Step:45/68  GlobalStep:115000  TotalLoss:0.00206  PostnetLoss:0.00100  DecoderLoss:0.00106  StopLoss:0.06766  GradNorm:0.00330  GradNormST:0.02475  AvgTextLen:34.4  AvgSpecLen:118.6  StepTime:0.67  LR:0.000100\n",
      " | | > Checkpoint saving : /vm/TTS/models/cartman_models/mozilla-no-loc-fattn-stopnet-sigmoid-loss_masking-August-15-2019_11+26AM-de6aeb9/checkpoint_115000.pth.tar\n",
      "   | > Step:55/68  GlobalStep:115010  TotalLoss:0.00230  PostnetLoss:0.00112  DecoderLoss:0.00118  StopLoss:0.05732  GradNorm:0.00363  GradNormST:0.01509  AvgTextLen:43.9  AvgSpecLen:145.2  StepTime:0.80  LR:0.000100\n",
      "   | > Step:65/68  GlobalStep:115020  TotalLoss:0.00254  PostnetLoss:0.00123  DecoderLoss:0.00131  StopLoss:0.04275  GradNorm:0.00286  GradNormST:0.01273  AvgTextLen:64.2  AvgSpecLen:207.4  StepTime:1.18  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:115023  AvgTotalLoss:0.05498  AvgPostnetLoss:0.00094  AvgDecoderLoss:0.00099  AvgStopLoss:0.05305  EpochTime:42.25  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.09283   PostnetLoss: 0.00543   DecoderLoss:0.00572  StopLoss: 0.08167  \n",
      "   | > TotalLoss: 0.10254   PostnetLoss: 0.00948   DecoderLoss:0.00990  StopLoss: 0.08316  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00094   Validation Loss: 0.00849\n",
      "\n",
      " > Epoch 667/1000\n",
      "   | > Step:6/68  GlobalStep:115030  TotalLoss:0.00136  PostnetLoss:0.00067  DecoderLoss:0.00070  StopLoss:0.03338  GradNorm:0.00425  GradNormST:0.01224  AvgTextLen:6.0  AvgSpecLen:34.8  StepTime:0.39  LR:0.000100\n",
      "   | > Step:16/68  GlobalStep:115040  TotalLoss:0.00164  PostnetLoss:0.00080  DecoderLoss:0.00084  StopLoss:0.04358  GradNorm:0.00356  GradNormST:0.01574  AvgTextLen:13.8  AvgSpecLen:60.6  StepTime:0.39  LR:0.000100\n",
      "   | > Step:26/68  GlobalStep:115050  TotalLoss:0.00182  PostnetLoss:0.00089  DecoderLoss:0.00093  StopLoss:0.06691  GradNorm:0.00324  GradNormST:0.02514  AvgTextLen:20.3  AvgSpecLen:77.5  StepTime:0.51  LR:0.000100\n",
      "   | > Step:36/68  GlobalStep:115060  TotalLoss:0.00195  PostnetLoss:0.00095  DecoderLoss:0.00100  StopLoss:0.05624  GradNorm:0.00296  GradNormST:0.01299  AvgTextLen:28.5  AvgSpecLen:100.2  StepTime:0.47  LR:0.000100\n",
      "   | > Step:46/68  GlobalStep:115070  TotalLoss:0.00208  PostnetLoss:0.00101  DecoderLoss:0.00107  StopLoss:0.04458  GradNorm:0.00298  GradNormST:0.01671  AvgTextLen:35.4  AvgSpecLen:122.6  StepTime:0.69  LR:0.000100\n",
      "   | > Step:56/68  GlobalStep:115080  TotalLoss:0.00228  PostnetLoss:0.00111  DecoderLoss:0.00117  StopLoss:0.05130  GradNorm:0.00372  GradNormST:0.03059  AvgTextLen:45.8  AvgSpecLen:145.4  StepTime:0.95  LR:0.000100\n",
      "   | > Step:66/68  GlobalStep:115090  TotalLoss:0.00265  PostnetLoss:0.00129  DecoderLoss:0.00137  StopLoss:0.03677  GradNorm:0.00306  GradNormST:0.01445  AvgTextLen:71.2  AvgSpecLen:220.8  StepTime:1.05  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:115092  AvgTotalLoss:0.05450  AvgPostnetLoss:0.00094  AvgDecoderLoss:0.00099  AvgStopLoss:0.05258  EpochTime:41.73  AvgStepTime:0.60\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.09130   PostnetLoss: 0.00543   DecoderLoss:0.00571  StopLoss: 0.08016  \n",
      "   | > TotalLoss: 0.10401   PostnetLoss: 0.00970   DecoderLoss:0.01012  StopLoss: 0.08419  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00094   Validation Loss: 0.00865\n",
      "\n",
      " > Epoch 668/1000\n",
      "   | > Step:7/68  GlobalStep:115100  TotalLoss:0.00141  PostnetLoss:0.00069  DecoderLoss:0.00072  StopLoss:0.04417  GradNorm:0.00365  GradNormST:0.02365  AvgTextLen:7.4  AvgSpecLen:44.6  StepTime:0.38  LR:0.000100\n",
      "   | > Step:17/68  GlobalStep:115110  TotalLoss:0.00161  PostnetLoss:0.00079  DecoderLoss:0.00083  StopLoss:0.05344  GradNorm:0.00332  GradNormST:0.01555  AvgTextLen:14.7  AvgSpecLen:66.0  StepTime:0.46  LR:0.000100\n",
      "   | > Step:27/68  GlobalStep:115120  TotalLoss:0.00184  PostnetLoss:0.00090  DecoderLoss:0.00094  StopLoss:0.07120  GradNorm:0.00309  GradNormST:0.02220  AvgTextLen:21.0  AvgSpecLen:80.2  StepTime:0.68  LR:0.000100\n",
      "   | > Step:37/68  GlobalStep:115130  TotalLoss:0.00197  PostnetLoss:0.00096  DecoderLoss:0.00101  StopLoss:0.05572  GradNorm:0.00293  GradNormST:0.01189  AvgTextLen:29.3  AvgSpecLen:102.4  StepTime:0.62  LR:0.000100\n",
      "   | > Step:47/68  GlobalStep:115140  TotalLoss:0.00217  PostnetLoss:0.00106  DecoderLoss:0.00111  StopLoss:0.04810  GradNorm:0.00292  GradNormST:0.01346  AvgTextLen:36.7  AvgSpecLen:126.9  StepTime:0.74  LR:0.000100\n",
      "   | > Step:57/68  GlobalStep:115150  TotalLoss:0.00234  PostnetLoss:0.00114  DecoderLoss:0.00120  StopLoss:0.04267  GradNorm:0.00311  GradNormST:0.01081  AvgTextLen:47.1  AvgSpecLen:161.0  StepTime:0.84  LR:0.000100\n",
      "   | > Step:67/68  GlobalStep:115160  TotalLoss:0.00268  PostnetLoss:0.00130  DecoderLoss:0.00138  StopLoss:0.03634  GradNorm:0.00270  GradNormST:0.01730  AvgTextLen:80.6  AvgSpecLen:246.5  StepTime:1.22  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:115161  AvgTotalLoss:0.05343  AvgPostnetLoss:0.00093  AvgDecoderLoss:0.00098  AvgStopLoss:0.05151  EpochTime:43.01  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.09099   PostnetLoss: 0.00547   DecoderLoss:0.00576  StopLoss: 0.07976  \n",
      "   | > TotalLoss: 0.10910   PostnetLoss: 0.01003   DecoderLoss:0.01046  StopLoss: 0.08860  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00093   Validation Loss: 0.00885\n",
      "\n",
      " > Epoch 669/1000\n",
      "   | > Step:8/68  GlobalStep:115170  TotalLoss:0.00147  PostnetLoss:0.00072  DecoderLoss:0.00075  StopLoss:0.05902  GradNorm:0.00402  GradNormST:0.01927  AvgTextLen:8.3  AvgSpecLen:44.0  StepTime:0.25  LR:0.000100\n",
      "   | > Step:18/68  GlobalStep:115180  TotalLoss:0.00159  PostnetLoss:0.00078  DecoderLoss:0.00081  StopLoss:0.05563  GradNorm:0.00335  GradNormST:0.01796  AvgTextLen:15.2  AvgSpecLen:64.5  StepTime:0.41  LR:0.000100\n",
      "   | > Step:28/68  GlobalStep:115190  TotalLoss:0.00185  PostnetLoss:0.00090  DecoderLoss:0.00095  StopLoss:0.06182  GradNorm:0.00332  GradNormST:0.02105  AvgTextLen:22.1  AvgSpecLen:85.4  StepTime:0.54  LR:0.000100\n",
      "   | > Step:38/68  GlobalStep:115200  TotalLoss:0.00198  PostnetLoss:0.00097  DecoderLoss:0.00102  StopLoss:0.03946  GradNorm:0.00318  GradNormST:0.01240  AvgTextLen:29.4  AvgSpecLen:104.1  StepTime:0.61  LR:0.000100\n",
      "   | > Step:48/68  GlobalStep:115210  TotalLoss:0.00217  PostnetLoss:0.00106  DecoderLoss:0.00111  StopLoss:0.04802  GradNorm:0.00292  GradNormST:0.02137  AvgTextLen:37.5  AvgSpecLen:123.8  StepTime:0.67  LR:0.000100\n",
      "   | > Step:58/68  GlobalStep:115220  TotalLoss:0.00232  PostnetLoss:0.00113  DecoderLoss:0.00119  StopLoss:0.04371  GradNorm:0.00317  GradNormST:0.01389  AvgTextLen:47.8  AvgSpecLen:158.0  StepTime:0.74  LR:0.000100\n",
      "   | > Step:68/68  GlobalStep:115230  TotalLoss:0.00265  PostnetLoss:0.00128  DecoderLoss:0.00137  StopLoss:0.03544  GradNorm:0.00266  GradNormST:0.01504  AvgTextLen:103.0  AvgSpecLen:308.9  StepTime:1.86  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:115230  AvgTotalLoss:0.05213  AvgPostnetLoss:0.00094  AvgDecoderLoss:0.00098  AvgStopLoss:0.05021  EpochTime:42.79  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.08621   PostnetLoss: 0.00557   DecoderLoss:0.00586  StopLoss: 0.07477  \n",
      "   | > TotalLoss: 0.10573   PostnetLoss: 0.00989   DecoderLoss:0.01029  StopLoss: 0.08555  \n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00094   Validation Loss: 0.00881\n",
      "\n",
      " > Epoch 670/1000\n",
      "   | > Step:9/68  GlobalStep:115240  TotalLoss:0.00142  PostnetLoss:0.00069  DecoderLoss:0.00073  StopLoss:0.04375  GradNorm:0.00370  GradNormST:0.01795  AvgTextLen:9.0  AvgSpecLen:42.1  StepTime:0.43  LR:0.000100\n",
      "   | > Step:19/68  GlobalStep:115250  TotalLoss:0.00163  PostnetLoss:0.00079  DecoderLoss:0.00083  StopLoss:0.05846  GradNorm:0.00315  GradNormST:0.01617  AvgTextLen:15.5  AvgSpecLen:63.2  StepTime:0.42  LR:0.000100\n",
      "   | > Step:29/68  GlobalStep:115260  TotalLoss:0.00183  PostnetLoss:0.00089  DecoderLoss:0.00093  StopLoss:0.06267  GradNorm:0.00306  GradNormST:0.01446  AvgTextLen:21.8  AvgSpecLen:83.4  StepTime:0.46  LR:0.000100\n",
      "   | > Step:39/68  GlobalStep:115270  TotalLoss:0.00205  PostnetLoss:0.00100  DecoderLoss:0.00105  StopLoss:0.04516  GradNorm:0.00304  GradNormST:0.02508  AvgTextLen:30.2  AvgSpecLen:110.4  StepTime:0.69  LR:0.000100\n",
      "   | > Step:49/68  GlobalStep:115280  TotalLoss:0.00216  PostnetLoss:0.00105  DecoderLoss:0.00111  StopLoss:0.07488  GradNorm:0.00291  GradNormST:0.04884  AvgTextLen:38.2  AvgSpecLen:127.7  StepTime:0.77  LR:0.000100\n",
      "   | > Step:59/68  GlobalStep:115290  TotalLoss:0.00233  PostnetLoss:0.00113  DecoderLoss:0.00120  StopLoss:0.08990  GradNorm:0.00294  GradNormST:0.04811  AvgTextLen:50.5  AvgSpecLen:155.9  StepTime:0.87  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:115299  AvgTotalLoss:0.06446  AvgPostnetLoss:0.00094  AvgDecoderLoss:0.00099  AvgStopLoss:0.06253  EpochTime:42.62  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.08501   PostnetLoss: 0.00571   DecoderLoss:0.00601  StopLoss: 0.07329  \n",
      "   | > TotalLoss: 0.09839   PostnetLoss: 0.00976   DecoderLoss:0.01017  StopLoss: 0.07846  \n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00094   Validation Loss: 0.00866\n",
      "\n",
      " > Epoch 671/1000\n",
      "   | > Step:0/68  GlobalStep:115300  TotalLoss:0.00143  PostnetLoss:0.00069  DecoderLoss:0.00074  StopLoss:0.07228  GradNorm:0.00578  GradNormST:0.03332  AvgTextLen:3.0  AvgSpecLen:25.3  StepTime:0.37  LR:0.000100\n",
      "   | > Step:10/68  GlobalStep:115310  TotalLoss:0.00148  PostnetLoss:0.00072  DecoderLoss:0.00076  StopLoss:0.06890  GradNorm:0.00358  GradNormST:0.04490  AvgTextLen:9.8  AvgSpecLen:48.6  StepTime:0.67  LR:0.000100\n",
      "   | > Step:20/68  GlobalStep:115320  TotalLoss:0.00175  PostnetLoss:0.00086  DecoderLoss:0.00089  StopLoss:0.05961  GradNorm:0.00399  GradNormST:0.03506  AvgTextLen:16.4  AvgSpecLen:64.8  StepTime:0.50  LR:0.000100\n",
      "   | > Step:30/68  GlobalStep:115330  TotalLoss:0.00186  PostnetLoss:0.00091  DecoderLoss:0.00095  StopLoss:0.06335  GradNorm:0.00357  GradNormST:0.02457  AvgTextLen:22.9  AvgSpecLen:85.8  StepTime:0.50  LR:0.000100\n",
      "   | > Step:40/68  GlobalStep:115340  TotalLoss:0.00198  PostnetLoss:0.00097  DecoderLoss:0.00101  StopLoss:0.06168  GradNorm:0.00289  GradNormST:0.02347  AvgTextLen:30.9  AvgSpecLen:105.3  StepTime:0.60  LR:0.000100\n",
      "   | > Step:50/68  GlobalStep:115350  TotalLoss:0.00217  PostnetLoss:0.00106  DecoderLoss:0.00111  StopLoss:0.05417  GradNorm:0.00329  GradNormST:0.02803  AvgTextLen:38.8  AvgSpecLen:132.0  StepTime:0.67  LR:0.000100\n",
      "   | > Step:60/68  GlobalStep:115360  TotalLoss:0.00241  PostnetLoss:0.00117  DecoderLoss:0.00124  StopLoss:0.05386  GradNorm:0.00303  GradNormST:0.03256  AvgTextLen:52.0  AvgSpecLen:172.7  StepTime:0.99  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:115368  AvgTotalLoss:0.06695  AvgPostnetLoss:0.00094  AvgDecoderLoss:0.00099  AvgStopLoss:0.06502  EpochTime:43.40  AvgStepTime:0.63\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.09308   PostnetLoss: 0.00568   DecoderLoss:0.00596  StopLoss: 0.08145  \n",
      "   | > TotalLoss: 0.10432   PostnetLoss: 0.01005   DecoderLoss:0.01045  StopLoss: 0.08381  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00094   Validation Loss: 0.00889\n",
      "\n",
      " > Epoch 672/1000\n",
      "   | > Step:1/68  GlobalStep:115370  TotalLoss:0.00136  PostnetLoss:0.00066  DecoderLoss:0.00070  StopLoss:0.04377  GradNorm:0.00507  GradNormST:0.02446  AvgTextLen:3.7  AvgSpecLen:28.6  StepTime:0.30  LR:0.000100\n",
      "   | > Step:11/68  GlobalStep:115380  TotalLoss:0.00145  PostnetLoss:0.00071  DecoderLoss:0.00074  StopLoss:0.05214  GradNorm:0.00335  GradNormST:0.01833  AvgTextLen:10.3  AvgSpecLen:49.4  StepTime:0.41  LR:0.000100\n",
      "   | > Step:21/68  GlobalStep:115390  TotalLoss:0.00169  PostnetLoss:0.00083  DecoderLoss:0.00087  StopLoss:0.05313  GradNorm:0.00358  GradNormST:0.01580  AvgTextLen:16.6  AvgSpecLen:66.3  StepTime:0.38  LR:0.000100\n",
      "   | > Step:31/68  GlobalStep:115400  TotalLoss:0.00188  PostnetLoss:0.00092  DecoderLoss:0.00096  StopLoss:0.06339  GradNorm:0.00371  GradNormST:0.02812  AvgTextLen:24.1  AvgSpecLen:82.7  StepTime:0.54  LR:0.000100\n",
      "   | > Step:41/68  GlobalStep:115410  TotalLoss:0.00207  PostnetLoss:0.00101  DecoderLoss:0.00106  StopLoss:0.05113  GradNorm:0.00386  GradNormST:0.02428  AvgTextLen:32.3  AvgSpecLen:109.4  StepTime:0.52  LR:0.000100\n",
      "   | > Step:51/68  GlobalStep:115420  TotalLoss:0.00216  PostnetLoss:0.00105  DecoderLoss:0.00111  StopLoss:0.05326  GradNorm:0.00312  GradNormST:0.01870  AvgTextLen:39.2  AvgSpecLen:126.9  StepTime:0.80  LR:0.000100\n",
      "   | > Step:61/68  GlobalStep:115430  TotalLoss:0.00235  PostnetLoss:0.00114  DecoderLoss:0.00121  StopLoss:0.05898  GradNorm:0.00274  GradNormST:0.03053  AvgTextLen:54.6  AvgSpecLen:165.5  StepTime:0.92  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:115437  AvgTotalLoss:0.06174  AvgPostnetLoss:0.00094  AvgDecoderLoss:0.00099  AvgStopLoss:0.05981  EpochTime:41.73  AvgStepTime:0.60\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.09286   PostnetLoss: 0.00555   DecoderLoss:0.00584  StopLoss: 0.08147  \n",
      "   | > TotalLoss: 0.10456   PostnetLoss: 0.00980   DecoderLoss:0.01023  StopLoss: 0.08453  \n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00094   Validation Loss: 0.00883\n",
      "\n",
      " > Epoch 673/1000\n",
      "   | > Step:2/68  GlobalStep:115440  TotalLoss:0.00127  PostnetLoss:0.00062  DecoderLoss:0.00065  StopLoss:0.05490  GradNorm:0.00425  GradNormST:0.01883  AvgTextLen:4.0  AvgSpecLen:31.3  StepTime:0.50  LR:0.000100\n",
      "   | > Step:12/68  GlobalStep:115450  TotalLoss:0.00148  PostnetLoss:0.00072  DecoderLoss:0.00076  StopLoss:0.05615  GradNorm:0.00351  GradNormST:0.01902  AvgTextLen:11.0  AvgSpecLen:49.3  StepTime:0.41  LR:0.000100\n",
      "   | > Step:22/68  GlobalStep:115460  TotalLoss:0.00173  PostnetLoss:0.00084  DecoderLoss:0.00088  StopLoss:0.07291  GradNorm:0.00327  GradNormST:0.02156  AvgTextLen:18.0  AvgSpecLen:72.0  StepTime:0.36  LR:0.000100\n",
      "   | > Step:32/68  GlobalStep:115470  TotalLoss:0.00187  PostnetLoss:0.00091  DecoderLoss:0.00096  StopLoss:0.09253  GradNorm:0.00304  GradNormST:0.04173  AvgTextLen:24.4  AvgSpecLen:91.5  StepTime:0.56  LR:0.000100\n",
      "   | > Step:42/68  GlobalStep:115480  TotalLoss:0.00207  PostnetLoss:0.00101  DecoderLoss:0.00106  StopLoss:0.06914  GradNorm:0.00398  GradNormST:0.02852  AvgTextLen:33.1  AvgSpecLen:109.7  StepTime:0.48  LR:0.000100\n",
      "   | > Step:52/68  GlobalStep:115490  TotalLoss:0.00225  PostnetLoss:0.00110  DecoderLoss:0.00116  StopLoss:0.07939  GradNorm:0.00446  GradNormST:0.02163  AvgTextLen:42.1  AvgSpecLen:137.5  StepTime:0.80  LR:0.000100\n",
      "   | > Step:62/68  GlobalStep:115500  TotalLoss:0.00243  PostnetLoss:0.00118  DecoderLoss:0.00125  StopLoss:0.05265  GradNorm:0.00298  GradNormST:0.01855  AvgTextLen:54.0  AvgSpecLen:174.7  StepTime:0.75  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:115506  AvgTotalLoss:0.06136  AvgPostnetLoss:0.00094  AvgDecoderLoss:0.00099  AvgStopLoss:0.05944  EpochTime:42.06  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.09127   PostnetLoss: 0.00548   DecoderLoss:0.00578  StopLoss: 0.08000  \n",
      "   | > TotalLoss: 0.10718   PostnetLoss: 0.01020   DecoderLoss:0.01062  StopLoss: 0.08636  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00094   Validation Loss: 0.00902\n",
      "\n",
      " > Epoch 674/1000\n",
      "   | > Step:3/68  GlobalStep:115510  TotalLoss:0.00127  PostnetLoss:0.00062  DecoderLoss:0.00065  StopLoss:0.07765  GradNorm:0.00361  GradNormST:0.03088  AvgTextLen:4.7  AvgSpecLen:29.7  StepTime:0.36  LR:0.000100\n",
      "   | > Step:13/68  GlobalStep:115520  TotalLoss:0.00149  PostnetLoss:0.00073  DecoderLoss:0.00076  StopLoss:0.07190  GradNorm:0.00310  GradNormST:0.02815  AvgTextLen:11.5  AvgSpecLen:55.2  StepTime:0.43  LR:0.000100\n",
      "   | > Step:23/68  GlobalStep:115530  TotalLoss:0.00174  PostnetLoss:0.00085  DecoderLoss:0.00089  StopLoss:0.05489  GradNorm:0.00309  GradNormST:0.01418  AvgTextLen:18.3  AvgSpecLen:72.2  StepTime:0.38  LR:0.000100\n",
      "   | > Step:33/68  GlobalStep:115540  TotalLoss:0.00191  PostnetLoss:0.00094  DecoderLoss:0.00098  StopLoss:0.06415  GradNorm:0.00312  GradNormST:0.01967  AvgTextLen:25.9  AvgSpecLen:88.9  StepTime:0.54  LR:0.000100\n",
      "   | > Step:43/68  GlobalStep:115550  TotalLoss:0.00206  PostnetLoss:0.00100  DecoderLoss:0.00106  StopLoss:0.05483  GradNorm:0.00358  GradNormST:0.01512  AvgTextLen:33.4  AvgSpecLen:115.7  StepTime:0.63  LR:0.000100\n",
      "   | > Step:53/68  GlobalStep:115560  TotalLoss:0.00221  PostnetLoss:0.00108  DecoderLoss:0.00114  StopLoss:0.04355  GradNorm:0.00375  GradNormST:0.01508  AvgTextLen:42.4  AvgSpecLen:137.6  StepTime:0.81  LR:0.000100\n",
      "   | > Step:63/68  GlobalStep:115570  TotalLoss:0.00252  PostnetLoss:0.00122  DecoderLoss:0.00130  StopLoss:0.03127  GradNorm:0.00393  GradNormST:0.01160  AvgTextLen:58.6  AvgSpecLen:185.1  StepTime:0.91  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:115575  AvgTotalLoss:0.05972  AvgPostnetLoss:0.00094  AvgDecoderLoss:0.00099  AvgStopLoss:0.05780  EpochTime:42.33  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.09704   PostnetLoss: 0.00572   DecoderLoss:0.00601  StopLoss: 0.08532  \n",
      "   | > TotalLoss: 0.10097   PostnetLoss: 0.00957   DecoderLoss:0.00996  StopLoss: 0.08144  \n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00094   Validation Loss: 0.00869\n",
      "\n",
      " > Epoch 675/1000\n",
      "   | > Step:4/68  GlobalStep:115580  TotalLoss:0.00136  PostnetLoss:0.00066  DecoderLoss:0.00070  StopLoss:0.08136  GradNorm:0.00453  GradNormST:0.02982  AvgTextLen:4.8  AvgSpecLen:31.1  StepTime:0.27  LR:0.000100\n",
      "   | > Step:14/68  GlobalStep:115590  TotalLoss:0.00153  PostnetLoss:0.00075  DecoderLoss:0.00078  StopLoss:0.05167  GradNorm:0.00314  GradNormST:0.02037  AvgTextLen:12.2  AvgSpecLen:52.0  StepTime:0.33  LR:0.000100\n",
      "   | > Step:24/68  GlobalStep:115600  TotalLoss:0.00181  PostnetLoss:0.00088  DecoderLoss:0.00093  StopLoss:0.05311  GradNorm:0.00312  GradNormST:0.01507  AvgTextLen:19.2  AvgSpecLen:79.0  StepTime:0.53  LR:0.000100\n",
      "   | > Step:34/68  GlobalStep:115610  TotalLoss:0.00194  PostnetLoss:0.00094  DecoderLoss:0.00099  StopLoss:0.05857  GradNorm:0.00309  GradNormST:0.02284  AvgTextLen:26.2  AvgSpecLen:95.9  StepTime:0.47  LR:0.000100\n",
      "   | > Step:44/68  GlobalStep:115620  TotalLoss:0.00213  PostnetLoss:0.00104  DecoderLoss:0.00109  StopLoss:0.08719  GradNorm:0.00387  GradNormST:0.03757  AvgTextLen:33.8  AvgSpecLen:116.3  StepTime:0.58  LR:0.000100\n",
      "   | > Step:54/68  GlobalStep:115630  TotalLoss:0.00224  PostnetLoss:0.00109  DecoderLoss:0.00115  StopLoss:0.04588  GradNorm:0.00346  GradNormST:0.01304  AvgTextLen:42.2  AvgSpecLen:144.9  StepTime:0.81  LR:0.000100\n",
      "   | > Step:64/68  GlobalStep:115640  TotalLoss:0.00236  PostnetLoss:0.00115  DecoderLoss:0.00122  StopLoss:0.04045  GradNorm:0.00364  GradNormST:0.01142  AvgTextLen:60.6  AvgSpecLen:177.6  StepTime:0.81  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:115644  AvgTotalLoss:0.05773  AvgPostnetLoss:0.00094  AvgDecoderLoss:0.00099  AvgStopLoss:0.05581  EpochTime:41.68  AvgStepTime:0.60\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.09879   PostnetLoss: 0.00566   DecoderLoss:0.00595  StopLoss: 0.08719  \n",
      "   | > TotalLoss: 0.10621   PostnetLoss: 0.01002   DecoderLoss:0.01044  StopLoss: 0.08575  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00094   Validation Loss: 0.00902\n",
      "\n",
      " > Epoch 676/1000\n",
      "   | > Step:5/68  GlobalStep:115650  TotalLoss:0.00135  PostnetLoss:0.00066  DecoderLoss:0.00070  StopLoss:0.05201  GradNorm:0.00338  GradNormST:0.02130  AvgTextLen:5.9  AvgSpecLen:33.2  StepTime:0.25  LR:0.000100\n",
      "   | > Step:15/68  GlobalStep:115660  TotalLoss:0.00161  PostnetLoss:0.00079  DecoderLoss:0.00082  StopLoss:0.04490  GradNorm:0.00344  GradNormST:0.01735  AvgTextLen:12.8  AvgSpecLen:59.1  StepTime:0.51  LR:0.000100\n",
      "   | > Step:25/68  GlobalStep:115670  TotalLoss:0.00178  PostnetLoss:0.00087  DecoderLoss:0.00091  StopLoss:0.04521  GradNorm:0.00336  GradNormST:0.01049  AvgTextLen:19.8  AvgSpecLen:73.3  StepTime:0.37  LR:0.000100\n",
      "   | > Step:35/68  GlobalStep:115680  TotalLoss:0.00198  PostnetLoss:0.00096  DecoderLoss:0.00101  StopLoss:0.05353  GradNorm:0.00334  GradNormST:0.02817  AvgTextLen:27.1  AvgSpecLen:104.7  StepTime:0.63  LR:0.000100\n",
      "   | > Step:45/68  GlobalStep:115690  TotalLoss:0.00207  PostnetLoss:0.00100  DecoderLoss:0.00106  StopLoss:0.06760  GradNorm:0.00311  GradNormST:0.02411  AvgTextLen:34.4  AvgSpecLen:118.6  StepTime:0.54  LR:0.000100\n",
      "   | > Step:55/68  GlobalStep:115700  TotalLoss:0.00232  PostnetLoss:0.00113  DecoderLoss:0.00119  StopLoss:0.05467  GradNorm:0.00390  GradNormST:0.01577  AvgTextLen:43.9  AvgSpecLen:145.2  StepTime:0.72  LR:0.000100\n",
      "   | > Step:65/68  GlobalStep:115710  TotalLoss:0.00255  PostnetLoss:0.00124  DecoderLoss:0.00131  StopLoss:0.04614  GradNorm:0.00394  GradNormST:0.01472  AvgTextLen:64.2  AvgSpecLen:207.4  StepTime:1.18  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:115713  AvgTotalLoss:0.05765  AvgPostnetLoss:0.00094  AvgDecoderLoss:0.00099  AvgStopLoss:0.05573  EpochTime:41.69  AvgStepTime:0.60\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.09351   PostnetLoss: 0.00558   DecoderLoss:0.00586  StopLoss: 0.08207  \n",
      "   | > TotalLoss: 0.10773   PostnetLoss: 0.01009   DecoderLoss:0.01050  StopLoss: 0.08714  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00094   Validation Loss: 0.00886\n",
      "\n",
      " > Epoch 677/1000\n",
      "   | > Step:6/68  GlobalStep:115720  TotalLoss:0.00134  PostnetLoss:0.00066  DecoderLoss:0.00068  StopLoss:0.06125  GradNorm:0.00398  GradNormST:0.03131  AvgTextLen:6.0  AvgSpecLen:34.8  StepTime:0.38  LR:0.000100\n",
      "   | > Step:16/68  GlobalStep:115730  TotalLoss:0.00161  PostnetLoss:0.00079  DecoderLoss:0.00082  StopLoss:0.07716  GradNorm:0.00322  GradNormST:0.02026  AvgTextLen:13.8  AvgSpecLen:60.6  StepTime:0.29  LR:0.000100\n",
      "   | > Step:26/68  GlobalStep:115740  TotalLoss:0.00179  PostnetLoss:0.00088  DecoderLoss:0.00092  StopLoss:0.05874  GradNorm:0.00323  GradNormST:0.02490  AvgTextLen:20.3  AvgSpecLen:77.5  StepTime:0.47  LR:0.000100\n",
      "   | > Step:36/68  GlobalStep:115750  TotalLoss:0.00197  PostnetLoss:0.00096  DecoderLoss:0.00101  StopLoss:0.08549  GradNorm:0.00362  GradNormST:0.02157  AvgTextLen:28.5  AvgSpecLen:100.2  StepTime:0.69  LR:0.000100\n",
      "   | > Step:46/68  GlobalStep:115760  TotalLoss:0.00209  PostnetLoss:0.00102  DecoderLoss:0.00107  StopLoss:0.05899  GradNorm:0.00341  GradNormST:0.03590  AvgTextLen:35.4  AvgSpecLen:122.6  StepTime:0.77  LR:0.000100\n",
      "   | > Step:56/68  GlobalStep:115770  TotalLoss:0.00229  PostnetLoss:0.00111  DecoderLoss:0.00118  StopLoss:0.07170  GradNorm:0.00390  GradNormST:0.05759  AvgTextLen:45.8  AvgSpecLen:145.4  StepTime:0.83  LR:0.000100\n",
      "   | > Step:66/68  GlobalStep:115780  TotalLoss:0.00266  PostnetLoss:0.00129  DecoderLoss:0.00137  StopLoss:0.04341  GradNorm:0.00321  GradNormST:0.01843  AvgTextLen:71.2  AvgSpecLen:220.8  StepTime:1.04  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:115782  AvgTotalLoss:0.05958  AvgPostnetLoss:0.00093  AvgDecoderLoss:0.00098  AvgStopLoss:0.05766  EpochTime:42.72  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.09253   PostnetLoss: 0.00561   DecoderLoss:0.00590  StopLoss: 0.08102  \n",
      "   | > TotalLoss: 0.10972   PostnetLoss: 0.01044   DecoderLoss:0.01083  StopLoss: 0.08845  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00093   Validation Loss: 0.00936\n",
      "\n",
      " > Epoch 678/1000\n",
      "   | > Step:7/68  GlobalStep:115790  TotalLoss:0.00142  PostnetLoss:0.00070  DecoderLoss:0.00073  StopLoss:0.04776  GradNorm:0.00365  GradNormST:0.02043  AvgTextLen:7.4  AvgSpecLen:44.6  StepTime:0.33  LR:0.000100\n",
      "   | > Step:17/68  GlobalStep:115800  TotalLoss:0.00159  PostnetLoss:0.00078  DecoderLoss:0.00081  StopLoss:0.05444  GradNorm:0.00326  GradNormST:0.01973  AvgTextLen:14.7  AvgSpecLen:66.0  StepTime:0.33  LR:0.000100\n",
      "   | > Step:27/68  GlobalStep:115810  TotalLoss:0.00182  PostnetLoss:0.00089  DecoderLoss:0.00093  StopLoss:0.06488  GradNorm:0.00312  GradNormST:0.01996  AvgTextLen:21.0  AvgSpecLen:80.2  StepTime:0.62  LR:0.000100\n",
      "   | > Step:37/68  GlobalStep:115820  TotalLoss:0.00196  PostnetLoss:0.00095  DecoderLoss:0.00100  StopLoss:0.06037  GradNorm:0.00292  GradNormST:0.01376  AvgTextLen:29.3  AvgSpecLen:102.4  StepTime:0.48  LR:0.000100\n",
      "   | > Step:47/68  GlobalStep:115830  TotalLoss:0.00220  PostnetLoss:0.00107  DecoderLoss:0.00113  StopLoss:0.05259  GradNorm:0.00420  GradNormST:0.01490  AvgTextLen:36.7  AvgSpecLen:126.9  StepTime:0.66  LR:0.000100\n",
      "   | > Step:57/68  GlobalStep:115840  TotalLoss:0.00238  PostnetLoss:0.00116  DecoderLoss:0.00122  StopLoss:0.03518  GradNorm:0.00411  GradNormST:0.01277  AvgTextLen:47.1  AvgSpecLen:161.0  StepTime:0.84  LR:0.000100\n",
      "   | > Step:67/68  GlobalStep:115850  TotalLoss:0.00269  PostnetLoss:0.00130  DecoderLoss:0.00139  StopLoss:0.03979  GradNorm:0.00351  GradNormST:0.01770  AvgTextLen:80.6  AvgSpecLen:246.5  StepTime:1.21  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:115851  AvgTotalLoss:0.05729  AvgPostnetLoss:0.00094  AvgDecoderLoss:0.00099  AvgStopLoss:0.05537  EpochTime:41.42  AvgStepTime:0.60\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.09616   PostnetLoss: 0.00555   DecoderLoss:0.00583  StopLoss: 0.08479  \n",
      "   | > TotalLoss: 0.10632   PostnetLoss: 0.01013   DecoderLoss:0.01054  StopLoss: 0.08565  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00094   Validation Loss: 0.00903\n",
      "\n",
      " > Epoch 679/1000\n",
      "   | > Step:8/68  GlobalStep:115860  TotalLoss:0.00144  PostnetLoss:0.00071  DecoderLoss:0.00074  StopLoss:0.05656  GradNorm:0.00393  GradNormST:0.02318  AvgTextLen:8.3  AvgSpecLen:44.0  StepTime:0.25  LR:0.000100\n",
      "   | > Step:18/68  GlobalStep:115870  TotalLoss:0.00160  PostnetLoss:0.00078  DecoderLoss:0.00082  StopLoss:0.04822  GradNorm:0.00324  GradNormST:0.01446  AvgTextLen:15.2  AvgSpecLen:64.5  StepTime:0.33  LR:0.000100\n",
      "   | > Step:28/68  GlobalStep:115880  TotalLoss:0.00181  PostnetLoss:0.00088  DecoderLoss:0.00093  StopLoss:0.05476  GradNorm:0.00282  GradNormST:0.02018  AvgTextLen:22.1  AvgSpecLen:85.4  StepTime:0.53  LR:0.000100\n",
      "   | > Step:38/68  GlobalStep:115890  TotalLoss:0.00196  PostnetLoss:0.00096  DecoderLoss:0.00101  StopLoss:0.05221  GradNorm:0.00328  GradNormST:0.01816  AvgTextLen:29.4  AvgSpecLen:104.1  StepTime:0.61  LR:0.000100\n",
      "   | > Step:48/68  GlobalStep:115900  TotalLoss:0.00217  PostnetLoss:0.00105  DecoderLoss:0.00111  StopLoss:0.05267  GradNorm:0.00341  GradNormST:0.02104  AvgTextLen:37.5  AvgSpecLen:123.8  StepTime:0.72  LR:0.000100\n",
      "   | > Step:58/68  GlobalStep:115910  TotalLoss:0.00236  PostnetLoss:0.00115  DecoderLoss:0.00121  StopLoss:0.05424  GradNorm:0.00433  GradNormST:0.01791  AvgTextLen:47.8  AvgSpecLen:158.0  StepTime:0.91  LR:0.000100\n",
      "   | > Step:68/68  GlobalStep:115920  TotalLoss:0.00265  PostnetLoss:0.00128  DecoderLoss:0.00136  StopLoss:0.03439  GradNorm:0.00284  GradNormST:0.01199  AvgTextLen:103.0  AvgSpecLen:308.9  StepTime:1.87  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:115920  AvgTotalLoss:0.05766  AvgPostnetLoss:0.00093  AvgDecoderLoss:0.00098  AvgStopLoss:0.05574  EpochTime:41.90  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.09388   PostnetLoss: 0.00555   DecoderLoss:0.00583  StopLoss: 0.08250  \n",
      "   | > TotalLoss: 0.10209   PostnetLoss: 0.01029   DecoderLoss:0.01067  StopLoss: 0.08113  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00093   Validation Loss: 0.00915\n",
      "\n",
      " > Epoch 680/1000\n",
      "   | > Step:9/68  GlobalStep:115930  TotalLoss:0.00140  PostnetLoss:0.00068  DecoderLoss:0.00072  StopLoss:0.04799  GradNorm:0.00356  GradNormST:0.01992  AvgTextLen:9.0  AvgSpecLen:42.1  StepTime:0.44  LR:0.000100\n",
      "   | > Step:19/68  GlobalStep:115940  TotalLoss:0.00163  PostnetLoss:0.00080  DecoderLoss:0.00083  StopLoss:0.07300  GradNorm:0.00343  GradNormST:0.02624  AvgTextLen:15.5  AvgSpecLen:63.2  StepTime:0.32  LR:0.000100\n",
      "   | > Step:29/68  GlobalStep:115950  TotalLoss:0.00181  PostnetLoss:0.00089  DecoderLoss:0.00093  StopLoss:0.06781  GradNorm:0.00319  GradNormST:0.01484  AvgTextLen:21.8  AvgSpecLen:83.4  StepTime:0.59  LR:0.000100\n",
      "   | > Step:39/68  GlobalStep:115960  TotalLoss:0.00202  PostnetLoss:0.00098  DecoderLoss:0.00103  StopLoss:0.04671  GradNorm:0.00281  GradNormST:0.01633  AvgTextLen:30.2  AvgSpecLen:110.4  StepTime:0.53  LR:0.000100\n",
      "   | > Step:49/68  GlobalStep:115970  TotalLoss:0.00217  PostnetLoss:0.00106  DecoderLoss:0.00111  StopLoss:0.04279  GradNorm:0.00318  GradNormST:0.01294  AvgTextLen:38.2  AvgSpecLen:127.7  StepTime:0.76  LR:0.000100\n",
      "   | > Step:59/68  GlobalStep:115980  TotalLoss:0.00232  PostnetLoss:0.00113  DecoderLoss:0.00119  StopLoss:0.04430  GradNorm:0.00379  GradNormST:0.01272  AvgTextLen:50.5  AvgSpecLen:155.9  StepTime:0.97  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:115989  AvgTotalLoss:0.05373  AvgPostnetLoss:0.00093  AvgDecoderLoss:0.00098  AvgStopLoss:0.05181  EpochTime:42.53  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.09294   PostnetLoss: 0.00556   DecoderLoss:0.00585  StopLoss: 0.08154  \n",
      "   | > TotalLoss: 0.10266   PostnetLoss: 0.01002   DecoderLoss:0.01044  StopLoss: 0.08220  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00093   Validation Loss: 0.00893\n",
      "\n",
      " > Epoch 681/1000\n",
      "   | > Step:0/68  GlobalStep:115990  TotalLoss:0.00142  PostnetLoss:0.00068  DecoderLoss:0.00074  StopLoss:0.08707  GradNorm:0.00563  GradNormST:0.04111  AvgTextLen:3.0  AvgSpecLen:25.3  StepTime:0.35  LR:0.000100\n",
      "   | > Step:10/68  GlobalStep:116000  TotalLoss:0.00151  PostnetLoss:0.00073  DecoderLoss:0.00077  StopLoss:0.03778  GradNorm:0.00366  GradNormST:0.01371  AvgTextLen:9.8  AvgSpecLen:48.6  StepTime:0.43  LR:0.000100\n",
      " | | > Checkpoint saving : /vm/TTS/models/cartman_models/mozilla-no-loc-fattn-stopnet-sigmoid-loss_masking-August-15-2019_11+26AM-de6aeb9/checkpoint_116000.pth.tar\n",
      "warning: audio amplitude out of range, auto clipped.\n",
      "   | > Step:20/68  GlobalStep:116010  TotalLoss:0.00172  PostnetLoss:0.00084  DecoderLoss:0.00088  StopLoss:0.03929  GradNorm:0.00344  GradNormST:0.01288  AvgTextLen:16.4  AvgSpecLen:64.8  StepTime:0.49  LR:0.000100\n",
      "   | > Step:30/68  GlobalStep:116020  TotalLoss:0.00184  PostnetLoss:0.00090  DecoderLoss:0.00094  StopLoss:0.05300  GradNorm:0.00317  GradNormST:0.01315  AvgTextLen:22.9  AvgSpecLen:85.8  StepTime:0.59  LR:0.000100\n",
      "   | > Step:40/68  GlobalStep:116030  TotalLoss:0.00199  PostnetLoss:0.00097  DecoderLoss:0.00102  StopLoss:0.03938  GradNorm:0.00309  GradNormST:0.00952  AvgTextLen:30.9  AvgSpecLen:105.3  StepTime:0.49  LR:0.000100\n",
      "   | > Step:50/68  GlobalStep:116040  TotalLoss:0.00216  PostnetLoss:0.00105  DecoderLoss:0.00111  StopLoss:0.06049  GradNorm:0.00316  GradNormST:0.02856  AvgTextLen:38.8  AvgSpecLen:132.0  StepTime:0.81  LR:0.000100\n",
      "   | > Step:60/68  GlobalStep:116050  TotalLoss:0.00241  PostnetLoss:0.00117  DecoderLoss:0.00124  StopLoss:0.04519  GradNorm:0.00356  GradNormST:0.01839  AvgTextLen:52.0  AvgSpecLen:172.7  StepTime:0.79  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:116058  AvgTotalLoss:0.05462  AvgPostnetLoss:0.00094  AvgDecoderLoss:0.00099  AvgStopLoss:0.05269  EpochTime:41.77  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.09682   PostnetLoss: 0.00563   DecoderLoss:0.00591  StopLoss: 0.08528  \n",
      "   | > TotalLoss: 0.10613   PostnetLoss: 0.01032   DecoderLoss:0.01073  StopLoss: 0.08507  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00094   Validation Loss: 0.00917\n",
      "\n",
      " > Epoch 682/1000\n",
      "   | > Step:1/68  GlobalStep:116060  TotalLoss:0.00134  PostnetLoss:0.00065  DecoderLoss:0.00069  StopLoss:0.05483  GradNorm:0.00461  GradNormST:0.02119  AvgTextLen:3.7  AvgSpecLen:28.6  StepTime:0.34  LR:0.000100\n",
      "   | > Step:11/68  GlobalStep:116070  TotalLoss:0.00147  PostnetLoss:0.00072  DecoderLoss:0.00075  StopLoss:0.04204  GradNorm:0.00377  GradNormST:0.01137  AvgTextLen:10.3  AvgSpecLen:49.4  StepTime:0.44  LR:0.000100\n",
      "   | > Step:21/68  GlobalStep:116080  TotalLoss:0.00170  PostnetLoss:0.00083  DecoderLoss:0.00087  StopLoss:0.04964  GradNorm:0.00331  GradNormST:0.01302  AvgTextLen:16.6  AvgSpecLen:66.3  StepTime:0.44  LR:0.000100\n",
      "   | > Step:31/68  GlobalStep:116090  TotalLoss:0.00184  PostnetLoss:0.00090  DecoderLoss:0.00094  StopLoss:0.05081  GradNorm:0.00299  GradNormST:0.01274  AvgTextLen:24.1  AvgSpecLen:82.7  StepTime:0.51  LR:0.000100\n",
      "   | > Step:41/68  GlobalStep:116100  TotalLoss:0.00204  PostnetLoss:0.00100  DecoderLoss:0.00105  StopLoss:0.04144  GradNorm:0.00299  GradNormST:0.01247  AvgTextLen:32.3  AvgSpecLen:109.4  StepTime:0.65  LR:0.000100\n",
      "   | > Step:51/68  GlobalStep:116110  TotalLoss:0.00214  PostnetLoss:0.00104  DecoderLoss:0.00110  StopLoss:0.04974  GradNorm:0.00293  GradNormST:0.01440  AvgTextLen:39.2  AvgSpecLen:126.9  StepTime:0.61  LR:0.000100\n",
      "   | > Step:61/68  GlobalStep:116120  TotalLoss:0.00237  PostnetLoss:0.00115  DecoderLoss:0.00122  StopLoss:0.03401  GradNorm:0.00339  GradNormST:0.01004  AvgTextLen:54.6  AvgSpecLen:165.5  StepTime:0.90  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:116127  AvgTotalLoss:0.05271  AvgPostnetLoss:0.00093  AvgDecoderLoss:0.00098  AvgStopLoss:0.05079  EpochTime:42.34  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.09330   PostnetLoss: 0.00557   DecoderLoss:0.00586  StopLoss: 0.08187  \n",
      "   | > TotalLoss: 0.10213   PostnetLoss: 0.01032   DecoderLoss:0.01073  StopLoss: 0.08109  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00093   Validation Loss: 0.00917\n",
      "\n",
      " > Epoch 683/1000\n",
      "   | > Step:2/68  GlobalStep:116130  TotalLoss:0.00129  PostnetLoss:0.00063  DecoderLoss:0.00066  StopLoss:0.04548  GradNorm:0.00424  GradNormST:0.01681  AvgTextLen:4.0  AvgSpecLen:31.3  StepTime:0.40  LR:0.000100\n",
      "   | > Step:12/68  GlobalStep:116140  TotalLoss:0.00147  PostnetLoss:0.00072  DecoderLoss:0.00075  StopLoss:0.04705  GradNorm:0.00386  GradNormST:0.02087  AvgTextLen:11.0  AvgSpecLen:49.3  StepTime:0.38  LR:0.000100\n",
      "   | > Step:22/68  GlobalStep:116150  TotalLoss:0.00170  PostnetLoss:0.00083  DecoderLoss:0.00087  StopLoss:0.06042  GradNorm:0.00317  GradNormST:0.01540  AvgTextLen:18.0  AvgSpecLen:72.0  StepTime:0.46  LR:0.000100\n",
      "   | > Step:32/68  GlobalStep:116160  TotalLoss:0.00188  PostnetLoss:0.00091  DecoderLoss:0.00096  StopLoss:0.05401  GradNorm:0.00298  GradNormST:0.01219  AvgTextLen:24.4  AvgSpecLen:91.5  StepTime:0.49  LR:0.000100\n",
      "   | > Step:42/68  GlobalStep:116170  TotalLoss:0.00204  PostnetLoss:0.00099  DecoderLoss:0.00105  StopLoss:0.06303  GradNorm:0.00332  GradNormST:0.02202  AvgTextLen:33.1  AvgSpecLen:109.7  StepTime:0.48  LR:0.000100\n",
      "   | > Step:52/68  GlobalStep:116180  TotalLoss:0.00222  PostnetLoss:0.00108  DecoderLoss:0.00114  StopLoss:0.06298  GradNorm:0.00290  GradNormST:0.01389  AvgTextLen:42.1  AvgSpecLen:137.5  StepTime:0.84  LR:0.000100\n",
      "   | > Step:62/68  GlobalStep:116190  TotalLoss:0.00244  PostnetLoss:0.00118  DecoderLoss:0.00125  StopLoss:0.04367  GradNorm:0.00303  GradNormST:0.01306  AvgTextLen:54.0  AvgSpecLen:174.7  StepTime:0.75  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:116196  AvgTotalLoss:0.05091  AvgPostnetLoss:0.00094  AvgDecoderLoss:0.00099  AvgStopLoss:0.04899  EpochTime:41.92  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.09276   PostnetLoss: 0.00561   DecoderLoss:0.00589  StopLoss: 0.08125  \n",
      "   | > TotalLoss: 0.10302   PostnetLoss: 0.01049   DecoderLoss:0.01087  StopLoss: 0.08166  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00094   Validation Loss: 0.00930\n",
      "\n",
      " > Epoch 684/1000\n",
      "   | > Step:3/68  GlobalStep:116200  TotalLoss:0.00128  PostnetLoss:0.00062  DecoderLoss:0.00065  StopLoss:0.06698  GradNorm:0.00385  GradNormST:0.02492  AvgTextLen:4.7  AvgSpecLen:29.7  StepTime:0.29  LR:0.000100\n",
      "   | > Step:13/68  GlobalStep:116210  TotalLoss:0.00149  PostnetLoss:0.00073  DecoderLoss:0.00076  StopLoss:0.06576  GradNorm:0.00394  GradNormST:0.03619  AvgTextLen:11.5  AvgSpecLen:55.2  StepTime:0.42  LR:0.000100\n",
      "   | > Step:23/68  GlobalStep:116220  TotalLoss:0.00173  PostnetLoss:0.00085  DecoderLoss:0.00089  StopLoss:0.04708  GradNorm:0.00324  GradNormST:0.01094  AvgTextLen:18.3  AvgSpecLen:72.2  StepTime:0.33  LR:0.000100\n",
      "   | > Step:33/68  GlobalStep:116230  TotalLoss:0.00194  PostnetLoss:0.00095  DecoderLoss:0.00099  StopLoss:0.05667  GradNorm:0.00318  GradNormST:0.01706  AvgTextLen:25.9  AvgSpecLen:88.9  StepTime:0.56  LR:0.000100\n",
      "   | > Step:43/68  GlobalStep:116240  TotalLoss:0.00204  PostnetLoss:0.00099  DecoderLoss:0.00105  StopLoss:0.05909  GradNorm:0.00337  GradNormST:0.01452  AvgTextLen:33.4  AvgSpecLen:115.7  StepTime:0.66  LR:0.000100\n",
      "   | > Step:53/68  GlobalStep:116250  TotalLoss:0.00218  PostnetLoss:0.00106  DecoderLoss:0.00112  StopLoss:0.03514  GradNorm:0.00295  GradNormST:0.00781  AvgTextLen:42.4  AvgSpecLen:137.6  StepTime:0.76  LR:0.000100\n",
      "   | > Step:63/68  GlobalStep:116260  TotalLoss:0.00248  PostnetLoss:0.00120  DecoderLoss:0.00127  StopLoss:0.02876  GradNorm:0.00295  GradNormST:0.00722  AvgTextLen:58.6  AvgSpecLen:185.1  StepTime:0.91  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:116265  AvgTotalLoss:0.05072  AvgPostnetLoss:0.00094  AvgDecoderLoss:0.00099  AvgStopLoss:0.04880  EpochTime:42.50  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.08852   PostnetLoss: 0.00562   DecoderLoss:0.00590  StopLoss: 0.07700  \n",
      "   | > TotalLoss: 0.10093   PostnetLoss: 0.01034   DecoderLoss:0.01072  StopLoss: 0.07987  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00094   Validation Loss: 0.00916\n",
      "\n",
      " > Epoch 685/1000\n",
      "   | > Step:4/68  GlobalStep:116270  TotalLoss:0.00136  PostnetLoss:0.00066  DecoderLoss:0.00070  StopLoss:0.10107  GradNorm:0.00427  GradNormST:0.04756  AvgTextLen:4.8  AvgSpecLen:31.1  StepTime:0.24  LR:0.000100\n",
      "   | > Step:14/68  GlobalStep:116280  TotalLoss:0.00155  PostnetLoss:0.00075  DecoderLoss:0.00079  StopLoss:0.04696  GradNorm:0.00374  GradNormST:0.02343  AvgTextLen:12.2  AvgSpecLen:52.0  StepTime:0.34  LR:0.000100\n",
      "   | > Step:24/68  GlobalStep:116290  TotalLoss:0.00178  PostnetLoss:0.00087  DecoderLoss:0.00091  StopLoss:0.05108  GradNorm:0.00308  GradNormST:0.01283  AvgTextLen:19.2  AvgSpecLen:79.0  StepTime:0.50  LR:0.000100\n",
      "   | > Step:34/68  GlobalStep:116300  TotalLoss:0.00195  PostnetLoss:0.00095  DecoderLoss:0.00100  StopLoss:0.05999  GradNorm:0.00307  GradNormST:0.02938  AvgTextLen:26.2  AvgSpecLen:95.9  StepTime:0.59  LR:0.000100\n",
      "   | > Step:44/68  GlobalStep:116310  TotalLoss:0.00210  PostnetLoss:0.00103  DecoderLoss:0.00108  StopLoss:0.04071  GradNorm:0.00290  GradNormST:0.01936  AvgTextLen:33.8  AvgSpecLen:116.3  StepTime:0.53  LR:0.000100\n",
      "   | > Step:54/68  GlobalStep:116320  TotalLoss:0.00227  PostnetLoss:0.00111  DecoderLoss:0.00117  StopLoss:0.04026  GradNorm:0.00327  GradNormST:0.01347  AvgTextLen:42.2  AvgSpecLen:144.9  StepTime:0.68  LR:0.000100\n",
      "   | > Step:64/68  GlobalStep:116330  TotalLoss:0.00236  PostnetLoss:0.00114  DecoderLoss:0.00121  StopLoss:0.03366  GradNorm:0.00296  GradNormST:0.01091  AvgTextLen:60.6  AvgSpecLen:177.6  StepTime:0.82  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:116334  AvgTotalLoss:0.05319  AvgPostnetLoss:0.00094  AvgDecoderLoss:0.00098  AvgStopLoss:0.05127  EpochTime:42.32  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.09247   PostnetLoss: 0.00566   DecoderLoss:0.00595  StopLoss: 0.08086  \n",
      "   | > TotalLoss: 0.10182   PostnetLoss: 0.01014   DecoderLoss:0.01054  StopLoss: 0.08114  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00094   Validation Loss: 0.00900\n",
      "\n",
      " > Epoch 686/1000\n",
      "   | > Step:5/68  GlobalStep:116340  TotalLoss:0.00137  PostnetLoss:0.00067  DecoderLoss:0.00070  StopLoss:0.05449  GradNorm:0.00368  GradNormST:0.01625  AvgTextLen:5.9  AvgSpecLen:33.2  StepTime:0.36  LR:0.000100\n",
      "   | > Step:15/68  GlobalStep:116350  TotalLoss:0.00162  PostnetLoss:0.00079  DecoderLoss:0.00083  StopLoss:0.04459  GradNorm:0.00326  GradNormST:0.01268  AvgTextLen:12.8  AvgSpecLen:59.1  StepTime:0.53  LR:0.000100\n",
      "   | > Step:25/68  GlobalStep:116360  TotalLoss:0.00174  PostnetLoss:0.00085  DecoderLoss:0.00089  StopLoss:0.04854  GradNorm:0.00314  GradNormST:0.01252  AvgTextLen:19.8  AvgSpecLen:73.3  StepTime:0.40  LR:0.000100\n",
      "   | > Step:35/68  GlobalStep:116370  TotalLoss:0.00196  PostnetLoss:0.00095  DecoderLoss:0.00100  StopLoss:0.03632  GradNorm:0.00285  GradNormST:0.01452  AvgTextLen:27.1  AvgSpecLen:104.7  StepTime:0.65  LR:0.000100\n",
      "   | > Step:45/68  GlobalStep:116380  TotalLoss:0.00203  PostnetLoss:0.00099  DecoderLoss:0.00104  StopLoss:0.06210  GradNorm:0.00290  GradNormST:0.02401  AvgTextLen:34.4  AvgSpecLen:118.6  StepTime:0.70  LR:0.000100\n",
      "   | > Step:55/68  GlobalStep:116390  TotalLoss:0.00229  PostnetLoss:0.00111  DecoderLoss:0.00117  StopLoss:0.04580  GradNorm:0.00307  GradNormST:0.01383  AvgTextLen:43.9  AvgSpecLen:145.2  StepTime:0.66  LR:0.000100\n",
      "   | > Step:65/68  GlobalStep:116400  TotalLoss:0.00254  PostnetLoss:0.00123  DecoderLoss:0.00131  StopLoss:0.03944  GradNorm:0.00323  GradNormST:0.01264  AvgTextLen:64.2  AvgSpecLen:207.4  StepTime:1.18  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:116403  AvgTotalLoss:0.05225  AvgPostnetLoss:0.00094  AvgDecoderLoss:0.00099  AvgStopLoss:0.05032  EpochTime:42.25  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.08965   PostnetLoss: 0.00554   DecoderLoss:0.00582  StopLoss: 0.07828  \n",
      "   | > TotalLoss: 0.10069   PostnetLoss: 0.01018   DecoderLoss:0.01058  StopLoss: 0.07993  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00094   Validation Loss: 0.00906\n",
      "\n",
      " > Epoch 687/1000\n",
      "   | > Step:6/68  GlobalStep:116410  TotalLoss:0.00135  PostnetLoss:0.00066  DecoderLoss:0.00069  StopLoss:0.05681  GradNorm:0.00381  GradNormST:0.02985  AvgTextLen:6.0  AvgSpecLen:34.8  StepTime:0.37  LR:0.000100\n",
      "   | > Step:16/68  GlobalStep:116420  TotalLoss:0.00165  PostnetLoss:0.00081  DecoderLoss:0.00084  StopLoss:0.04406  GradNorm:0.00341  GradNormST:0.01533  AvgTextLen:13.8  AvgSpecLen:60.6  StepTime:0.37  LR:0.000100\n",
      "   | > Step:26/68  GlobalStep:116430  TotalLoss:0.00182  PostnetLoss:0.00089  DecoderLoss:0.00093  StopLoss:0.04936  GradNorm:0.00320  GradNormST:0.01259  AvgTextLen:20.3  AvgSpecLen:77.5  StepTime:0.40  LR:0.000100\n",
      "   | > Step:36/68  GlobalStep:116440  TotalLoss:0.00191  PostnetLoss:0.00093  DecoderLoss:0.00098  StopLoss:0.05613  GradNorm:0.00277  GradNormST:0.01054  AvgTextLen:28.5  AvgSpecLen:100.2  StepTime:0.68  LR:0.000100\n",
      "   | > Step:46/68  GlobalStep:116450  TotalLoss:0.00211  PostnetLoss:0.00103  DecoderLoss:0.00108  StopLoss:0.04849  GradNorm:0.00330  GradNormST:0.02195  AvgTextLen:35.4  AvgSpecLen:122.6  StepTime:0.77  LR:0.000100\n",
      "   | > Step:56/68  GlobalStep:116460  TotalLoss:0.00228  PostnetLoss:0.00111  DecoderLoss:0.00117  StopLoss:0.03700  GradNorm:0.00309  GradNormST:0.01874  AvgTextLen:45.8  AvgSpecLen:145.4  StepTime:0.96  LR:0.000100\n",
      "   | > Step:66/68  GlobalStep:116470  TotalLoss:0.00268  PostnetLoss:0.00130  DecoderLoss:0.00138  StopLoss:0.04254  GradNorm:0.00335  GradNormST:0.02223  AvgTextLen:71.2  AvgSpecLen:220.8  StepTime:1.03  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:116472  AvgTotalLoss:0.05050  AvgPostnetLoss:0.00094  AvgDecoderLoss:0.00099  AvgStopLoss:0.04858  EpochTime:42.76  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.09033   PostnetLoss: 0.00546   DecoderLoss:0.00572  StopLoss: 0.07915  \n",
      "   | > TotalLoss: 0.09980   PostnetLoss: 0.01013   DecoderLoss:0.01048  StopLoss: 0.07920  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00094   Validation Loss: 0.00913\n",
      "\n",
      " > Epoch 688/1000\n",
      "   | > Step:7/68  GlobalStep:116480  TotalLoss:0.00144  PostnetLoss:0.00070  DecoderLoss:0.00074  StopLoss:0.05386  GradNorm:0.00408  GradNormST:0.03237  AvgTextLen:7.4  AvgSpecLen:44.6  StepTime:0.40  LR:0.000100\n",
      "   | > Step:17/68  GlobalStep:116490  TotalLoss:0.00161  PostnetLoss:0.00079  DecoderLoss:0.00082  StopLoss:0.04989  GradNorm:0.00320  GradNormST:0.01572  AvgTextLen:14.7  AvgSpecLen:66.0  StepTime:0.44  LR:0.000100\n",
      "   | > Step:27/68  GlobalStep:116500  TotalLoss:0.00186  PostnetLoss:0.00091  DecoderLoss:0.00095  StopLoss:0.06534  GradNorm:0.00331  GradNormST:0.01985  AvgTextLen:21.0  AvgSpecLen:80.2  StepTime:0.67  LR:0.000100\n",
      "   | > Step:37/68  GlobalStep:116510  TotalLoss:0.00201  PostnetLoss:0.00098  DecoderLoss:0.00103  StopLoss:0.04649  GradNorm:0.00311  GradNormST:0.00926  AvgTextLen:29.3  AvgSpecLen:102.4  StepTime:0.63  LR:0.000100\n",
      "   | > Step:47/68  GlobalStep:116520  TotalLoss:0.00243  PostnetLoss:0.00119  DecoderLoss:0.00124  StopLoss:0.03727  GradNorm:0.00718  GradNormST:0.01294  AvgTextLen:36.7  AvgSpecLen:126.9  StepTime:0.70  LR:0.000100\n",
      "   | > Step:57/68  GlobalStep:116530  TotalLoss:0.00235  PostnetLoss:0.00114  DecoderLoss:0.00121  StopLoss:0.03880  GradNorm:0.00430  GradNormST:0.00848  AvgTextLen:47.1  AvgSpecLen:161.0  StepTime:0.90  LR:0.000100\n",
      "   | > Step:67/68  GlobalStep:116540  TotalLoss:0.00269  PostnetLoss:0.00130  DecoderLoss:0.00139  StopLoss:0.03830  GradNorm:0.00428  GradNormST:0.02030  AvgTextLen:80.6  AvgSpecLen:246.5  StepTime:1.22  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:116541  AvgTotalLoss:0.04975  AvgPostnetLoss:0.00095  AvgDecoderLoss:0.00100  AvgStopLoss:0.04780  EpochTime:42.00  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.08479   PostnetLoss: 0.00542   DecoderLoss:0.00569  StopLoss: 0.07368  \n",
      "   | > TotalLoss: 0.09692   PostnetLoss: 0.01023   DecoderLoss:0.01062  StopLoss: 0.07608  \n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00095   Validation Loss: 0.00893\n",
      "\n",
      " > Epoch 689/1000\n",
      "   | > Step:8/68  GlobalStep:116550  TotalLoss:0.00148  PostnetLoss:0.00073  DecoderLoss:0.00076  StopLoss:0.05497  GradNorm:0.00414  GradNormST:0.01569  AvgTextLen:8.3  AvgSpecLen:44.0  StepTime:0.24  LR:0.000100\n",
      "   | > Step:18/68  GlobalStep:116560  TotalLoss:0.00164  PostnetLoss:0.00080  DecoderLoss:0.00084  StopLoss:0.04946  GradNorm:0.00398  GradNormST:0.01112  AvgTextLen:15.2  AvgSpecLen:64.5  StepTime:0.38  LR:0.000100\n",
      "   | > Step:28/68  GlobalStep:116570  TotalLoss:0.00185  PostnetLoss:0.00091  DecoderLoss:0.00095  StopLoss:0.04982  GradNorm:0.00312  GradNormST:0.01188  AvgTextLen:22.1  AvgSpecLen:85.4  StepTime:0.53  LR:0.000100\n",
      "   | > Step:38/68  GlobalStep:116580  TotalLoss:0.00198  PostnetLoss:0.00096  DecoderLoss:0.00101  StopLoss:0.03565  GradNorm:0.00322  GradNormST:0.00853  AvgTextLen:29.4  AvgSpecLen:104.1  StepTime:0.51  LR:0.000100\n",
      "   | > Step:48/68  GlobalStep:116590  TotalLoss:0.00215  PostnetLoss:0.00105  DecoderLoss:0.00110  StopLoss:0.04477  GradNorm:0.00294  GradNormST:0.01633  AvgTextLen:37.5  AvgSpecLen:123.8  StepTime:0.72  LR:0.000100\n",
      "   | > Step:58/68  GlobalStep:116600  TotalLoss:0.00235  PostnetLoss:0.00114  DecoderLoss:0.00120  StopLoss:0.04686  GradNorm:0.00386  GradNormST:0.01250  AvgTextLen:47.8  AvgSpecLen:158.0  StepTime:0.71  LR:0.000100\n",
      "   | > Step:68/68  GlobalStep:116610  TotalLoss:0.00264  PostnetLoss:0.00128  DecoderLoss:0.00136  StopLoss:0.02398  GradNorm:0.00404  GradNormST:0.01192  AvgTextLen:103.0  AvgSpecLen:308.9  StepTime:1.86  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:116610  AvgTotalLoss:0.04922  AvgPostnetLoss:0.00094  AvgDecoderLoss:0.00099  AvgStopLoss:0.04728  EpochTime:42.17  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.08178   PostnetLoss: 0.00548   DecoderLoss:0.00576  StopLoss: 0.07054  \n",
      "   | > TotalLoss: 0.09772   PostnetLoss: 0.00969   DecoderLoss:0.01007  StopLoss: 0.07796  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00094   Validation Loss: 0.00876\n",
      "\n",
      " > Epoch 690/1000\n",
      "   | > Step:9/68  GlobalStep:116620  TotalLoss:0.00143  PostnetLoss:0.00070  DecoderLoss:0.00073  StopLoss:0.04354  GradNorm:0.00404  GradNormST:0.02056  AvgTextLen:9.0  AvgSpecLen:42.1  StepTime:0.46  LR:0.000100\n",
      "   | > Step:19/68  GlobalStep:116630  TotalLoss:0.00165  PostnetLoss:0.00081  DecoderLoss:0.00084  StopLoss:0.06749  GradNorm:0.00351  GradNormST:0.03010  AvgTextLen:15.5  AvgSpecLen:63.2  StepTime:0.39  LR:0.000100\n",
      "   | > Step:29/68  GlobalStep:116640  TotalLoss:0.00181  PostnetLoss:0.00088  DecoderLoss:0.00093  StopLoss:0.06256  GradNorm:0.00322  GradNormST:0.01436  AvgTextLen:21.8  AvgSpecLen:83.4  StepTime:0.45  LR:0.000100\n",
      "   | > Step:39/68  GlobalStep:116650  TotalLoss:0.00203  PostnetLoss:0.00099  DecoderLoss:0.00104  StopLoss:0.03868  GradNorm:0.00304  GradNormST:0.01317  AvgTextLen:30.2  AvgSpecLen:110.4  StepTime:0.54  LR:0.000100\n",
      "   | > Step:49/68  GlobalStep:116660  TotalLoss:0.00216  PostnetLoss:0.00105  DecoderLoss:0.00111  StopLoss:0.04159  GradNorm:0.00338  GradNormST:0.01150  AvgTextLen:38.2  AvgSpecLen:127.7  StepTime:0.78  LR:0.000100\n",
      "   | > Step:59/68  GlobalStep:116670  TotalLoss:0.00231  PostnetLoss:0.00112  DecoderLoss:0.00119  StopLoss:0.04262  GradNorm:0.00334  GradNormST:0.01563  AvgTextLen:50.5  AvgSpecLen:155.9  StepTime:0.88  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:116679  AvgTotalLoss:0.04956  AvgPostnetLoss:0.00094  AvgDecoderLoss:0.00099  AvgStopLoss:0.04764  EpochTime:41.62  AvgStepTime:0.60\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.08512   PostnetLoss: 0.00573   DecoderLoss:0.00601  StopLoss: 0.07338  \n",
      "   | > TotalLoss: 0.09585   PostnetLoss: 0.00964   DecoderLoss:0.01004  StopLoss: 0.07616  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00094   Validation Loss: 0.00864\n",
      "\n",
      " > Epoch 691/1000\n",
      "   | > Step:0/68  GlobalStep:116680  TotalLoss:0.00144  PostnetLoss:0.00069  DecoderLoss:0.00075  StopLoss:0.05236  GradNorm:0.00728  GradNormST:0.01835  AvgTextLen:3.0  AvgSpecLen:25.3  StepTime:0.41  LR:0.000100\n",
      "   | > Step:10/68  GlobalStep:116690  TotalLoss:0.00151  PostnetLoss:0.00074  DecoderLoss:0.00077  StopLoss:0.04397  GradNorm:0.00400  GradNormST:0.02493  AvgTextLen:9.8  AvgSpecLen:48.6  StepTime:0.55  LR:0.000100\n",
      "   | > Step:20/68  GlobalStep:116700  TotalLoss:0.00174  PostnetLoss:0.00085  DecoderLoss:0.00089  StopLoss:0.03716  GradNorm:0.00394  GradNormST:0.01348  AvgTextLen:16.4  AvgSpecLen:64.8  StepTime:0.50  LR:0.000100\n",
      "   | > Step:30/68  GlobalStep:116710  TotalLoss:0.00182  PostnetLoss:0.00089  DecoderLoss:0.00093  StopLoss:0.05652  GradNorm:0.00306  GradNormST:0.01800  AvgTextLen:22.9  AvgSpecLen:85.8  StepTime:0.58  LR:0.000100\n",
      "   | > Step:40/68  GlobalStep:116720  TotalLoss:0.00199  PostnetLoss:0.00097  DecoderLoss:0.00102  StopLoss:0.04377  GradNorm:0.00292  GradNormST:0.01161  AvgTextLen:30.9  AvgSpecLen:105.3  StepTime:0.63  LR:0.000100\n",
      "   | > Step:50/68  GlobalStep:116730  TotalLoss:0.00218  PostnetLoss:0.00106  DecoderLoss:0.00112  StopLoss:0.03618  GradNorm:0.00345  GradNormST:0.00992  AvgTextLen:38.8  AvgSpecLen:132.0  StepTime:0.86  LR:0.000100\n",
      "   | > Step:60/68  GlobalStep:116740  TotalLoss:0.00242  PostnetLoss:0.00118  DecoderLoss:0.00124  StopLoss:0.03413  GradNorm:0.00436  GradNormST:0.01092  AvgTextLen:52.0  AvgSpecLen:172.7  StepTime:0.93  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:116748  AvgTotalLoss:0.04951  AvgPostnetLoss:0.00094  AvgDecoderLoss:0.00099  AvgStopLoss:0.04759  EpochTime:42.79  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.08634   PostnetLoss: 0.00567   DecoderLoss:0.00595  StopLoss: 0.07472  \n",
      "   | > TotalLoss: 0.09753   PostnetLoss: 0.00985   DecoderLoss:0.01023  StopLoss: 0.07744  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00094   Validation Loss: 0.00877\n",
      "\n",
      " > Epoch 692/1000\n",
      "   | > Step:1/68  GlobalStep:116750  TotalLoss:0.00134  PostnetLoss:0.00065  DecoderLoss:0.00069  StopLoss:0.05045  GradNorm:0.00576  GradNormST:0.02312  AvgTextLen:3.7  AvgSpecLen:28.6  StepTime:0.42  LR:0.000100\n",
      "   | > Step:11/68  GlobalStep:116760  TotalLoss:0.00146  PostnetLoss:0.00072  DecoderLoss:0.00075  StopLoss:0.04310  GradNorm:0.00359  GradNormST:0.01310  AvgTextLen:10.3  AvgSpecLen:49.4  StepTime:0.39  LR:0.000100\n",
      "   | > Step:21/68  GlobalStep:116770  TotalLoss:0.00170  PostnetLoss:0.00083  DecoderLoss:0.00087  StopLoss:0.04525  GradNorm:0.00337  GradNormST:0.01036  AvgTextLen:16.6  AvgSpecLen:66.3  StepTime:0.42  LR:0.000100\n",
      "   | > Step:31/68  GlobalStep:116780  TotalLoss:0.00185  PostnetLoss:0.00090  DecoderLoss:0.00095  StopLoss:0.05451  GradNorm:0.00300  GradNormST:0.02122  AvgTextLen:24.1  AvgSpecLen:82.7  StepTime:0.54  LR:0.000100\n",
      "   | > Step:41/68  GlobalStep:116790  TotalLoss:0.00206  PostnetLoss:0.00100  DecoderLoss:0.00105  StopLoss:0.03054  GradNorm:0.00305  GradNormST:0.01041  AvgTextLen:32.3  AvgSpecLen:109.4  StepTime:0.64  LR:0.000100\n",
      "   | > Step:51/68  GlobalStep:116800  TotalLoss:0.00215  PostnetLoss:0.00105  DecoderLoss:0.00110  StopLoss:0.04871  GradNorm:0.00290  GradNormST:0.01082  AvgTextLen:39.2  AvgSpecLen:126.9  StepTime:0.74  LR:0.000100\n",
      "   | > Step:61/68  GlobalStep:116810  TotalLoss:0.00238  PostnetLoss:0.00116  DecoderLoss:0.00122  StopLoss:0.04021  GradNorm:0.00418  GradNormST:0.01036  AvgTextLen:54.6  AvgSpecLen:165.5  StepTime:0.90  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:116817  AvgTotalLoss:0.05093  AvgPostnetLoss:0.00094  AvgDecoderLoss:0.00099  AvgStopLoss:0.04901  EpochTime:43.51  AvgStepTime:0.63\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.08323   PostnetLoss: 0.00564   DecoderLoss:0.00593  StopLoss: 0.07166  \n",
      "   | > TotalLoss: 0.09495   PostnetLoss: 0.00952   DecoderLoss:0.00993  StopLoss: 0.07550  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00094   Validation Loss: 0.00862\n",
      "\n",
      " > Epoch 693/1000\n",
      "   | > Step:2/68  GlobalStep:116820  TotalLoss:0.00126  PostnetLoss:0.00061  DecoderLoss:0.00064  StopLoss:0.04553  GradNorm:0.00472  GradNormST:0.01758  AvgTextLen:4.0  AvgSpecLen:31.3  StepTime:0.50  LR:0.000100\n",
      "   | > Step:12/68  GlobalStep:116830  TotalLoss:0.00146  PostnetLoss:0.00072  DecoderLoss:0.00075  StopLoss:0.05766  GradNorm:0.00393  GradNormST:0.03431  AvgTextLen:11.0  AvgSpecLen:49.3  StepTime:0.42  LR:0.000100\n",
      "   | > Step:22/68  GlobalStep:116840  TotalLoss:0.00174  PostnetLoss:0.00085  DecoderLoss:0.00089  StopLoss:0.05852  GradNorm:0.00357  GradNormST:0.01315  AvgTextLen:18.0  AvgSpecLen:72.0  StepTime:0.36  LR:0.000100\n",
      "   | > Step:32/68  GlobalStep:116850  TotalLoss:0.00185  PostnetLoss:0.00090  DecoderLoss:0.00095  StopLoss:0.06025  GradNorm:0.00299  GradNormST:0.01643  AvgTextLen:24.4  AvgSpecLen:91.5  StepTime:0.60  LR:0.000100\n",
      "   | > Step:42/68  GlobalStep:116860  TotalLoss:0.00203  PostnetLoss:0.00099  DecoderLoss:0.00105  StopLoss:0.05643  GradNorm:0.00290  GradNormST:0.02515  AvgTextLen:33.1  AvgSpecLen:109.7  StepTime:0.48  LR:0.000100\n",
      "   | > Step:52/68  GlobalStep:116870  TotalLoss:0.00223  PostnetLoss:0.00108  DecoderLoss:0.00114  StopLoss:0.06264  GradNorm:0.00326  GradNormST:0.01082  AvgTextLen:42.1  AvgSpecLen:137.5  StepTime:0.81  LR:0.000100\n",
      "   | > Step:62/68  GlobalStep:116880  TotalLoss:0.00242  PostnetLoss:0.00118  DecoderLoss:0.00125  StopLoss:0.03691  GradNorm:0.00336  GradNormST:0.01168  AvgTextLen:54.0  AvgSpecLen:174.7  StepTime:0.75  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:116886  AvgTotalLoss:0.05088  AvgPostnetLoss:0.00093  AvgDecoderLoss:0.00098  AvgStopLoss:0.04896  EpochTime:42.37  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.08204   PostnetLoss: 0.00558   DecoderLoss:0.00587  StopLoss: 0.07059  \n",
      "   | > TotalLoss: 0.09230   PostnetLoss: 0.00939   DecoderLoss:0.00976  StopLoss: 0.07316  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00093   Validation Loss: 0.00845\n",
      "\n",
      " > Epoch 694/1000\n",
      "   | > Step:3/68  GlobalStep:116890  TotalLoss:0.00130  PostnetLoss:0.00063  DecoderLoss:0.00066  StopLoss:0.06428  GradNorm:0.00409  GradNormST:0.02002  AvgTextLen:4.7  AvgSpecLen:29.7  StepTime:0.28  LR:0.000100\n",
      "   | > Step:13/68  GlobalStep:116900  TotalLoss:0.00149  PostnetLoss:0.00073  DecoderLoss:0.00076  StopLoss:0.05777  GradNorm:0.00377  GradNormST:0.01990  AvgTextLen:11.5  AvgSpecLen:55.2  StepTime:0.41  LR:0.000100\n",
      "   | > Step:23/68  GlobalStep:116910  TotalLoss:0.00179  PostnetLoss:0.00088  DecoderLoss:0.00092  StopLoss:0.05283  GradNorm:0.00406  GradNormST:0.01395  AvgTextLen:18.3  AvgSpecLen:72.2  StepTime:0.34  LR:0.000100\n",
      "   | > Step:33/68  GlobalStep:116920  TotalLoss:0.00190  PostnetLoss:0.00093  DecoderLoss:0.00098  StopLoss:0.08759  GradNorm:0.00298  GradNormST:0.02898  AvgTextLen:25.9  AvgSpecLen:88.9  StepTime:0.58  LR:0.000100\n",
      "   | > Step:43/68  GlobalStep:116930  TotalLoss:0.00204  PostnetLoss:0.00099  DecoderLoss:0.00104  StopLoss:0.04828  GradNorm:0.00290  GradNormST:0.01186  AvgTextLen:33.4  AvgSpecLen:115.7  StepTime:0.51  LR:0.000100\n",
      "   | > Step:53/68  GlobalStep:116940  TotalLoss:0.00222  PostnetLoss:0.00108  DecoderLoss:0.00114  StopLoss:0.04387  GradNorm:0.00372  GradNormST:0.01038  AvgTextLen:42.4  AvgSpecLen:137.6  StepTime:0.76  LR:0.000100\n",
      "   | > Step:63/68  GlobalStep:116950  TotalLoss:0.00247  PostnetLoss:0.00120  DecoderLoss:0.00127  StopLoss:0.02707  GradNorm:0.00312  GradNormST:0.00653  AvgTextLen:58.6  AvgSpecLen:185.1  StepTime:0.91  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:116955  AvgTotalLoss:0.05075  AvgPostnetLoss:0.00093  AvgDecoderLoss:0.00098  AvgStopLoss:0.04883  EpochTime:42.56  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.08334   PostnetLoss: 0.00556   DecoderLoss:0.00585  StopLoss: 0.07193  \n",
      "   | > TotalLoss: 0.08936   PostnetLoss: 0.00916   DecoderLoss:0.00957  StopLoss: 0.07063  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00093   Validation Loss: 0.00834\n",
      "\n",
      " > Epoch 695/1000\n",
      "   | > Step:4/68  GlobalStep:116960  TotalLoss:0.00131  PostnetLoss:0.00064  DecoderLoss:0.00067  StopLoss:0.07071  GradNorm:0.00414  GradNormST:0.02601  AvgTextLen:4.8  AvgSpecLen:31.1  StepTime:0.27  LR:0.000100\n",
      "   | > Step:14/68  GlobalStep:116970  TotalLoss:0.00155  PostnetLoss:0.00076  DecoderLoss:0.00079  StopLoss:0.05642  GradNorm:0.00345  GradNormST:0.01874  AvgTextLen:12.2  AvgSpecLen:52.0  StepTime:0.26  LR:0.000100\n",
      "   | > Step:24/68  GlobalStep:116980  TotalLoss:0.00184  PostnetLoss:0.00090  DecoderLoss:0.00094  StopLoss:0.05407  GradNorm:0.00368  GradNormST:0.01904  AvgTextLen:19.2  AvgSpecLen:79.0  StepTime:0.49  LR:0.000100\n",
      "   | > Step:34/68  GlobalStep:116990  TotalLoss:0.00195  PostnetLoss:0.00095  DecoderLoss:0.00100  StopLoss:0.04090  GradNorm:0.00337  GradNormST:0.01637  AvgTextLen:26.2  AvgSpecLen:95.9  StepTime:0.58  LR:0.000100\n",
      "   | > Step:44/68  GlobalStep:117000  TotalLoss:0.00210  PostnetLoss:0.00102  DecoderLoss:0.00108  StopLoss:0.03706  GradNorm:0.00291  GradNormST:0.01242  AvgTextLen:33.8  AvgSpecLen:116.3  StepTime:0.68  LR:0.000100\n",
      " | | > Checkpoint saving : /vm/TTS/models/cartman_models/mozilla-no-loc-fattn-stopnet-sigmoid-loss_masking-August-15-2019_11+26AM-de6aeb9/checkpoint_117000.pth.tar\n",
      "warning: audio amplitude out of range, auto clipped.\n",
      "   | > Step:54/68  GlobalStep:117010  TotalLoss:0.00224  PostnetLoss:0.00109  DecoderLoss:0.00115  StopLoss:0.03746  GradNorm:0.00343  GradNormST:0.00992  AvgTextLen:42.2  AvgSpecLen:144.9  StepTime:0.68  LR:0.000100\n",
      "   | > Step:64/68  GlobalStep:117020  TotalLoss:0.00236  PostnetLoss:0.00115  DecoderLoss:0.00121  StopLoss:0.03362  GradNorm:0.00400  GradNormST:0.00910  AvgTextLen:60.6  AvgSpecLen:177.6  StepTime:0.80  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:117024  AvgTotalLoss:0.04991  AvgPostnetLoss:0.00093  AvgDecoderLoss:0.00098  AvgStopLoss:0.04799  EpochTime:42.14  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.08454   PostnetLoss: 0.00560   DecoderLoss:0.00589  StopLoss: 0.07304  \n",
      "   | > TotalLoss: 0.09162   PostnetLoss: 0.00962   DecoderLoss:0.01000  StopLoss: 0.07200  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00093   Validation Loss: 0.00865\n",
      "\n",
      " > Epoch 696/1000\n",
      "   | > Step:5/68  GlobalStep:117030  TotalLoss:0.00135  PostnetLoss:0.00065  DecoderLoss:0.00069  StopLoss:0.06264  GradNorm:0.00393  GradNormST:0.02174  AvgTextLen:5.9  AvgSpecLen:33.2  StepTime:0.38  LR:0.000100\n",
      "   | > Step:15/68  GlobalStep:117040  TotalLoss:0.00161  PostnetLoss:0.00079  DecoderLoss:0.00082  StopLoss:0.05833  GradNorm:0.00369  GradNormST:0.03243  AvgTextLen:12.8  AvgSpecLen:59.1  StepTime:0.55  LR:0.000100\n",
      "   | > Step:25/68  GlobalStep:117050  TotalLoss:0.00175  PostnetLoss:0.00086  DecoderLoss:0.00090  StopLoss:0.04109  GradNorm:0.00380  GradNormST:0.01545  AvgTextLen:19.8  AvgSpecLen:73.3  StepTime:0.45  LR:0.000100\n",
      "   | > Step:35/68  GlobalStep:117060  TotalLoss:0.00197  PostnetLoss:0.00096  DecoderLoss:0.00101  StopLoss:0.07952  GradNorm:0.00357  GradNormST:0.04811  AvgTextLen:27.1  AvgSpecLen:104.7  StepTime:0.65  LR:0.000100\n",
      "   | > Step:45/68  GlobalStep:117070  TotalLoss:0.00206  PostnetLoss:0.00100  DecoderLoss:0.00106  StopLoss:0.06221  GradNorm:0.00292  GradNormST:0.01835  AvgTextLen:34.4  AvgSpecLen:118.6  StepTime:0.68  LR:0.000100\n",
      "   | > Step:55/68  GlobalStep:117080  TotalLoss:0.00227  PostnetLoss:0.00111  DecoderLoss:0.00117  StopLoss:0.04211  GradNorm:0.00267  GradNormST:0.00997  AvgTextLen:43.9  AvgSpecLen:145.2  StepTime:0.81  LR:0.000100\n",
      "   | > Step:65/68  GlobalStep:117090  TotalLoss:0.00250  PostnetLoss:0.00121  DecoderLoss:0.00129  StopLoss:0.03326  GradNorm:0.00345  GradNormST:0.00973  AvgTextLen:64.2  AvgSpecLen:207.4  StepTime:1.23  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:117093  AvgTotalLoss:0.05220  AvgPostnetLoss:0.00093  AvgDecoderLoss:0.00098  AvgStopLoss:0.05029  EpochTime:42.13  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.08091   PostnetLoss: 0.00560   DecoderLoss:0.00588  StopLoss: 0.06943  \n",
      "   | > TotalLoss: 0.08755   PostnetLoss: 0.00935   DecoderLoss:0.00974  StopLoss: 0.06845  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00093   Validation Loss: 0.00847\n",
      "\n",
      " > Epoch 697/1000\n",
      "   | > Step:6/68  GlobalStep:117100  TotalLoss:0.00140  PostnetLoss:0.00068  DecoderLoss:0.00071  StopLoss:0.04233  GradNorm:0.00476  GradNormST:0.01899  AvgTextLen:6.0  AvgSpecLen:34.8  StepTime:0.45  LR:0.000100\n",
      "   | > Step:16/68  GlobalStep:117110  TotalLoss:0.00163  PostnetLoss:0.00079  DecoderLoss:0.00083  StopLoss:0.04660  GradNorm:0.00363  GradNormST:0.01326  AvgTextLen:13.8  AvgSpecLen:60.6  StepTime:0.29  LR:0.000100\n",
      "   | > Step:26/68  GlobalStep:117120  TotalLoss:0.00182  PostnetLoss:0.00089  DecoderLoss:0.00093  StopLoss:0.05224  GradNorm:0.00366  GradNormST:0.01638  AvgTextLen:20.3  AvgSpecLen:77.5  StepTime:0.38  LR:0.000100\n",
      "   | > Step:36/68  GlobalStep:117130  TotalLoss:0.00195  PostnetLoss:0.00095  DecoderLoss:0.00100  StopLoss:0.05680  GradNorm:0.00319  GradNormST:0.01019  AvgTextLen:28.5  AvgSpecLen:100.2  StepTime:0.58  LR:0.000100\n",
      "   | > Step:46/68  GlobalStep:117140  TotalLoss:0.00208  PostnetLoss:0.00101  DecoderLoss:0.00107  StopLoss:0.04769  GradNorm:0.00297  GradNormST:0.02150  AvgTextLen:35.4  AvgSpecLen:122.6  StepTime:0.78  LR:0.000100\n",
      "   | > Step:56/68  GlobalStep:117150  TotalLoss:0.00225  PostnetLoss:0.00109  DecoderLoss:0.00115  StopLoss:0.04189  GradNorm:0.00305  GradNormST:0.02251  AvgTextLen:45.8  AvgSpecLen:145.4  StepTime:0.99  LR:0.000100\n",
      "   | > Step:66/68  GlobalStep:117160  TotalLoss:0.00263  PostnetLoss:0.00127  DecoderLoss:0.00136  StopLoss:0.03966  GradNorm:0.00292  GradNormST:0.01641  AvgTextLen:71.2  AvgSpecLen:220.8  StepTime:1.04  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:117162  AvgTotalLoss:0.05182  AvgPostnetLoss:0.00093  AvgDecoderLoss:0.00098  AvgStopLoss:0.04990  EpochTime:42.27  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.08155   PostnetLoss: 0.00571   DecoderLoss:0.00600  StopLoss: 0.06984  \n",
      "   | > TotalLoss: 0.08728   PostnetLoss: 0.00893   DecoderLoss:0.00933  StopLoss: 0.06902  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00093   Validation Loss: 0.00816\n",
      "\n",
      " > Epoch 698/1000\n",
      "   | > Step:7/68  GlobalStep:117170  TotalLoss:0.00147  PostnetLoss:0.00072  DecoderLoss:0.00075  StopLoss:0.04533  GradNorm:0.00469  GradNormST:0.01765  AvgTextLen:7.4  AvgSpecLen:44.6  StepTime:0.39  LR:0.000100\n",
      "   | > Step:17/68  GlobalStep:117180  TotalLoss:0.00161  PostnetLoss:0.00079  DecoderLoss:0.00082  StopLoss:0.05212  GradNorm:0.00355  GradNormST:0.01378  AvgTextLen:14.7  AvgSpecLen:66.0  StepTime:0.33  LR:0.000100\n",
      "   | > Step:27/68  GlobalStep:117190  TotalLoss:0.00184  PostnetLoss:0.00090  DecoderLoss:0.00094  StopLoss:0.08265  GradNorm:0.00376  GradNormST:0.02860  AvgTextLen:21.0  AvgSpecLen:80.2  StepTime:0.66  LR:0.000100\n",
      "   | > Step:37/68  GlobalStep:117200  TotalLoss:0.00196  PostnetLoss:0.00096  DecoderLoss:0.00101  StopLoss:0.04945  GradNorm:0.00321  GradNormST:0.00987  AvgTextLen:29.3  AvgSpecLen:102.4  StepTime:0.49  LR:0.000100\n",
      "   | > Step:47/68  GlobalStep:117210  TotalLoss:0.00218  PostnetLoss:0.00106  DecoderLoss:0.00112  StopLoss:0.04611  GradNorm:0.00315  GradNormST:0.01518  AvgTextLen:36.7  AvgSpecLen:126.9  StepTime:0.75  LR:0.000100\n",
      "   | > Step:57/68  GlobalStep:117220  TotalLoss:0.00230  PostnetLoss:0.00112  DecoderLoss:0.00118  StopLoss:0.03359  GradNorm:0.00281  GradNormST:0.00854  AvgTextLen:47.1  AvgSpecLen:161.0  StepTime:0.83  LR:0.000100\n",
      "   | > Step:67/68  GlobalStep:117230  TotalLoss:0.00265  PostnetLoss:0.00128  DecoderLoss:0.00137  StopLoss:0.03372  GradNorm:0.00248  GradNormST:0.01824  AvgTextLen:80.6  AvgSpecLen:246.5  StepTime:1.22  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:117231  AvgTotalLoss:0.04941  AvgPostnetLoss:0.00094  AvgDecoderLoss:0.00099  AvgStopLoss:0.04749  EpochTime:42.91  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.08807   PostnetLoss: 0.00566   DecoderLoss:0.00594  StopLoss: 0.07647  \n",
      "   | > TotalLoss: 0.09524   PostnetLoss: 0.00938   DecoderLoss:0.00978  StopLoss: 0.07608  \n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00094   Validation Loss: 0.00847\n",
      "\n",
      " > Epoch 699/1000\n",
      "   | > Step:8/68  GlobalStep:117240  TotalLoss:0.00144  PostnetLoss:0.00070  DecoderLoss:0.00073  StopLoss:0.05612  GradNorm:0.00369  GradNormST:0.01976  AvgTextLen:8.3  AvgSpecLen:44.0  StepTime:0.31  LR:0.000100\n",
      "   | > Step:18/68  GlobalStep:117250  TotalLoss:0.00159  PostnetLoss:0.00078  DecoderLoss:0.00081  StopLoss:0.04769  GradNorm:0.00398  GradNormST:0.01397  AvgTextLen:15.2  AvgSpecLen:64.5  StepTime:0.41  LR:0.000100\n",
      "   | > Step:28/68  GlobalStep:117260  TotalLoss:0.00186  PostnetLoss:0.00091  DecoderLoss:0.00095  StopLoss:0.05185  GradNorm:0.00401  GradNormST:0.01418  AvgTextLen:22.1  AvgSpecLen:85.4  StepTime:0.44  LR:0.000100\n",
      "   | > Step:38/68  GlobalStep:117270  TotalLoss:0.00197  PostnetLoss:0.00096  DecoderLoss:0.00101  StopLoss:0.03926  GradNorm:0.00333  GradNormST:0.01065  AvgTextLen:29.4  AvgSpecLen:104.1  StepTime:0.61  LR:0.000100\n",
      "   | > Step:48/68  GlobalStep:117280  TotalLoss:0.00212  PostnetLoss:0.00104  DecoderLoss:0.00109  StopLoss:0.04328  GradNorm:0.00328  GradNormST:0.01763  AvgTextLen:37.5  AvgSpecLen:123.8  StepTime:0.74  LR:0.000100\n",
      "   | > Step:58/68  GlobalStep:117290  TotalLoss:0.00231  PostnetLoss:0.00113  DecoderLoss:0.00119  StopLoss:0.04072  GradNorm:0.00273  GradNormST:0.01233  AvgTextLen:47.8  AvgSpecLen:158.0  StepTime:0.85  LR:0.000100\n",
      "   | > Step:68/68  GlobalStep:117300  TotalLoss:0.00263  PostnetLoss:0.00128  DecoderLoss:0.00136  StopLoss:0.02307  GradNorm:0.00274  GradNormST:0.01450  AvgTextLen:103.0  AvgSpecLen:308.9  StepTime:1.87  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:117300  AvgTotalLoss:0.04796  AvgPostnetLoss:0.00093  AvgDecoderLoss:0.00098  AvgStopLoss:0.04605  EpochTime:42.44  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.08746   PostnetLoss: 0.00550   DecoderLoss:0.00578  StopLoss: 0.07617  \n",
      "   | > TotalLoss: 0.09167   PostnetLoss: 0.00933   DecoderLoss:0.00971  StopLoss: 0.07263  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00093   Validation Loss: 0.00846\n",
      "\n",
      " > Epoch 700/1000\n",
      "   | > Step:9/68  GlobalStep:117310  TotalLoss:0.00143  PostnetLoss:0.00070  DecoderLoss:0.00073  StopLoss:0.06789  GradNorm:0.00363  GradNormST:0.03608  AvgTextLen:9.0  AvgSpecLen:42.1  StepTime:0.40  LR:0.000100\n",
      "   | > Step:19/68  GlobalStep:117320  TotalLoss:0.00163  PostnetLoss:0.00080  DecoderLoss:0.00083  StopLoss:0.05640  GradNorm:0.00344  GradNormST:0.02624  AvgTextLen:15.5  AvgSpecLen:63.2  StepTime:0.47  LR:0.000100\n",
      "   | > Step:29/68  GlobalStep:117330  TotalLoss:0.00180  PostnetLoss:0.00088  DecoderLoss:0.00092  StopLoss:0.06889  GradNorm:0.00368  GradNormST:0.01599  AvgTextLen:21.8  AvgSpecLen:83.4  StepTime:0.58  LR:0.000100\n",
      "   | > Step:39/68  GlobalStep:117340  TotalLoss:0.00202  PostnetLoss:0.00099  DecoderLoss:0.00104  StopLoss:0.04115  GradNorm:0.00353  GradNormST:0.01952  AvgTextLen:30.2  AvgSpecLen:110.4  StepTime:0.52  LR:0.000100\n",
      "   | > Step:49/68  GlobalStep:117350  TotalLoss:0.00216  PostnetLoss:0.00105  DecoderLoss:0.00111  StopLoss:0.03962  GradNorm:0.00349  GradNormST:0.01256  AvgTextLen:38.2  AvgSpecLen:127.7  StepTime:0.61  LR:0.000100\n",
      "   | > Step:59/68  GlobalStep:117360  TotalLoss:0.00230  PostnetLoss:0.00112  DecoderLoss:0.00118  StopLoss:0.03764  GradNorm:0.00313  GradNormST:0.00933  AvgTextLen:50.5  AvgSpecLen:155.9  StepTime:0.74  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:117369  AvgTotalLoss:0.05005  AvgPostnetLoss:0.00093  AvgDecoderLoss:0.00098  AvgStopLoss:0.04814  EpochTime:42.23  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.08708   PostnetLoss: 0.00559   DecoderLoss:0.00588  StopLoss: 0.07561  \n",
      "   | > TotalLoss: 0.09281   PostnetLoss: 0.00923   DecoderLoss:0.00963  StopLoss: 0.07394  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00093   Validation Loss: 0.00839\n",
      "\n",
      " > Epoch 701/1000\n",
      "   | > Step:0/68  GlobalStep:117370  TotalLoss:0.00135  PostnetLoss:0.00065  DecoderLoss:0.00070  StopLoss:0.07134  GradNorm:0.00542  GradNormST:0.02720  AvgTextLen:3.0  AvgSpecLen:25.3  StepTime:0.51  LR:0.000100\n",
      "   | > Step:10/68  GlobalStep:117380  TotalLoss:0.00149  PostnetLoss:0.00073  DecoderLoss:0.00076  StopLoss:0.08329  GradNorm:0.00371  GradNormST:0.05741  AvgTextLen:9.8  AvgSpecLen:48.6  StepTime:0.51  LR:0.000100\n",
      "   | > Step:20/68  GlobalStep:117390  TotalLoss:0.00171  PostnetLoss:0.00083  DecoderLoss:0.00087  StopLoss:0.03962  GradNorm:0.00333  GradNormST:0.01640  AvgTextLen:16.4  AvgSpecLen:64.8  StepTime:0.39  LR:0.000100\n",
      "   | > Step:30/68  GlobalStep:117400  TotalLoss:0.00182  PostnetLoss:0.00089  DecoderLoss:0.00093  StopLoss:0.05777  GradNorm:0.00334  GradNormST:0.01332  AvgTextLen:22.9  AvgSpecLen:85.8  StepTime:0.60  LR:0.000100\n",
      "   | > Step:40/68  GlobalStep:117410  TotalLoss:0.00199  PostnetLoss:0.00097  DecoderLoss:0.00102  StopLoss:0.04002  GradNorm:0.00354  GradNormST:0.01515  AvgTextLen:30.9  AvgSpecLen:105.3  StepTime:0.63  LR:0.000100\n",
      "   | > Step:50/68  GlobalStep:117420  TotalLoss:0.00218  PostnetLoss:0.00106  DecoderLoss:0.00112  StopLoss:0.05004  GradNorm:0.00371  GradNormST:0.01377  AvgTextLen:38.8  AvgSpecLen:132.0  StepTime:0.86  LR:0.000100\n",
      "   | > Step:60/68  GlobalStep:117430  TotalLoss:0.00236  PostnetLoss:0.00115  DecoderLoss:0.00121  StopLoss:0.04180  GradNorm:0.00321  GradNormST:0.01225  AvgTextLen:52.0  AvgSpecLen:172.7  StepTime:0.94  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:117438  AvgTotalLoss:0.05224  AvgPostnetLoss:0.00093  AvgDecoderLoss:0.00098  AvgStopLoss:0.05033  EpochTime:42.18  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.08715   PostnetLoss: 0.00552   DecoderLoss:0.00581  StopLoss: 0.07582  \n",
      "   | > TotalLoss: 0.09073   PostnetLoss: 0.00911   DecoderLoss:0.00950  StopLoss: 0.07212  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00093   Validation Loss: 0.00828\n",
      "\n",
      " > Epoch 702/1000\n",
      "   | > Step:1/68  GlobalStep:117440  TotalLoss:0.00134  PostnetLoss:0.00065  DecoderLoss:0.00069  StopLoss:0.06127  GradNorm:0.00438  GradNormST:0.02311  AvgTextLen:3.7  AvgSpecLen:28.6  StepTime:0.39  LR:0.000100\n",
      "   | > Step:11/68  GlobalStep:117450  TotalLoss:0.00147  PostnetLoss:0.00072  DecoderLoss:0.00075  StopLoss:0.04838  GradNorm:0.00340  GradNormST:0.02083  AvgTextLen:10.3  AvgSpecLen:49.4  StepTime:0.41  LR:0.000100\n",
      "   | > Step:21/68  GlobalStep:117460  TotalLoss:0.00169  PostnetLoss:0.00083  DecoderLoss:0.00086  StopLoss:0.05282  GradNorm:0.00320  GradNormST:0.01294  AvgTextLen:16.6  AvgSpecLen:66.3  StepTime:0.43  LR:0.000100\n",
      "   | > Step:31/68  GlobalStep:117470  TotalLoss:0.00185  PostnetLoss:0.00090  DecoderLoss:0.00095  StopLoss:0.05243  GradNorm:0.00339  GradNormST:0.02312  AvgTextLen:24.1  AvgSpecLen:82.7  StepTime:0.43  LR:0.000100\n",
      "   | > Step:41/68  GlobalStep:117480  TotalLoss:0.00209  PostnetLoss:0.00102  DecoderLoss:0.00107  StopLoss:0.04096  GradNorm:0.00352  GradNormST:0.01526  AvgTextLen:32.3  AvgSpecLen:109.4  StepTime:0.52  LR:0.000100\n",
      "   | > Step:51/68  GlobalStep:117490  TotalLoss:0.00217  PostnetLoss:0.00106  DecoderLoss:0.00111  StopLoss:0.04606  GradNorm:0.00359  GradNormST:0.01096  AvgTextLen:39.2  AvgSpecLen:126.9  StepTime:0.79  LR:0.000100\n",
      "   | > Step:61/68  GlobalStep:117500  TotalLoss:0.00234  PostnetLoss:0.00114  DecoderLoss:0.00120  StopLoss:0.03787  GradNorm:0.00325  GradNormST:0.01317  AvgTextLen:54.6  AvgSpecLen:165.5  StepTime:0.91  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:117507  AvgTotalLoss:0.05593  AvgPostnetLoss:0.00093  AvgDecoderLoss:0.00098  AvgStopLoss:0.05402  EpochTime:42.62  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.08657   PostnetLoss: 0.00556   DecoderLoss:0.00584  StopLoss: 0.07517  \n",
      "   | > TotalLoss: 0.09437   PostnetLoss: 0.00941   DecoderLoss:0.00981  StopLoss: 0.07515  \n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00093   Validation Loss: 0.00837\n",
      "\n",
      " > Epoch 703/1000\n",
      "   | > Step:2/68  GlobalStep:117510  TotalLoss:0.00126  PostnetLoss:0.00061  DecoderLoss:0.00064  StopLoss:0.04860  GradNorm:0.00423  GradNormST:0.02218  AvgTextLen:4.0  AvgSpecLen:31.3  StepTime:0.47  LR:0.000100\n",
      "   | > Step:12/68  GlobalStep:117520  TotalLoss:0.00146  PostnetLoss:0.00071  DecoderLoss:0.00075  StopLoss:0.06158  GradNorm:0.00320  GradNormST:0.03809  AvgTextLen:11.0  AvgSpecLen:49.3  StepTime:0.42  LR:0.000100\n",
      "   | > Step:22/68  GlobalStep:117530  TotalLoss:0.00170  PostnetLoss:0.00083  DecoderLoss:0.00087  StopLoss:0.04711  GradNorm:0.00315  GradNormST:0.01317  AvgTextLen:18.0  AvgSpecLen:72.0  StepTime:0.47  LR:0.000100\n",
      "   | > Step:32/68  GlobalStep:117540  TotalLoss:0.00187  PostnetLoss:0.00091  DecoderLoss:0.00096  StopLoss:0.09995  GradNorm:0.00342  GradNormST:0.03771  AvgTextLen:24.4  AvgSpecLen:91.5  StepTime:0.60  LR:0.000100\n",
      "   | > Step:42/68  GlobalStep:117550  TotalLoss:0.00203  PostnetLoss:0.00099  DecoderLoss:0.00104  StopLoss:0.05171  GradNorm:0.00330  GradNormST:0.02046  AvgTextLen:33.1  AvgSpecLen:109.7  StepTime:0.48  LR:0.000100\n",
      "   | > Step:52/68  GlobalStep:117560  TotalLoss:0.00223  PostnetLoss:0.00108  DecoderLoss:0.00114  StopLoss:0.05595  GradNorm:0.00372  GradNormST:0.01281  AvgTextLen:42.1  AvgSpecLen:137.5  StepTime:0.83  LR:0.000100\n",
      "   | > Step:62/68  GlobalStep:117570  TotalLoss:0.00241  PostnetLoss:0.00117  DecoderLoss:0.00124  StopLoss:0.04359  GradNorm:0.00324  GradNormST:0.01325  AvgTextLen:54.0  AvgSpecLen:174.7  StepTime:0.75  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:117576  AvgTotalLoss:0.05498  AvgPostnetLoss:0.00093  AvgDecoderLoss:0.00098  AvgStopLoss:0.05307  EpochTime:42.29  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.08408   PostnetLoss: 0.00561   DecoderLoss:0.00589  StopLoss: 0.07258  \n",
      "   | > TotalLoss: 0.09109   PostnetLoss: 0.00941   DecoderLoss:0.00979  StopLoss: 0.07190  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00093   Validation Loss: 0.00839\n",
      "\n",
      " > Epoch 704/1000\n",
      "   | > Step:3/68  GlobalStep:117580  TotalLoss:0.00130  PostnetLoss:0.00064  DecoderLoss:0.00066  StopLoss:0.06241  GradNorm:0.00398  GradNormST:0.02194  AvgTextLen:4.7  AvgSpecLen:29.7  StepTime:0.30  LR:0.000100\n",
      "   | > Step:13/68  GlobalStep:117590  TotalLoss:0.00152  PostnetLoss:0.00074  DecoderLoss:0.00077  StopLoss:0.07377  GradNorm:0.00430  GradNormST:0.02297  AvgTextLen:11.5  AvgSpecLen:55.2  StepTime:0.32  LR:0.000100\n",
      "   | > Step:23/68  GlobalStep:117600  TotalLoss:0.00175  PostnetLoss:0.00085  DecoderLoss:0.00089  StopLoss:0.06161  GradNorm:0.00339  GradNormST:0.01712  AvgTextLen:18.3  AvgSpecLen:72.2  StepTime:0.34  LR:0.000100\n",
      "   | > Step:33/68  GlobalStep:117610  TotalLoss:0.00192  PostnetLoss:0.00094  DecoderLoss:0.00098  StopLoss:0.05686  GradNorm:0.00332  GradNormST:0.02598  AvgTextLen:25.9  AvgSpecLen:88.9  StepTime:0.53  LR:0.000100\n",
      "   | > Step:43/68  GlobalStep:117620  TotalLoss:0.00204  PostnetLoss:0.00099  DecoderLoss:0.00105  StopLoss:0.05467  GradNorm:0.00347  GradNormST:0.01584  AvgTextLen:33.4  AvgSpecLen:115.7  StepTime:0.66  LR:0.000100\n",
      "   | > Step:53/68  GlobalStep:117630  TotalLoss:0.00219  PostnetLoss:0.00107  DecoderLoss:0.00112  StopLoss:0.05769  GradNorm:0.00423  GradNormST:0.01778  AvgTextLen:42.4  AvgSpecLen:137.6  StepTime:0.76  LR:0.000100\n",
      "   | > Step:63/68  GlobalStep:117640  TotalLoss:0.00249  PostnetLoss:0.00121  DecoderLoss:0.00128  StopLoss:0.04333  GradNorm:0.00386  GradNormST:0.01805  AvgTextLen:58.6  AvgSpecLen:185.1  StepTime:0.91  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:117645  AvgTotalLoss:0.05554  AvgPostnetLoss:0.00094  AvgDecoderLoss:0.00098  AvgStopLoss:0.05362  EpochTime:42.33  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.08714   PostnetLoss: 0.00556   DecoderLoss:0.00584  StopLoss: 0.07574  \n",
      "   | > TotalLoss: 0.09203   PostnetLoss: 0.00959   DecoderLoss:0.00998  StopLoss: 0.07247  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00094   Validation Loss: 0.00855\n",
      "\n",
      " > Epoch 705/1000\n",
      "   | > Step:4/68  GlobalStep:117650  TotalLoss:0.00131  PostnetLoss:0.00064  DecoderLoss:0.00067  StopLoss:0.06526  GradNorm:0.00406  GradNormST:0.02788  AvgTextLen:4.8  AvgSpecLen:31.1  StepTime:0.26  LR:0.000100\n",
      "   | > Step:14/68  GlobalStep:117660  TotalLoss:0.00153  PostnetLoss:0.00075  DecoderLoss:0.00079  StopLoss:0.04193  GradNorm:0.00367  GradNormST:0.01447  AvgTextLen:12.2  AvgSpecLen:52.0  StepTime:0.35  LR:0.000100\n",
      "   | > Step:24/68  GlobalStep:117670  TotalLoss:0.00182  PostnetLoss:0.00089  DecoderLoss:0.00093  StopLoss:0.06248  GradNorm:0.00389  GradNormST:0.01902  AvgTextLen:19.2  AvgSpecLen:79.0  StepTime:0.52  LR:0.000100\n",
      "   | > Step:34/68  GlobalStep:117680  TotalLoss:0.00195  PostnetLoss:0.00095  DecoderLoss:0.00100  StopLoss:0.06350  GradNorm:0.00343  GradNormST:0.02920  AvgTextLen:26.2  AvgSpecLen:95.9  StepTime:0.56  LR:0.000100\n",
      "   | > Step:44/68  GlobalStep:117690  TotalLoss:0.00212  PostnetLoss:0.00103  DecoderLoss:0.00109  StopLoss:0.03419  GradNorm:0.00333  GradNormST:0.01307  AvgTextLen:33.8  AvgSpecLen:116.3  StepTime:0.64  LR:0.000100\n",
      "   | > Step:54/68  GlobalStep:117700  TotalLoss:0.00228  PostnetLoss:0.00111  DecoderLoss:0.00117  StopLoss:0.04062  GradNorm:0.00386  GradNormST:0.01256  AvgTextLen:42.2  AvgSpecLen:144.9  StepTime:0.79  LR:0.000100\n",
      "   | > Step:64/68  GlobalStep:117710  TotalLoss:0.00233  PostnetLoss:0.00113  DecoderLoss:0.00120  StopLoss:0.03614  GradNorm:0.00292  GradNormST:0.00975  AvgTextLen:60.6  AvgSpecLen:177.6  StepTime:0.80  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:117714  AvgTotalLoss:0.05398  AvgPostnetLoss:0.00094  AvgDecoderLoss:0.00098  AvgStopLoss:0.05206  EpochTime:42.41  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.08193   PostnetLoss: 0.00557   DecoderLoss:0.00585  StopLoss: 0.07051  \n",
      "   | > TotalLoss: 0.09072   PostnetLoss: 0.00929   DecoderLoss:0.00969  StopLoss: 0.07174  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00094   Validation Loss: 0.00832\n",
      "\n",
      " > Epoch 706/1000\n",
      "   | > Step:5/68  GlobalStep:117720  TotalLoss:0.00139  PostnetLoss:0.00068  DecoderLoss:0.00072  StopLoss:0.06409  GradNorm:0.00370  GradNormST:0.02488  AvgTextLen:5.9  AvgSpecLen:33.2  StepTime:0.35  LR:0.000100\n",
      "   | > Step:15/68  GlobalStep:117730  TotalLoss:0.00160  PostnetLoss:0.00078  DecoderLoss:0.00082  StopLoss:0.06565  GradNorm:0.00360  GradNormST:0.04385  AvgTextLen:12.8  AvgSpecLen:59.1  StepTime:0.55  LR:0.000100\n",
      "   | > Step:25/68  GlobalStep:117740  TotalLoss:0.00177  PostnetLoss:0.00087  DecoderLoss:0.00091  StopLoss:0.04801  GradNorm:0.00376  GradNormST:0.01921  AvgTextLen:19.8  AvgSpecLen:73.3  StepTime:0.46  LR:0.000100\n",
      "   | > Step:35/68  GlobalStep:117750  TotalLoss:0.00196  PostnetLoss:0.00096  DecoderLoss:0.00100  StopLoss:0.07707  GradNorm:0.00320  GradNormST:0.04233  AvgTextLen:27.1  AvgSpecLen:104.7  StepTime:0.74  LR:0.000100\n",
      "   | > Step:45/68  GlobalStep:117760  TotalLoss:0.00203  PostnetLoss:0.00099  DecoderLoss:0.00104  StopLoss:0.06544  GradNorm:0.00316  GradNormST:0.01999  AvgTextLen:34.4  AvgSpecLen:118.6  StepTime:0.53  LR:0.000100\n",
      "   | > Step:55/68  GlobalStep:117770  TotalLoss:0.00227  PostnetLoss:0.00111  DecoderLoss:0.00117  StopLoss:0.04189  GradNorm:0.00342  GradNormST:0.01222  AvgTextLen:43.9  AvgSpecLen:145.2  StepTime:0.66  LR:0.000100\n",
      "   | > Step:65/68  GlobalStep:117780  TotalLoss:0.00251  PostnetLoss:0.00122  DecoderLoss:0.00129  StopLoss:0.03752  GradNorm:0.00281  GradNormST:0.01229  AvgTextLen:64.2  AvgSpecLen:207.4  StepTime:1.18  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:117783  AvgTotalLoss:0.05564  AvgPostnetLoss:0.00094  AvgDecoderLoss:0.00098  AvgStopLoss:0.05372  EpochTime:41.82  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.08898   PostnetLoss: 0.00565   DecoderLoss:0.00594  StopLoss: 0.07739  \n",
      "   | > TotalLoss: 0.09241   PostnetLoss: 0.00991   DecoderLoss:0.01031  StopLoss: 0.07219  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00094   Validation Loss: 0.00857\n",
      "\n",
      " > Epoch 707/1000\n",
      "   | > Step:6/68  GlobalStep:117790  TotalLoss:0.00135  PostnetLoss:0.00066  DecoderLoss:0.00069  StopLoss:0.03984  GradNorm:0.00363  GradNormST:0.01363  AvgTextLen:6.0  AvgSpecLen:34.8  StepTime:0.36  LR:0.000100\n",
      "   | > Step:16/68  GlobalStep:117800  TotalLoss:0.00163  PostnetLoss:0.00079  DecoderLoss:0.00083  StopLoss:0.04763  GradNorm:0.00346  GradNormST:0.01293  AvgTextLen:13.8  AvgSpecLen:60.6  StepTime:0.28  LR:0.000100\n",
      "   | > Step:26/68  GlobalStep:117810  TotalLoss:0.00181  PostnetLoss:0.00089  DecoderLoss:0.00093  StopLoss:0.04178  GradNorm:0.00370  GradNormST:0.01513  AvgTextLen:20.3  AvgSpecLen:77.5  StepTime:0.38  LR:0.000100\n",
      "   | > Step:36/68  GlobalStep:117820  TotalLoss:0.00192  PostnetLoss:0.00094  DecoderLoss:0.00099  StopLoss:0.05042  GradNorm:0.00305  GradNormST:0.01009  AvgTextLen:28.5  AvgSpecLen:100.2  StepTime:0.59  LR:0.000100\n",
      "   | > Step:46/68  GlobalStep:117830  TotalLoss:0.00209  PostnetLoss:0.00102  DecoderLoss:0.00107  StopLoss:0.03647  GradNorm:0.00325  GradNormST:0.01280  AvgTextLen:35.4  AvgSpecLen:122.6  StepTime:0.76  LR:0.000100\n",
      "   | > Step:56/68  GlobalStep:117840  TotalLoss:0.00225  PostnetLoss:0.00109  DecoderLoss:0.00115  StopLoss:0.03394  GradNorm:0.00337  GradNormST:0.01129  AvgTextLen:45.8  AvgSpecLen:145.4  StepTime:0.96  LR:0.000100\n",
      "   | > Step:66/68  GlobalStep:117850  TotalLoss:0.00267  PostnetLoss:0.00129  DecoderLoss:0.00137  StopLoss:0.04398  GradNorm:0.00323  GradNormST:0.01498  AvgTextLen:71.2  AvgSpecLen:220.8  StepTime:1.04  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:117852  AvgTotalLoss:0.05132  AvgPostnetLoss:0.00094  AvgDecoderLoss:0.00099  AvgStopLoss:0.04939  EpochTime:42.59  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.08321   PostnetLoss: 0.00563   DecoderLoss:0.00590  StopLoss: 0.07168  \n",
      "   | > TotalLoss: 0.09363   PostnetLoss: 0.00992   DecoderLoss:0.01029  StopLoss: 0.07342  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00094   Validation Loss: 0.00877\n",
      "\n",
      " > Epoch 708/1000\n",
      "   | > Step:7/68  GlobalStep:117860  TotalLoss:0.00146  PostnetLoss:0.00072  DecoderLoss:0.00075  StopLoss:0.04508  GradNorm:0.00354  GradNormST:0.02775  AvgTextLen:7.4  AvgSpecLen:44.6  StepTime:0.33  LR:0.000100\n",
      "   | > Step:17/68  GlobalStep:117870  TotalLoss:0.00159  PostnetLoss:0.00078  DecoderLoss:0.00081  StopLoss:0.05608  GradNorm:0.00348  GradNormST:0.01685  AvgTextLen:14.7  AvgSpecLen:66.0  StepTime:0.45  LR:0.000100\n",
      "   | > Step:27/68  GlobalStep:117880  TotalLoss:0.00184  PostnetLoss:0.00090  DecoderLoss:0.00094  StopLoss:0.06365  GradNorm:0.00370  GradNormST:0.01716  AvgTextLen:21.0  AvgSpecLen:80.2  StepTime:0.62  LR:0.000100\n",
      "   | > Step:37/68  GlobalStep:117890  TotalLoss:0.00202  PostnetLoss:0.00099  DecoderLoss:0.00103  StopLoss:0.04771  GradNorm:0.00408  GradNormST:0.01409  AvgTextLen:29.3  AvgSpecLen:102.4  StepTime:0.63  LR:0.000100\n",
      "   | > Step:47/68  GlobalStep:117900  TotalLoss:0.00217  PostnetLoss:0.00105  DecoderLoss:0.00111  StopLoss:0.04265  GradNorm:0.00343  GradNormST:0.01137  AvgTextLen:36.7  AvgSpecLen:126.9  StepTime:0.74  LR:0.000100\n",
      "   | > Step:57/68  GlobalStep:117910  TotalLoss:0.00234  PostnetLoss:0.00114  DecoderLoss:0.00120  StopLoss:0.03771  GradNorm:0.00323  GradNormST:0.00871  AvgTextLen:47.1  AvgSpecLen:161.0  StepTime:0.69  LR:0.000100\n",
      "   | > Step:67/68  GlobalStep:117920  TotalLoss:0.00265  PostnetLoss:0.00128  DecoderLoss:0.00137  StopLoss:0.04560  GradNorm:0.00329  GradNormST:0.01815  AvgTextLen:80.6  AvgSpecLen:246.5  StepTime:1.21  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:117921  AvgTotalLoss:0.05088  AvgPostnetLoss:0.00094  AvgDecoderLoss:0.00099  AvgStopLoss:0.04896  EpochTime:42.75  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.08098   PostnetLoss: 0.00569   DecoderLoss:0.00598  StopLoss: 0.06932  \n",
      "   | > TotalLoss: 0.09086   PostnetLoss: 0.00955   DecoderLoss:0.00995  StopLoss: 0.07135  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00094   Validation Loss: 0.00851\n",
      "\n",
      " > Epoch 709/1000\n",
      "   | > Step:8/68  GlobalStep:117930  TotalLoss:0.00149  PostnetLoss:0.00073  DecoderLoss:0.00076  StopLoss:0.05273  GradNorm:0.00506  GradNormST:0.02533  AvgTextLen:8.3  AvgSpecLen:44.0  StepTime:0.25  LR:0.000100\n",
      "   | > Step:18/68  GlobalStep:117940  TotalLoss:0.00158  PostnetLoss:0.00077  DecoderLoss:0.00081  StopLoss:0.04172  GradNorm:0.00335  GradNormST:0.01305  AvgTextLen:15.2  AvgSpecLen:64.5  StepTime:0.42  LR:0.000100\n",
      "   | > Step:28/68  GlobalStep:117950  TotalLoss:0.00181  PostnetLoss:0.00088  DecoderLoss:0.00092  StopLoss:0.05146  GradNorm:0.00344  GradNormST:0.01446  AvgTextLen:22.1  AvgSpecLen:85.4  StepTime:0.58  LR:0.000100\n",
      "   | > Step:38/68  GlobalStep:117960  TotalLoss:0.00197  PostnetLoss:0.00096  DecoderLoss:0.00101  StopLoss:0.04437  GradNorm:0.00345  GradNormST:0.01652  AvgTextLen:29.4  AvgSpecLen:104.1  StepTime:0.59  LR:0.000100\n",
      "   | > Step:48/68  GlobalStep:117970  TotalLoss:0.00214  PostnetLoss:0.00104  DecoderLoss:0.00110  StopLoss:0.03144  GradNorm:0.00321  GradNormST:0.00931  AvgTextLen:37.5  AvgSpecLen:123.8  StepTime:0.56  LR:0.000100\n",
      "   | > Step:58/68  GlobalStep:117980  TotalLoss:0.00234  PostnetLoss:0.00114  DecoderLoss:0.00120  StopLoss:0.05148  GradNorm:0.00367  GradNormST:0.01297  AvgTextLen:47.8  AvgSpecLen:158.0  StepTime:0.83  LR:0.000100\n",
      "   | > Step:68/68  GlobalStep:117990  TotalLoss:0.00265  PostnetLoss:0.00128  DecoderLoss:0.00136  StopLoss:0.03876  GradNorm:0.00359  GradNormST:0.01759  AvgTextLen:103.0  AvgSpecLen:308.9  StepTime:1.86  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:117990  AvgTotalLoss:0.04793  AvgPostnetLoss:0.00094  AvgDecoderLoss:0.00099  AvgStopLoss:0.04599  EpochTime:42.31  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.07457   PostnetLoss: 0.00552   DecoderLoss:0.00579  StopLoss: 0.06326  \n",
      "   | > TotalLoss: 0.09593   PostnetLoss: 0.01012   DecoderLoss:0.01050  StopLoss: 0.07531  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00094   Validation Loss: 0.00889\n",
      "\n",
      " > Epoch 710/1000\n",
      "   | > Step:9/68  GlobalStep:118000  TotalLoss:0.00143  PostnetLoss:0.00070  DecoderLoss:0.00073  StopLoss:0.03476  GradNorm:0.00390  GradNormST:0.01159  AvgTextLen:9.0  AvgSpecLen:42.1  StepTime:0.42  LR:0.000100\n",
      " | | > Checkpoint saving : /vm/TTS/models/cartman_models/mozilla-no-loc-fattn-stopnet-sigmoid-loss_masking-August-15-2019_11+26AM-de6aeb9/checkpoint_118000.pth.tar\n",
      "warning: audio amplitude out of range, auto clipped.\n",
      "   | > Step:19/68  GlobalStep:118010  TotalLoss:0.00161  PostnetLoss:0.00078  DecoderLoss:0.00082  StopLoss:0.06059  GradNorm:0.00397  GradNormST:0.01940  AvgTextLen:15.5  AvgSpecLen:63.2  StepTime:0.32  LR:0.000100\n",
      "   | > Step:29/68  GlobalStep:118020  TotalLoss:0.00181  PostnetLoss:0.00089  DecoderLoss:0.00093  StopLoss:0.05628  GradNorm:0.00377  GradNormST:0.01974  AvgTextLen:21.8  AvgSpecLen:83.4  StepTime:0.58  LR:0.000100\n",
      "   | > Step:39/68  GlobalStep:118030  TotalLoss:0.00204  PostnetLoss:0.00100  DecoderLoss:0.00104  StopLoss:0.03522  GradNorm:0.00404  GradNormST:0.01255  AvgTextLen:30.2  AvgSpecLen:110.4  StepTime:0.52  LR:0.000100\n",
      "   | > Step:49/68  GlobalStep:118040  TotalLoss:0.00215  PostnetLoss:0.00105  DecoderLoss:0.00110  StopLoss:0.03764  GradNorm:0.00370  GradNormST:0.01281  AvgTextLen:38.2  AvgSpecLen:127.7  StepTime:0.59  LR:0.000100\n",
      "   | > Step:59/68  GlobalStep:118050  TotalLoss:0.00232  PostnetLoss:0.00113  DecoderLoss:0.00119  StopLoss:0.05702  GradNorm:0.00339  GradNormST:0.02485  AvgTextLen:50.5  AvgSpecLen:155.9  StepTime:0.71  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:118059  AvgTotalLoss:0.04738  AvgPostnetLoss:0.00094  AvgDecoderLoss:0.00099  AvgStopLoss:0.04546  EpochTime:41.90  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.07734   PostnetLoss: 0.00564   DecoderLoss:0.00593  StopLoss: 0.06576  \n",
      "   | > TotalLoss: 0.08093   PostnetLoss: 0.00966   DecoderLoss:0.01006  StopLoss: 0.06121  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00094   Validation Loss: 0.00853\n",
      "\n",
      " > Epoch 711/1000\n",
      "   | > Step:0/68  GlobalStep:118060  TotalLoss:0.00147  PostnetLoss:0.00071  DecoderLoss:0.00076  StopLoss:0.07251  GradNorm:0.00833  GradNormST:0.02084  AvgTextLen:3.0  AvgSpecLen:25.3  StepTime:0.45  LR:0.000100\n",
      "   | > Step:10/68  GlobalStep:118070  TotalLoss:0.00147  PostnetLoss:0.00072  DecoderLoss:0.00075  StopLoss:0.03851  GradNorm:0.00357  GradNormST:0.01957  AvgTextLen:9.8  AvgSpecLen:48.6  StepTime:0.54  LR:0.000100\n",
      "   | > Step:20/68  GlobalStep:118080  TotalLoss:0.00167  PostnetLoss:0.00082  DecoderLoss:0.00086  StopLoss:0.03077  GradNorm:0.00407  GradNormST:0.01183  AvgTextLen:16.4  AvgSpecLen:64.8  StepTime:0.49  LR:0.000100\n",
      "   | > Step:30/68  GlobalStep:118090  TotalLoss:0.00187  PostnetLoss:0.00091  DecoderLoss:0.00096  StopLoss:0.05204  GradNorm:0.00399  GradNormST:0.01323  AvgTextLen:22.9  AvgSpecLen:85.8  StepTime:0.57  LR:0.000100\n",
      "   | > Step:40/68  GlobalStep:118100  TotalLoss:0.00259  PostnetLoss:0.00127  DecoderLoss:0.00132  StopLoss:0.04591  GradNorm:0.01004  GradNormST:0.01483  AvgTextLen:30.9  AvgSpecLen:105.3  StepTime:0.62  LR:0.000100\n",
      "   | > Step:50/68  GlobalStep:118110  TotalLoss:0.00228  PostnetLoss:0.00111  DecoderLoss:0.00117  StopLoss:0.03176  GradNorm:0.00412  GradNormST:0.01040  AvgTextLen:38.8  AvgSpecLen:132.0  StepTime:0.78  LR:0.000100\n",
      "   | > Step:60/68  GlobalStep:118120  TotalLoss:0.00243  PostnetLoss:0.00118  DecoderLoss:0.00125  StopLoss:0.04128  GradNorm:0.00343  GradNormST:0.01221  AvgTextLen:52.0  AvgSpecLen:172.7  StepTime:0.94  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:118128  AvgTotalLoss:0.04695  AvgPostnetLoss:0.00096  AvgDecoderLoss:0.00101  AvgStopLoss:0.04497  EpochTime:43.00  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.06827   PostnetLoss: 0.00515   DecoderLoss:0.00544  StopLoss: 0.05768  \n",
      "   | > TotalLoss: 0.08293   PostnetLoss: 0.00947   DecoderLoss:0.00985  StopLoss: 0.06360  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00096   Validation Loss: 0.00834\n",
      "\n",
      " > Epoch 712/1000\n",
      "   | > Step:1/68  GlobalStep:118130  TotalLoss:0.00140  PostnetLoss:0.00068  DecoderLoss:0.00072  StopLoss:0.04395  GradNorm:0.00644  GradNormST:0.02173  AvgTextLen:3.7  AvgSpecLen:28.6  StepTime:0.29  LR:0.000100\n",
      "   | > Step:11/68  GlobalStep:118140  TotalLoss:0.00149  PostnetLoss:0.00073  DecoderLoss:0.00076  StopLoss:0.03038  GradNorm:0.00393  GradNormST:0.00953  AvgTextLen:10.3  AvgSpecLen:49.4  StepTime:0.40  LR:0.000100\n",
      "   | > Step:21/68  GlobalStep:118150  TotalLoss:0.00172  PostnetLoss:0.00084  DecoderLoss:0.00088  StopLoss:0.04582  GradNorm:0.00376  GradNormST:0.01502  AvgTextLen:16.6  AvgSpecLen:66.3  StepTime:0.42  LR:0.000100\n",
      "   | > Step:31/68  GlobalStep:118160  TotalLoss:0.00187  PostnetLoss:0.00092  DecoderLoss:0.00096  StopLoss:0.04256  GradNorm:0.00448  GradNormST:0.01466  AvgTextLen:24.1  AvgSpecLen:82.7  StepTime:0.53  LR:0.000100\n",
      "   | > Step:41/68  GlobalStep:118170  TotalLoss:0.00218  PostnetLoss:0.00106  DecoderLoss:0.00112  StopLoss:0.02912  GradNorm:0.00516  GradNormST:0.01043  AvgTextLen:32.3  AvgSpecLen:109.4  StepTime:0.61  LR:0.000100\n",
      "   | > Step:51/68  GlobalStep:118180  TotalLoss:0.00251  PostnetLoss:0.00122  DecoderLoss:0.00129  StopLoss:0.04512  GradNorm:0.00654  GradNormST:0.01727  AvgTextLen:39.2  AvgSpecLen:126.9  StepTime:0.62  LR:0.000100\n",
      "   | > Step:61/68  GlobalStep:118190  TotalLoss:0.00277  PostnetLoss:0.00135  DecoderLoss:0.00142  StopLoss:0.03459  GradNorm:0.00506  GradNormST:0.00874  AvgTextLen:54.6  AvgSpecLen:165.5  StepTime:0.92  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:118197  AvgTotalLoss:0.04636  AvgPostnetLoss:0.00102  AvgDecoderLoss:0.00107  AvgStopLoss:0.04427  EpochTime:41.66  AvgStepTime:0.60\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.07833   PostnetLoss: 0.00536   DecoderLoss:0.00569  StopLoss: 0.06728  \n",
      "   | > TotalLoss: 0.08198   PostnetLoss: 0.00852   DecoderLoss:0.00896  StopLoss: 0.06450  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00102   Validation Loss: 0.00775\n",
      "\n",
      " > Epoch 713/1000\n",
      "   | > Step:2/68  GlobalStep:118200  TotalLoss:0.00171  PostnetLoss:0.00084  DecoderLoss:0.00088  StopLoss:0.04629  GradNorm:0.01036  GradNormST:0.01897  AvgTextLen:4.0  AvgSpecLen:31.3  StepTime:0.40  LR:0.000100\n",
      "   | > Step:12/68  GlobalStep:118210  TotalLoss:0.00177  PostnetLoss:0.00086  DecoderLoss:0.00090  StopLoss:0.05304  GradNorm:0.00516  GradNormST:0.02039  AvgTextLen:11.0  AvgSpecLen:49.3  StepTime:0.42  LR:0.000100\n",
      "   | > Step:22/68  GlobalStep:118220  TotalLoss:0.00183  PostnetLoss:0.00089  DecoderLoss:0.00094  StopLoss:0.04485  GradNorm:0.00422  GradNormST:0.01151  AvgTextLen:18.0  AvgSpecLen:72.0  StepTime:0.45  LR:0.000100\n",
      "   | > Step:32/68  GlobalStep:118230  TotalLoss:0.00203  PostnetLoss:0.00099  DecoderLoss:0.00104  StopLoss:0.05917  GradNorm:0.00485  GradNormST:0.02373  AvgTextLen:24.4  AvgSpecLen:91.5  StepTime:0.46  LR:0.000100\n",
      "   | > Step:42/68  GlobalStep:118240  TotalLoss:0.00225  PostnetLoss:0.00110  DecoderLoss:0.00116  StopLoss:0.05859  GradNorm:0.00513  GradNormST:0.01634  AvgTextLen:33.1  AvgSpecLen:109.7  StepTime:0.63  LR:0.000100\n",
      "   | > Step:52/68  GlobalStep:118250  TotalLoss:0.00256  PostnetLoss:0.00125  DecoderLoss:0.00131  StopLoss:0.05461  GradNorm:0.00677  GradNormST:0.01979  AvgTextLen:42.1  AvgSpecLen:137.5  StepTime:0.67  LR:0.000100\n",
      "   | > Step:62/68  GlobalStep:118260  TotalLoss:0.00277  PostnetLoss:0.00135  DecoderLoss:0.00142  StopLoss:0.03179  GradNorm:0.00554  GradNormST:0.00972  AvgTextLen:54.0  AvgSpecLen:174.7  StepTime:0.75  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:118266  AvgTotalLoss:0.04613  AvgPostnetLoss:0.00104  AvgDecoderLoss:0.00110  AvgStopLoss:0.04398  EpochTime:41.95  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.07243   PostnetLoss: 0.00511   DecoderLoss:0.00541  StopLoss: 0.06192  \n",
      "   | > TotalLoss: 0.07882   PostnetLoss: 0.00866   DecoderLoss:0.00904  StopLoss: 0.06112  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00104   Validation Loss: 0.00782\n",
      "\n",
      " > Epoch 714/1000\n",
      "   | > Step:3/68  GlobalStep:118270  TotalLoss:0.00145  PostnetLoss:0.00071  DecoderLoss:0.00074  StopLoss:0.07743  GradNorm:0.00521  GradNormST:0.02195  AvgTextLen:4.7  AvgSpecLen:29.7  StepTime:0.32  LR:0.000100\n",
      "   | > Step:13/68  GlobalStep:118280  TotalLoss:0.00159  PostnetLoss:0.00078  DecoderLoss:0.00081  StopLoss:0.05837  GradNorm:0.00376  GradNormST:0.02520  AvgTextLen:11.5  AvgSpecLen:55.2  StepTime:0.42  LR:0.000100\n",
      "   | > Step:23/68  GlobalStep:118290  TotalLoss:0.00191  PostnetLoss:0.00093  DecoderLoss:0.00098  StopLoss:0.04538  GradNorm:0.00442  GradNormST:0.01115  AvgTextLen:18.3  AvgSpecLen:72.2  StepTime:0.42  LR:0.000100\n",
      "   | > Step:33/68  GlobalStep:118300  TotalLoss:0.00210  PostnetLoss:0.00103  DecoderLoss:0.00107  StopLoss:0.04864  GradNorm:0.00581  GradNormST:0.01281  AvgTextLen:25.9  AvgSpecLen:88.9  StepTime:0.52  LR:0.000100\n",
      "   | > Step:43/68  GlobalStep:118310  TotalLoss:0.00226  PostnetLoss:0.00111  DecoderLoss:0.00116  StopLoss:0.04249  GradNorm:0.00571  GradNormST:0.00787  AvgTextLen:33.4  AvgSpecLen:115.7  StepTime:0.63  LR:0.000100\n",
      "   | > Step:53/68  GlobalStep:118320  TotalLoss:0.00238  PostnetLoss:0.00116  DecoderLoss:0.00122  StopLoss:0.03368  GradNorm:0.00663  GradNormST:0.00970  AvgTextLen:42.4  AvgSpecLen:137.6  StepTime:0.74  LR:0.000100\n",
      "   | > Step:63/68  GlobalStep:118330  TotalLoss:0.00275  PostnetLoss:0.00134  DecoderLoss:0.00141  StopLoss:0.02954  GradNorm:0.00753  GradNormST:0.01722  AvgTextLen:58.6  AvgSpecLen:185.1  StepTime:0.92  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:118335  AvgTotalLoss:0.04540  AvgPostnetLoss:0.00101  AvgDecoderLoss:0.00106  AvgStopLoss:0.04333  EpochTime:42.22  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.06905   PostnetLoss: 0.00524   DecoderLoss:0.00552  StopLoss: 0.05829  \n",
      "   | > TotalLoss: 0.07400   PostnetLoss: 0.00851   DecoderLoss:0.00889  StopLoss: 0.05660  \n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00101   Validation Loss: 0.00755\n",
      "\n",
      " > Epoch 715/1000\n",
      "   | > Step:4/68  GlobalStep:118340  TotalLoss:0.00138  PostnetLoss:0.00067  DecoderLoss:0.00071  StopLoss:0.07586  GradNorm:0.00484  GradNormST:0.02993  AvgTextLen:4.8  AvgSpecLen:31.1  StepTime:0.26  LR:0.000100\n",
      "   | > Step:14/68  GlobalStep:118350  TotalLoss:0.00161  PostnetLoss:0.00079  DecoderLoss:0.00083  StopLoss:0.04631  GradNorm:0.00343  GradNormST:0.01294  AvgTextLen:12.2  AvgSpecLen:52.0  StepTime:0.36  LR:0.000100\n",
      "   | > Step:24/68  GlobalStep:118360  TotalLoss:0.00190  PostnetLoss:0.00093  DecoderLoss:0.00098  StopLoss:0.03849  GradNorm:0.00426  GradNormST:0.00983  AvgTextLen:19.2  AvgSpecLen:79.0  StepTime:0.50  LR:0.000100\n",
      "   | > Step:34/68  GlobalStep:118370  TotalLoss:0.00206  PostnetLoss:0.00101  DecoderLoss:0.00105  StopLoss:0.03620  GradNorm:0.00452  GradNormST:0.00961  AvgTextLen:26.2  AvgSpecLen:95.9  StepTime:0.59  LR:0.000100\n",
      "   | > Step:44/68  GlobalStep:118380  TotalLoss:0.00224  PostnetLoss:0.00109  DecoderLoss:0.00114  StopLoss:0.04204  GradNorm:0.00573  GradNormST:0.02605  AvgTextLen:33.8  AvgSpecLen:116.3  StepTime:0.65  LR:0.000100\n",
      "   | > Step:54/68  GlobalStep:118390  TotalLoss:0.00245  PostnetLoss:0.00119  DecoderLoss:0.00126  StopLoss:0.03568  GradNorm:0.00691  GradNormST:0.01234  AvgTextLen:42.2  AvgSpecLen:144.9  StepTime:0.81  LR:0.000100\n",
      "   | > Step:64/68  GlobalStep:118400  TotalLoss:0.00254  PostnetLoss:0.00123  DecoderLoss:0.00130  StopLoss:0.03361  GradNorm:0.00658  GradNormST:0.01103  AvgTextLen:60.6  AvgSpecLen:177.6  StepTime:0.81  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:118404  AvgTotalLoss:0.04691  AvgPostnetLoss:0.00099  AvgDecoderLoss:0.00104  AvgStopLoss:0.04489  EpochTime:41.81  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.07150   PostnetLoss: 0.00535   DecoderLoss:0.00564  StopLoss: 0.06050  \n",
      "   | > TotalLoss: 0.07601   PostnetLoss: 0.00852   DecoderLoss:0.00890  StopLoss: 0.05858  \n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00099   Validation Loss: 0.00757\n",
      "\n",
      " > Epoch 716/1000\n",
      "   | > Step:5/68  GlobalStep:118410  TotalLoss:0.00136  PostnetLoss:0.00066  DecoderLoss:0.00070  StopLoss:0.04560  GradNorm:0.00356  GradNormST:0.01703  AvgTextLen:5.9  AvgSpecLen:33.2  StepTime:0.34  LR:0.000100\n",
      "   | > Step:15/68  GlobalStep:118420  TotalLoss:0.00161  PostnetLoss:0.00079  DecoderLoss:0.00082  StopLoss:0.03353  GradNorm:0.00327  GradNormST:0.01428  AvgTextLen:12.8  AvgSpecLen:59.1  StepTime:0.54  LR:0.000100\n",
      "   | > Step:25/68  GlobalStep:118430  TotalLoss:0.00181  PostnetLoss:0.00088  DecoderLoss:0.00093  StopLoss:0.04355  GradNorm:0.00358  GradNormST:0.01371  AvgTextLen:19.8  AvgSpecLen:73.3  StepTime:0.46  LR:0.000100\n",
      "   | > Step:35/68  GlobalStep:118440  TotalLoss:0.00207  PostnetLoss:0.00101  DecoderLoss:0.00106  StopLoss:0.04131  GradNorm:0.00483  GradNormST:0.01636  AvgTextLen:27.1  AvgSpecLen:104.7  StepTime:0.74  LR:0.000100\n",
      "   | > Step:45/68  GlobalStep:118450  TotalLoss:0.00213  PostnetLoss:0.00104  DecoderLoss:0.00109  StopLoss:0.05601  GradNorm:0.00425  GradNormST:0.01427  AvgTextLen:34.4  AvgSpecLen:118.6  StepTime:0.54  LR:0.000100\n",
      "   | > Step:55/68  GlobalStep:118460  TotalLoss:0.00245  PostnetLoss:0.00119  DecoderLoss:0.00125  StopLoss:0.03407  GradNorm:0.00686  GradNormST:0.01107  AvgTextLen:43.9  AvgSpecLen:145.2  StepTime:0.85  LR:0.000100\n",
      "   | > Step:65/68  GlobalStep:118470  TotalLoss:0.00275  PostnetLoss:0.00134  DecoderLoss:0.00141  StopLoss:0.04891  GradNorm:0.00668  GradNormST:0.01898  AvgTextLen:64.2  AvgSpecLen:207.4  StepTime:1.19  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:118473  AvgTotalLoss:0.04618  AvgPostnetLoss:0.00097  AvgDecoderLoss:0.00102  AvgStopLoss:0.04420  EpochTime:42.82  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.07139   PostnetLoss: 0.00532   DecoderLoss:0.00559  StopLoss: 0.06048  \n",
      "   | > TotalLoss: 0.07680   PostnetLoss: 0.00834   DecoderLoss:0.00872  StopLoss: 0.05974  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00097   Validation Loss: 0.00761\n",
      "\n",
      " > Epoch 717/1000\n",
      "   | > Step:6/68  GlobalStep:118480  TotalLoss:0.00141  PostnetLoss:0.00069  DecoderLoss:0.00072  StopLoss:0.04072  GradNorm:0.00368  GradNormST:0.02519  AvgTextLen:6.0  AvgSpecLen:34.8  StepTime:0.38  LR:0.000100\n",
      "   | > Step:16/68  GlobalStep:118490  TotalLoss:0.00166  PostnetLoss:0.00081  DecoderLoss:0.00085  StopLoss:0.03486  GradNorm:0.00339  GradNormST:0.01141  AvgTextLen:13.8  AvgSpecLen:60.6  StepTime:0.28  LR:0.000100\n",
      "   | > Step:26/68  GlobalStep:118500  TotalLoss:0.00182  PostnetLoss:0.00089  DecoderLoss:0.00093  StopLoss:0.04497  GradNorm:0.00322  GradNormST:0.01316  AvgTextLen:20.3  AvgSpecLen:77.5  StepTime:0.51  LR:0.000100\n",
      "   | > Step:36/68  GlobalStep:118510  TotalLoss:0.00201  PostnetLoss:0.00098  DecoderLoss:0.00103  StopLoss:0.05117  GradNorm:0.00352  GradNormST:0.01315  AvgTextLen:28.5  AvgSpecLen:100.2  StepTime:0.68  LR:0.000100\n",
      "   | > Step:46/68  GlobalStep:118520  TotalLoss:0.00217  PostnetLoss:0.00106  DecoderLoss:0.00111  StopLoss:0.05221  GradNorm:0.00477  GradNormST:0.03485  AvgTextLen:35.4  AvgSpecLen:122.6  StepTime:0.78  LR:0.000100\n",
      "   | > Step:56/68  GlobalStep:118530  TotalLoss:0.00241  PostnetLoss:0.00117  DecoderLoss:0.00123  StopLoss:0.03162  GradNorm:0.00586  GradNormST:0.00985  AvgTextLen:45.8  AvgSpecLen:145.4  StepTime:0.96  LR:0.000100\n",
      "   | > Step:66/68  GlobalStep:118540  TotalLoss:0.00281  PostnetLoss:0.00137  DecoderLoss:0.00144  StopLoss:0.03418  GradNorm:0.00671  GradNormST:0.00831  AvgTextLen:71.2  AvgSpecLen:220.8  StepTime:1.03  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:118542  AvgTotalLoss:0.04784  AvgPostnetLoss:0.00096  AvgDecoderLoss:0.00101  AvgStopLoss:0.04587  EpochTime:42.22  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.07271   PostnetLoss: 0.00536   DecoderLoss:0.00563  StopLoss: 0.06173  \n",
      "   | > TotalLoss: 0.07769   PostnetLoss: 0.00836   DecoderLoss:0.00874  StopLoss: 0.06059  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00096   Validation Loss: 0.00773\n",
      "\n",
      " > Epoch 718/1000\n",
      "   | > Step:7/68  GlobalStep:118550  TotalLoss:0.00144  PostnetLoss:0.00070  DecoderLoss:0.00074  StopLoss:0.03337  GradNorm:0.00386  GradNormST:0.02085  AvgTextLen:7.4  AvgSpecLen:44.6  StepTime:0.41  LR:0.000100\n",
      "   | > Step:17/68  GlobalStep:118560  TotalLoss:0.00166  PostnetLoss:0.00081  DecoderLoss:0.00085  StopLoss:0.04107  GradNorm:0.00337  GradNormST:0.01171  AvgTextLen:14.7  AvgSpecLen:66.0  StepTime:0.43  LR:0.000100\n",
      "   | > Step:27/68  GlobalStep:118570  TotalLoss:0.00185  PostnetLoss:0.00090  DecoderLoss:0.00095  StopLoss:0.05618  GradNorm:0.00319  GradNormST:0.01601  AvgTextLen:21.0  AvgSpecLen:80.2  StepTime:0.63  LR:0.000100\n",
      "   | > Step:37/68  GlobalStep:118580  TotalLoss:0.00203  PostnetLoss:0.00099  DecoderLoss:0.00104  StopLoss:0.05153  GradNorm:0.00363  GradNormST:0.01136  AvgTextLen:29.3  AvgSpecLen:102.4  StepTime:0.58  LR:0.000100\n",
      "   | > Step:47/68  GlobalStep:118590  TotalLoss:0.00224  PostnetLoss:0.00109  DecoderLoss:0.00115  StopLoss:0.04017  GradNorm:0.00533  GradNormST:0.01346  AvgTextLen:36.7  AvgSpecLen:126.9  StepTime:0.58  LR:0.000100\n",
      "   | > Step:57/68  GlobalStep:118600  TotalLoss:0.00243  PostnetLoss:0.00119  DecoderLoss:0.00125  StopLoss:0.03355  GradNorm:0.00533  GradNormST:0.01111  AvgTextLen:47.1  AvgSpecLen:161.0  StepTime:0.89  LR:0.000100\n",
      "   | > Step:67/68  GlobalStep:118610  TotalLoss:0.00277  PostnetLoss:0.00134  DecoderLoss:0.00143  StopLoss:0.03402  GradNorm:0.00470  GradNormST:0.01130  AvgTextLen:80.6  AvgSpecLen:246.5  StepTime:1.23  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:118611  AvgTotalLoss:0.04561  AvgPostnetLoss:0.00095  AvgDecoderLoss:0.00100  AvgStopLoss:0.04365  EpochTime:41.22  AvgStepTime:0.60\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.06911   PostnetLoss: 0.00539   DecoderLoss:0.00567  StopLoss: 0.05805  \n",
      "   | > TotalLoss: 0.07642   PostnetLoss: 0.00834   DecoderLoss:0.00871  StopLoss: 0.05936  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00095   Validation Loss: 0.00772\n",
      "\n",
      " > Epoch 719/1000\n",
      "   | > Step:8/68  GlobalStep:118620  TotalLoss:0.00148  PostnetLoss:0.00072  DecoderLoss:0.00076  StopLoss:0.04743  GradNorm:0.00360  GradNormST:0.01320  AvgTextLen:8.3  AvgSpecLen:44.0  StepTime:0.27  LR:0.000100\n",
      "   | > Step:18/68  GlobalStep:118630  TotalLoss:0.00162  PostnetLoss:0.00079  DecoderLoss:0.00083  StopLoss:0.04078  GradNorm:0.00326  GradNormST:0.01013  AvgTextLen:15.2  AvgSpecLen:64.5  StepTime:0.42  LR:0.000100\n",
      "   | > Step:28/68  GlobalStep:118640  TotalLoss:0.00187  PostnetLoss:0.00091  DecoderLoss:0.00096  StopLoss:0.03691  GradNorm:0.00322  GradNormST:0.01116  AvgTextLen:22.1  AvgSpecLen:85.4  StepTime:0.44  LR:0.000100\n",
      "   | > Step:38/68  GlobalStep:118650  TotalLoss:0.00201  PostnetLoss:0.00098  DecoderLoss:0.00103  StopLoss:0.03979  GradNorm:0.00387  GradNormST:0.00937  AvgTextLen:29.4  AvgSpecLen:104.1  StepTime:0.62  LR:0.000100\n",
      "   | > Step:48/68  GlobalStep:118660  TotalLoss:0.00218  PostnetLoss:0.00106  DecoderLoss:0.00112  StopLoss:0.04371  GradNorm:0.00396  GradNormST:0.02274  AvgTextLen:37.5  AvgSpecLen:123.8  StepTime:0.72  LR:0.000100\n",
      "   | > Step:58/68  GlobalStep:118670  TotalLoss:0.00240  PostnetLoss:0.00117  DecoderLoss:0.00123  StopLoss:0.04651  GradNorm:0.00508  GradNormST:0.01191  AvgTextLen:47.8  AvgSpecLen:158.0  StepTime:0.87  LR:0.000100\n",
      "   | > Step:68/68  GlobalStep:118680  TotalLoss:0.00268  PostnetLoss:0.00129  DecoderLoss:0.00138  StopLoss:0.02635  GradNorm:0.00895  GradNormST:0.01349  AvgTextLen:103.0  AvgSpecLen:308.9  StepTime:1.87  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:118680  AvgTotalLoss:0.04555  AvgPostnetLoss:0.00095  AvgDecoderLoss:0.00100  AvgStopLoss:0.04360  EpochTime:41.74  AvgStepTime:0.60\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.07333   PostnetLoss: 0.00513   DecoderLoss:0.00539  StopLoss: 0.06280  \n",
      "   | > TotalLoss: 0.08103   PostnetLoss: 0.00852   DecoderLoss:0.00890  StopLoss: 0.06361  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00095   Validation Loss: 0.00777\n",
      "\n",
      " > Epoch 720/1000\n",
      "   | > Step:9/68  GlobalStep:118690  TotalLoss:0.00148  PostnetLoss:0.00073  DecoderLoss:0.00076  StopLoss:0.04412  GradNorm:0.00463  GradNormST:0.01860  AvgTextLen:9.0  AvgSpecLen:42.1  StepTime:0.41  LR:0.000100\n",
      "   | > Step:19/68  GlobalStep:118700  TotalLoss:0.00166  PostnetLoss:0.00081  DecoderLoss:0.00085  StopLoss:0.04873  GradNorm:0.00347  GradNormST:0.02073  AvgTextLen:15.5  AvgSpecLen:63.2  StepTime:0.43  LR:0.000100\n",
      "   | > Step:29/68  GlobalStep:118710  TotalLoss:0.00182  PostnetLoss:0.00089  DecoderLoss:0.00093  StopLoss:0.05549  GradNorm:0.00313  GradNormST:0.01814  AvgTextLen:21.8  AvgSpecLen:83.4  StepTime:0.56  LR:0.000100\n",
      "   | > Step:39/68  GlobalStep:118720  TotalLoss:0.00204  PostnetLoss:0.00099  DecoderLoss:0.00105  StopLoss:0.03828  GradNorm:0.00336  GradNormST:0.01825  AvgTextLen:30.2  AvgSpecLen:110.4  StepTime:0.57  LR:0.000100\n",
      "   | > Step:49/68  GlobalStep:118730  TotalLoss:0.00222  PostnetLoss:0.00108  DecoderLoss:0.00114  StopLoss:0.05013  GradNorm:0.00402  GradNormST:0.02788  AvgTextLen:38.2  AvgSpecLen:127.7  StepTime:0.77  LR:0.000100\n",
      "   | > Step:59/68  GlobalStep:118740  TotalLoss:0.00236  PostnetLoss:0.00115  DecoderLoss:0.00121  StopLoss:0.05320  GradNorm:0.00392  GradNormST:0.02079  AvgTextLen:50.5  AvgSpecLen:155.9  StepTime:0.92  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:118749  AvgTotalLoss:0.04642  AvgPostnetLoss:0.00095  AvgDecoderLoss:0.00100  AvgStopLoss:0.04448  EpochTime:42.10  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.07501   PostnetLoss: 0.00516   DecoderLoss:0.00544  StopLoss: 0.06442  \n",
      "   | > TotalLoss: 0.08253   PostnetLoss: 0.00870   DecoderLoss:0.00908  StopLoss: 0.06475  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00095   Validation Loss: 0.00775\n",
      "\n",
      " > Epoch 721/1000\n",
      "   | > Step:0/68  GlobalStep:118750  TotalLoss:0.00143  PostnetLoss:0.00069  DecoderLoss:0.00074  StopLoss:0.05494  GradNorm:0.00756  GradNormST:0.03202  AvgTextLen:3.0  AvgSpecLen:25.3  StepTime:0.51  LR:0.000100\n",
      "   | > Step:10/68  GlobalStep:118760  TotalLoss:0.00155  PostnetLoss:0.00076  DecoderLoss:0.00080  StopLoss:0.03676  GradNorm:0.00446  GradNormST:0.02166  AvgTextLen:9.8  AvgSpecLen:48.6  StepTime:0.54  LR:0.000100\n",
      "   | > Step:20/68  GlobalStep:118770  TotalLoss:0.00175  PostnetLoss:0.00086  DecoderLoss:0.00090  StopLoss:0.02620  GradNorm:0.00363  GradNormST:0.00733  AvgTextLen:16.4  AvgSpecLen:64.8  StepTime:0.40  LR:0.000100\n",
      "   | > Step:30/68  GlobalStep:118780  TotalLoss:0.00185  PostnetLoss:0.00090  DecoderLoss:0.00094  StopLoss:0.05249  GradNorm:0.00298  GradNormST:0.01100  AvgTextLen:22.9  AvgSpecLen:85.8  StepTime:0.57  LR:0.000100\n",
      "   | > Step:40/68  GlobalStep:118790  TotalLoss:0.00204  PostnetLoss:0.00100  DecoderLoss:0.00104  StopLoss:0.05091  GradNorm:0.00394  GradNormST:0.01247  AvgTextLen:30.9  AvgSpecLen:105.3  StepTime:0.61  LR:0.000100\n",
      "   | > Step:50/68  GlobalStep:118800  TotalLoss:0.00220  PostnetLoss:0.00107  DecoderLoss:0.00113  StopLoss:0.04101  GradNorm:0.00412  GradNormST:0.02258  AvgTextLen:38.8  AvgSpecLen:132.0  StepTime:0.79  LR:0.000100\n",
      "   | > Step:60/68  GlobalStep:118810  TotalLoss:0.00241  PostnetLoss:0.00117  DecoderLoss:0.00124  StopLoss:0.03327  GradNorm:0.00486  GradNormST:0.01233  AvgTextLen:52.0  AvgSpecLen:172.7  StepTime:1.02  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:118818  AvgTotalLoss:0.04557  AvgPostnetLoss:0.00095  AvgDecoderLoss:0.00099  AvgStopLoss:0.04363  EpochTime:43.00  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.07374   PostnetLoss: 0.00539   DecoderLoss:0.00566  StopLoss: 0.06270  \n",
      "   | > TotalLoss: 0.08161   PostnetLoss: 0.00851   DecoderLoss:0.00886  StopLoss: 0.06423  \n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00095   Validation Loss: 0.00769\n",
      "\n",
      " > Epoch 722/1000\n",
      "   | > Step:1/68  GlobalStep:118820  TotalLoss:0.00139  PostnetLoss:0.00068  DecoderLoss:0.00071  StopLoss:0.05416  GradNorm:0.00480  GradNormST:0.01925  AvgTextLen:3.7  AvgSpecLen:28.6  StepTime:0.25  LR:0.000100\n",
      "   | > Step:11/68  GlobalStep:118830  TotalLoss:0.00154  PostnetLoss:0.00075  DecoderLoss:0.00078  StopLoss:0.05303  GradNorm:0.00561  GradNormST:0.02176  AvgTextLen:10.3  AvgSpecLen:49.4  StepTime:0.39  LR:0.000100\n",
      "   | > Step:21/68  GlobalStep:118840  TotalLoss:0.00169  PostnetLoss:0.00083  DecoderLoss:0.00086  StopLoss:0.03882  GradNorm:0.00357  GradNormST:0.01169  AvgTextLen:16.6  AvgSpecLen:66.3  StepTime:0.44  LR:0.000100\n",
      "   | > Step:31/68  GlobalStep:118850  TotalLoss:0.00186  PostnetLoss:0.00091  DecoderLoss:0.00095  StopLoss:0.04695  GradNorm:0.00348  GradNormST:0.02144  AvgTextLen:24.1  AvgSpecLen:82.7  StepTime:0.54  LR:0.000100\n",
      "   | > Step:41/68  GlobalStep:118860  TotalLoss:0.00205  PostnetLoss:0.00100  DecoderLoss:0.00105  StopLoss:0.03086  GradNorm:0.00301  GradNormST:0.01481  AvgTextLen:32.3  AvgSpecLen:109.4  StepTime:0.62  LR:0.000100\n",
      "   | > Step:51/68  GlobalStep:118870  TotalLoss:0.00215  PostnetLoss:0.00105  DecoderLoss:0.00110  StopLoss:0.04609  GradNorm:0.00348  GradNormST:0.02063  AvgTextLen:39.2  AvgSpecLen:126.9  StepTime:0.64  LR:0.000100\n",
      "   | > Step:61/68  GlobalStep:118880  TotalLoss:0.00236  PostnetLoss:0.00115  DecoderLoss:0.00121  StopLoss:0.03748  GradNorm:0.00414  GradNormST:0.01917  AvgTextLen:54.6  AvgSpecLen:165.5  StepTime:0.89  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:118887  AvgTotalLoss:0.04831  AvgPostnetLoss:0.00094  AvgDecoderLoss:0.00099  AvgStopLoss:0.04638  EpochTime:42.51  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.07746   PostnetLoss: 0.00536   DecoderLoss:0.00562  StopLoss: 0.06648  \n",
      "   | > TotalLoss: 0.08410   PostnetLoss: 0.00857   DecoderLoss:0.00894  StopLoss: 0.06659  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00094   Validation Loss: 0.00776\n",
      "\n",
      " > Epoch 723/1000\n",
      "   | > Step:2/68  GlobalStep:118890  TotalLoss:0.00127  PostnetLoss:0.00062  DecoderLoss:0.00065  StopLoss:0.03221  GradNorm:0.00374  GradNormST:0.01329  AvgTextLen:4.0  AvgSpecLen:31.3  StepTime:0.35  LR:0.000100\n",
      "   | > Step:12/68  GlobalStep:118900  TotalLoss:0.00149  PostnetLoss:0.00073  DecoderLoss:0.00076  StopLoss:0.05080  GradNorm:0.00393  GradNormST:0.01424  AvgTextLen:11.0  AvgSpecLen:49.3  StepTime:0.34  LR:0.000100\n",
      "   | > Step:22/68  GlobalStep:118910  TotalLoss:0.00171  PostnetLoss:0.00084  DecoderLoss:0.00088  StopLoss:0.04725  GradNorm:0.00322  GradNormST:0.01018  AvgTextLen:18.0  AvgSpecLen:72.0  StepTime:0.47  LR:0.000100\n",
      "   | > Step:32/68  GlobalStep:118920  TotalLoss:0.00187  PostnetLoss:0.00091  DecoderLoss:0.00096  StopLoss:0.05918  GradNorm:0.00302  GradNormST:0.01837  AvgTextLen:24.4  AvgSpecLen:91.5  StepTime:0.54  LR:0.000100\n",
      "   | > Step:42/68  GlobalStep:118930  TotalLoss:0.00203  PostnetLoss:0.00099  DecoderLoss:0.00104  StopLoss:0.05328  GradNorm:0.00278  GradNormST:0.01281  AvgTextLen:33.1  AvgSpecLen:109.7  StepTime:0.60  LR:0.000100\n",
      "   | > Step:52/68  GlobalStep:118940  TotalLoss:0.00223  PostnetLoss:0.00109  DecoderLoss:0.00115  StopLoss:0.05715  GradNorm:0.00359  GradNormST:0.02127  AvgTextLen:42.1  AvgSpecLen:137.5  StepTime:0.70  LR:0.000100\n",
      "   | > Step:62/68  GlobalStep:118950  TotalLoss:0.00242  PostnetLoss:0.00118  DecoderLoss:0.00124  StopLoss:0.03622  GradNorm:0.00381  GradNormST:0.01219  AvgTextLen:54.0  AvgSpecLen:174.7  StepTime:0.76  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:118956  AvgTotalLoss:0.04554  AvgPostnetLoss:0.00093  AvgDecoderLoss:0.00098  AvgStopLoss:0.04364  EpochTime:41.39  AvgStepTime:0.60\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.07710   PostnetLoss: 0.00526   DecoderLoss:0.00553  StopLoss: 0.06631  \n",
      "   | > TotalLoss: 0.09001   PostnetLoss: 0.00901   DecoderLoss:0.00938  StopLoss: 0.07162  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00093   Validation Loss: 0.00800\n",
      "\n",
      " > Epoch 724/1000\n",
      "   | > Step:3/68  GlobalStep:118960  TotalLoss:0.00127  PostnetLoss:0.00062  DecoderLoss:0.00065  StopLoss:0.07662  GradNorm:0.00366  GradNormST:0.03369  AvgTextLen:4.7  AvgSpecLen:29.7  StepTime:0.32  LR:0.000100\n",
      "   | > Step:13/68  GlobalStep:118970  TotalLoss:0.00149  PostnetLoss:0.00073  DecoderLoss:0.00076  StopLoss:0.06211  GradNorm:0.00418  GradNormST:0.02052  AvgTextLen:11.5  AvgSpecLen:55.2  StepTime:0.44  LR:0.000100\n",
      "   | > Step:23/68  GlobalStep:118980  TotalLoss:0.00174  PostnetLoss:0.00085  DecoderLoss:0.00089  StopLoss:0.03909  GradNorm:0.00322  GradNormST:0.00844  AvgTextLen:18.3  AvgSpecLen:72.2  StepTime:0.38  LR:0.000100\n",
      "   | > Step:33/68  GlobalStep:118990  TotalLoss:0.00188  PostnetLoss:0.00092  DecoderLoss:0.00096  StopLoss:0.05203  GradNorm:0.00306  GradNormST:0.01565  AvgTextLen:25.9  AvgSpecLen:88.9  StepTime:0.51  LR:0.000100\n",
      "   | > Step:43/68  GlobalStep:119000  TotalLoss:0.00203  PostnetLoss:0.00099  DecoderLoss:0.00104  StopLoss:0.04804  GradNorm:0.00270  GradNormST:0.01192  AvgTextLen:33.4  AvgSpecLen:115.7  StepTime:0.51  LR:0.000100\n",
      " | | > Checkpoint saving : /vm/TTS/models/cartman_models/mozilla-no-loc-fattn-stopnet-sigmoid-loss_masking-August-15-2019_11+26AM-de6aeb9/checkpoint_119000.pth.tar\n",
      "   | > Step:53/68  GlobalStep:119010  TotalLoss:0.00217  PostnetLoss:0.00105  DecoderLoss:0.00111  StopLoss:0.04161  GradNorm:0.00308  GradNormST:0.01446  AvgTextLen:42.4  AvgSpecLen:137.6  StepTime:0.77  LR:0.000100\n",
      "   | > Step:63/68  GlobalStep:119020  TotalLoss:0.00246  PostnetLoss:0.00119  DecoderLoss:0.00126  StopLoss:0.02870  GradNorm:0.00284  GradNormST:0.01355  AvgTextLen:58.6  AvgSpecLen:185.1  StepTime:0.90  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:119025  AvgTotalLoss:0.04546  AvgPostnetLoss:0.00093  AvgDecoderLoss:0.00098  AvgStopLoss:0.04356  EpochTime:42.94  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.07695   PostnetLoss: 0.00528   DecoderLoss:0.00556  StopLoss: 0.06611  \n",
      "   | > TotalLoss: 0.09078   PostnetLoss: 0.00915   DecoderLoss:0.00952  StopLoss: 0.07211  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00093   Validation Loss: 0.00805\n",
      "\n",
      " > Epoch 725/1000\n",
      "   | > Step:4/68  GlobalStep:119030  TotalLoss:0.00127  PostnetLoss:0.00062  DecoderLoss:0.00065  StopLoss:0.05882  GradNorm:0.00350  GradNormST:0.02762  AvgTextLen:4.8  AvgSpecLen:31.1  StepTime:0.30  LR:0.000100\n",
      "   | > Step:14/68  GlobalStep:119040  TotalLoss:0.00157  PostnetLoss:0.00076  DecoderLoss:0.00080  StopLoss:0.04892  GradNorm:0.00393  GradNormST:0.01201  AvgTextLen:12.2  AvgSpecLen:52.0  StepTime:0.33  LR:0.000100\n",
      "   | > Step:24/68  GlobalStep:119050  TotalLoss:0.00181  PostnetLoss:0.00088  DecoderLoss:0.00093  StopLoss:0.03843  GradNorm:0.00342  GradNormST:0.01054  AvgTextLen:19.2  AvgSpecLen:79.0  StepTime:0.52  LR:0.000100\n",
      "   | > Step:34/68  GlobalStep:119060  TotalLoss:0.00194  PostnetLoss:0.00094  DecoderLoss:0.00099  StopLoss:0.03615  GradNorm:0.00308  GradNormST:0.00965  AvgTextLen:26.2  AvgSpecLen:95.9  StepTime:0.62  LR:0.000100\n",
      "   | > Step:44/68  GlobalStep:119070  TotalLoss:0.00208  PostnetLoss:0.00101  DecoderLoss:0.00107  StopLoss:0.03326  GradNorm:0.00287  GradNormST:0.02009  AvgTextLen:33.8  AvgSpecLen:116.3  StepTime:0.60  LR:0.000100\n",
      "   | > Step:54/68  GlobalStep:119080  TotalLoss:0.00223  PostnetLoss:0.00108  DecoderLoss:0.00115  StopLoss:0.03400  GradNorm:0.00291  GradNormST:0.00912  AvgTextLen:42.2  AvgSpecLen:144.9  StepTime:0.81  LR:0.000100\n",
      "   | > Step:64/68  GlobalStep:119090  TotalLoss:0.00232  PostnetLoss:0.00113  DecoderLoss:0.00120  StopLoss:0.03100  GradNorm:0.00263  GradNormST:0.00748  AvgTextLen:60.6  AvgSpecLen:177.6  StepTime:0.80  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:119094  AvgTotalLoss:0.04452  AvgPostnetLoss:0.00093  AvgDecoderLoss:0.00098  AvgStopLoss:0.04262  EpochTime:42.74  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.07653   PostnetLoss: 0.00527   DecoderLoss:0.00555  StopLoss: 0.06571  \n",
      "   | > TotalLoss: 0.08807   PostnetLoss: 0.00930   DecoderLoss:0.00967  StopLoss: 0.06910  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00093   Validation Loss: 0.00813\n",
      "\n",
      " > Epoch 726/1000\n",
      "   | > Step:5/68  GlobalStep:119100  TotalLoss:0.00133  PostnetLoss:0.00065  DecoderLoss:0.00069  StopLoss:0.04887  GradNorm:0.00358  GradNormST:0.01945  AvgTextLen:5.9  AvgSpecLen:33.2  StepTime:0.36  LR:0.000100\n",
      "   | > Step:15/68  GlobalStep:119110  TotalLoss:0.00159  PostnetLoss:0.00078  DecoderLoss:0.00081  StopLoss:0.03246  GradNorm:0.00369  GradNormST:0.01230  AvgTextLen:12.8  AvgSpecLen:59.1  StepTime:0.51  LR:0.000100\n",
      "   | > Step:25/68  GlobalStep:119120  TotalLoss:0.00171  PostnetLoss:0.00083  DecoderLoss:0.00087  StopLoss:0.04229  GradNorm:0.00339  GradNormST:0.01205  AvgTextLen:19.8  AvgSpecLen:73.3  StepTime:0.48  LR:0.000100\n",
      "   | > Step:35/68  GlobalStep:119130  TotalLoss:0.00196  PostnetLoss:0.00095  DecoderLoss:0.00100  StopLoss:0.04135  GradNorm:0.00294  GradNormST:0.01376  AvgTextLen:27.1  AvgSpecLen:104.7  StepTime:0.81  LR:0.000100\n",
      "   | > Step:45/68  GlobalStep:119140  TotalLoss:0.00205  PostnetLoss:0.00099  DecoderLoss:0.00105  StopLoss:0.04914  GradNorm:0.00298  GradNormST:0.01217  AvgTextLen:34.4  AvgSpecLen:118.6  StepTime:0.64  LR:0.000100\n",
      "   | > Step:55/68  GlobalStep:119150  TotalLoss:0.00225  PostnetLoss:0.00109  DecoderLoss:0.00115  StopLoss:0.04065  GradNorm:0.00272  GradNormST:0.01326  AvgTextLen:43.9  AvgSpecLen:145.2  StepTime:0.80  LR:0.000100\n",
      "   | > Step:65/68  GlobalStep:119160  TotalLoss:0.00247  PostnetLoss:0.00120  DecoderLoss:0.00127  StopLoss:0.04112  GradNorm:0.00254  GradNormST:0.01187  AvgTextLen:64.2  AvgSpecLen:207.4  StepTime:1.18  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:119163  AvgTotalLoss:0.04436  AvgPostnetLoss:0.00092  AvgDecoderLoss:0.00097  AvgStopLoss:0.04247  EpochTime:42.57  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.07788   PostnetLoss: 0.00538   DecoderLoss:0.00565  StopLoss: 0.06684  \n",
      "   | > TotalLoss: 0.09025   PostnetLoss: 0.00928   DecoderLoss:0.00966  StopLoss: 0.07132  \n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00092   Validation Loss: 0.00809\n",
      "\n",
      " > Epoch 727/1000\n",
      "   | > Step:6/68  GlobalStep:119170  TotalLoss:0.00132  PostnetLoss:0.00065  DecoderLoss:0.00068  StopLoss:0.03127  GradNorm:0.00346  GradNormST:0.01018  AvgTextLen:6.0  AvgSpecLen:34.8  StepTime:0.37  LR:0.000100\n",
      "   | > Step:16/68  GlobalStep:119180  TotalLoss:0.00163  PostnetLoss:0.00079  DecoderLoss:0.00083  StopLoss:0.03404  GradNorm:0.00355  GradNormST:0.01149  AvgTextLen:13.8  AvgSpecLen:60.6  StepTime:0.29  LR:0.000100\n",
      "   | > Step:26/68  GlobalStep:119190  TotalLoss:0.00178  PostnetLoss:0.00087  DecoderLoss:0.00091  StopLoss:0.03581  GradNorm:0.00340  GradNormST:0.00811  AvgTextLen:20.3  AvgSpecLen:77.5  StepTime:0.50  LR:0.000100\n",
      "   | > Step:36/68  GlobalStep:119200  TotalLoss:0.00192  PostnetLoss:0.00094  DecoderLoss:0.00099  StopLoss:0.04423  GradNorm:0.00302  GradNormST:0.01432  AvgTextLen:28.5  AvgSpecLen:100.2  StepTime:0.58  LR:0.000100\n",
      "   | > Step:46/68  GlobalStep:119210  TotalLoss:0.00207  PostnetLoss:0.00101  DecoderLoss:0.00106  StopLoss:0.04072  GradNorm:0.00330  GradNormST:0.02378  AvgTextLen:35.4  AvgSpecLen:122.6  StepTime:0.80  LR:0.000100\n",
      "   | > Step:56/68  GlobalStep:119220  TotalLoss:0.00222  PostnetLoss:0.00108  DecoderLoss:0.00114  StopLoss:0.03943  GradNorm:0.00267  GradNormST:0.02118  AvgTextLen:45.8  AvgSpecLen:145.4  StepTime:0.83  LR:0.000100\n",
      "   | > Step:66/68  GlobalStep:119230  TotalLoss:0.00262  PostnetLoss:0.00127  DecoderLoss:0.00135  StopLoss:0.03436  GradNorm:0.00256  GradNormST:0.01440  AvgTextLen:71.2  AvgSpecLen:220.8  StepTime:1.04  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:119232  AvgTotalLoss:0.04389  AvgPostnetLoss:0.00092  AvgDecoderLoss:0.00097  AvgStopLoss:0.04200  EpochTime:42.12  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.07968   PostnetLoss: 0.00545   DecoderLoss:0.00573  StopLoss: 0.06849  \n",
      "   | > TotalLoss: 0.09298   PostnetLoss: 0.00952   DecoderLoss:0.00991  StopLoss: 0.07355  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00092   Validation Loss: 0.00829\n",
      "\n",
      " > Epoch 728/1000\n",
      "   | > Step:7/68  GlobalStep:119240  TotalLoss:0.00140  PostnetLoss:0.00069  DecoderLoss:0.00072  StopLoss:0.02312  GradNorm:0.00342  GradNormST:0.00878  AvgTextLen:7.4  AvgSpecLen:44.6  StepTime:0.30  LR:0.000100\n",
      "   | > Step:17/68  GlobalStep:119250  TotalLoss:0.00157  PostnetLoss:0.00077  DecoderLoss:0.00080  StopLoss:0.04502  GradNorm:0.00332  GradNormST:0.01037  AvgTextLen:14.7  AvgSpecLen:66.0  StepTime:0.40  LR:0.000100\n",
      "   | > Step:27/68  GlobalStep:119260  TotalLoss:0.00179  PostnetLoss:0.00088  DecoderLoss:0.00092  StopLoss:0.05765  GradNorm:0.00309  GradNormST:0.01808  AvgTextLen:21.0  AvgSpecLen:80.2  StepTime:0.62  LR:0.000100\n",
      "   | > Step:37/68  GlobalStep:119270  TotalLoss:0.00196  PostnetLoss:0.00095  DecoderLoss:0.00100  StopLoss:0.05107  GradNorm:0.00316  GradNormST:0.01750  AvgTextLen:29.3  AvgSpecLen:102.4  StepTime:0.57  LR:0.000100\n",
      "   | > Step:47/68  GlobalStep:119280  TotalLoss:0.00214  PostnetLoss:0.00104  DecoderLoss:0.00109  StopLoss:0.03668  GradNorm:0.00389  GradNormST:0.00878  AvgTextLen:36.7  AvgSpecLen:126.9  StepTime:0.74  LR:0.000100\n",
      "   | > Step:57/68  GlobalStep:119290  TotalLoss:0.00230  PostnetLoss:0.00112  DecoderLoss:0.00118  StopLoss:0.02980  GradNorm:0.00304  GradNormST:0.01186  AvgTextLen:47.1  AvgSpecLen:161.0  StepTime:0.69  LR:0.000100\n",
      "   | > Step:67/68  GlobalStep:119300  TotalLoss:0.00261  PostnetLoss:0.00126  DecoderLoss:0.00134  StopLoss:0.03590  GradNorm:0.00270  GradNormST:0.01462  AvgTextLen:80.6  AvgSpecLen:246.5  StepTime:1.22  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:119301  AvgTotalLoss:0.04338  AvgPostnetLoss:0.00092  AvgDecoderLoss:0.00097  AvgStopLoss:0.04149  EpochTime:42.32  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.07609   PostnetLoss: 0.00542   DecoderLoss:0.00570  StopLoss: 0.06498  \n",
      "   | > TotalLoss: 0.08941   PostnetLoss: 0.00941   DecoderLoss:0.00979  StopLoss: 0.07021  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00092   Validation Loss: 0.00828\n",
      "\n",
      " > Epoch 729/1000\n",
      "   | > Step:8/68  GlobalStep:119310  TotalLoss:0.00143  PostnetLoss:0.00070  DecoderLoss:0.00073  StopLoss:0.05149  GradNorm:0.00390  GradNormST:0.02437  AvgTextLen:8.3  AvgSpecLen:44.0  StepTime:0.30  LR:0.000100\n",
      "   | > Step:18/68  GlobalStep:119320  TotalLoss:0.00161  PostnetLoss:0.00078  DecoderLoss:0.00082  StopLoss:0.04812  GradNorm:0.00312  GradNormST:0.00981  AvgTextLen:15.2  AvgSpecLen:64.5  StepTime:0.43  LR:0.000100\n",
      "   | > Step:28/68  GlobalStep:119330  TotalLoss:0.00178  PostnetLoss:0.00087  DecoderLoss:0.00091  StopLoss:0.04309  GradNorm:0.00310  GradNormST:0.01189  AvgTextLen:22.1  AvgSpecLen:85.4  StepTime:0.46  LR:0.000100\n",
      "   | > Step:38/68  GlobalStep:119340  TotalLoss:0.00194  PostnetLoss:0.00095  DecoderLoss:0.00100  StopLoss:0.03838  GradNorm:0.00362  GradNormST:0.01152  AvgTextLen:29.4  AvgSpecLen:104.1  StepTime:0.60  LR:0.000100\n",
      "   | > Step:48/68  GlobalStep:119350  TotalLoss:0.00210  PostnetLoss:0.00102  DecoderLoss:0.00108  StopLoss:0.03724  GradNorm:0.00329  GradNormST:0.01671  AvgTextLen:37.5  AvgSpecLen:123.8  StepTime:0.65  LR:0.000100\n",
      "   | > Step:58/68  GlobalStep:119360  TotalLoss:0.00229  PostnetLoss:0.00111  DecoderLoss:0.00117  StopLoss:0.04138  GradNorm:0.00339  GradNormST:0.01014  AvgTextLen:47.8  AvgSpecLen:158.0  StepTime:0.75  LR:0.000100\n",
      "   | > Step:68/68  GlobalStep:119370  TotalLoss:0.00257  PostnetLoss:0.00125  DecoderLoss:0.00132  StopLoss:0.02485  GradNorm:0.00254  GradNormST:0.01147  AvgTextLen:103.0  AvgSpecLen:308.9  StepTime:1.87  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:119370  AvgTotalLoss:0.04284  AvgPostnetLoss:0.00092  AvgDecoderLoss:0.00097  AvgStopLoss:0.04095  EpochTime:41.74  AvgStepTime:0.60\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.07537   PostnetLoss: 0.00538   DecoderLoss:0.00565  StopLoss: 0.06434  \n",
      "   | > TotalLoss: 0.09004   PostnetLoss: 0.00926   DecoderLoss:0.00965  StopLoss: 0.07113  \n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00092   Validation Loss: 0.00809\n",
      "\n",
      " > Epoch 730/1000\n",
      "   | > Step:9/68  GlobalStep:119380  TotalLoss:0.00137  PostnetLoss:0.00067  DecoderLoss:0.00070  StopLoss:0.03449  GradNorm:0.00323  GradNormST:0.01118  AvgTextLen:9.0  AvgSpecLen:42.1  StepTime:0.43  LR:0.000100\n",
      "   | > Step:19/68  GlobalStep:119390  TotalLoss:0.00159  PostnetLoss:0.00078  DecoderLoss:0.00081  StopLoss:0.04656  GradNorm:0.00331  GradNormST:0.01785  AvgTextLen:15.5  AvgSpecLen:63.2  StepTime:0.33  LR:0.000100\n",
      "   | > Step:29/68  GlobalStep:119400  TotalLoss:0.00178  PostnetLoss:0.00087  DecoderLoss:0.00091  StopLoss:0.05138  GradNorm:0.00296  GradNormST:0.01540  AvgTextLen:21.8  AvgSpecLen:83.4  StepTime:0.58  LR:0.000100\n",
      "   | > Step:39/68  GlobalStep:119410  TotalLoss:0.00200  PostnetLoss:0.00098  DecoderLoss:0.00103  StopLoss:0.03479  GradNorm:0.00319  GradNormST:0.00937  AvgTextLen:30.2  AvgSpecLen:110.4  StepTime:0.64  LR:0.000100\n",
      "   | > Step:49/68  GlobalStep:119420  TotalLoss:0.00212  PostnetLoss:0.00103  DecoderLoss:0.00109  StopLoss:0.04341  GradNorm:0.00348  GradNormST:0.01435  AvgTextLen:38.2  AvgSpecLen:127.7  StepTime:0.78  LR:0.000100\n",
      "   | > Step:59/68  GlobalStep:119430  TotalLoss:0.00230  PostnetLoss:0.00112  DecoderLoss:0.00118  StopLoss:0.04606  GradNorm:0.00345  GradNormST:0.01686  AvgTextLen:50.5  AvgSpecLen:155.9  StepTime:0.75  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:119439  AvgTotalLoss:0.04303  AvgPostnetLoss:0.00092  AvgDecoderLoss:0.00097  AvgStopLoss:0.04115  EpochTime:42.14  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.07765   PostnetLoss: 0.00529   DecoderLoss:0.00556  StopLoss: 0.06680  \n",
      "   | > TotalLoss: 0.09048   PostnetLoss: 0.00918   DecoderLoss:0.00957  StopLoss: 0.07173  \n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00092   Validation Loss: 0.00810\n",
      "\n",
      " > Epoch 731/1000\n",
      "   | > Step:0/68  GlobalStep:119440  TotalLoss:0.00136  PostnetLoss:0.00065  DecoderLoss:0.00071  StopLoss:0.08148  GradNorm:0.00598  GradNormST:0.03208  AvgTextLen:3.0  AvgSpecLen:25.3  StepTime:0.45  LR:0.000100\n",
      "   | > Step:10/68  GlobalStep:119450  TotalLoss:0.00147  PostnetLoss:0.00072  DecoderLoss:0.00075  StopLoss:0.03616  GradNorm:0.00349  GradNormST:0.01617  AvgTextLen:9.8  AvgSpecLen:48.6  StepTime:0.52  LR:0.000100\n",
      "   | > Step:20/68  GlobalStep:119460  TotalLoss:0.00168  PostnetLoss:0.00082  DecoderLoss:0.00086  StopLoss:0.03884  GradNorm:0.00378  GradNormST:0.01422  AvgTextLen:16.4  AvgSpecLen:64.8  StepTime:0.48  LR:0.000100\n",
      "   | > Step:30/68  GlobalStep:119470  TotalLoss:0.00183  PostnetLoss:0.00089  DecoderLoss:0.00094  StopLoss:0.03820  GradNorm:0.00313  GradNormST:0.00794  AvgTextLen:22.9  AvgSpecLen:85.8  StepTime:0.60  LR:0.000100\n",
      "   | > Step:40/68  GlobalStep:119480  TotalLoss:0.00195  PostnetLoss:0.00095  DecoderLoss:0.00100  StopLoss:0.03961  GradNorm:0.00341  GradNormST:0.01032  AvgTextLen:30.9  AvgSpecLen:105.3  StepTime:0.59  LR:0.000100\n",
      "   | > Step:50/68  GlobalStep:119490  TotalLoss:0.00212  PostnetLoss:0.00103  DecoderLoss:0.00109  StopLoss:0.03815  GradNorm:0.00366  GradNormST:0.01438  AvgTextLen:38.8  AvgSpecLen:132.0  StepTime:0.86  LR:0.000100\n",
      "   | > Step:60/68  GlobalStep:119500  TotalLoss:0.00236  PostnetLoss:0.00115  DecoderLoss:0.00121  StopLoss:0.03306  GradNorm:0.00365  GradNormST:0.01086  AvgTextLen:52.0  AvgSpecLen:172.7  StepTime:0.84  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:119508  AvgTotalLoss:0.04411  AvgPostnetLoss:0.00092  AvgDecoderLoss:0.00097  AvgStopLoss:0.04222  EpochTime:42.45  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.07411   PostnetLoss: 0.00523   DecoderLoss:0.00550  StopLoss: 0.06337  \n",
      "   | > TotalLoss: 0.08615   PostnetLoss: 0.00908   DecoderLoss:0.00946  StopLoss: 0.06761  \n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00092   Validation Loss: 0.00805\n",
      "\n",
      " > Epoch 732/1000\n",
      "   | > Step:1/68  GlobalStep:119510  TotalLoss:0.00134  PostnetLoss:0.00065  DecoderLoss:0.00069  StopLoss:0.05430  GradNorm:0.00469  GradNormST:0.01701  AvgTextLen:3.7  AvgSpecLen:28.6  StepTime:0.48  LR:0.000100\n",
      "   | > Step:11/68  GlobalStep:119520  TotalLoss:0.00141  PostnetLoss:0.00069  DecoderLoss:0.00072  StopLoss:0.03915  GradNorm:0.00313  GradNormST:0.01353  AvgTextLen:10.3  AvgSpecLen:49.4  StepTime:0.39  LR:0.000100\n",
      "   | > Step:21/68  GlobalStep:119530  TotalLoss:0.00164  PostnetLoss:0.00080  DecoderLoss:0.00084  StopLoss:0.03889  GradNorm:0.00343  GradNormST:0.01550  AvgTextLen:16.6  AvgSpecLen:66.3  StepTime:0.34  LR:0.000100\n",
      "   | > Step:31/68  GlobalStep:119540  TotalLoss:0.00181  PostnetLoss:0.00088  DecoderLoss:0.00093  StopLoss:0.04949  GradNorm:0.00303  GradNormST:0.01596  AvgTextLen:24.1  AvgSpecLen:82.7  StepTime:0.57  LR:0.000100\n",
      "   | > Step:41/68  GlobalStep:119550  TotalLoss:0.00202  PostnetLoss:0.00099  DecoderLoss:0.00104  StopLoss:0.03265  GradNorm:0.00333  GradNormST:0.01076  AvgTextLen:32.3  AvgSpecLen:109.4  StepTime:0.68  LR:0.000100\n",
      "   | > Step:51/68  GlobalStep:119560  TotalLoss:0.00215  PostnetLoss:0.00105  DecoderLoss:0.00111  StopLoss:0.04529  GradNorm:0.00326  GradNormST:0.01149  AvgTextLen:39.2  AvgSpecLen:126.9  StepTime:0.72  LR:0.000100\n",
      "   | > Step:61/68  GlobalStep:119570  TotalLoss:0.00233  PostnetLoss:0.00113  DecoderLoss:0.00120  StopLoss:0.03080  GradNorm:0.00296  GradNormST:0.00915  AvgTextLen:54.6  AvgSpecLen:165.5  StepTime:0.89  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:119577  AvgTotalLoss:0.04474  AvgPostnetLoss:0.00092  AvgDecoderLoss:0.00097  AvgStopLoss:0.04285  EpochTime:42.15  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.07776   PostnetLoss: 0.00544   DecoderLoss:0.00572  StopLoss: 0.06659  \n",
      "   | > TotalLoss: 0.09070   PostnetLoss: 0.00926   DecoderLoss:0.00965  StopLoss: 0.07178  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00092   Validation Loss: 0.00826\n",
      "\n",
      " > Epoch 733/1000\n",
      "   | > Step:2/68  GlobalStep:119580  TotalLoss:0.00122  PostnetLoss:0.00060  DecoderLoss:0.00063  StopLoss:0.03051  GradNorm:0.00361  GradNormST:0.01673  AvgTextLen:4.0  AvgSpecLen:31.3  StepTime:0.38  LR:0.000100\n",
      "   | > Step:12/68  GlobalStep:119590  TotalLoss:0.00143  PostnetLoss:0.00070  DecoderLoss:0.00073  StopLoss:0.03448  GradNorm:0.00329  GradNormST:0.00883  AvgTextLen:11.0  AvgSpecLen:49.3  StepTime:0.34  LR:0.000100\n",
      "   | > Step:22/68  GlobalStep:119600  TotalLoss:0.00168  PostnetLoss:0.00082  DecoderLoss:0.00086  StopLoss:0.04091  GradNorm:0.00310  GradNormST:0.00890  AvgTextLen:18.0  AvgSpecLen:72.0  StepTime:0.46  LR:0.000100\n",
      "   | > Step:32/68  GlobalStep:119610  TotalLoss:0.00182  PostnetLoss:0.00089  DecoderLoss:0.00093  StopLoss:0.04869  GradNorm:0.00298  GradNormST:0.01920  AvgTextLen:24.4  AvgSpecLen:91.5  StepTime:0.61  LR:0.000100\n",
      "   | > Step:42/68  GlobalStep:119620  TotalLoss:0.00198  PostnetLoss:0.00097  DecoderLoss:0.00102  StopLoss:0.05176  GradNorm:0.00288  GradNormST:0.01750  AvgTextLen:33.1  AvgSpecLen:109.7  StepTime:0.48  LR:0.000100\n",
      "   | > Step:52/68  GlobalStep:119630  TotalLoss:0.00219  PostnetLoss:0.00107  DecoderLoss:0.00112  StopLoss:0.05243  GradNorm:0.00383  GradNormST:0.01446  AvgTextLen:42.1  AvgSpecLen:137.5  StepTime:0.67  LR:0.000100\n",
      "   | > Step:62/68  GlobalStep:119640  TotalLoss:0.00240  PostnetLoss:0.00117  DecoderLoss:0.00123  StopLoss:0.03646  GradNorm:0.00355  GradNormST:0.01241  AvgTextLen:54.0  AvgSpecLen:174.7  StepTime:0.75  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:119646  AvgTotalLoss:0.04283  AvgPostnetLoss:0.00092  AvgDecoderLoss:0.00096  AvgStopLoss:0.04095  EpochTime:41.53  AvgStepTime:0.60\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.07822   PostnetLoss: 0.00543   DecoderLoss:0.00570  StopLoss: 0.06709  \n",
      "   | > TotalLoss: 0.08952   PostnetLoss: 0.00926   DecoderLoss:0.00963  StopLoss: 0.07064  \n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00092   Validation Loss: 0.00819\n",
      "\n",
      " > Epoch 734/1000\n",
      "   | > Step:3/68  GlobalStep:119650  TotalLoss:0.00126  PostnetLoss:0.00061  DecoderLoss:0.00064  StopLoss:0.05036  GradNorm:0.00358  GradNormST:0.01883  AvgTextLen:4.7  AvgSpecLen:29.7  StepTime:0.43  LR:0.000100\n",
      "   | > Step:13/68  GlobalStep:119660  TotalLoss:0.00142  PostnetLoss:0.00070  DecoderLoss:0.00073  StopLoss:0.05314  GradNorm:0.00298  GradNormST:0.02198  AvgTextLen:11.5  AvgSpecLen:55.2  StepTime:0.37  LR:0.000100\n",
      "   | > Step:23/68  GlobalStep:119670  TotalLoss:0.00173  PostnetLoss:0.00085  DecoderLoss:0.00088  StopLoss:0.03996  GradNorm:0.00322  GradNormST:0.01060  AvgTextLen:18.3  AvgSpecLen:72.2  StepTime:0.43  LR:0.000100\n",
      "   | > Step:33/68  GlobalStep:119680  TotalLoss:0.00190  PostnetLoss:0.00092  DecoderLoss:0.00097  StopLoss:0.04697  GradNorm:0.00293  GradNormST:0.01115  AvgTextLen:25.9  AvgSpecLen:88.9  StepTime:0.57  LR:0.000100\n",
      "   | > Step:43/68  GlobalStep:119690  TotalLoss:0.00201  PostnetLoss:0.00098  DecoderLoss:0.00103  StopLoss:0.04143  GradNorm:0.00307  GradNormST:0.01059  AvgTextLen:33.4  AvgSpecLen:115.7  StepTime:0.51  LR:0.000100\n",
      "   | > Step:53/68  GlobalStep:119700  TotalLoss:0.00214  PostnetLoss:0.00104  DecoderLoss:0.00110  StopLoss:0.03627  GradNorm:0.00363  GradNormST:0.00859  AvgTextLen:42.4  AvgSpecLen:137.6  StepTime:0.63  LR:0.000100\n",
      "   | > Step:63/68  GlobalStep:119710  TotalLoss:0.00242  PostnetLoss:0.00118  DecoderLoss:0.00125  StopLoss:0.02503  GradNorm:0.00358  GradNormST:0.00682  AvgTextLen:58.6  AvgSpecLen:185.1  StepTime:0.90  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:119715  AvgTotalLoss:0.04110  AvgPostnetLoss:0.00092  AvgDecoderLoss:0.00096  AvgStopLoss:0.03922  EpochTime:41.95  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.07709   PostnetLoss: 0.00544   DecoderLoss:0.00570  StopLoss: 0.06594  \n",
      "   | > TotalLoss: 0.08946   PostnetLoss: 0.00906   DecoderLoss:0.00943  StopLoss: 0.07097  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00092   Validation Loss: 0.00811\n",
      "\n",
      " > Epoch 735/1000\n",
      "   | > Step:4/68  GlobalStep:119720  TotalLoss:0.00130  PostnetLoss:0.00063  DecoderLoss:0.00067  StopLoss:0.06455  GradNorm:0.00373  GradNormST:0.03533  AvgTextLen:4.8  AvgSpecLen:31.1  StepTime:0.27  LR:0.000100\n",
      "   | > Step:14/68  GlobalStep:119730  TotalLoss:0.00151  PostnetLoss:0.00074  DecoderLoss:0.00078  StopLoss:0.04582  GradNorm:0.00344  GradNormST:0.01269  AvgTextLen:12.2  AvgSpecLen:52.0  StepTime:0.29  LR:0.000100\n",
      "   | > Step:24/68  GlobalStep:119740  TotalLoss:0.00175  PostnetLoss:0.00085  DecoderLoss:0.00089  StopLoss:0.03346  GradNorm:0.00300  GradNormST:0.01019  AvgTextLen:19.2  AvgSpecLen:79.0  StepTime:0.49  LR:0.000100\n",
      "   | > Step:34/68  GlobalStep:119750  TotalLoss:0.00191  PostnetLoss:0.00093  DecoderLoss:0.00098  StopLoss:0.02760  GradNorm:0.00290  GradNormST:0.00858  AvgTextLen:26.2  AvgSpecLen:95.9  StepTime:0.48  LR:0.000100\n",
      "   | > Step:44/68  GlobalStep:119760  TotalLoss:0.00208  PostnetLoss:0.00101  DecoderLoss:0.00106  StopLoss:0.03238  GradNorm:0.00309  GradNormST:0.01280  AvgTextLen:33.8  AvgSpecLen:116.3  StepTime:0.65  LR:0.000100\n",
      "   | > Step:54/68  GlobalStep:119770  TotalLoss:0.00221  PostnetLoss:0.00108  DecoderLoss:0.00114  StopLoss:0.03315  GradNorm:0.00324  GradNormST:0.00855  AvgTextLen:42.2  AvgSpecLen:144.9  StepTime:0.71  LR:0.000100\n",
      "   | > Step:64/68  GlobalStep:119780  TotalLoss:0.00230  PostnetLoss:0.00112  DecoderLoss:0.00118  StopLoss:0.03602  GradNorm:0.00276  GradNormST:0.00910  AvgTextLen:60.6  AvgSpecLen:177.6  StepTime:0.82  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:119784  AvgTotalLoss:0.04157  AvgPostnetLoss:0.00092  AvgDecoderLoss:0.00096  AvgStopLoss:0.03968  EpochTime:42.33  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.08095   PostnetLoss: 0.00538   DecoderLoss:0.00565  StopLoss: 0.06992  \n",
      "   | > TotalLoss: 0.09464   PostnetLoss: 0.00949   DecoderLoss:0.00986  StopLoss: 0.07529  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00092   Validation Loss: 0.00836\n",
      "\n",
      " > Epoch 736/1000\n",
      "   | > Step:5/68  GlobalStep:119790  TotalLoss:0.00135  PostnetLoss:0.00066  DecoderLoss:0.00069  StopLoss:0.05805  GradNorm:0.00407  GradNormST:0.01648  AvgTextLen:5.9  AvgSpecLen:33.2  StepTime:0.35  LR:0.000100\n",
      "   | > Step:15/68  GlobalStep:119800  TotalLoss:0.00157  PostnetLoss:0.00077  DecoderLoss:0.00080  StopLoss:0.03364  GradNorm:0.00320  GradNormST:0.01062  AvgTextLen:12.8  AvgSpecLen:59.1  StepTime:0.52  LR:0.000100\n",
      "   | > Step:25/68  GlobalStep:119810  TotalLoss:0.00174  PostnetLoss:0.00085  DecoderLoss:0.00089  StopLoss:0.04917  GradNorm:0.00324  GradNormST:0.01505  AvgTextLen:19.8  AvgSpecLen:73.3  StepTime:0.48  LR:0.000100\n",
      "   | > Step:35/68  GlobalStep:119820  TotalLoss:0.00193  PostnetLoss:0.00094  DecoderLoss:0.00099  StopLoss:0.03712  GradNorm:0.00279  GradNormST:0.01394  AvgTextLen:27.1  AvgSpecLen:104.7  StepTime:0.63  LR:0.000100\n",
      "   | > Step:45/68  GlobalStep:119830  TotalLoss:0.00200  PostnetLoss:0.00097  DecoderLoss:0.00103  StopLoss:0.05131  GradNorm:0.00276  GradNormST:0.01752  AvgTextLen:34.4  AvgSpecLen:118.6  StepTime:0.68  LR:0.000100\n",
      "   | > Step:55/68  GlobalStep:119840  TotalLoss:0.00224  PostnetLoss:0.00109  DecoderLoss:0.00115  StopLoss:0.03984  GradNorm:0.00318  GradNormST:0.00854  AvgTextLen:43.9  AvgSpecLen:145.2  StepTime:0.80  LR:0.000100\n",
      "   | > Step:65/68  GlobalStep:119850  TotalLoss:0.00247  PostnetLoss:0.00120  DecoderLoss:0.00127  StopLoss:0.05194  GradNorm:0.00265  GradNormST:0.02188  AvgTextLen:64.2  AvgSpecLen:207.4  StepTime:1.19  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:119853  AvgTotalLoss:0.04280  AvgPostnetLoss:0.00092  AvgDecoderLoss:0.00096  AvgStopLoss:0.04092  EpochTime:42.72  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.07812   PostnetLoss: 0.00540   DecoderLoss:0.00569  StopLoss: 0.06703  \n",
      "   | > TotalLoss: 0.09113   PostnetLoss: 0.00940   DecoderLoss:0.00980  StopLoss: 0.07193  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00092   Validation Loss: 0.00827\n",
      "\n",
      " > Epoch 737/1000\n",
      "   | > Step:6/68  GlobalStep:119860  TotalLoss:0.00130  PostnetLoss:0.00064  DecoderLoss:0.00066  StopLoss:0.04082  GradNorm:0.00388  GradNormST:0.01742  AvgTextLen:6.0  AvgSpecLen:34.8  StepTime:0.32  LR:0.000100\n",
      "   | > Step:16/68  GlobalStep:119870  TotalLoss:0.00161  PostnetLoss:0.00079  DecoderLoss:0.00082  StopLoss:0.04578  GradNorm:0.00333  GradNormST:0.01190  AvgTextLen:13.8  AvgSpecLen:60.6  StepTime:0.39  LR:0.000100\n",
      "   | > Step:26/68  GlobalStep:119880  TotalLoss:0.00178  PostnetLoss:0.00087  DecoderLoss:0.00091  StopLoss:0.03225  GradNorm:0.00339  GradNormST:0.01291  AvgTextLen:20.3  AvgSpecLen:77.5  StepTime:0.50  LR:0.000100\n",
      "   | > Step:36/68  GlobalStep:119890  TotalLoss:0.00190  PostnetLoss:0.00093  DecoderLoss:0.00098  StopLoss:0.04489  GradNorm:0.00286  GradNormST:0.01176  AvgTextLen:28.5  AvgSpecLen:100.2  StepTime:0.68  LR:0.000100\n",
      "   | > Step:46/68  GlobalStep:119900  TotalLoss:0.00204  PostnetLoss:0.00099  DecoderLoss:0.00105  StopLoss:0.03624  GradNorm:0.00284  GradNormST:0.01497  AvgTextLen:35.4  AvgSpecLen:122.6  StepTime:0.78  LR:0.000100\n",
      "   | > Step:56/68  GlobalStep:119910  TotalLoss:0.00221  PostnetLoss:0.00107  DecoderLoss:0.00113  StopLoss:0.03198  GradNorm:0.00302  GradNormST:0.01221  AvgTextLen:45.8  AvgSpecLen:145.4  StepTime:0.97  LR:0.000100\n",
      "   | > Step:66/68  GlobalStep:119920  TotalLoss:0.00260  PostnetLoss:0.00126  DecoderLoss:0.00134  StopLoss:0.03413  GradNorm:0.00265  GradNormST:0.01652  AvgTextLen:71.2  AvgSpecLen:220.8  StepTime:1.04  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:119922  AvgTotalLoss:0.04244  AvgPostnetLoss:0.00091  AvgDecoderLoss:0.00096  AvgStopLoss:0.04057  EpochTime:42.13  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.07861   PostnetLoss: 0.00551   DecoderLoss:0.00579  StopLoss: 0.06731  \n",
      "   | > TotalLoss: 0.09307   PostnetLoss: 0.00962   DecoderLoss:0.01000  StopLoss: 0.07344  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00091   Validation Loss: 0.00841\n",
      "\n",
      " > Epoch 738/1000\n",
      "   | > Step:7/68  GlobalStep:119930  TotalLoss:0.00139  PostnetLoss:0.00068  DecoderLoss:0.00071  StopLoss:0.03134  GradNorm:0.00379  GradNormST:0.01029  AvgTextLen:7.4  AvgSpecLen:44.6  StepTime:0.40  LR:0.000100\n",
      "   | > Step:17/68  GlobalStep:119940  TotalLoss:0.00157  PostnetLoss:0.00077  DecoderLoss:0.00080  StopLoss:0.04775  GradNorm:0.00315  GradNormST:0.01216  AvgTextLen:14.7  AvgSpecLen:66.0  StepTime:0.44  LR:0.000100\n",
      "   | > Step:27/68  GlobalStep:119950  TotalLoss:0.00177  PostnetLoss:0.00087  DecoderLoss:0.00091  StopLoss:0.05416  GradNorm:0.00304  GradNormST:0.01400  AvgTextLen:21.0  AvgSpecLen:80.2  StepTime:0.62  LR:0.000100\n",
      "   | > Step:37/68  GlobalStep:119960  TotalLoss:0.00194  PostnetLoss:0.00095  DecoderLoss:0.00100  StopLoss:0.04124  GradNorm:0.00286  GradNormST:0.01389  AvgTextLen:29.3  AvgSpecLen:102.4  StepTime:0.49  LR:0.000100\n",
      "   | > Step:47/68  GlobalStep:119970  TotalLoss:0.00211  PostnetLoss:0.00103  DecoderLoss:0.00108  StopLoss:0.03085  GradNorm:0.00310  GradNormST:0.00684  AvgTextLen:36.7  AvgSpecLen:126.9  StepTime:0.62  LR:0.000100\n",
      "   | > Step:57/68  GlobalStep:119980  TotalLoss:0.00228  PostnetLoss:0.00111  DecoderLoss:0.00117  StopLoss:0.03430  GradNorm:0.00271  GradNormST:0.01045  AvgTextLen:47.1  AvgSpecLen:161.0  StepTime:0.90  LR:0.000100\n",
      "   | > Step:67/68  GlobalStep:119990  TotalLoss:0.00261  PostnetLoss:0.00126  DecoderLoss:0.00134  StopLoss:0.02897  GradNorm:0.00267  GradNormST:0.01201  AvgTextLen:80.6  AvgSpecLen:246.5  StepTime:1.21  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:119991  AvgTotalLoss:0.04180  AvgPostnetLoss:0.00091  AvgDecoderLoss:0.00096  AvgStopLoss:0.03993  EpochTime:41.87  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.07841   PostnetLoss: 0.00536   DecoderLoss:0.00564  StopLoss: 0.06742  \n",
      "   | > TotalLoss: 0.09119   PostnetLoss: 0.00979   DecoderLoss:0.01019  StopLoss: 0.07122  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00091   Validation Loss: 0.00848\n",
      "\n",
      " > Epoch 739/1000\n",
      "   | > Step:8/68  GlobalStep:120000  TotalLoss:0.00143  PostnetLoss:0.00070  DecoderLoss:0.00073  StopLoss:0.03896  GradNorm:0.00374  GradNormST:0.01178  AvgTextLen:8.3  AvgSpecLen:44.0  StepTime:0.25  LR:0.000100\n",
      " | | > Checkpoint saving : /vm/TTS/models/cartman_models/mozilla-no-loc-fattn-stopnet-sigmoid-loss_masking-August-15-2019_11+26AM-de6aeb9/checkpoint_120000.pth.tar\n",
      "warning: audio amplitude out of range, auto clipped.\n",
      "   | > Step:18/68  GlobalStep:120010  TotalLoss:0.00157  PostnetLoss:0.00076  DecoderLoss:0.00080  StopLoss:0.04490  GradNorm:0.00310  GradNormST:0.01224  AvgTextLen:15.2  AvgSpecLen:64.5  StepTime:0.43  LR:0.000100\n",
      "   | > Step:28/68  GlobalStep:120020  TotalLoss:0.00179  PostnetLoss:0.00088  DecoderLoss:0.00092  StopLoss:0.05521  GradNorm:0.00301  GradNormST:0.02590  AvgTextLen:22.1  AvgSpecLen:85.4  StepTime:0.57  LR:0.000100\n",
      "   | > Step:38/68  GlobalStep:120030  TotalLoss:0.00191  PostnetLoss:0.00093  DecoderLoss:0.00098  StopLoss:0.04123  GradNorm:0.00308  GradNormST:0.01725  AvgTextLen:29.4  AvgSpecLen:104.1  StepTime:0.62  LR:0.000100\n",
      "   | > Step:48/68  GlobalStep:120040  TotalLoss:0.00209  PostnetLoss:0.00102  DecoderLoss:0.00107  StopLoss:0.04016  GradNorm:0.00300  GradNormST:0.01667  AvgTextLen:37.5  AvgSpecLen:123.8  StepTime:0.69  LR:0.000100\n",
      "   | > Step:58/68  GlobalStep:120050  TotalLoss:0.00227  PostnetLoss:0.00110  DecoderLoss:0.00116  StopLoss:0.04414  GradNorm:0.00280  GradNormST:0.01415  AvgTextLen:47.8  AvgSpecLen:158.0  StepTime:0.85  LR:0.000100\n",
      "   | > Step:68/68  GlobalStep:120060  TotalLoss:0.00256  PostnetLoss:0.00124  DecoderLoss:0.00132  StopLoss:0.02004  GradNorm:0.00243  GradNormST:0.00984  AvgTextLen:103.0  AvgSpecLen:308.9  StepTime:1.92  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:120060  AvgTotalLoss:0.04569  AvgPostnetLoss:0.00091  AvgDecoderLoss:0.00096  AvgStopLoss:0.04382  EpochTime:42.12  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.07812   PostnetLoss: 0.00549   DecoderLoss:0.00578  StopLoss: 0.06685  \n",
      "   | > TotalLoss: 0.09243   PostnetLoss: 0.00974   DecoderLoss:0.01013  StopLoss: 0.07257  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00091   Validation Loss: 0.00850\n",
      "\n",
      " > Epoch 740/1000\n",
      "   | > Step:9/68  GlobalStep:120070  TotalLoss:0.00139  PostnetLoss:0.00068  DecoderLoss:0.00071  StopLoss:0.05529  GradNorm:0.00370  GradNormST:0.03598  AvgTextLen:9.0  AvgSpecLen:42.1  StepTime:0.41  LR:0.000100\n",
      "   | > Step:19/68  GlobalStep:120080  TotalLoss:0.00160  PostnetLoss:0.00078  DecoderLoss:0.00082  StopLoss:0.05162  GradNorm:0.00316  GradNormST:0.01446  AvgTextLen:15.5  AvgSpecLen:63.2  StepTime:0.36  LR:0.000100\n",
      "   | > Step:29/68  GlobalStep:120090  TotalLoss:0.00176  PostnetLoss:0.00086  DecoderLoss:0.00090  StopLoss:0.07261  GradNorm:0.00290  GradNormST:0.02673  AvgTextLen:21.8  AvgSpecLen:83.4  StepTime:0.54  LR:0.000100\n",
      "   | > Step:39/68  GlobalStep:120100  TotalLoss:0.00196  PostnetLoss:0.00095  DecoderLoss:0.00100  StopLoss:0.03973  GradNorm:0.00292  GradNormST:0.01294  AvgTextLen:30.2  AvgSpecLen:110.4  StepTime:0.64  LR:0.000100\n",
      "   | > Step:49/68  GlobalStep:120110  TotalLoss:0.00212  PostnetLoss:0.00103  DecoderLoss:0.00109  StopLoss:0.04000  GradNorm:0.00297  GradNormST:0.00872  AvgTextLen:38.2  AvgSpecLen:127.7  StepTime:0.75  LR:0.000100\n",
      "   | > Step:59/68  GlobalStep:120120  TotalLoss:0.00226  PostnetLoss:0.00110  DecoderLoss:0.00116  StopLoss:0.07096  GradNorm:0.00286  GradNormST:0.03332  AvgTextLen:50.5  AvgSpecLen:155.9  StepTime:0.72  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:120129  AvgTotalLoss:0.04431  AvgPostnetLoss:0.00091  AvgDecoderLoss:0.00096  AvgStopLoss:0.04244  EpochTime:43.07  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.08055   PostnetLoss: 0.00549   DecoderLoss:0.00576  StopLoss: 0.06930  \n",
      "   | > TotalLoss: 0.08852   PostnetLoss: 0.00959   DecoderLoss:0.00998  StopLoss: 0.06895  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00091   Validation Loss: 0.00849\n",
      "\n",
      " > Epoch 741/1000\n",
      "   | > Step:0/68  GlobalStep:120130  TotalLoss:0.00136  PostnetLoss:0.00065  DecoderLoss:0.00070  StopLoss:0.04542  GradNorm:0.00601  GradNormST:0.01796  AvgTextLen:3.0  AvgSpecLen:25.3  StepTime:0.43  LR:0.000100\n",
      "   | > Step:10/68  GlobalStep:120140  TotalLoss:0.00145  PostnetLoss:0.00071  DecoderLoss:0.00074  StopLoss:0.05153  GradNorm:0.00360  GradNormST:0.02691  AvgTextLen:9.8  AvgSpecLen:48.6  StepTime:0.43  LR:0.000100\n",
      "   | > Step:20/68  GlobalStep:120150  TotalLoss:0.00167  PostnetLoss:0.00081  DecoderLoss:0.00085  StopLoss:0.03396  GradNorm:0.00333  GradNormST:0.01387  AvgTextLen:16.4  AvgSpecLen:64.8  StepTime:0.50  LR:0.000100\n",
      "   | > Step:30/68  GlobalStep:120160  TotalLoss:0.00182  PostnetLoss:0.00089  DecoderLoss:0.00093  StopLoss:0.04080  GradNorm:0.00301  GradNormST:0.01130  AvgTextLen:22.9  AvgSpecLen:85.8  StepTime:0.56  LR:0.000100\n",
      "   | > Step:40/68  GlobalStep:120170  TotalLoss:0.00194  PostnetLoss:0.00095  DecoderLoss:0.00099  StopLoss:0.03663  GradNorm:0.00291  GradNormST:0.00939  AvgTextLen:30.9  AvgSpecLen:105.3  StepTime:0.49  LR:0.000100\n",
      "   | > Step:50/68  GlobalStep:120180  TotalLoss:0.00213  PostnetLoss:0.00103  DecoderLoss:0.00109  StopLoss:0.05043  GradNorm:0.00295  GradNormST:0.01798  AvgTextLen:38.8  AvgSpecLen:132.0  StepTime:0.68  LR:0.000100\n",
      "   | > Step:60/68  GlobalStep:120190  TotalLoss:0.00234  PostnetLoss:0.00114  DecoderLoss:0.00120  StopLoss:0.03586  GradNorm:0.00292  GradNormST:0.01331  AvgTextLen:52.0  AvgSpecLen:172.7  StepTime:0.90  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:120198  AvgTotalLoss:0.04414  AvgPostnetLoss:0.00091  AvgDecoderLoss:0.00096  AvgStopLoss:0.04226  EpochTime:41.95  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.08320   PostnetLoss: 0.00549   DecoderLoss:0.00577  StopLoss: 0.07194  \n",
      "   | > TotalLoss: 0.09163   PostnetLoss: 0.00976   DecoderLoss:0.01015  StopLoss: 0.07173  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00091   Validation Loss: 0.00862\n",
      "\n",
      " > Epoch 742/1000\n",
      "   | > Step:1/68  GlobalStep:120200  TotalLoss:0.00133  PostnetLoss:0.00065  DecoderLoss:0.00069  StopLoss:0.03810  GradNorm:0.00493  GradNormST:0.01315  AvgTextLen:3.7  AvgSpecLen:28.6  StepTime:0.43  LR:0.000100\n",
      "   | > Step:11/68  GlobalStep:120210  TotalLoss:0.00145  PostnetLoss:0.00071  DecoderLoss:0.00074  StopLoss:0.04363  GradNorm:0.00456  GradNormST:0.01577  AvgTextLen:10.3  AvgSpecLen:49.4  StepTime:0.39  LR:0.000100\n",
      "   | > Step:21/68  GlobalStep:120220  TotalLoss:0.00167  PostnetLoss:0.00082  DecoderLoss:0.00085  StopLoss:0.04123  GradNorm:0.00377  GradNormST:0.02195  AvgTextLen:16.6  AvgSpecLen:66.3  StepTime:0.43  LR:0.000100\n",
      "   | > Step:31/68  GlobalStep:120230  TotalLoss:0.00182  PostnetLoss:0.00089  DecoderLoss:0.00093  StopLoss:0.03804  GradNorm:0.00293  GradNormST:0.01042  AvgTextLen:24.1  AvgSpecLen:82.7  StepTime:0.52  LR:0.000100\n",
      "   | > Step:41/68  GlobalStep:120240  TotalLoss:0.00199  PostnetLoss:0.00097  DecoderLoss:0.00102  StopLoss:0.03661  GradNorm:0.00304  GradNormST:0.01210  AvgTextLen:32.3  AvgSpecLen:109.4  StepTime:0.57  LR:0.000100\n",
      "   | > Step:51/68  GlobalStep:120250  TotalLoss:0.00211  PostnetLoss:0.00103  DecoderLoss:0.00108  StopLoss:0.03593  GradNorm:0.00320  GradNormST:0.01141  AvgTextLen:39.2  AvgSpecLen:126.9  StepTime:0.79  LR:0.000100\n",
      "   | > Step:61/68  GlobalStep:120260  TotalLoss:0.00230  PostnetLoss:0.00112  DecoderLoss:0.00118  StopLoss:0.03353  GradNorm:0.00319  GradNormST:0.00871  AvgTextLen:54.6  AvgSpecLen:165.5  StepTime:0.93  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:120267  AvgTotalLoss:0.04520  AvgPostnetLoss:0.00091  AvgDecoderLoss:0.00096  AvgStopLoss:0.04333  EpochTime:42.35  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.08218   PostnetLoss: 0.00530   DecoderLoss:0.00557  StopLoss: 0.07131  \n",
      "   | > TotalLoss: 0.09681   PostnetLoss: 0.00999   DecoderLoss:0.01037  StopLoss: 0.07645  \n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00091   Validation Loss: 0.00874\n",
      "\n",
      " > Epoch 743/1000\n",
      "   | > Step:2/68  GlobalStep:120270  TotalLoss:0.00123  PostnetLoss:0.00060  DecoderLoss:0.00063  StopLoss:0.03376  GradNorm:0.00420  GradNormST:0.00976  AvgTextLen:4.0  AvgSpecLen:31.3  StepTime:0.34  LR:0.000100\n",
      "   | > Step:12/68  GlobalStep:120280  TotalLoss:0.00142  PostnetLoss:0.00069  DecoderLoss:0.00072  StopLoss:0.03942  GradNorm:0.00410  GradNormST:0.01389  AvgTextLen:11.0  AvgSpecLen:49.3  StepTime:0.36  LR:0.000100\n",
      "   | > Step:22/68  GlobalStep:120290  TotalLoss:0.00168  PostnetLoss:0.00082  DecoderLoss:0.00086  StopLoss:0.04586  GradNorm:0.00337  GradNormST:0.01371  AvgTextLen:18.0  AvgSpecLen:72.0  StepTime:0.45  LR:0.000100\n",
      "   | > Step:32/68  GlobalStep:120300  TotalLoss:0.00183  PostnetLoss:0.00089  DecoderLoss:0.00094  StopLoss:0.05240  GradNorm:0.00302  GradNormST:0.01930  AvgTextLen:24.4  AvgSpecLen:91.5  StepTime:0.54  LR:0.000100\n",
      "   | > Step:42/68  GlobalStep:120310  TotalLoss:0.00199  PostnetLoss:0.00097  DecoderLoss:0.00102  StopLoss:0.05046  GradNorm:0.00287  GradNormST:0.01947  AvgTextLen:33.1  AvgSpecLen:109.7  StepTime:0.58  LR:0.000100\n",
      "   | > Step:52/68  GlobalStep:120320  TotalLoss:0.00215  PostnetLoss:0.00105  DecoderLoss:0.00110  StopLoss:0.05375  GradNorm:0.00296  GradNormST:0.01253  AvgTextLen:42.1  AvgSpecLen:137.5  StepTime:0.80  LR:0.000100\n",
      "   | > Step:62/68  GlobalStep:120330  TotalLoss:0.00235  PostnetLoss:0.00114  DecoderLoss:0.00121  StopLoss:0.02967  GradNorm:0.00291  GradNormST:0.01034  AvgTextLen:54.0  AvgSpecLen:174.7  StepTime:0.76  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:120336  AvgTotalLoss:0.04347  AvgPostnetLoss:0.00091  AvgDecoderLoss:0.00096  AvgStopLoss:0.04161  EpochTime:42.68  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.08232   PostnetLoss: 0.00552   DecoderLoss:0.00580  StopLoss: 0.07099  \n",
      "   | > TotalLoss: 0.09243   PostnetLoss: 0.00994   DecoderLoss:0.01033  StopLoss: 0.07216  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00091   Validation Loss: 0.00870\n",
      "\n",
      " > Epoch 744/1000\n",
      "   | > Step:3/68  GlobalStep:120340  TotalLoss:0.00128  PostnetLoss:0.00062  DecoderLoss:0.00065  StopLoss:0.04971  GradNorm:0.00387  GradNormST:0.01656  AvgTextLen:4.7  AvgSpecLen:29.7  StepTime:0.26  LR:0.000100\n",
      "   | > Step:13/68  GlobalStep:120350  TotalLoss:0.00148  PostnetLoss:0.00072  DecoderLoss:0.00076  StopLoss:0.05199  GradNorm:0.00382  GradNormST:0.02044  AvgTextLen:11.5  AvgSpecLen:55.2  StepTime:0.36  LR:0.000100\n",
      "   | > Step:23/68  GlobalStep:120360  TotalLoss:0.00172  PostnetLoss:0.00084  DecoderLoss:0.00088  StopLoss:0.05189  GradNorm:0.00367  GradNormST:0.01157  AvgTextLen:18.3  AvgSpecLen:72.2  StepTime:0.34  LR:0.000100\n",
      "   | > Step:33/68  GlobalStep:120370  TotalLoss:0.00185  PostnetLoss:0.00091  DecoderLoss:0.00095  StopLoss:0.05603  GradNorm:0.00317  GradNormST:0.02116  AvgTextLen:25.9  AvgSpecLen:88.9  StepTime:0.54  LR:0.000100\n",
      "   | > Step:43/68  GlobalStep:120380  TotalLoss:0.00197  PostnetLoss:0.00096  DecoderLoss:0.00101  StopLoss:0.04560  GradNorm:0.00284  GradNormST:0.01658  AvgTextLen:33.4  AvgSpecLen:115.7  StepTime:0.62  LR:0.000100\n",
      "   | > Step:53/68  GlobalStep:120390  TotalLoss:0.00211  PostnetLoss:0.00103  DecoderLoss:0.00109  StopLoss:0.04033  GradNorm:0.00306  GradNormST:0.01255  AvgTextLen:42.4  AvgSpecLen:137.6  StepTime:0.74  LR:0.000100\n",
      "   | > Step:63/68  GlobalStep:120400  TotalLoss:0.00240  PostnetLoss:0.00117  DecoderLoss:0.00123  StopLoss:0.02762  GradNorm:0.00264  GradNormST:0.01004  AvgTextLen:58.6  AvgSpecLen:185.1  StepTime:0.90  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:120405  AvgTotalLoss:0.04218  AvgPostnetLoss:0.00091  AvgDecoderLoss:0.00096  AvgStopLoss:0.04031  EpochTime:42.78  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.08099   PostnetLoss: 0.00551   DecoderLoss:0.00580  StopLoss: 0.06968  \n",
      "   | > TotalLoss: 0.09303   PostnetLoss: 0.00991   DecoderLoss:0.01030  StopLoss: 0.07282  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00091   Validation Loss: 0.00883\n",
      "\n",
      " > Epoch 745/1000\n",
      "   | > Step:4/68  GlobalStep:120410  TotalLoss:0.00127  PostnetLoss:0.00062  DecoderLoss:0.00065  StopLoss:0.07084  GradNorm:0.00347  GradNormST:0.02422  AvgTextLen:4.8  AvgSpecLen:31.1  StepTime:0.23  LR:0.000100\n",
      "   | > Step:14/68  GlobalStep:120420  TotalLoss:0.00151  PostnetLoss:0.00074  DecoderLoss:0.00078  StopLoss:0.03692  GradNorm:0.00334  GradNormST:0.01488  AvgTextLen:12.2  AvgSpecLen:52.0  StepTime:0.29  LR:0.000100\n",
      "   | > Step:24/68  GlobalStep:120430  TotalLoss:0.00176  PostnetLoss:0.00086  DecoderLoss:0.00090  StopLoss:0.03904  GradNorm:0.00317  GradNormST:0.00911  AvgTextLen:19.2  AvgSpecLen:79.0  StepTime:0.50  LR:0.000100\n",
      "   | > Step:34/68  GlobalStep:120440  TotalLoss:0.00188  PostnetLoss:0.00092  DecoderLoss:0.00096  StopLoss:0.03417  GradNorm:0.00307  GradNormST:0.00854  AvgTextLen:26.2  AvgSpecLen:95.9  StepTime:0.57  LR:0.000100\n",
      "   | > Step:44/68  GlobalStep:120450  TotalLoss:0.00205  PostnetLoss:0.00100  DecoderLoss:0.00105  StopLoss:0.02921  GradNorm:0.00289  GradNormST:0.01112  AvgTextLen:33.8  AvgSpecLen:116.3  StepTime:0.66  LR:0.000100\n",
      "   | > Step:54/68  GlobalStep:120460  TotalLoss:0.00220  PostnetLoss:0.00107  DecoderLoss:0.00113  StopLoss:0.03619  GradNorm:0.00275  GradNormST:0.01123  AvgTextLen:42.2  AvgSpecLen:144.9  StepTime:0.84  LR:0.000100\n",
      "   | > Step:64/68  GlobalStep:120470  TotalLoss:0.00230  PostnetLoss:0.00112  DecoderLoss:0.00118  StopLoss:0.02946  GradNorm:0.00271  GradNormST:0.00737  AvgTextLen:60.6  AvgSpecLen:177.6  StepTime:0.81  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:120474  AvgTotalLoss:0.04378  AvgPostnetLoss:0.00091  AvgDecoderLoss:0.00096  AvgStopLoss:0.04192  EpochTime:42.80  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.07766   PostnetLoss: 0.00548   DecoderLoss:0.00577  StopLoss: 0.06641  \n",
      "   | > TotalLoss: 0.08931   PostnetLoss: 0.00989   DecoderLoss:0.01029  StopLoss: 0.06914  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00091   Validation Loss: 0.00873\n",
      "\n",
      " > Epoch 746/1000\n",
      "   | > Step:5/68  GlobalStep:120480  TotalLoss:0.00133  PostnetLoss:0.00065  DecoderLoss:0.00069  StopLoss:0.04278  GradNorm:0.00342  GradNormST:0.01571  AvgTextLen:5.9  AvgSpecLen:33.2  StepTime:0.34  LR:0.000100\n",
      "   | > Step:15/68  GlobalStep:120490  TotalLoss:0.00157  PostnetLoss:0.00077  DecoderLoss:0.00080  StopLoss:0.03777  GradNorm:0.00352  GradNormST:0.01130  AvgTextLen:12.8  AvgSpecLen:59.1  StepTime:0.52  LR:0.000100\n",
      "   | > Step:25/68  GlobalStep:120500  TotalLoss:0.00170  PostnetLoss:0.00083  DecoderLoss:0.00087  StopLoss:0.04249  GradNorm:0.00365  GradNormST:0.01168  AvgTextLen:19.8  AvgSpecLen:73.3  StepTime:0.45  LR:0.000100\n",
      "   | > Step:35/68  GlobalStep:120510  TotalLoss:0.00192  PostnetLoss:0.00094  DecoderLoss:0.00098  StopLoss:0.04639  GradNorm:0.00320  GradNormST:0.01886  AvgTextLen:27.1  AvgSpecLen:104.7  StepTime:0.80  LR:0.000100\n",
      "   | > Step:45/68  GlobalStep:120520  TotalLoss:0.00199  PostnetLoss:0.00097  DecoderLoss:0.00102  StopLoss:0.04431  GradNorm:0.00285  GradNormST:0.01512  AvgTextLen:34.4  AvgSpecLen:118.6  StepTime:0.69  LR:0.000100\n",
      "   | > Step:55/68  GlobalStep:120530  TotalLoss:0.00221  PostnetLoss:0.00108  DecoderLoss:0.00114  StopLoss:0.03923  GradNorm:0.00266  GradNormST:0.01132  AvgTextLen:43.9  AvgSpecLen:145.2  StepTime:0.78  LR:0.000100\n",
      "   | > Step:65/68  GlobalStep:120540  TotalLoss:0.00245  PostnetLoss:0.00119  DecoderLoss:0.00126  StopLoss:0.03917  GradNorm:0.00262  GradNormST:0.01471  AvgTextLen:64.2  AvgSpecLen:207.4  StepTime:1.19  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:120543  AvgTotalLoss:0.04284  AvgPostnetLoss:0.00091  AvgDecoderLoss:0.00095  AvgStopLoss:0.04098  EpochTime:42.60  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.07769   PostnetLoss: 0.00573   DecoderLoss:0.00602  StopLoss: 0.06595  \n",
      "   | > TotalLoss: 0.08820   PostnetLoss: 0.00952   DecoderLoss:0.00990  StopLoss: 0.06878  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00091   Validation Loss: 0.00849\n",
      "\n",
      " > Epoch 747/1000\n",
      "   | > Step:6/68  GlobalStep:120550  TotalLoss:0.00132  PostnetLoss:0.00064  DecoderLoss:0.00067  StopLoss:0.03231  GradNorm:0.00349  GradNormST:0.01468  AvgTextLen:6.0  AvgSpecLen:34.8  StepTime:0.36  LR:0.000100\n",
      "   | > Step:16/68  GlobalStep:120560  TotalLoss:0.00158  PostnetLoss:0.00077  DecoderLoss:0.00081  StopLoss:0.03596  GradNorm:0.00359  GradNormST:0.01221  AvgTextLen:13.8  AvgSpecLen:60.6  StepTime:0.39  LR:0.000100\n",
      "   | > Step:26/68  GlobalStep:120570  TotalLoss:0.00175  PostnetLoss:0.00086  DecoderLoss:0.00090  StopLoss:0.04949  GradNorm:0.00331  GradNormST:0.01324  AvgTextLen:20.3  AvgSpecLen:77.5  StepTime:0.38  LR:0.000100\n",
      "   | > Step:36/68  GlobalStep:120580  TotalLoss:0.00189  PostnetLoss:0.00092  DecoderLoss:0.00097  StopLoss:0.04954  GradNorm:0.00294  GradNormST:0.01510  AvgTextLen:28.5  AvgSpecLen:100.2  StepTime:0.69  LR:0.000100\n",
      "   | > Step:46/68  GlobalStep:120590  TotalLoss:0.00203  PostnetLoss:0.00099  DecoderLoss:0.00104  StopLoss:0.03935  GradNorm:0.00282  GradNormST:0.01205  AvgTextLen:35.4  AvgSpecLen:122.6  StepTime:0.77  LR:0.000100\n",
      "   | > Step:56/68  GlobalStep:120600  TotalLoss:0.00221  PostnetLoss:0.00107  DecoderLoss:0.00113  StopLoss:0.03824  GradNorm:0.00269  GradNormST:0.01578  AvgTextLen:45.8  AvgSpecLen:145.4  StepTime:0.83  LR:0.000100\n",
      "   | > Step:66/68  GlobalStep:120610  TotalLoss:0.00259  PostnetLoss:0.00126  DecoderLoss:0.00134  StopLoss:0.04235  GradNorm:0.00261  GradNormST:0.01402  AvgTextLen:71.2  AvgSpecLen:220.8  StepTime:1.04  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:120612  AvgTotalLoss:0.04376  AvgPostnetLoss:0.00091  AvgDecoderLoss:0.00096  AvgStopLoss:0.04190  EpochTime:43.22  AvgStepTime:0.63\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.07666   PostnetLoss: 0.00550   DecoderLoss:0.00578  StopLoss: 0.06538  \n",
      "   | > TotalLoss: 0.09099   PostnetLoss: 0.00974   DecoderLoss:0.01013  StopLoss: 0.07113  \n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00091   Validation Loss: 0.00863\n",
      "\n",
      " > Epoch 748/1000\n",
      "   | > Step:7/68  GlobalStep:120620  TotalLoss:0.00138  PostnetLoss:0.00067  DecoderLoss:0.00070  StopLoss:0.02614  GradNorm:0.00337  GradNormST:0.01426  AvgTextLen:7.4  AvgSpecLen:44.6  StepTime:0.30  LR:0.000100\n",
      "   | > Step:17/68  GlobalStep:120630  TotalLoss:0.00156  PostnetLoss:0.00076  DecoderLoss:0.00080  StopLoss:0.04188  GradNorm:0.00319  GradNormST:0.01236  AvgTextLen:14.7  AvgSpecLen:66.0  StepTime:0.40  LR:0.000100\n",
      "   | > Step:27/68  GlobalStep:120640  TotalLoss:0.00176  PostnetLoss:0.00086  DecoderLoss:0.00090  StopLoss:0.05173  GradNorm:0.00308  GradNormST:0.01816  AvgTextLen:21.0  AvgSpecLen:80.2  StepTime:0.67  LR:0.000100\n",
      "   | > Step:37/68  GlobalStep:120650  TotalLoss:0.00192  PostnetLoss:0.00094  DecoderLoss:0.00098  StopLoss:0.04373  GradNorm:0.00296  GradNormST:0.01515  AvgTextLen:29.3  AvgSpecLen:102.4  StepTime:0.61  LR:0.000100\n",
      "   | > Step:47/68  GlobalStep:120660  TotalLoss:0.00208  PostnetLoss:0.00102  DecoderLoss:0.00107  StopLoss:0.03988  GradNorm:0.00289  GradNormST:0.01245  AvgTextLen:36.7  AvgSpecLen:126.9  StepTime:0.58  LR:0.000100\n",
      "   | > Step:57/68  GlobalStep:120670  TotalLoss:0.00225  PostnetLoss:0.00110  DecoderLoss:0.00116  StopLoss:0.03205  GradNorm:0.00261  GradNormST:0.01035  AvgTextLen:47.1  AvgSpecLen:161.0  StepTime:0.83  LR:0.000100\n",
      "   | > Step:67/68  GlobalStep:120680  TotalLoss:0.00258  PostnetLoss:0.00125  DecoderLoss:0.00133  StopLoss:0.03064  GradNorm:0.00234  GradNormST:0.00924  AvgTextLen:80.6  AvgSpecLen:246.5  StepTime:1.22  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:120681  AvgTotalLoss:0.04282  AvgPostnetLoss:0.00091  AvgDecoderLoss:0.00095  AvgStopLoss:0.04097  EpochTime:42.37  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.07828   PostnetLoss: 0.00552   DecoderLoss:0.00580  StopLoss: 0.06696  \n",
      "   | > TotalLoss: 0.08716   PostnetLoss: 0.00975   DecoderLoss:0.01015  StopLoss: 0.06725  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00091   Validation Loss: 0.00859\n",
      "\n",
      " > Epoch 749/1000\n",
      "   | > Step:8/68  GlobalStep:120690  TotalLoss:0.00141  PostnetLoss:0.00069  DecoderLoss:0.00072  StopLoss:0.04725  GradNorm:0.00375  GradNormST:0.01353  AvgTextLen:8.3  AvgSpecLen:44.0  StepTime:0.32  LR:0.000100\n",
      "   | > Step:18/68  GlobalStep:120700  TotalLoss:0.00156  PostnetLoss:0.00076  DecoderLoss:0.00080  StopLoss:0.04976  GradNorm:0.00317  GradNormST:0.01588  AvgTextLen:15.2  AvgSpecLen:64.5  StepTime:0.37  LR:0.000100\n",
      "   | > Step:28/68  GlobalStep:120710  TotalLoss:0.00177  PostnetLoss:0.00087  DecoderLoss:0.00091  StopLoss:0.04303  GradNorm:0.00296  GradNormST:0.01634  AvgTextLen:22.1  AvgSpecLen:85.4  StepTime:0.58  LR:0.000100\n",
      "   | > Step:38/68  GlobalStep:120720  TotalLoss:0.00190  PostnetLoss:0.00093  DecoderLoss:0.00098  StopLoss:0.03761  GradNorm:0.00296  GradNormST:0.00944  AvgTextLen:29.4  AvgSpecLen:104.1  StepTime:0.59  LR:0.000100\n",
      "   | > Step:48/68  GlobalStep:120730  TotalLoss:0.00210  PostnetLoss:0.00102  DecoderLoss:0.00107  StopLoss:0.02824  GradNorm:0.00294  GradNormST:0.00720  AvgTextLen:37.5  AvgSpecLen:123.8  StepTime:0.56  LR:0.000100\n",
      "   | > Step:58/68  GlobalStep:120740  TotalLoss:0.00224  PostnetLoss:0.00109  DecoderLoss:0.00115  StopLoss:0.05462  GradNorm:0.00269  GradNormST:0.01997  AvgTextLen:47.8  AvgSpecLen:158.0  StepTime:0.85  LR:0.000100\n",
      "   | > Step:68/68  GlobalStep:120750  TotalLoss:0.00257  PostnetLoss:0.00124  DecoderLoss:0.00132  StopLoss:0.02807  GradNorm:0.00265  GradNormST:0.01059  AvgTextLen:103.0  AvgSpecLen:308.9  StepTime:1.88  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:120750  AvgTotalLoss:0.04502  AvgPostnetLoss:0.00091  AvgDecoderLoss:0.00095  AvgStopLoss:0.04316  EpochTime:41.82  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.07870   PostnetLoss: 0.00545   DecoderLoss:0.00574  StopLoss: 0.06751  \n",
      "   | > TotalLoss: 0.08674   PostnetLoss: 0.00971   DecoderLoss:0.01010  StopLoss: 0.06694  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00091   Validation Loss: 0.00859\n",
      "\n",
      " > Epoch 750/1000\n",
      "   | > Step:9/68  GlobalStep:120760  TotalLoss:0.00136  PostnetLoss:0.00066  DecoderLoss:0.00069  StopLoss:0.04201  GradNorm:0.00348  GradNormST:0.01555  AvgTextLen:9.0  AvgSpecLen:42.1  StepTime:0.41  LR:0.000100\n",
      "   | > Step:19/68  GlobalStep:120770  TotalLoss:0.00158  PostnetLoss:0.00077  DecoderLoss:0.00081  StopLoss:0.06326  GradNorm:0.00323  GradNormST:0.02372  AvgTextLen:15.5  AvgSpecLen:63.2  StepTime:0.41  LR:0.000100\n",
      "   | > Step:29/68  GlobalStep:120780  TotalLoss:0.00176  PostnetLoss:0.00086  DecoderLoss:0.00090  StopLoss:0.04708  GradNorm:0.00320  GradNormST:0.01449  AvgTextLen:21.8  AvgSpecLen:83.4  StepTime:0.56  LR:0.000100\n",
      "   | > Step:39/68  GlobalStep:120790  TotalLoss:0.00195  PostnetLoss:0.00095  DecoderLoss:0.00100  StopLoss:0.03223  GradNorm:0.00329  GradNormST:0.01075  AvgTextLen:30.2  AvgSpecLen:110.4  StepTime:0.69  LR:0.000100\n",
      "   | > Step:49/68  GlobalStep:120800  TotalLoss:0.00212  PostnetLoss:0.00103  DecoderLoss:0.00109  StopLoss:0.04757  GradNorm:0.00285  GradNormST:0.01261  AvgTextLen:38.2  AvgSpecLen:127.7  StepTime:0.72  LR:0.000100\n",
      "   | > Step:59/68  GlobalStep:120810  TotalLoss:0.00224  PostnetLoss:0.00109  DecoderLoss:0.00115  StopLoss:0.05632  GradNorm:0.00275  GradNormST:0.02787  AvgTextLen:50.5  AvgSpecLen:155.9  StepTime:0.74  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:120819  AvgTotalLoss:0.04332  AvgPostnetLoss:0.00091  AvgDecoderLoss:0.00095  AvgStopLoss:0.04146  EpochTime:42.61  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.08051   PostnetLoss: 0.00550   DecoderLoss:0.00579  StopLoss: 0.06922  \n",
      "   | > TotalLoss: 0.08669   PostnetLoss: 0.00959   DecoderLoss:0.00999  StopLoss: 0.06711  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00091   Validation Loss: 0.00855\n",
      "\n",
      " > Epoch 751/1000\n",
      "   | > Step:0/68  GlobalStep:120820  TotalLoss:0.00137  PostnetLoss:0.00066  DecoderLoss:0.00071  StopLoss:0.05938  GradNorm:0.00600  GradNormST:0.01737  AvgTextLen:3.0  AvgSpecLen:25.3  StepTime:0.37  LR:0.000100\n",
      "   | > Step:10/68  GlobalStep:120830  TotalLoss:0.00146  PostnetLoss:0.00071  DecoderLoss:0.00075  StopLoss:0.03648  GradNorm:0.00409  GradNormST:0.02044  AvgTextLen:9.8  AvgSpecLen:48.6  StepTime:0.53  LR:0.000100\n",
      "   | > Step:20/68  GlobalStep:120840  TotalLoss:0.00164  PostnetLoss:0.00080  DecoderLoss:0.00084  StopLoss:0.06530  GradNorm:0.00399  GradNormST:0.02815  AvgTextLen:16.4  AvgSpecLen:64.8  StepTime:0.49  LR:0.000100\n",
      "   | > Step:30/68  GlobalStep:120850  TotalLoss:0.00180  PostnetLoss:0.00088  DecoderLoss:0.00092  StopLoss:0.04078  GradNorm:0.00354  GradNormST:0.00921  AvgTextLen:22.9  AvgSpecLen:85.8  StepTime:0.60  LR:0.000100\n",
      "   | > Step:40/68  GlobalStep:120860  TotalLoss:0.00195  PostnetLoss:0.00095  DecoderLoss:0.00100  StopLoss:0.03543  GradNorm:0.00328  GradNormST:0.00871  AvgTextLen:30.9  AvgSpecLen:105.3  StepTime:0.49  LR:0.000100\n",
      "   | > Step:50/68  GlobalStep:120870  TotalLoss:0.00210  PostnetLoss:0.00102  DecoderLoss:0.00108  StopLoss:0.03121  GradNorm:0.00291  GradNormST:0.00729  AvgTextLen:38.8  AvgSpecLen:132.0  StepTime:0.78  LR:0.000100\n",
      "   | > Step:60/68  GlobalStep:120880  TotalLoss:0.00233  PostnetLoss:0.00113  DecoderLoss:0.00120  StopLoss:0.03412  GradNorm:0.00281  GradNormST:0.01294  AvgTextLen:52.0  AvgSpecLen:172.7  StepTime:0.82  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:120888  AvgTotalLoss:0.04274  AvgPostnetLoss:0.00091  AvgDecoderLoss:0.00095  AvgStopLoss:0.04088  EpochTime:42.21  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.08422   PostnetLoss: 0.00543   DecoderLoss:0.00571  StopLoss: 0.07308  \n",
      "   | > TotalLoss: 0.09295   PostnetLoss: 0.00973   DecoderLoss:0.01012  StopLoss: 0.07310  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00091   Validation Loss: 0.00852\n",
      "\n",
      " > Epoch 752/1000\n",
      "   | > Step:1/68  GlobalStep:120890  TotalLoss:0.00135  PostnetLoss:0.00066  DecoderLoss:0.00069  StopLoss:0.03717  GradNorm:0.00561  GradNormST:0.01529  AvgTextLen:3.7  AvgSpecLen:28.6  StepTime:0.38  LR:0.000100\n",
      "   | > Step:11/68  GlobalStep:120900  TotalLoss:0.00145  PostnetLoss:0.00071  DecoderLoss:0.00074  StopLoss:0.03539  GradNorm:0.00377  GradNormST:0.00889  AvgTextLen:10.3  AvgSpecLen:49.4  StepTime:0.41  LR:0.000100\n",
      "   | > Step:21/68  GlobalStep:120910  TotalLoss:0.00166  PostnetLoss:0.00081  DecoderLoss:0.00085  StopLoss:0.03879  GradNorm:0.00351  GradNormST:0.01317  AvgTextLen:16.6  AvgSpecLen:66.3  StepTime:0.34  LR:0.000100\n",
      "   | > Step:31/68  GlobalStep:120920  TotalLoss:0.00178  PostnetLoss:0.00087  DecoderLoss:0.00091  StopLoss:0.04537  GradNorm:0.00355  GradNormST:0.01821  AvgTextLen:24.1  AvgSpecLen:82.7  StepTime:0.53  LR:0.000100\n",
      "   | > Step:41/68  GlobalStep:120930  TotalLoss:0.00200  PostnetLoss:0.00098  DecoderLoss:0.00103  StopLoss:0.02847  GradNorm:0.00355  GradNormST:0.00878  AvgTextLen:32.3  AvgSpecLen:109.4  StepTime:0.63  LR:0.000100\n",
      "   | > Step:51/68  GlobalStep:120940  TotalLoss:0.00208  PostnetLoss:0.00101  DecoderLoss:0.00107  StopLoss:0.03729  GradNorm:0.00326  GradNormST:0.01531  AvgTextLen:39.2  AvgSpecLen:126.9  StepTime:0.62  LR:0.000100\n",
      "   | > Step:61/68  GlobalStep:120950  TotalLoss:0.00229  PostnetLoss:0.00111  DecoderLoss:0.00118  StopLoss:0.04361  GradNorm:0.00282  GradNormST:0.01158  AvgTextLen:54.6  AvgSpecLen:165.5  StepTime:0.92  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:120957  AvgTotalLoss:0.04422  AvgPostnetLoss:0.00091  AvgDecoderLoss:0.00096  AvgStopLoss:0.04236  EpochTime:41.71  AvgStepTime:0.60\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.08030   PostnetLoss: 0.00551   DecoderLoss:0.00579  StopLoss: 0.06900  \n",
      "   | > TotalLoss: 0.08519   PostnetLoss: 0.00951   DecoderLoss:0.00991  StopLoss: 0.06577  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00091   Validation Loss: 0.00837\n",
      "\n",
      " > Epoch 753/1000\n",
      "   | > Step:2/68  GlobalStep:120960  TotalLoss:0.00124  PostnetLoss:0.00061  DecoderLoss:0.00064  StopLoss:0.03411  GradNorm:0.00403  GradNormST:0.01069  AvgTextLen:4.0  AvgSpecLen:31.3  StepTime:0.43  LR:0.000100\n",
      "   | > Step:12/68  GlobalStep:120970  TotalLoss:0.00142  PostnetLoss:0.00069  DecoderLoss:0.00073  StopLoss:0.07171  GradNorm:0.00376  GradNormST:0.02844  AvgTextLen:11.0  AvgSpecLen:49.3  StepTime:0.44  LR:0.000100\n",
      "   | > Step:22/68  GlobalStep:120980  TotalLoss:0.00170  PostnetLoss:0.00083  DecoderLoss:0.00087  StopLoss:0.05098  GradNorm:0.00320  GradNormST:0.01359  AvgTextLen:18.0  AvgSpecLen:72.0  StepTime:0.46  LR:0.000100\n",
      "   | > Step:32/68  GlobalStep:120990  TotalLoss:0.00181  PostnetLoss:0.00088  DecoderLoss:0.00093  StopLoss:0.07701  GradNorm:0.00325  GradNormST:0.02127  AvgTextLen:24.4  AvgSpecLen:91.5  StepTime:0.56  LR:0.000100\n",
      "   | > Step:42/68  GlobalStep:121000  TotalLoss:0.00199  PostnetLoss:0.00097  DecoderLoss:0.00102  StopLoss:0.05031  GradNorm:0.00325  GradNormST:0.02041  AvgTextLen:33.1  AvgSpecLen:109.7  StepTime:0.52  LR:0.000100\n",
      " | | > Checkpoint saving : /vm/TTS/models/cartman_models/mozilla-no-loc-fattn-stopnet-sigmoid-loss_masking-August-15-2019_11+26AM-de6aeb9/checkpoint_121000.pth.tar\n",
      "warning: audio amplitude out of range, auto clipped.\n",
      "   | > Step:52/68  GlobalStep:121010  TotalLoss:0.00216  PostnetLoss:0.00106  DecoderLoss:0.00111  StopLoss:0.05670  GradNorm:0.00344  GradNormST:0.01352  AvgTextLen:42.1  AvgSpecLen:137.5  StepTime:0.87  LR:0.000100\n",
      "   | > Step:62/68  GlobalStep:121020  TotalLoss:0.00239  PostnetLoss:0.00116  DecoderLoss:0.00123  StopLoss:0.03502  GradNorm:0.00334  GradNormST:0.00919  AvgTextLen:54.0  AvgSpecLen:174.7  StepTime:0.76  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:121026  AvgTotalLoss:0.04794  AvgPostnetLoss:0.00091  AvgDecoderLoss:0.00096  AvgStopLoss:0.04608  EpochTime:42.58  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.08206   PostnetLoss: 0.00555   DecoderLoss:0.00583  StopLoss: 0.07068  \n",
      "   | > TotalLoss: 0.08831   PostnetLoss: 0.00988   DecoderLoss:0.01025  StopLoss: 0.06819  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00091   Validation Loss: 0.00874\n",
      "\n",
      " > Epoch 754/1000\n",
      "   | > Step:3/68  GlobalStep:121030  TotalLoss:0.00127  PostnetLoss:0.00062  DecoderLoss:0.00065  StopLoss:0.05459  GradNorm:0.00364  GradNormST:0.01676  AvgTextLen:4.7  AvgSpecLen:29.7  StepTime:0.23  LR:0.000100\n",
      "   | > Step:13/68  GlobalStep:121040  TotalLoss:0.00144  PostnetLoss:0.00071  DecoderLoss:0.00074  StopLoss:0.05354  GradNorm:0.00312  GradNormST:0.01448  AvgTextLen:11.5  AvgSpecLen:55.2  StepTime:0.42  LR:0.000100\n",
      "   | > Step:23/68  GlobalStep:121050  TotalLoss:0.00171  PostnetLoss:0.00083  DecoderLoss:0.00087  StopLoss:0.04697  GradNorm:0.00341  GradNormST:0.01193  AvgTextLen:18.3  AvgSpecLen:72.2  StepTime:0.43  LR:0.000100\n",
      "   | > Step:33/68  GlobalStep:121060  TotalLoss:0.00186  PostnetLoss:0.00091  DecoderLoss:0.00095  StopLoss:0.04178  GradNorm:0.00339  GradNormST:0.01401  AvgTextLen:25.9  AvgSpecLen:88.9  StepTime:0.47  LR:0.000100\n",
      "   | > Step:43/68  GlobalStep:121070  TotalLoss:0.00198  PostnetLoss:0.00096  DecoderLoss:0.00101  StopLoss:0.04109  GradNorm:0.00324  GradNormST:0.01228  AvgTextLen:33.4  AvgSpecLen:115.7  StepTime:0.51  LR:0.000100\n",
      "   | > Step:53/68  GlobalStep:121080  TotalLoss:0.00213  PostnetLoss:0.00104  DecoderLoss:0.00110  StopLoss:0.03487  GradNorm:0.00309  GradNormST:0.01158  AvgTextLen:42.4  AvgSpecLen:137.6  StepTime:0.81  LR:0.000100\n",
      "   | > Step:63/68  GlobalStep:121090  TotalLoss:0.00240  PostnetLoss:0.00117  DecoderLoss:0.00124  StopLoss:0.03057  GradNorm:0.00272  GradNormST:0.00753  AvgTextLen:58.6  AvgSpecLen:185.1  StepTime:0.90  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:121095  AvgTotalLoss:0.04599  AvgPostnetLoss:0.00091  AvgDecoderLoss:0.00095  AvgStopLoss:0.04413  EpochTime:42.28  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.08285   PostnetLoss: 0.00546   DecoderLoss:0.00575  StopLoss: 0.07165  \n",
      "   | > TotalLoss: 0.08678   PostnetLoss: 0.00963   DecoderLoss:0.01003  StopLoss: 0.06711  \n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00091   Validation Loss: 0.00858\n",
      "\n",
      " > Epoch 755/1000\n",
      "   | > Step:4/68  GlobalStep:121100  TotalLoss:0.00130  PostnetLoss:0.00063  DecoderLoss:0.00067  StopLoss:0.06697  GradNorm:0.00361  GradNormST:0.02720  AvgTextLen:4.8  AvgSpecLen:31.1  StepTime:0.27  LR:0.000100\n",
      "   | > Step:14/68  GlobalStep:121110  TotalLoss:0.00150  PostnetLoss:0.00073  DecoderLoss:0.00077  StopLoss:0.03737  GradNorm:0.00311  GradNormST:0.01157  AvgTextLen:12.2  AvgSpecLen:52.0  StepTime:0.28  LR:0.000100\n",
      "   | > Step:24/68  GlobalStep:121120  TotalLoss:0.00176  PostnetLoss:0.00086  DecoderLoss:0.00090  StopLoss:0.03972  GradNorm:0.00314  GradNormST:0.01034  AvgTextLen:19.2  AvgSpecLen:79.0  StepTime:0.52  LR:0.000100\n",
      "   | > Step:34/68  GlobalStep:121130  TotalLoss:0.00188  PostnetLoss:0.00092  DecoderLoss:0.00097  StopLoss:0.03099  GradNorm:0.00333  GradNormST:0.00753  AvgTextLen:26.2  AvgSpecLen:95.9  StepTime:0.57  LR:0.000100\n",
      "   | > Step:44/68  GlobalStep:121140  TotalLoss:0.00206  PostnetLoss:0.00100  DecoderLoss:0.00106  StopLoss:0.02889  GradNorm:0.00343  GradNormST:0.01191  AvgTextLen:33.8  AvgSpecLen:116.3  StepTime:0.69  LR:0.000100\n",
      "   | > Step:54/68  GlobalStep:121150  TotalLoss:0.00217  PostnetLoss:0.00106  DecoderLoss:0.00111  StopLoss:0.03842  GradNorm:0.00342  GradNormST:0.00861  AvgTextLen:42.2  AvgSpecLen:144.9  StepTime:0.81  LR:0.000100\n",
      "   | > Step:64/68  GlobalStep:121160  TotalLoss:0.00225  PostnetLoss:0.00109  DecoderLoss:0.00116  StopLoss:0.03381  GradNorm:0.00274  GradNormST:0.00901  AvgTextLen:60.6  AvgSpecLen:177.6  StepTime:0.80  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:121164  AvgTotalLoss:0.04490  AvgPostnetLoss:0.00091  AvgDecoderLoss:0.00095  AvgStopLoss:0.04305  EpochTime:42.98  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.08622   PostnetLoss: 0.00563   DecoderLoss:0.00592  StopLoss: 0.07467  \n",
      "   | > TotalLoss: 0.08808   PostnetLoss: 0.00986   DecoderLoss:0.01026  StopLoss: 0.06796  \n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00091   Validation Loss: 0.00880\n",
      "\n",
      " > Epoch 756/1000\n",
      "   | > Step:5/68  GlobalStep:121170  TotalLoss:0.00133  PostnetLoss:0.00065  DecoderLoss:0.00068  StopLoss:0.05321  GradNorm:0.00355  GradNormST:0.01889  AvgTextLen:5.9  AvgSpecLen:33.2  StepTime:0.29  LR:0.000100\n",
      "   | > Step:15/68  GlobalStep:121180  TotalLoss:0.00157  PostnetLoss:0.00077  DecoderLoss:0.00080  StopLoss:0.05261  GradNorm:0.00320  GradNormST:0.01544  AvgTextLen:12.8  AvgSpecLen:59.1  StepTime:0.52  LR:0.000100\n",
      "   | > Step:25/68  GlobalStep:121190  TotalLoss:0.00170  PostnetLoss:0.00083  DecoderLoss:0.00087  StopLoss:0.03425  GradNorm:0.00312  GradNormST:0.00731  AvgTextLen:19.8  AvgSpecLen:73.3  StepTime:0.36  LR:0.000100\n",
      "   | > Step:35/68  GlobalStep:121200  TotalLoss:0.00192  PostnetLoss:0.00094  DecoderLoss:0.00098  StopLoss:0.04021  GradNorm:0.00293  GradNormST:0.01816  AvgTextLen:27.1  AvgSpecLen:104.7  StepTime:0.63  LR:0.000100\n",
      "   | > Step:45/68  GlobalStep:121210  TotalLoss:0.00195  PostnetLoss:0.00095  DecoderLoss:0.00100  StopLoss:0.05012  GradNorm:0.00287  GradNormST:0.02050  AvgTextLen:34.4  AvgSpecLen:118.6  StepTime:0.63  LR:0.000100\n",
      "   | > Step:55/68  GlobalStep:121220  TotalLoss:0.00224  PostnetLoss:0.00109  DecoderLoss:0.00115  StopLoss:0.03836  GradNorm:0.00335  GradNormST:0.01069  AvgTextLen:43.9  AvgSpecLen:145.2  StepTime:0.70  LR:0.000100\n",
      "   | > Step:65/68  GlobalStep:121230  TotalLoss:0.00247  PostnetLoss:0.00120  DecoderLoss:0.00127  StopLoss:0.05330  GradNorm:0.00293  GradNormST:0.01856  AvgTextLen:64.2  AvgSpecLen:207.4  StepTime:1.19  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:121233  AvgTotalLoss:0.04470  AvgPostnetLoss:0.00090  AvgDecoderLoss:0.00095  AvgStopLoss:0.04285  EpochTime:41.52  AvgStepTime:0.60\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.08576   PostnetLoss: 0.00550   DecoderLoss:0.00579  StopLoss: 0.07447  \n",
      "   | > TotalLoss: 0.08733   PostnetLoss: 0.00992   DecoderLoss:0.01032  StopLoss: 0.06709  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00090   Validation Loss: 0.00870\n",
      "\n",
      " > Epoch 757/1000\n",
      "   | > Step:6/68  GlobalStep:121240  TotalLoss:0.00127  PostnetLoss:0.00062  DecoderLoss:0.00065  StopLoss:0.03561  GradNorm:0.00336  GradNormST:0.01345  AvgTextLen:6.0  AvgSpecLen:34.8  StepTime:0.29  LR:0.000100\n",
      "   | > Step:16/68  GlobalStep:121250  TotalLoss:0.00158  PostnetLoss:0.00077  DecoderLoss:0.00081  StopLoss:0.03272  GradNorm:0.00321  GradNormST:0.01204  AvgTextLen:13.8  AvgSpecLen:60.6  StepTime:0.39  LR:0.000100\n",
      "   | > Step:26/68  GlobalStep:121260  TotalLoss:0.00176  PostnetLoss:0.00086  DecoderLoss:0.00090  StopLoss:0.05194  GradNorm:0.00314  GradNormST:0.01469  AvgTextLen:20.3  AvgSpecLen:77.5  StepTime:0.41  LR:0.000100\n",
      "   | > Step:36/68  GlobalStep:121270  TotalLoss:0.00190  PostnetLoss:0.00093  DecoderLoss:0.00098  StopLoss:0.04721  GradNorm:0.00316  GradNormST:0.01010  AvgTextLen:28.5  AvgSpecLen:100.2  StepTime:0.60  LR:0.000100\n",
      "   | > Step:46/68  GlobalStep:121280  TotalLoss:0.00205  PostnetLoss:0.00100  DecoderLoss:0.00105  StopLoss:0.04365  GradNorm:0.00328  GradNormST:0.01289  AvgTextLen:35.4  AvgSpecLen:122.6  StepTime:0.85  LR:0.000100\n",
      "   | > Step:56/68  GlobalStep:121290  TotalLoss:0.00221  PostnetLoss:0.00108  DecoderLoss:0.00114  StopLoss:0.03194  GradNorm:0.00345  GradNormST:0.01511  AvgTextLen:45.8  AvgSpecLen:145.4  StepTime:1.04  LR:0.000100\n",
      "   | > Step:66/68  GlobalStep:121300  TotalLoss:0.00257  PostnetLoss:0.00125  DecoderLoss:0.00133  StopLoss:0.03380  GradNorm:0.00296  GradNormST:0.01693  AvgTextLen:71.2  AvgSpecLen:220.8  StepTime:1.03  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:121302  AvgTotalLoss:0.04656  AvgPostnetLoss:0.00090  AvgDecoderLoss:0.00095  AvgStopLoss:0.04470  EpochTime:42.17  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.08642   PostnetLoss: 0.00549   DecoderLoss:0.00577  StopLoss: 0.07516  \n",
      "   | > TotalLoss: 0.08916   PostnetLoss: 0.00990   DecoderLoss:0.01030  StopLoss: 0.06896  \n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00090   Validation Loss: 0.00877\n",
      "\n",
      " > Epoch 758/1000\n",
      "   | > Step:7/68  GlobalStep:121310  TotalLoss:0.00139  PostnetLoss:0.00068  DecoderLoss:0.00071  StopLoss:0.03609  GradNorm:0.00348  GradNormST:0.01332  AvgTextLen:7.4  AvgSpecLen:44.6  StepTime:0.30  LR:0.000100\n",
      "   | > Step:17/68  GlobalStep:121320  TotalLoss:0.00156  PostnetLoss:0.00076  DecoderLoss:0.00080  StopLoss:0.04950  GradNorm:0.00311  GradNormST:0.01166  AvgTextLen:14.7  AvgSpecLen:66.0  StepTime:0.47  LR:0.000100\n",
      "   | > Step:27/68  GlobalStep:121330  TotalLoss:0.00178  PostnetLoss:0.00087  DecoderLoss:0.00091  StopLoss:0.06629  GradNorm:0.00309  GradNormST:0.01832  AvgTextLen:21.0  AvgSpecLen:80.2  StepTime:0.68  LR:0.000100\n",
      "   | > Step:37/68  GlobalStep:121340  TotalLoss:0.00192  PostnetLoss:0.00093  DecoderLoss:0.00098  StopLoss:0.04542  GradNorm:0.00293  GradNormST:0.01057  AvgTextLen:29.3  AvgSpecLen:102.4  StepTime:0.59  LR:0.000100\n",
      "   | > Step:47/68  GlobalStep:121350  TotalLoss:0.00209  PostnetLoss:0.00102  DecoderLoss:0.00107  StopLoss:0.03746  GradNorm:0.00337  GradNormST:0.00940  AvgTextLen:36.7  AvgSpecLen:126.9  StepTime:0.62  LR:0.000100\n",
      "   | > Step:57/68  GlobalStep:121360  TotalLoss:0.00226  PostnetLoss:0.00110  DecoderLoss:0.00116  StopLoss:0.03251  GradNorm:0.00320  GradNormST:0.01064  AvgTextLen:47.1  AvgSpecLen:161.0  StepTime:0.70  LR:0.000100\n",
      "   | > Step:67/68  GlobalStep:121370  TotalLoss:0.00259  PostnetLoss:0.00125  DecoderLoss:0.00133  StopLoss:0.02996  GradNorm:0.00252  GradNormST:0.01051  AvgTextLen:80.6  AvgSpecLen:246.5  StepTime:1.23  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:121371  AvgTotalLoss:0.04803  AvgPostnetLoss:0.00090  AvgDecoderLoss:0.00095  AvgStopLoss:0.04617  EpochTime:42.49  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.08180   PostnetLoss: 0.00536   DecoderLoss:0.00565  StopLoss: 0.07080  \n",
      "   | > TotalLoss: 0.09107   PostnetLoss: 0.01012   DecoderLoss:0.01053  StopLoss: 0.07042  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00090   Validation Loss: 0.00883\n",
      "\n",
      " > Epoch 759/1000\n",
      "   | > Step:8/68  GlobalStep:121380  TotalLoss:0.00141  PostnetLoss:0.00069  DecoderLoss:0.00072  StopLoss:0.03909  GradNorm:0.00361  GradNormST:0.01205  AvgTextLen:8.3  AvgSpecLen:44.0  StepTime:0.33  LR:0.000100\n",
      "   | > Step:18/68  GlobalStep:121390  TotalLoss:0.00156  PostnetLoss:0.00076  DecoderLoss:0.00080  StopLoss:0.04148  GradNorm:0.00310  GradNormST:0.01406  AvgTextLen:15.2  AvgSpecLen:64.5  StepTime:0.43  LR:0.000100\n",
      "   | > Step:28/68  GlobalStep:121400  TotalLoss:0.00180  PostnetLoss:0.00088  DecoderLoss:0.00092  StopLoss:0.04668  GradNorm:0.00323  GradNormST:0.01614  AvgTextLen:22.1  AvgSpecLen:85.4  StepTime:0.55  LR:0.000100\n",
      "   | > Step:38/68  GlobalStep:121410  TotalLoss:0.00193  PostnetLoss:0.00094  DecoderLoss:0.00099  StopLoss:0.04311  GradNorm:0.00302  GradNormST:0.01298  AvgTextLen:29.4  AvgSpecLen:104.1  StepTime:0.65  LR:0.000100\n",
      "   | > Step:48/68  GlobalStep:121420  TotalLoss:0.00209  PostnetLoss:0.00102  DecoderLoss:0.00107  StopLoss:0.04226  GradNorm:0.00304  GradNormST:0.01753  AvgTextLen:37.5  AvgSpecLen:123.8  StepTime:0.70  LR:0.000100\n",
      "   | > Step:58/68  GlobalStep:121430  TotalLoss:0.00229  PostnetLoss:0.00111  DecoderLoss:0.00117  StopLoss:0.04693  GradNorm:0.00336  GradNormST:0.01228  AvgTextLen:47.8  AvgSpecLen:158.0  StepTime:0.83  LR:0.000100\n",
      "   | > Step:68/68  GlobalStep:121440  TotalLoss:0.00255  PostnetLoss:0.00124  DecoderLoss:0.00131  StopLoss:0.02882  GradNorm:0.81256  GradNormST:0.00844  AvgTextLen:103.0  AvgSpecLen:308.9  StepTime:1.87  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:121440  AvgTotalLoss:0.04816  AvgPostnetLoss:0.00091  AvgDecoderLoss:0.00096  AvgStopLoss:0.04629  EpochTime:42.28  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.07828   PostnetLoss: 0.00545   DecoderLoss:0.00575  StopLoss: 0.06707  \n",
      "   | > TotalLoss: 0.08897   PostnetLoss: 0.01000   DecoderLoss:0.01041  StopLoss: 0.06855  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00091   Validation Loss: 0.00889\n",
      "\n",
      " > Epoch 760/1000\n",
      "   | > Step:9/68  GlobalStep:121450  TotalLoss:0.00198  PostnetLoss:0.00097  DecoderLoss:0.00101  StopLoss:0.08991  GradNorm:0.01159  GradNormST:0.04926  AvgTextLen:9.0  AvgSpecLen:42.1  StepTime:0.43  LR:0.000100\n",
      "   | > Step:19/68  GlobalStep:121460  TotalLoss:0.00201  PostnetLoss:0.00098  DecoderLoss:0.00103  StopLoss:0.06847  GradNorm:0.00511  GradNormST:0.01752  AvgTextLen:15.5  AvgSpecLen:63.2  StepTime:0.41  LR:0.000100\n",
      "   | > Step:29/68  GlobalStep:121470  TotalLoss:0.00240  PostnetLoss:0.00117  DecoderLoss:0.00123  StopLoss:0.08300  GradNorm:0.00645  GradNormST:0.03389  AvgTextLen:21.8  AvgSpecLen:83.4  StepTime:0.57  LR:0.000100\n",
      "   | > Step:39/68  GlobalStep:121480  TotalLoss:0.00245  PostnetLoss:0.00119  DecoderLoss:0.00126  StopLoss:0.05484  GradNorm:0.00493  GradNormST:0.02824  AvgTextLen:30.2  AvgSpecLen:110.4  StepTime:0.63  LR:0.000100\n",
      "   | > Step:49/68  GlobalStep:121490  TotalLoss:0.00255  PostnetLoss:0.00124  DecoderLoss:0.00131  StopLoss:0.05366  GradNorm:0.00699  GradNormST:0.01797  AvgTextLen:38.2  AvgSpecLen:127.7  StepTime:0.71  LR:0.000100\n",
      "   | > Step:59/68  GlobalStep:121500  TotalLoss:0.00337  PostnetLoss:0.00163  DecoderLoss:0.00173  StopLoss:0.05741  GradNorm:0.01041  GradNormST:0.02155  AvgTextLen:50.5  AvgSpecLen:155.9  StepTime:0.86  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:121509  AvgTotalLoss:0.07253  AvgPostnetLoss:0.00118  AvgDecoderLoss:0.00125  AvgStopLoss:0.07010  EpochTime:42.28  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.06551   PostnetLoss: 0.00495   DecoderLoss:0.00532  StopLoss: 0.05524  \n",
      "   | > TotalLoss: 0.07999   PostnetLoss: 0.00841   DecoderLoss:0.00895  StopLoss: 0.06262  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00118   Validation Loss: 0.00753\n",
      "\n",
      " > Epoch 761/1000\n",
      "   | > Step:0/68  GlobalStep:121510  TotalLoss:0.00170  PostnetLoss:0.00082  DecoderLoss:0.00088  StopLoss:0.08634  GradNorm:0.00893  GradNormST:0.03968  AvgTextLen:3.0  AvgSpecLen:25.3  StepTime:0.45  LR:0.000100\n",
      "   | > Step:10/68  GlobalStep:121520  TotalLoss:0.00200  PostnetLoss:0.00097  DecoderLoss:0.00102  StopLoss:0.05269  GradNorm:0.01502  GradNormST:0.02873  AvgTextLen:9.8  AvgSpecLen:48.6  StepTime:0.47  LR:0.000100\n",
      "   | > Step:20/68  GlobalStep:121530  TotalLoss:0.00205  PostnetLoss:0.00100  DecoderLoss:0.00105  StopLoss:0.05385  GradNorm:0.00566  GradNormST:0.03441  AvgTextLen:16.4  AvgSpecLen:64.8  StepTime:0.39  LR:0.000100\n",
      "   | > Step:30/68  GlobalStep:121540  TotalLoss:0.00226  PostnetLoss:0.00110  DecoderLoss:0.00116  StopLoss:0.07724  GradNorm:0.00657  GradNormST:0.04015  AvgTextLen:22.9  AvgSpecLen:85.8  StepTime:0.58  LR:0.000100\n",
      "   | > Step:40/68  GlobalStep:121550  TotalLoss:0.00223  PostnetLoss:0.00108  DecoderLoss:0.00114  StopLoss:0.06333  GradNorm:0.00528  GradNormST:0.02753  AvgTextLen:30.9  AvgSpecLen:105.3  StepTime:0.59  LR:0.000100\n",
      "   | > Step:50/68  GlobalStep:121560  TotalLoss:0.00246  PostnetLoss:0.00119  DecoderLoss:0.00127  StopLoss:0.07747  GradNorm:0.00400  GradNormST:0.03516  AvgTextLen:38.8  AvgSpecLen:132.0  StepTime:0.67  LR:0.000100\n",
      "   | > Step:60/68  GlobalStep:121570  TotalLoss:0.00282  PostnetLoss:0.00137  DecoderLoss:0.00145  StopLoss:0.04534  GradNorm:0.00507  GradNormST:0.01472  AvgTextLen:52.0  AvgSpecLen:172.7  StepTime:0.78  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:121578  AvgTotalLoss:0.06742  AvgPostnetLoss:0.00110  AvgDecoderLoss:0.00116  AvgStopLoss:0.06516  EpochTime:42.56  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.06630   PostnetLoss: 0.00517   DecoderLoss:0.00550  StopLoss: 0.05563  \n",
      "   | > TotalLoss: 0.07585   PostnetLoss: 0.00855   DecoderLoss:0.00899  StopLoss: 0.05830  \n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00110   Validation Loss: 0.00763\n",
      "\n",
      " > Epoch 762/1000\n",
      "   | > Step:1/68  GlobalStep:121580  TotalLoss:0.00144  PostnetLoss:0.00070  DecoderLoss:0.00074  StopLoss:0.05150  GradNorm:0.00597  GradNormST:0.01702  AvgTextLen:3.7  AvgSpecLen:28.6  StepTime:0.20  LR:0.000100\n",
      "   | > Step:11/68  GlobalStep:121590  TotalLoss:0.00158  PostnetLoss:0.00078  DecoderLoss:0.00081  StopLoss:0.09480  GradNorm:0.00403  GradNormST:0.04163  AvgTextLen:10.3  AvgSpecLen:49.4  StepTime:0.30  LR:0.000100\n",
      "   | > Step:21/68  GlobalStep:121600  TotalLoss:0.00185  PostnetLoss:0.00091  DecoderLoss:0.00095  StopLoss:0.04940  GradNorm:0.00391  GradNormST:0.01737  AvgTextLen:16.6  AvgSpecLen:66.3  StepTime:0.44  LR:0.000100\n",
      "   | > Step:31/68  GlobalStep:121610  TotalLoss:0.00203  PostnetLoss:0.00099  DecoderLoss:0.00104  StopLoss:0.05831  GradNorm:0.00464  GradNormST:0.02302  AvgTextLen:24.1  AvgSpecLen:82.7  StepTime:0.56  LR:0.000100\n",
      "   | > Step:41/68  GlobalStep:121620  TotalLoss:0.00225  PostnetLoss:0.00110  DecoderLoss:0.00116  StopLoss:0.03378  GradNorm:0.00359  GradNormST:0.01029  AvgTextLen:32.3  AvgSpecLen:109.4  StepTime:0.64  LR:0.000100\n",
      "   | > Step:51/68  GlobalStep:121630  TotalLoss:0.00244  PostnetLoss:0.00118  DecoderLoss:0.00125  StopLoss:0.04915  GradNorm:0.00358  GradNormST:0.01069  AvgTextLen:39.2  AvgSpecLen:126.9  StepTime:0.72  LR:0.000100\n",
      "   | > Step:61/68  GlobalStep:121640  TotalLoss:0.00257  PostnetLoss:0.00124  DecoderLoss:0.00132  StopLoss:0.05291  GradNorm:0.00654  GradNormST:0.01610  AvgTextLen:54.6  AvgSpecLen:165.5  StepTime:0.89  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:121647  AvgTotalLoss:0.05845  AvgPostnetLoss:0.00102  AvgDecoderLoss:0.00108  AvgStopLoss:0.05635  EpochTime:42.16  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.06839   PostnetLoss: 0.00522   DecoderLoss:0.00554  StopLoss: 0.05763  \n",
      "   | > TotalLoss: 0.08282   PostnetLoss: 0.00861   DecoderLoss:0.00904  StopLoss: 0.06517  \n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00102   Validation Loss: 0.00769\n",
      "\n",
      " > Epoch 763/1000\n",
      "   | > Step:2/68  GlobalStep:121650  TotalLoss:0.00132  PostnetLoss:0.00064  DecoderLoss:0.00068  StopLoss:0.06299  GradNorm:0.00500  GradNormST:0.02335  AvgTextLen:4.0  AvgSpecLen:31.3  StepTime:0.46  LR:0.000100\n",
      "   | > Step:12/68  GlobalStep:121660  TotalLoss:0.00157  PostnetLoss:0.00077  DecoderLoss:0.00080  StopLoss:0.07836  GradNorm:0.00390  GradNormST:0.03793  AvgTextLen:11.0  AvgSpecLen:49.3  StepTime:0.43  LR:0.000100\n",
      "   | > Step:22/68  GlobalStep:121670  TotalLoss:0.00181  PostnetLoss:0.00088  DecoderLoss:0.00093  StopLoss:0.05015  GradNorm:0.00371  GradNormST:0.01518  AvgTextLen:18.0  AvgSpecLen:72.0  StepTime:0.36  LR:0.000100\n",
      "   | > Step:32/68  GlobalStep:121680  TotalLoss:0.00197  PostnetLoss:0.00096  DecoderLoss:0.00101  StopLoss:0.08869  GradNorm:0.00336  GradNormST:0.03100  AvgTextLen:24.4  AvgSpecLen:91.5  StepTime:0.51  LR:0.000100\n",
      "   | > Step:42/68  GlobalStep:121690  TotalLoss:0.00212  PostnetLoss:0.00103  DecoderLoss:0.00109  StopLoss:0.06312  GradNorm:0.00306  GradNormST:0.02566  AvgTextLen:33.1  AvgSpecLen:109.7  StepTime:0.57  LR:0.000100\n",
      "   | > Step:52/68  GlobalStep:121700  TotalLoss:0.00239  PostnetLoss:0.00116  DecoderLoss:0.00123  StopLoss:0.07217  GradNorm:0.00335  GradNormST:0.02366  AvgTextLen:42.1  AvgSpecLen:137.5  StepTime:0.85  LR:0.000100\n",
      "   | > Step:62/68  GlobalStep:121710  TotalLoss:0.00254  PostnetLoss:0.00123  DecoderLoss:0.00131  StopLoss:0.05620  GradNorm:0.00373  GradNormST:0.01837  AvgTextLen:54.0  AvgSpecLen:174.7  StepTime:0.76  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:121716  AvgTotalLoss:0.05824  AvgPostnetLoss:0.00098  AvgDecoderLoss:0.00104  AvgStopLoss:0.05622  EpochTime:42.74  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.06613   PostnetLoss: 0.00524   DecoderLoss:0.00554  StopLoss: 0.05534  \n",
      "   | > TotalLoss: 0.08530   PostnetLoss: 0.00885   DecoderLoss:0.00927  StopLoss: 0.06717  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00098   Validation Loss: 0.00778\n",
      "\n",
      " > Epoch 764/1000\n",
      "   | > Step:3/68  GlobalStep:121720  TotalLoss:0.00135  PostnetLoss:0.00066  DecoderLoss:0.00069  StopLoss:0.05919  GradNorm:0.00523  GradNormST:0.01248  AvgTextLen:4.7  AvgSpecLen:29.7  StepTime:0.32  LR:0.000100\n",
      "   | > Step:13/68  GlobalStep:121730  TotalLoss:0.00149  PostnetLoss:0.00073  DecoderLoss:0.00076  StopLoss:0.07148  GradNorm:0.00335  GradNormST:0.01923  AvgTextLen:11.5  AvgSpecLen:55.2  StepTime:0.33  LR:0.000100\n",
      "   | > Step:23/68  GlobalStep:121740  TotalLoss:0.00177  PostnetLoss:0.00087  DecoderLoss:0.00091  StopLoss:0.05254  GradNorm:0.00345  GradNormST:0.01445  AvgTextLen:18.3  AvgSpecLen:72.2  StepTime:0.42  LR:0.000100\n",
      "   | > Step:33/68  GlobalStep:121750  TotalLoss:0.00199  PostnetLoss:0.00097  DecoderLoss:0.00102  StopLoss:0.05756  GradNorm:0.00322  GradNormST:0.01674  AvgTextLen:25.9  AvgSpecLen:88.9  StepTime:0.46  LR:0.000100\n",
      "   | > Step:43/68  GlobalStep:121760  TotalLoss:0.00212  PostnetLoss:0.00103  DecoderLoss:0.00109  StopLoss:0.06068  GradNorm:0.00309  GradNormST:0.02080  AvgTextLen:33.4  AvgSpecLen:115.7  StepTime:0.66  LR:0.000100\n",
      "   | > Step:53/68  GlobalStep:121770  TotalLoss:0.00227  PostnetLoss:0.00111  DecoderLoss:0.00117  StopLoss:0.04228  GradNorm:0.00307  GradNormST:0.01422  AvgTextLen:42.4  AvgSpecLen:137.6  StepTime:0.63  LR:0.000100\n",
      "   | > Step:63/68  GlobalStep:121780  TotalLoss:0.00258  PostnetLoss:0.00125  DecoderLoss:0.00133  StopLoss:0.02994  GradNorm:0.00288  GradNormST:0.01018  AvgTextLen:58.6  AvgSpecLen:185.1  StepTime:0.91  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:121785  AvgTotalLoss:0.05317  AvgPostnetLoss:0.00096  AvgDecoderLoss:0.00102  AvgStopLoss:0.05119  EpochTime:42.04  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.07020   PostnetLoss: 0.00520   DecoderLoss:0.00551  StopLoss: 0.05949  \n",
      "   | > TotalLoss: 0.08216   PostnetLoss: 0.00865   DecoderLoss:0.00906  StopLoss: 0.06445  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00096   Validation Loss: 0.00774\n",
      "\n",
      " > Epoch 765/1000\n",
      "   | > Step:4/68  GlobalStep:121790  TotalLoss:0.00132  PostnetLoss:0.00064  DecoderLoss:0.00067  StopLoss:0.10685  GradNorm:0.00378  GradNormST:0.03647  AvgTextLen:4.8  AvgSpecLen:31.1  StepTime:0.34  LR:0.000100\n",
      "   | > Step:14/68  GlobalStep:121800  TotalLoss:0.00155  PostnetLoss:0.00076  DecoderLoss:0.00079  StopLoss:0.03865  GradNorm:0.00381  GradNormST:0.01589  AvgTextLen:12.2  AvgSpecLen:52.0  StepTime:0.35  LR:0.000100\n",
      "   | > Step:24/68  GlobalStep:121810  TotalLoss:0.00184  PostnetLoss:0.00090  DecoderLoss:0.00094  StopLoss:0.05253  GradNorm:0.00379  GradNormST:0.01401  AvgTextLen:19.2  AvgSpecLen:79.0  StepTime:0.48  LR:0.000100\n",
      "   | > Step:34/68  GlobalStep:121820  TotalLoss:0.00197  PostnetLoss:0.00096  DecoderLoss:0.00101  StopLoss:0.03133  GradNorm:0.00287  GradNormST:0.00914  AvgTextLen:26.2  AvgSpecLen:95.9  StepTime:0.52  LR:0.000100\n",
      "   | > Step:44/68  GlobalStep:121830  TotalLoss:0.00214  PostnetLoss:0.00104  DecoderLoss:0.00110  StopLoss:0.03447  GradNorm:0.00322  GradNormST:0.00924  AvgTextLen:33.8  AvgSpecLen:116.3  StepTime:0.54  LR:0.000100\n",
      "   | > Step:54/68  GlobalStep:121840  TotalLoss:0.00228  PostnetLoss:0.00111  DecoderLoss:0.00117  StopLoss:0.04951  GradNorm:0.00302  GradNormST:0.01508  AvgTextLen:42.2  AvgSpecLen:144.9  StepTime:0.81  LR:0.000100\n",
      "   | > Step:64/68  GlobalStep:121850  TotalLoss:0.00236  PostnetLoss:0.00114  DecoderLoss:0.00122  StopLoss:0.04164  GradNorm:0.00282  GradNormST:0.01322  AvgTextLen:60.6  AvgSpecLen:177.6  StepTime:0.80  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:121854  AvgTotalLoss:0.05215  AvgPostnetLoss:0.00095  AvgDecoderLoss:0.00100  AvgStopLoss:0.05020  EpochTime:41.73  AvgStepTime:0.60\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.07116   PostnetLoss: 0.00517   DecoderLoss:0.00548  StopLoss: 0.06051  \n",
      "   | > TotalLoss: 0.08229   PostnetLoss: 0.00852   DecoderLoss:0.00892  StopLoss: 0.06485  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00095   Validation Loss: 0.00768\n",
      "\n",
      " > Epoch 766/1000\n",
      "   | > Step:5/68  GlobalStep:121860  TotalLoss:0.00139  PostnetLoss:0.00067  DecoderLoss:0.00072  StopLoss:0.06741  GradNorm:0.00434  GradNormST:0.02752  AvgTextLen:5.9  AvgSpecLen:33.2  StepTime:0.35  LR:0.000100\n",
      "   | > Step:15/68  GlobalStep:121870  TotalLoss:0.00163  PostnetLoss:0.00080  DecoderLoss:0.00084  StopLoss:0.05225  GradNorm:0.00354  GradNormST:0.01918  AvgTextLen:12.8  AvgSpecLen:59.1  StepTime:0.42  LR:0.000100\n",
      "   | > Step:25/68  GlobalStep:121880  TotalLoss:0.00174  PostnetLoss:0.00085  DecoderLoss:0.00089  StopLoss:0.05380  GradNorm:0.00307  GradNormST:0.01368  AvgTextLen:19.8  AvgSpecLen:73.3  StepTime:0.37  LR:0.000100\n",
      "   | > Step:35/68  GlobalStep:121890  TotalLoss:0.00198  PostnetLoss:0.00097  DecoderLoss:0.00102  StopLoss:0.04143  GradNorm:0.00304  GradNormST:0.01675  AvgTextLen:27.1  AvgSpecLen:104.7  StepTime:0.66  LR:0.000100\n",
      "   | > Step:45/68  GlobalStep:121900  TotalLoss:0.00208  PostnetLoss:0.00101  DecoderLoss:0.00107  StopLoss:0.06002  GradNorm:0.00292  GradNormST:0.01994  AvgTextLen:34.4  AvgSpecLen:118.6  StepTime:0.65  LR:0.000100\n",
      "   | > Step:55/68  GlobalStep:121910  TotalLoss:0.00232  PostnetLoss:0.00113  DecoderLoss:0.00119  StopLoss:0.03710  GradNorm:0.00283  GradNormST:0.01118  AvgTextLen:43.9  AvgSpecLen:145.2  StepTime:0.70  LR:0.000100\n",
      "   | > Step:65/68  GlobalStep:121920  TotalLoss:0.00255  PostnetLoss:0.00124  DecoderLoss:0.00132  StopLoss:0.06147  GradNorm:0.00299  GradNormST:0.02290  AvgTextLen:64.2  AvgSpecLen:207.4  StepTime:1.18  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:121923  AvgTotalLoss:0.05206  AvgPostnetLoss:0.00094  AvgDecoderLoss:0.00099  AvgStopLoss:0.05013  EpochTime:41.81  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.07310   PostnetLoss: 0.00525   DecoderLoss:0.00556  StopLoss: 0.06229  \n",
      "   | > TotalLoss: 0.08387   PostnetLoss: 0.00858   DecoderLoss:0.00899  StopLoss: 0.06630  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00094   Validation Loss: 0.00772\n",
      "\n",
      " > Epoch 767/1000\n",
      "   | > Step:6/68  GlobalStep:121930  TotalLoss:0.00137  PostnetLoss:0.00067  DecoderLoss:0.00070  StopLoss:0.06727  GradNorm:0.00401  GradNormST:0.03879  AvgTextLen:6.0  AvgSpecLen:34.8  StepTime:0.37  LR:0.000100\n",
      "   | > Step:16/68  GlobalStep:121940  TotalLoss:0.00160  PostnetLoss:0.00078  DecoderLoss:0.00082  StopLoss:0.04247  GradNorm:0.00321  GradNormST:0.01755  AvgTextLen:13.8  AvgSpecLen:60.6  StepTime:0.38  LR:0.000100\n",
      "   | > Step:26/68  GlobalStep:121950  TotalLoss:0.00179  PostnetLoss:0.00088  DecoderLoss:0.00092  StopLoss:0.04740  GradNorm:0.00325  GradNormST:0.01349  AvgTextLen:20.3  AvgSpecLen:77.5  StepTime:0.47  LR:0.000100\n",
      "   | > Step:36/68  GlobalStep:121960  TotalLoss:0.00193  PostnetLoss:0.00094  DecoderLoss:0.00099  StopLoss:0.04796  GradNorm:0.00309  GradNormST:0.01516  AvgTextLen:28.5  AvgSpecLen:100.2  StepTime:0.55  LR:0.000100\n",
      "   | > Step:46/68  GlobalStep:121970  TotalLoss:0.00213  PostnetLoss:0.00104  DecoderLoss:0.00109  StopLoss:0.04762  GradNorm:0.00336  GradNormST:0.01324  AvgTextLen:35.4  AvgSpecLen:122.6  StepTime:0.78  LR:0.000100\n",
      "   | > Step:56/68  GlobalStep:121980  TotalLoss:0.00228  PostnetLoss:0.00111  DecoderLoss:0.00117  StopLoss:0.04511  GradNorm:0.00301  GradNormST:0.01889  AvgTextLen:45.8  AvgSpecLen:145.4  StepTime:0.96  LR:0.000100\n",
      "   | > Step:66/68  GlobalStep:121990  TotalLoss:0.00267  PostnetLoss:0.00129  DecoderLoss:0.00138  StopLoss:0.05409  GradNorm:0.00288  GradNormST:0.02117  AvgTextLen:71.2  AvgSpecLen:220.8  StepTime:1.04  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:121992  AvgTotalLoss:0.05270  AvgPostnetLoss:0.00093  AvgDecoderLoss:0.00098  AvgStopLoss:0.05079  EpochTime:42.28  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.07709   PostnetLoss: 0.00530   DecoderLoss:0.00560  StopLoss: 0.06620  \n",
      "   | > TotalLoss: 0.08783   PostnetLoss: 0.00892   DecoderLoss:0.00933  StopLoss: 0.06957  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00093   Validation Loss: 0.00799\n",
      "\n",
      " > Epoch 768/1000\n",
      "   | > Step:7/68  GlobalStep:122000  TotalLoss:0.00144  PostnetLoss:0.00070  DecoderLoss:0.00074  StopLoss:0.06667  GradNorm:0.00471  GradNormST:0.03770  AvgTextLen:7.4  AvgSpecLen:44.6  StepTime:0.35  LR:0.000100\n",
      " | | > Checkpoint saving : /vm/TTS/models/cartman_models/mozilla-no-loc-fattn-stopnet-sigmoid-loss_masking-August-15-2019_11+26AM-de6aeb9/checkpoint_122000.pth.tar\n",
      "   | > Step:17/68  GlobalStep:122010  TotalLoss:0.00160  PostnetLoss:0.00078  DecoderLoss:0.00082  StopLoss:0.06847  GradNorm:0.00327  GradNormST:0.01826  AvgTextLen:14.7  AvgSpecLen:66.0  StepTime:0.42  LR:0.000100\n",
      "   | > Step:27/68  GlobalStep:122020  TotalLoss:0.00180  PostnetLoss:0.00088  DecoderLoss:0.00092  StopLoss:0.06419  GradNorm:0.00350  GradNormST:0.02384  AvgTextLen:21.0  AvgSpecLen:80.2  StepTime:0.71  LR:0.000100\n",
      "   | > Step:37/68  GlobalStep:122030  TotalLoss:0.00199  PostnetLoss:0.00097  DecoderLoss:0.00102  StopLoss:0.04989  GradNorm:0.00354  GradNormST:0.01190  AvgTextLen:29.3  AvgSpecLen:102.4  StepTime:0.49  LR:0.000100\n",
      "   | > Step:47/68  GlobalStep:122040  TotalLoss:0.00217  PostnetLoss:0.00106  DecoderLoss:0.00112  StopLoss:0.03694  GradNorm:0.00319  GradNormST:0.01284  AvgTextLen:36.7  AvgSpecLen:126.9  StepTime:0.75  LR:0.000100\n",
      "   | > Step:57/68  GlobalStep:122050  TotalLoss:0.00232  PostnetLoss:0.00113  DecoderLoss:0.00119  StopLoss:0.03852  GradNorm:0.00301  GradNormST:0.01180  AvgTextLen:47.1  AvgSpecLen:161.0  StepTime:0.69  LR:0.000100\n",
      "   | > Step:67/68  GlobalStep:122060  TotalLoss:0.00272  PostnetLoss:0.00132  DecoderLoss:0.00140  StopLoss:0.04501  GradNorm:0.00265  GradNormST:0.01574  AvgTextLen:80.6  AvgSpecLen:246.5  StepTime:1.21  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:122061  AvgTotalLoss:0.05387  AvgPostnetLoss:0.00093  AvgDecoderLoss:0.00098  AvgStopLoss:0.05196  EpochTime:42.26  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.07203   PostnetLoss: 0.00523   DecoderLoss:0.00553  StopLoss: 0.06127  \n",
      "   | > TotalLoss: 0.08331   PostnetLoss: 0.00869   DecoderLoss:0.00910  StopLoss: 0.06552  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00093   Validation Loss: 0.00780\n",
      "\n",
      " > Epoch 769/1000\n",
      "   | > Step:8/68  GlobalStep:122070  TotalLoss:0.00143  PostnetLoss:0.00070  DecoderLoss:0.00073  StopLoss:0.04835  GradNorm:0.00393  GradNormST:0.01465  AvgTextLen:8.3  AvgSpecLen:44.0  StepTime:0.32  LR:0.000100\n",
      "   | > Step:18/68  GlobalStep:122080  TotalLoss:0.00160  PostnetLoss:0.00079  DecoderLoss:0.00082  StopLoss:0.04723  GradNorm:0.00400  GradNormST:0.01678  AvgTextLen:15.2  AvgSpecLen:64.5  StepTime:0.32  LR:0.000100\n",
      "   | > Step:28/68  GlobalStep:122090  TotalLoss:0.00179  PostnetLoss:0.00087  DecoderLoss:0.00092  StopLoss:0.04277  GradNorm:0.00297  GradNormST:0.01258  AvgTextLen:22.1  AvgSpecLen:85.4  StepTime:0.53  LR:0.000100\n",
      "   | > Step:38/68  GlobalStep:122100  TotalLoss:0.00193  PostnetLoss:0.00094  DecoderLoss:0.00099  StopLoss:0.04039  GradNorm:0.00303  GradNormST:0.01167  AvgTextLen:29.4  AvgSpecLen:104.1  StepTime:0.61  LR:0.000100\n",
      "   | > Step:48/68  GlobalStep:122110  TotalLoss:0.00212  PostnetLoss:0.00103  DecoderLoss:0.00109  StopLoss:0.03959  GradNorm:0.00283  GradNormST:0.01169  AvgTextLen:37.5  AvgSpecLen:123.8  StepTime:0.73  LR:0.000100\n",
      "   | > Step:58/68  GlobalStep:122120  TotalLoss:0.00232  PostnetLoss:0.00113  DecoderLoss:0.00120  StopLoss:0.04618  GradNorm:0.00304  GradNormST:0.01399  AvgTextLen:47.8  AvgSpecLen:158.0  StepTime:0.91  LR:0.000100\n",
      "   | > Step:68/68  GlobalStep:122130  TotalLoss:0.00277  PostnetLoss:0.00134  DecoderLoss:0.00143  StopLoss:0.04787  GradNorm:0.00318  GradNormST:0.01919  AvgTextLen:103.0  AvgSpecLen:308.9  StepTime:1.86  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:122130  AvgTotalLoss:0.05112  AvgPostnetLoss:0.00092  AvgDecoderLoss:0.00097  AvgStopLoss:0.04923  EpochTime:42.14  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.07102   PostnetLoss: 0.00518   DecoderLoss:0.00548  StopLoss: 0.06036  \n",
      "   | > TotalLoss: 0.08120   PostnetLoss: 0.00861   DecoderLoss:0.00902  StopLoss: 0.06357  \n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00092   Validation Loss: 0.00777\n",
      "\n",
      " > Epoch 770/1000\n",
      "   | > Step:9/68  GlobalStep:122140  TotalLoss:0.00139  PostnetLoss:0.00068  DecoderLoss:0.00071  StopLoss:0.04601  GradNorm:0.00347  GradNormST:0.02057  AvgTextLen:9.0  AvgSpecLen:42.1  StepTime:0.43  LR:0.000100\n",
      "   | > Step:19/68  GlobalStep:122150  TotalLoss:0.00160  PostnetLoss:0.00078  DecoderLoss:0.00082  StopLoss:0.06128  GradNorm:0.00340  GradNormST:0.02333  AvgTextLen:15.5  AvgSpecLen:63.2  StepTime:0.41  LR:0.000100\n",
      "   | > Step:29/68  GlobalStep:122160  TotalLoss:0.00179  PostnetLoss:0.00087  DecoderLoss:0.00092  StopLoss:0.05919  GradNorm:0.00294  GradNormST:0.01156  AvgTextLen:21.8  AvgSpecLen:83.4  StepTime:0.54  LR:0.000100\n",
      "   | > Step:39/68  GlobalStep:122170  TotalLoss:0.00197  PostnetLoss:0.00096  DecoderLoss:0.00101  StopLoss:0.04267  GradNorm:0.00270  GradNormST:0.01337  AvgTextLen:30.2  AvgSpecLen:110.4  StepTime:0.67  LR:0.000100\n",
      "   | > Step:49/68  GlobalStep:122180  TotalLoss:0.00212  PostnetLoss:0.00103  DecoderLoss:0.00109  StopLoss:0.04621  GradNorm:0.00320  GradNormST:0.01387  AvgTextLen:38.2  AvgSpecLen:127.7  StepTime:0.59  LR:0.000100\n",
      "   | > Step:59/68  GlobalStep:122190  TotalLoss:0.00230  PostnetLoss:0.00112  DecoderLoss:0.00118  StopLoss:0.04191  GradNorm:0.00422  GradNormST:0.01738  AvgTextLen:50.5  AvgSpecLen:155.9  StepTime:0.74  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:122199  AvgTotalLoss:0.05125  AvgPostnetLoss:0.00092  AvgDecoderLoss:0.00097  AvgStopLoss:0.04936  EpochTime:42.30  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.07395   PostnetLoss: 0.00524   DecoderLoss:0.00555  StopLoss: 0.06317  \n",
      "   | > TotalLoss: 0.08489   PostnetLoss: 0.00886   DecoderLoss:0.00928  StopLoss: 0.06675  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00092   Validation Loss: 0.00793\n",
      "\n",
      " > Epoch 771/1000\n",
      "   | > Step:0/68  GlobalStep:122200  TotalLoss:0.00137  PostnetLoss:0.00066  DecoderLoss:0.00071  StopLoss:0.07739  GradNorm:0.00557  GradNormST:0.03455  AvgTextLen:3.0  AvgSpecLen:25.3  StepTime:0.53  LR:0.000100\n",
      "   | > Step:10/68  GlobalStep:122210  TotalLoss:0.00147  PostnetLoss:0.00071  DecoderLoss:0.00075  StopLoss:0.04606  GradNorm:0.00353  GradNormST:0.02589  AvgTextLen:9.8  AvgSpecLen:48.6  StepTime:0.52  LR:0.000100\n",
      "   | > Step:20/68  GlobalStep:122220  TotalLoss:0.00170  PostnetLoss:0.00083  DecoderLoss:0.00087  StopLoss:0.04120  GradNorm:0.00347  GradNormST:0.01448  AvgTextLen:16.4  AvgSpecLen:64.8  StepTime:0.39  LR:0.000100\n",
      "   | > Step:30/68  GlobalStep:122230  TotalLoss:0.00182  PostnetLoss:0.00089  DecoderLoss:0.00093  StopLoss:0.05973  GradNorm:0.00293  GradNormST:0.01415  AvgTextLen:22.9  AvgSpecLen:85.8  StepTime:0.50  LR:0.000100\n",
      "   | > Step:40/68  GlobalStep:122240  TotalLoss:0.00196  PostnetLoss:0.00096  DecoderLoss:0.00101  StopLoss:0.03971  GradNorm:0.00315  GradNormST:0.01090  AvgTextLen:30.9  AvgSpecLen:105.3  StepTime:0.60  LR:0.000100\n",
      "   | > Step:50/68  GlobalStep:122250  TotalLoss:0.00217  PostnetLoss:0.00105  DecoderLoss:0.00111  StopLoss:0.03674  GradNorm:0.00325  GradNormST:0.00977  AvgTextLen:38.8  AvgSpecLen:132.0  StepTime:0.85  LR:0.000100\n",
      "   | > Step:60/68  GlobalStep:122260  TotalLoss:0.00233  PostnetLoss:0.00114  DecoderLoss:0.00120  StopLoss:0.03623  GradNorm:0.00317  GradNormST:0.01209  AvgTextLen:52.0  AvgSpecLen:172.7  StepTime:0.93  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:122268  AvgTotalLoss:0.05121  AvgPostnetLoss:0.00092  AvgDecoderLoss:0.00097  AvgStopLoss:0.04932  EpochTime:42.17  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.07354   PostnetLoss: 0.00527   DecoderLoss:0.00557  StopLoss: 0.06270  \n",
      "   | > TotalLoss: 0.08314   PostnetLoss: 0.00872   DecoderLoss:0.00913  StopLoss: 0.06528  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00092   Validation Loss: 0.00786\n",
      "\n",
      " > Epoch 772/1000\n",
      "   | > Step:1/68  GlobalStep:122270  TotalLoss:0.00132  PostnetLoss:0.00064  DecoderLoss:0.00068  StopLoss:0.04748  GradNorm:0.00386  GradNormST:0.02093  AvgTextLen:3.7  AvgSpecLen:28.6  StepTime:0.32  LR:0.000100\n",
      "   | > Step:11/68  GlobalStep:122280  TotalLoss:0.00142  PostnetLoss:0.00070  DecoderLoss:0.00073  StopLoss:0.03207  GradNorm:0.00357  GradNormST:0.01267  AvgTextLen:10.3  AvgSpecLen:49.4  StepTime:0.39  LR:0.000100\n",
      "   | > Step:21/68  GlobalStep:122290  TotalLoss:0.00162  PostnetLoss:0.00079  DecoderLoss:0.00083  StopLoss:0.04236  GradNorm:0.00339  GradNormST:0.01139  AvgTextLen:16.6  AvgSpecLen:66.3  StepTime:0.42  LR:0.000100\n",
      "   | > Step:31/68  GlobalStep:122300  TotalLoss:0.00183  PostnetLoss:0.00089  DecoderLoss:0.00094  StopLoss:0.06129  GradNorm:0.00297  GradNormST:0.02163  AvgTextLen:24.1  AvgSpecLen:82.7  StepTime:0.56  LR:0.000100\n",
      "   | > Step:41/68  GlobalStep:122310  TotalLoss:0.00204  PostnetLoss:0.00099  DecoderLoss:0.00105  StopLoss:0.03668  GradNorm:0.00294  GradNormST:0.01455  AvgTextLen:32.3  AvgSpecLen:109.4  StepTime:0.67  LR:0.000100\n",
      "   | > Step:51/68  GlobalStep:122320  TotalLoss:0.00212  PostnetLoss:0.00103  DecoderLoss:0.00109  StopLoss:0.03837  GradNorm:0.00285  GradNormST:0.00933  AvgTextLen:39.2  AvgSpecLen:126.9  StepTime:0.79  LR:0.000100\n",
      "   | > Step:61/68  GlobalStep:122330  TotalLoss:0.00232  PostnetLoss:0.00113  DecoderLoss:0.00119  StopLoss:0.05576  GradNorm:0.00346  GradNormST:0.01995  AvgTextLen:54.6  AvgSpecLen:165.5  StepTime:0.89  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:122337  AvgTotalLoss:0.05325  AvgPostnetLoss:0.00092  AvgDecoderLoss:0.00097  AvgStopLoss:0.05136  EpochTime:42.35  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.07505   PostnetLoss: 0.00533   DecoderLoss:0.00563  StopLoss: 0.06408  \n",
      "   | > TotalLoss: 0.08319   PostnetLoss: 0.00870   DecoderLoss:0.00910  StopLoss: 0.06539  \n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00092   Validation Loss: 0.00782\n",
      "\n",
      " > Epoch 773/1000\n",
      "   | > Step:2/68  GlobalStep:122340  TotalLoss:0.00125  PostnetLoss:0.00061  DecoderLoss:0.00064  StopLoss:0.03793  GradNorm:0.00346  GradNormST:0.01769  AvgTextLen:4.0  AvgSpecLen:31.3  StepTime:0.38  LR:0.000100\n",
      "   | > Step:12/68  GlobalStep:122350  TotalLoss:0.00143  PostnetLoss:0.00070  DecoderLoss:0.00073  StopLoss:0.05754  GradNorm:0.00412  GradNormST:0.02352  AvgTextLen:11.0  AvgSpecLen:49.3  StepTime:0.33  LR:0.000100\n",
      "   | > Step:22/68  GlobalStep:122360  TotalLoss:0.00169  PostnetLoss:0.00083  DecoderLoss:0.00087  StopLoss:0.05649  GradNorm:0.00366  GradNormST:0.01541  AvgTextLen:18.0  AvgSpecLen:72.0  StepTime:0.44  LR:0.000100\n",
      "   | > Step:32/68  GlobalStep:122370  TotalLoss:0.00182  PostnetLoss:0.00089  DecoderLoss:0.00093  StopLoss:0.05759  GradNorm:0.00283  GradNormST:0.01677  AvgTextLen:24.4  AvgSpecLen:91.5  StepTime:0.49  LR:0.000100\n",
      "   | > Step:42/68  GlobalStep:122380  TotalLoss:0.00199  PostnetLoss:0.00097  DecoderLoss:0.00102  StopLoss:0.04759  GradNorm:0.00294  GradNormST:0.02159  AvgTextLen:33.1  AvgSpecLen:109.7  StepTime:0.58  LR:0.000100\n",
      "   | > Step:52/68  GlobalStep:122390  TotalLoss:0.00223  PostnetLoss:0.00109  DecoderLoss:0.00115  StopLoss:0.05540  GradNorm:0.00321  GradNormST:0.01748  AvgTextLen:42.1  AvgSpecLen:137.5  StepTime:0.71  LR:0.000100\n",
      "   | > Step:62/68  GlobalStep:122400  TotalLoss:0.00240  PostnetLoss:0.00116  DecoderLoss:0.00123  StopLoss:0.04302  GradNorm:0.00293  GradNormST:0.01312  AvgTextLen:54.0  AvgSpecLen:174.7  StepTime:0.75  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:122406  AvgTotalLoss:0.05166  AvgPostnetLoss:0.00091  AvgDecoderLoss:0.00096  AvgStopLoss:0.04978  EpochTime:41.33  AvgStepTime:0.60\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.07604   PostnetLoss: 0.00531   DecoderLoss:0.00560  StopLoss: 0.06513  \n",
      "   | > TotalLoss: 0.08253   PostnetLoss: 0.00876   DecoderLoss:0.00918  StopLoss: 0.06460  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00091   Validation Loss: 0.00785\n",
      "\n",
      " > Epoch 774/1000\n",
      "   | > Step:3/68  GlobalStep:122410  TotalLoss:0.00128  PostnetLoss:0.00063  DecoderLoss:0.00066  StopLoss:0.05599  GradNorm:0.00368  GradNormST:0.01701  AvgTextLen:4.7  AvgSpecLen:29.7  StepTime:0.30  LR:0.000100\n",
      "   | > Step:13/68  GlobalStep:122420  TotalLoss:0.00144  PostnetLoss:0.00071  DecoderLoss:0.00074  StopLoss:0.05716  GradNorm:0.00361  GradNormST:0.01682  AvgTextLen:11.5  AvgSpecLen:55.2  StepTime:0.42  LR:0.000100\n",
      "   | > Step:23/68  GlobalStep:122430  TotalLoss:0.00173  PostnetLoss:0.00084  DecoderLoss:0.00088  StopLoss:0.04519  GradNorm:0.00331  GradNormST:0.01128  AvgTextLen:18.3  AvgSpecLen:72.2  StepTime:0.43  LR:0.000100\n",
      "   | > Step:33/68  GlobalStep:122440  TotalLoss:0.00189  PostnetLoss:0.00092  DecoderLoss:0.00097  StopLoss:0.06243  GradNorm:0.00291  GradNormST:0.02553  AvgTextLen:25.9  AvgSpecLen:88.9  StepTime:0.54  LR:0.000100\n",
      "   | > Step:43/68  GlobalStep:122450  TotalLoss:0.00200  PostnetLoss:0.00098  DecoderLoss:0.00103  StopLoss:0.05482  GradNorm:0.00287  GradNormST:0.01816  AvgTextLen:33.4  AvgSpecLen:115.7  StepTime:0.51  LR:0.000100\n",
      "   | > Step:53/68  GlobalStep:122460  TotalLoss:0.00213  PostnetLoss:0.00104  DecoderLoss:0.00109  StopLoss:0.03752  GradNorm:0.00308  GradNormST:0.01002  AvgTextLen:42.4  AvgSpecLen:137.6  StepTime:0.82  LR:0.000100\n",
      "   | > Step:63/68  GlobalStep:122470  TotalLoss:0.00244  PostnetLoss:0.00119  DecoderLoss:0.00126  StopLoss:0.03465  GradNorm:0.00580  GradNormST:0.01225  AvgTextLen:58.6  AvgSpecLen:185.1  StepTime:0.91  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:122475  AvgTotalLoss:0.05112  AvgPostnetLoss:0.00091  AvgDecoderLoss:0.00096  AvgStopLoss:0.04924  EpochTime:42.24  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.07689   PostnetLoss: 0.00531   DecoderLoss:0.00560  StopLoss: 0.06599  \n",
      "   | > TotalLoss: 0.08227   PostnetLoss: 0.00885   DecoderLoss:0.00926  StopLoss: 0.06416  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00091   Validation Loss: 0.00789\n",
      "\n",
      " > Epoch 775/1000\n",
      "   | > Step:4/68  GlobalStep:122480  TotalLoss:0.00131  PostnetLoss:0.00064  DecoderLoss:0.00067  StopLoss:0.08053  GradNorm:0.00372  GradNormST:0.03443  AvgTextLen:4.8  AvgSpecLen:31.1  StepTime:0.30  LR:0.000100\n",
      "   | > Step:14/68  GlobalStep:122490  TotalLoss:0.00152  PostnetLoss:0.00074  DecoderLoss:0.00078  StopLoss:0.06012  GradNorm:0.00339  GradNormST:0.02245  AvgTextLen:12.2  AvgSpecLen:52.0  StepTime:0.32  LR:0.000100\n",
      "   | > Step:24/68  GlobalStep:122500  TotalLoss:0.00178  PostnetLoss:0.00087  DecoderLoss:0.00091  StopLoss:0.05515  GradNorm:0.00342  GradNormST:0.01815  AvgTextLen:19.2  AvgSpecLen:79.0  StepTime:0.53  LR:0.000100\n",
      "   | > Step:34/68  GlobalStep:122510  TotalLoss:0.00192  PostnetLoss:0.00093  DecoderLoss:0.00098  StopLoss:0.03010  GradNorm:0.00296  GradNormST:0.00831  AvgTextLen:26.2  AvgSpecLen:95.9  StepTime:0.61  LR:0.000100\n",
      "   | > Step:44/68  GlobalStep:122520  TotalLoss:0.00204  PostnetLoss:0.00099  DecoderLoss:0.00104  StopLoss:0.03349  GradNorm:0.00296  GradNormST:0.01072  AvgTextLen:33.8  AvgSpecLen:116.3  StepTime:0.65  LR:0.000100\n",
      "   | > Step:54/68  GlobalStep:122530  TotalLoss:0.00221  PostnetLoss:0.00108  DecoderLoss:0.00114  StopLoss:0.04864  GradNorm:0.00296  GradNormST:0.00947  AvgTextLen:42.2  AvgSpecLen:144.9  StepTime:0.71  LR:0.000100\n",
      "   | > Step:64/68  GlobalStep:122540  TotalLoss:0.00229  PostnetLoss:0.00111  DecoderLoss:0.00118  StopLoss:0.03261  GradNorm:0.00257  GradNormST:0.00835  AvgTextLen:60.6  AvgSpecLen:177.6  StepTime:0.80  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:122544  AvgTotalLoss:0.04852  AvgPostnetLoss:0.00091  AvgDecoderLoss:0.00096  AvgStopLoss:0.04664  EpochTime:42.69  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.07529   PostnetLoss: 0.00530   DecoderLoss:0.00559  StopLoss: 0.06440  \n",
      "   | > TotalLoss: 0.08154   PostnetLoss: 0.00875   DecoderLoss:0.00915  StopLoss: 0.06363  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00091   Validation Loss: 0.00787\n",
      "\n",
      " > Epoch 776/1000\n",
      "   | > Step:5/68  GlobalStep:122550  TotalLoss:0.00136  PostnetLoss:0.00066  DecoderLoss:0.00070  StopLoss:0.04939  GradNorm:0.00395  GradNormST:0.01854  AvgTextLen:5.9  AvgSpecLen:33.2  StepTime:0.26  LR:0.000100\n",
      "   | > Step:15/68  GlobalStep:122560  TotalLoss:0.00155  PostnetLoss:0.00076  DecoderLoss:0.00079  StopLoss:0.04098  GradNorm:0.00321  GradNormST:0.01047  AvgTextLen:12.8  AvgSpecLen:59.1  StepTime:0.49  LR:0.000100\n",
      "   | > Step:25/68  GlobalStep:122570  TotalLoss:0.00170  PostnetLoss:0.00083  DecoderLoss:0.00087  StopLoss:0.04795  GradNorm:0.00331  GradNormST:0.01664  AvgTextLen:19.8  AvgSpecLen:73.3  StepTime:0.48  LR:0.000100\n",
      "   | > Step:35/68  GlobalStep:122580  TotalLoss:0.00194  PostnetLoss:0.00094  DecoderLoss:0.00099  StopLoss:0.04372  GradNorm:0.00290  GradNormST:0.01789  AvgTextLen:27.1  AvgSpecLen:104.7  StepTime:0.75  LR:0.000100\n",
      "   | > Step:45/68  GlobalStep:122590  TotalLoss:0.00200  PostnetLoss:0.00097  DecoderLoss:0.00102  StopLoss:0.06076  GradNorm:0.00267  GradNormST:0.02522  AvgTextLen:34.4  AvgSpecLen:118.6  StepTime:0.54  LR:0.000100\n",
      "   | > Step:55/68  GlobalStep:122600  TotalLoss:0.00224  PostnetLoss:0.00109  DecoderLoss:0.00115  StopLoss:0.03994  GradNorm:0.00287  GradNormST:0.01033  AvgTextLen:43.9  AvgSpecLen:145.2  StepTime:0.71  LR:0.000100\n",
      "   | > Step:65/68  GlobalStep:122610  TotalLoss:0.00245  PostnetLoss:0.00119  DecoderLoss:0.00126  StopLoss:0.04194  GradNorm:0.00298  GradNormST:0.01190  AvgTextLen:64.2  AvgSpecLen:207.4  StepTime:1.17  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:122613  AvgTotalLoss:0.04721  AvgPostnetLoss:0.00091  AvgDecoderLoss:0.00096  AvgStopLoss:0.04535  EpochTime:42.30  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.07789   PostnetLoss: 0.00534   DecoderLoss:0.00564  StopLoss: 0.06692  \n",
      "   | > TotalLoss: 0.08190   PostnetLoss: 0.00876   DecoderLoss:0.00915  StopLoss: 0.06399  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00091   Validation Loss: 0.00783\n",
      "\n",
      " > Epoch 777/1000\n",
      "   | > Step:6/68  GlobalStep:122620  TotalLoss:0.00129  PostnetLoss:0.00063  DecoderLoss:0.00066  StopLoss:0.03412  GradNorm:0.00354  GradNormST:0.01123  AvgTextLen:6.0  AvgSpecLen:34.8  StepTime:0.30  LR:0.000100\n",
      "   | > Step:16/68  GlobalStep:122630  TotalLoss:0.00159  PostnetLoss:0.00078  DecoderLoss:0.00081  StopLoss:0.04641  GradNorm:0.00420  GradNormST:0.01635  AvgTextLen:13.8  AvgSpecLen:60.6  StepTime:0.37  LR:0.000100\n",
      "   | > Step:26/68  GlobalStep:122640  TotalLoss:0.00178  PostnetLoss:0.00087  DecoderLoss:0.00091  StopLoss:0.04025  GradNorm:0.00429  GradNormST:0.01105  AvgTextLen:20.3  AvgSpecLen:77.5  StepTime:0.47  LR:0.000100\n",
      "   | > Step:36/68  GlobalStep:122650  TotalLoss:0.00189  PostnetLoss:0.00092  DecoderLoss:0.00097  StopLoss:0.05255  GradNorm:0.00332  GradNormST:0.01137  AvgTextLen:28.5  AvgSpecLen:100.2  StepTime:0.57  LR:0.000100\n",
      "   | > Step:46/68  GlobalStep:122660  TotalLoss:0.00206  PostnetLoss:0.00100  DecoderLoss:0.00106  StopLoss:0.04664  GradNorm:0.00289  GradNormST:0.01186  AvgTextLen:35.4  AvgSpecLen:122.6  StepTime:0.69  LR:0.000100\n",
      "   | > Step:56/68  GlobalStep:122670  TotalLoss:0.00225  PostnetLoss:0.00109  DecoderLoss:0.00115  StopLoss:0.03252  GradNorm:0.00275  GradNormST:0.00933  AvgTextLen:45.8  AvgSpecLen:145.4  StepTime:0.96  LR:0.000100\n",
      "   | > Step:66/68  GlobalStep:122680  TotalLoss:0.00261  PostnetLoss:0.00126  DecoderLoss:0.00135  StopLoss:0.04294  GradNorm:0.00300  GradNormST:0.01476  AvgTextLen:71.2  AvgSpecLen:220.8  StepTime:1.04  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:122682  AvgTotalLoss:0.04855  AvgPostnetLoss:0.00091  AvgDecoderLoss:0.00096  AvgStopLoss:0.04668  EpochTime:42.03  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.07766   PostnetLoss: 0.00533   DecoderLoss:0.00562  StopLoss: 0.06670  \n",
      "   | > TotalLoss: 0.08203   PostnetLoss: 0.00887   DecoderLoss:0.00929  StopLoss: 0.06387  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00091   Validation Loss: 0.00790\n",
      "\n",
      " > Epoch 778/1000\n",
      "   | > Step:7/68  GlobalStep:122690  TotalLoss:0.00137  PostnetLoss:0.00067  DecoderLoss:0.00070  StopLoss:0.03924  GradNorm:0.00339  GradNormST:0.02154  AvgTextLen:7.4  AvgSpecLen:44.6  StepTime:0.34  LR:0.000100\n",
      "   | > Step:17/68  GlobalStep:122700  TotalLoss:0.00156  PostnetLoss:0.00076  DecoderLoss:0.00080  StopLoss:0.04980  GradNorm:0.00382  GradNormST:0.01040  AvgTextLen:14.7  AvgSpecLen:66.0  StepTime:0.38  LR:0.000100\n",
      "   | > Step:27/68  GlobalStep:122710  TotalLoss:0.00183  PostnetLoss:0.00089  DecoderLoss:0.00093  StopLoss:0.05935  GradNorm:0.00371  GradNormST:0.01705  AvgTextLen:21.0  AvgSpecLen:80.2  StepTime:0.63  LR:0.000100\n",
      "   | > Step:37/68  GlobalStep:122720  TotalLoss:0.00194  PostnetLoss:0.00094  DecoderLoss:0.00099  StopLoss:0.05738  GradNorm:0.00326  GradNormST:0.01148  AvgTextLen:29.3  AvgSpecLen:102.4  StepTime:0.60  LR:0.000100\n",
      "   | > Step:47/68  GlobalStep:122730  TotalLoss:0.00208  PostnetLoss:0.00101  DecoderLoss:0.00107  StopLoss:0.03872  GradNorm:0.00303  GradNormST:0.01391  AvgTextLen:36.7  AvgSpecLen:126.9  StepTime:0.76  LR:0.000100\n",
      "   | > Step:57/68  GlobalStep:122740  TotalLoss:0.00227  PostnetLoss:0.00110  DecoderLoss:0.00117  StopLoss:0.03693  GradNorm:0.00274  GradNormST:0.00956  AvgTextLen:47.1  AvgSpecLen:161.0  StepTime:0.82  LR:0.000100\n",
      "   | > Step:67/68  GlobalStep:122750  TotalLoss:0.00262  PostnetLoss:0.00127  DecoderLoss:0.00135  StopLoss:0.03037  GradNorm:0.00261  GradNormST:0.01395  AvgTextLen:80.6  AvgSpecLen:246.5  StepTime:1.21  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:122751  AvgTotalLoss:0.04757  AvgPostnetLoss:0.00091  AvgDecoderLoss:0.00096  AvgStopLoss:0.04571  EpochTime:41.90  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.07759   PostnetLoss: 0.00536   DecoderLoss:0.00565  StopLoss: 0.06658  \n",
      "   | > TotalLoss: 0.08174   PostnetLoss: 0.00892   DecoderLoss:0.00931  StopLoss: 0.06351  \n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00091   Validation Loss: 0.00796\n",
      "\n",
      " > Epoch 779/1000\n",
      "   | > Step:8/68  GlobalStep:122760  TotalLoss:0.00137  PostnetLoss:0.00067  DecoderLoss:0.00070  StopLoss:0.04188  GradNorm:0.00336  GradNormST:0.01223  AvgTextLen:8.3  AvgSpecLen:44.0  StepTime:0.26  LR:0.000100\n",
      "   | > Step:18/68  GlobalStep:122770  TotalLoss:0.00157  PostnetLoss:0.00076  DecoderLoss:0.00080  StopLoss:0.04611  GradNorm:0.00356  GradNormST:0.01014  AvgTextLen:15.2  AvgSpecLen:64.5  StepTime:0.34  LR:0.000100\n",
      "   | > Step:28/68  GlobalStep:122780  TotalLoss:0.00180  PostnetLoss:0.00088  DecoderLoss:0.00092  StopLoss:0.04568  GradNorm:0.00401  GradNormST:0.01309  AvgTextLen:22.1  AvgSpecLen:85.4  StepTime:0.55  LR:0.000100\n",
      "   | > Step:38/68  GlobalStep:122790  TotalLoss:0.00192  PostnetLoss:0.00094  DecoderLoss:0.00098  StopLoss:0.04831  GradNorm:0.00345  GradNormST:0.01321  AvgTextLen:29.4  AvgSpecLen:104.1  StepTime:0.61  LR:0.000100\n",
      "   | > Step:48/68  GlobalStep:122800  TotalLoss:0.00211  PostnetLoss:0.00103  DecoderLoss:0.00108  StopLoss:0.03438  GradNorm:0.00349  GradNormST:0.00865  AvgTextLen:37.5  AvgSpecLen:123.8  StepTime:0.55  LR:0.000100\n",
      "   | > Step:58/68  GlobalStep:122810  TotalLoss:0.00225  PostnetLoss:0.00110  DecoderLoss:0.00116  StopLoss:0.04004  GradNorm:0.00263  GradNormST:0.01148  AvgTextLen:47.8  AvgSpecLen:158.0  StepTime:0.85  LR:0.000100\n",
      "   | > Step:68/68  GlobalStep:122820  TotalLoss:0.00264  PostnetLoss:0.00128  DecoderLoss:0.00136  StopLoss:0.04130  GradNorm:0.00245  GradNormST:0.01727  AvgTextLen:103.0  AvgSpecLen:308.9  StepTime:1.87  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:122820  AvgTotalLoss:0.04746  AvgPostnetLoss:0.00091  AvgDecoderLoss:0.00095  AvgStopLoss:0.04560  EpochTime:42.33  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.07663   PostnetLoss: 0.00536   DecoderLoss:0.00565  StopLoss: 0.06562  \n",
      "   | > TotalLoss: 0.08053   PostnetLoss: 0.00887   DecoderLoss:0.00927  StopLoss: 0.06240  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00091   Validation Loss: 0.00795\n",
      "\n",
      " > Epoch 780/1000\n",
      "   | > Step:9/68  GlobalStep:122830  TotalLoss:0.00137  PostnetLoss:0.00067  DecoderLoss:0.00070  StopLoss:0.05967  GradNorm:0.00357  GradNormST:0.02445  AvgTextLen:9.0  AvgSpecLen:42.1  StepTime:0.43  LR:0.000100\n",
      "   | > Step:19/68  GlobalStep:122840  TotalLoss:0.00155  PostnetLoss:0.00076  DecoderLoss:0.00079  StopLoss:0.06064  GradNorm:0.00311  GradNormST:0.02096  AvgTextLen:15.5  AvgSpecLen:63.2  StepTime:0.41  LR:0.000100\n",
      "   | > Step:29/68  GlobalStep:122850  TotalLoss:0.00176  PostnetLoss:0.00086  DecoderLoss:0.00090  StopLoss:0.05308  GradNorm:0.00330  GradNormST:0.01172  AvgTextLen:21.8  AvgSpecLen:83.4  StepTime:0.54  LR:0.000100\n",
      "   | > Step:39/68  GlobalStep:122860  TotalLoss:0.00194  PostnetLoss:0.00095  DecoderLoss:0.00100  StopLoss:0.04393  GradNorm:0.00322  GradNormST:0.01755  AvgTextLen:30.2  AvgSpecLen:110.4  StepTime:0.67  LR:0.000100\n",
      "   | > Step:49/68  GlobalStep:122870  TotalLoss:0.00211  PostnetLoss:0.00103  DecoderLoss:0.00108  StopLoss:0.04526  GradNorm:0.00332  GradNormST:0.01109  AvgTextLen:38.2  AvgSpecLen:127.7  StepTime:0.77  LR:0.000100\n",
      "   | > Step:59/68  GlobalStep:122880  TotalLoss:0.00226  PostnetLoss:0.00110  DecoderLoss:0.00116  StopLoss:0.04734  GradNorm:0.00291  GradNormST:0.01130  AvgTextLen:50.5  AvgSpecLen:155.9  StepTime:0.71  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:122889  AvgTotalLoss:0.04584  AvgPostnetLoss:0.00090  AvgDecoderLoss:0.00095  AvgStopLoss:0.04398  EpochTime:41.74  AvgStepTime:0.60\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.07770   PostnetLoss: 0.00540   DecoderLoss:0.00568  StopLoss: 0.06663  \n",
      "   | > TotalLoss: 0.08069   PostnetLoss: 0.00894   DecoderLoss:0.00933  StopLoss: 0.06241  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00090   Validation Loss: 0.00803\n",
      "\n",
      " > Epoch 781/1000\n",
      "   | > Step:0/68  GlobalStep:122890  TotalLoss:0.00132  PostnetLoss:0.00063  DecoderLoss:0.00069  StopLoss:0.05491  GradNorm:0.00468  GradNormST:0.02062  AvgTextLen:3.0  AvgSpecLen:25.3  StepTime:0.52  LR:0.000100\n",
      "   | > Step:10/68  GlobalStep:122900  TotalLoss:0.00144  PostnetLoss:0.00070  DecoderLoss:0.00074  StopLoss:0.03438  GradNorm:0.00350  GradNormST:0.01445  AvgTextLen:9.8  AvgSpecLen:48.6  StepTime:0.55  LR:0.000100\n",
      "   | > Step:20/68  GlobalStep:122910  TotalLoss:0.00164  PostnetLoss:0.00080  DecoderLoss:0.00084  StopLoss:0.02970  GradNorm:0.00326  GradNormST:0.01080  AvgTextLen:16.4  AvgSpecLen:64.8  StepTime:0.41  LR:0.000100\n",
      "   | > Step:30/68  GlobalStep:122920  TotalLoss:0.00179  PostnetLoss:0.00087  DecoderLoss:0.00091  StopLoss:0.04707  GradNorm:0.00335  GradNormST:0.01143  AvgTextLen:22.9  AvgSpecLen:85.8  StepTime:0.56  LR:0.000100\n",
      "   | > Step:40/68  GlobalStep:122930  TotalLoss:0.00196  PostnetLoss:0.00096  DecoderLoss:0.00101  StopLoss:0.04082  GradNorm:0.00345  GradNormST:0.00961  AvgTextLen:30.9  AvgSpecLen:105.3  StepTime:0.62  LR:0.000100\n",
      "   | > Step:50/68  GlobalStep:122940  TotalLoss:0.00212  PostnetLoss:0.00103  DecoderLoss:0.00109  StopLoss:0.04148  GradNorm:0.00342  GradNormST:0.01016  AvgTextLen:38.8  AvgSpecLen:132.0  StepTime:0.78  LR:0.000100\n",
      "   | > Step:60/68  GlobalStep:122950  TotalLoss:0.00230  PostnetLoss:0.00112  DecoderLoss:0.00118  StopLoss:0.03241  GradNorm:0.00310  GradNormST:0.00857  AvgTextLen:52.0  AvgSpecLen:172.7  StepTime:0.98  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:122958  AvgTotalLoss:0.04710  AvgPostnetLoss:0.00090  AvgDecoderLoss:0.00095  AvgStopLoss:0.04525  EpochTime:41.78  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.07705   PostnetLoss: 0.00537   DecoderLoss:0.00566  StopLoss: 0.06603  \n",
      "   | > TotalLoss: 0.07999   PostnetLoss: 0.00881   DecoderLoss:0.00920  StopLoss: 0.06198  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00090   Validation Loss: 0.00798\n",
      "\n",
      " > Epoch 782/1000\n",
      "   | > Step:1/68  GlobalStep:122960  TotalLoss:0.00130  PostnetLoss:0.00063  DecoderLoss:0.00067  StopLoss:0.05558  GradNorm:0.00396  GradNormST:0.01600  AvgTextLen:3.7  AvgSpecLen:28.6  StepTime:0.28  LR:0.000100\n",
      "   | > Step:11/68  GlobalStep:122970  TotalLoss:0.00141  PostnetLoss:0.00069  DecoderLoss:0.00072  StopLoss:0.04681  GradNorm:0.00323  GradNormST:0.02182  AvgTextLen:10.3  AvgSpecLen:49.4  StepTime:0.36  LR:0.000100\n",
      "   | > Step:21/68  GlobalStep:122980  TotalLoss:0.00160  PostnetLoss:0.00078  DecoderLoss:0.00082  StopLoss:0.03936  GradNorm:0.00321  GradNormST:0.01032  AvgTextLen:16.6  AvgSpecLen:66.3  StepTime:0.43  LR:0.000100\n",
      "   | > Step:31/68  GlobalStep:122990  TotalLoss:0.00178  PostnetLoss:0.00087  DecoderLoss:0.00091  StopLoss:0.04156  GradNorm:0.00347  GradNormST:0.01504  AvgTextLen:24.1  AvgSpecLen:82.7  StepTime:0.52  LR:0.000100\n",
      "   | > Step:41/68  GlobalStep:123000  TotalLoss:0.00200  PostnetLoss:0.00098  DecoderLoss:0.00103  StopLoss:0.03932  GradNorm:0.00954  GradNormST:0.01043  AvgTextLen:32.3  AvgSpecLen:109.4  StepTime:0.61  LR:0.000100\n",
      " | | > Checkpoint saving : /vm/TTS/models/cartman_models/mozilla-no-loc-fattn-stopnet-sigmoid-loss_masking-August-15-2019_11+26AM-de6aeb9/checkpoint_123000.pth.tar\n",
      "   | > Step:51/68  GlobalStep:123010  TotalLoss:0.00211  PostnetLoss:0.00103  DecoderLoss:0.00108  StopLoss:0.03770  GradNorm:0.00343  GradNormST:0.00986  AvgTextLen:39.2  AvgSpecLen:126.9  StepTime:0.78  LR:0.000100\n",
      "   | > Step:61/68  GlobalStep:123020  TotalLoss:0.00229  PostnetLoss:0.00111  DecoderLoss:0.00118  StopLoss:0.03699  GradNorm:0.00344  GradNormST:0.00911  AvgTextLen:54.6  AvgSpecLen:165.5  StepTime:0.91  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:123027  AvgTotalLoss:0.04967  AvgPostnetLoss:0.00090  AvgDecoderLoss:0.00095  AvgStopLoss:0.04781  EpochTime:42.61  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.07850   PostnetLoss: 0.00534   DecoderLoss:0.00562  StopLoss: 0.06753  \n",
      "   | > TotalLoss: 0.07869   PostnetLoss: 0.00882   DecoderLoss:0.00922  StopLoss: 0.06065  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00090   Validation Loss: 0.00792\n",
      "\n",
      " > Epoch 783/1000\n",
      "   | > Step:2/68  GlobalStep:123030  TotalLoss:0.00121  PostnetLoss:0.00059  DecoderLoss:0.00062  StopLoss:0.04587  GradNorm:0.00374  GradNormST:0.01299  AvgTextLen:4.0  AvgSpecLen:31.3  StepTime:0.32  LR:0.000100\n",
      "   | > Step:12/68  GlobalStep:123040  TotalLoss:0.00142  PostnetLoss:0.00069  DecoderLoss:0.00072  StopLoss:0.06321  GradNorm:0.00313  GradNormST:0.02400  AvgTextLen:11.0  AvgSpecLen:49.3  StepTime:0.42  LR:0.000100\n",
      "   | > Step:22/68  GlobalStep:123050  TotalLoss:0.00166  PostnetLoss:0.00081  DecoderLoss:0.00085  StopLoss:0.04877  GradNorm:0.00303  GradNormST:0.01513  AvgTextLen:18.0  AvgSpecLen:72.0  StepTime:0.45  LR:0.000100\n",
      "   | > Step:32/68  GlobalStep:123060  TotalLoss:0.00180  PostnetLoss:0.00088  DecoderLoss:0.00092  StopLoss:0.06716  GradNorm:0.00320  GradNormST:0.01857  AvgTextLen:24.4  AvgSpecLen:91.5  StepTime:0.57  LR:0.000100\n",
      "   | > Step:42/68  GlobalStep:123070  TotalLoss:0.00198  PostnetLoss:0.00097  DecoderLoss:0.00102  StopLoss:0.05263  GradNorm:0.00328  GradNormST:0.01685  AvgTextLen:33.1  AvgSpecLen:109.7  StepTime:0.61  LR:0.000100\n",
      "   | > Step:52/68  GlobalStep:123080  TotalLoss:0.00218  PostnetLoss:0.00106  DecoderLoss:0.00111  StopLoss:0.05302  GradNorm:0.00366  GradNormST:0.01596  AvgTextLen:42.1  AvgSpecLen:137.5  StepTime:0.67  LR:0.000100\n",
      "   | > Step:62/68  GlobalStep:123090  TotalLoss:0.00236  PostnetLoss:0.00115  DecoderLoss:0.00122  StopLoss:0.03394  GradNorm:0.00326  GradNormST:0.01041  AvgTextLen:54.0  AvgSpecLen:174.7  StepTime:0.76  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:123096  AvgTotalLoss:0.04958  AvgPostnetLoss:0.00090  AvgDecoderLoss:0.00095  AvgStopLoss:0.04772  EpochTime:41.26  AvgStepTime:0.60\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.07863   PostnetLoss: 0.00539   DecoderLoss:0.00568  StopLoss: 0.06756  \n",
      "   | > TotalLoss: 0.07851   PostnetLoss: 0.00893   DecoderLoss:0.00931  StopLoss: 0.06027  \n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00090   Validation Loss: 0.00803\n",
      "\n",
      " > Epoch 784/1000\n",
      "   | > Step:3/68  GlobalStep:123100  TotalLoss:0.00127  PostnetLoss:0.00062  DecoderLoss:0.00065  StopLoss:0.06972  GradNorm:0.00367  GradNormST:0.02717  AvgTextLen:4.7  AvgSpecLen:29.7  StepTime:0.25  LR:0.000100\n",
      "   | > Step:13/68  GlobalStep:123110  TotalLoss:0.00145  PostnetLoss:0.00071  DecoderLoss:0.00074  StopLoss:0.07746  GradNorm:0.00348  GradNormST:0.02393  AvgTextLen:11.5  AvgSpecLen:55.2  StepTime:0.41  LR:0.000100\n",
      "   | > Step:23/68  GlobalStep:123120  TotalLoss:0.00168  PostnetLoss:0.00082  DecoderLoss:0.00086  StopLoss:0.04922  GradNorm:0.00303  GradNormST:0.01300  AvgTextLen:18.3  AvgSpecLen:72.2  StepTime:0.34  LR:0.000100\n",
      "   | > Step:33/68  GlobalStep:123130  TotalLoss:0.00188  PostnetLoss:0.00092  DecoderLoss:0.00096  StopLoss:0.06161  GradNorm:0.00355  GradNormST:0.02024  AvgTextLen:25.9  AvgSpecLen:88.9  StepTime:0.43  LR:0.000100\n",
      "   | > Step:43/68  GlobalStep:123140  TotalLoss:0.00199  PostnetLoss:0.00097  DecoderLoss:0.00102  StopLoss:0.05615  GradNorm:0.00312  GradNormST:0.02029  AvgTextLen:33.4  AvgSpecLen:115.7  StepTime:0.66  LR:0.000100\n",
      "   | > Step:53/68  GlobalStep:123150  TotalLoss:0.00215  PostnetLoss:0.00105  DecoderLoss:0.00111  StopLoss:0.03693  GradNorm:0.00382  GradNormST:0.00955  AvgTextLen:42.4  AvgSpecLen:137.6  StepTime:0.76  LR:0.000100\n",
      "   | > Step:63/68  GlobalStep:123160  TotalLoss:0.00239  PostnetLoss:0.00116  DecoderLoss:0.00123  StopLoss:0.03781  GradNorm:0.00307  GradNormST:0.01744  AvgTextLen:58.6  AvgSpecLen:185.1  StepTime:0.91  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:123165  AvgTotalLoss:0.04855  AvgPostnetLoss:0.00090  AvgDecoderLoss:0.00095  AvgStopLoss:0.04669  EpochTime:41.87  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.07928   PostnetLoss: 0.00528   DecoderLoss:0.00557  StopLoss: 0.06843  \n",
      "   | > TotalLoss: 0.08030   PostnetLoss: 0.00879   DecoderLoss:0.00919  StopLoss: 0.06232  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00090   Validation Loss: 0.00794\n",
      "\n",
      " > Epoch 785/1000\n",
      "   | > Step:4/68  GlobalStep:123170  TotalLoss:0.00131  PostnetLoss:0.00064  DecoderLoss:0.00067  StopLoss:0.06151  GradNorm:0.00348  GradNormST:0.02270  AvgTextLen:4.8  AvgSpecLen:31.1  StepTime:0.31  LR:0.000100\n",
      "   | > Step:14/68  GlobalStep:123180  TotalLoss:0.00149  PostnetLoss:0.00073  DecoderLoss:0.00076  StopLoss:0.04650  GradNorm:0.00366  GradNormST:0.01307  AvgTextLen:12.2  AvgSpecLen:52.0  StepTime:0.35  LR:0.000100\n",
      "   | > Step:24/68  GlobalStep:123190  TotalLoss:0.00173  PostnetLoss:0.00085  DecoderLoss:0.00089  StopLoss:0.05734  GradNorm:0.00298  GradNormST:0.01532  AvgTextLen:19.2  AvgSpecLen:79.0  StepTime:0.52  LR:0.000100\n",
      "   | > Step:34/68  GlobalStep:123200  TotalLoss:0.00188  PostnetLoss:0.00092  DecoderLoss:0.00096  StopLoss:0.03544  GradNorm:0.00302  GradNormST:0.01027  AvgTextLen:26.2  AvgSpecLen:95.9  StepTime:0.57  LR:0.000100\n",
      "   | > Step:44/68  GlobalStep:123210  TotalLoss:0.00205  PostnetLoss:0.00100  DecoderLoss:0.00105  StopLoss:0.02915  GradNorm:0.00317  GradNormST:0.00989  AvgTextLen:33.8  AvgSpecLen:116.3  StepTime:0.58  LR:0.000100\n",
      "   | > Step:54/68  GlobalStep:123220  TotalLoss:0.00220  PostnetLoss:0.00107  DecoderLoss:0.00113  StopLoss:0.04005  GradNorm:0.00369  GradNormST:0.01126  AvgTextLen:42.2  AvgSpecLen:144.9  StepTime:0.81  LR:0.000100\n",
      "   | > Step:64/68  GlobalStep:123230  TotalLoss:0.00225  PostnetLoss:0.00109  DecoderLoss:0.00116  StopLoss:0.03725  GradNorm:0.00293  GradNormST:0.00909  AvgTextLen:60.6  AvgSpecLen:177.6  StepTime:0.80  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:123234  AvgTotalLoss:0.04894  AvgPostnetLoss:0.00090  AvgDecoderLoss:0.00095  AvgStopLoss:0.04709  EpochTime:41.66  AvgStepTime:0.60\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.07892   PostnetLoss: 0.00530   DecoderLoss:0.00559  StopLoss: 0.06803  \n",
      "   | > TotalLoss: 0.08085   PostnetLoss: 0.00888   DecoderLoss:0.00926  StopLoss: 0.06272  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00090   Validation Loss: 0.00800\n",
      "\n",
      " > Epoch 786/1000\n",
      "   | > Step:5/68  GlobalStep:123240  TotalLoss:0.00130  PostnetLoss:0.00063  DecoderLoss:0.00067  StopLoss:0.04133  GradNorm:0.00347  GradNormST:0.01314  AvgTextLen:5.9  AvgSpecLen:33.2  StepTime:0.35  LR:0.000100\n",
      "   | > Step:15/68  GlobalStep:123250  TotalLoss:0.00155  PostnetLoss:0.00076  DecoderLoss:0.00079  StopLoss:0.05239  GradNorm:0.00309  GradNormST:0.02596  AvgTextLen:12.8  AvgSpecLen:59.1  StepTime:0.57  LR:0.000100\n",
      "   | > Step:25/68  GlobalStep:123260  TotalLoss:0.00167  PostnetLoss:0.00081  DecoderLoss:0.00085  StopLoss:0.04319  GradNorm:0.00302  GradNormST:0.01183  AvgTextLen:19.8  AvgSpecLen:73.3  StepTime:0.46  LR:0.000100\n",
      "   | > Step:35/68  GlobalStep:123270  TotalLoss:0.00189  PostnetLoss:0.00092  DecoderLoss:0.00097  StopLoss:0.03934  GradNorm:0.00292  GradNormST:0.01417  AvgTextLen:27.1  AvgSpecLen:104.7  StepTime:0.63  LR:0.000100\n",
      "   | > Step:45/68  GlobalStep:123280  TotalLoss:0.00198  PostnetLoss:0.00096  DecoderLoss:0.00101  StopLoss:0.05509  GradNorm:0.00269  GradNormST:0.01680  AvgTextLen:34.4  AvgSpecLen:118.6  StepTime:0.54  LR:0.000100\n",
      "   | > Step:55/68  GlobalStep:123290  TotalLoss:0.00222  PostnetLoss:0.00108  DecoderLoss:0.00114  StopLoss:0.03608  GradNorm:0.00350  GradNormST:0.01055  AvgTextLen:43.9  AvgSpecLen:145.2  StepTime:0.67  LR:0.000100\n",
      "   | > Step:65/68  GlobalStep:123300  TotalLoss:0.00243  PostnetLoss:0.00118  DecoderLoss:0.00125  StopLoss:0.03807  GradNorm:0.00257  GradNormST:0.01029  AvgTextLen:64.2  AvgSpecLen:207.4  StepTime:1.17  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:123303  AvgTotalLoss:0.04895  AvgPostnetLoss:0.00090  AvgDecoderLoss:0.00095  AvgStopLoss:0.04710  EpochTime:41.27  AvgStepTime:0.60\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.07753   PostnetLoss: 0.00539   DecoderLoss:0.00568  StopLoss: 0.06647  \n",
      "   | > TotalLoss: 0.08134   PostnetLoss: 0.00890   DecoderLoss:0.00931  StopLoss: 0.06314  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00090   Validation Loss: 0.00803\n",
      "\n",
      " > Epoch 787/1000\n",
      "   | > Step:6/68  GlobalStep:123310  TotalLoss:0.00130  PostnetLoss:0.00063  DecoderLoss:0.00067  StopLoss:0.04763  GradNorm:0.00348  GradNormST:0.01951  AvgTextLen:6.0  AvgSpecLen:34.8  StepTime:0.35  LR:0.000100\n",
      "   | > Step:16/68  GlobalStep:123320  TotalLoss:0.00154  PostnetLoss:0.00075  DecoderLoss:0.00079  StopLoss:0.04500  GradNorm:0.00326  GradNormST:0.01483  AvgTextLen:13.8  AvgSpecLen:60.6  StepTime:0.29  LR:0.000100\n",
      "   | > Step:26/68  GlobalStep:123330  TotalLoss:0.00174  PostnetLoss:0.00085  DecoderLoss:0.00089  StopLoss:0.03927  GradNorm:0.00316  GradNormST:0.01212  AvgTextLen:20.3  AvgSpecLen:77.5  StepTime:0.51  LR:0.000100\n",
      "   | > Step:36/68  GlobalStep:123340  TotalLoss:0.00187  PostnetLoss:0.00091  DecoderLoss:0.00096  StopLoss:0.05455  GradNorm:0.00277  GradNormST:0.01206  AvgTextLen:28.5  AvgSpecLen:100.2  StepTime:0.50  LR:0.000100\n",
      "   | > Step:46/68  GlobalStep:123350  TotalLoss:0.00203  PostnetLoss:0.00099  DecoderLoss:0.00104  StopLoss:0.04738  GradNorm:0.00279  GradNormST:0.01423  AvgTextLen:35.4  AvgSpecLen:122.6  StepTime:0.79  LR:0.000100\n",
      "   | > Step:56/68  GlobalStep:123360  TotalLoss:0.00219  PostnetLoss:0.00106  DecoderLoss:0.00112  StopLoss:0.06343  GradNorm:0.00683  GradNormST:0.03902  AvgTextLen:45.8  AvgSpecLen:145.4  StepTime:1.05  LR:0.000100\n",
      "   | > Step:66/68  GlobalStep:123370  TotalLoss:0.00258  PostnetLoss:0.00125  DecoderLoss:0.00133  StopLoss:0.03670  GradNorm:0.00279  GradNormST:0.01688  AvgTextLen:71.2  AvgSpecLen:220.8  StepTime:1.07  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:123372  AvgTotalLoss:0.05183  AvgPostnetLoss:0.00090  AvgDecoderLoss:0.00094  AvgStopLoss:0.04998  EpochTime:42.42  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.07823   PostnetLoss: 0.00536   DecoderLoss:0.00564  StopLoss: 0.06724  \n",
      "   | > TotalLoss: 0.10140   PostnetLoss: 0.00883   DecoderLoss:0.00920  StopLoss: 0.08337  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00090   Validation Loss: 0.00795\n",
      "\n",
      " > Epoch 788/1000\n",
      "   | > Step:7/68  GlobalStep:123380  TotalLoss:0.00138  PostnetLoss:0.00068  DecoderLoss:0.00071  StopLoss:0.03188  GradNorm:0.00349  GradNormST:0.01731  AvgTextLen:7.4  AvgSpecLen:44.6  StepTime:0.39  LR:0.000100\n",
      "   | > Step:17/68  GlobalStep:123390  TotalLoss:0.00157  PostnetLoss:0.00076  DecoderLoss:0.00080  StopLoss:0.04982  GradNorm:0.00340  GradNormST:0.01137  AvgTextLen:14.7  AvgSpecLen:66.0  StepTime:0.45  LR:0.000100\n",
      "   | > Step:27/68  GlobalStep:123400  TotalLoss:0.00176  PostnetLoss:0.00086  DecoderLoss:0.00090  StopLoss:0.04998  GradNorm:0.00305  GradNormST:0.01101  AvgTextLen:21.0  AvgSpecLen:80.2  StepTime:0.53  LR:0.000100\n",
      "   | > Step:37/68  GlobalStep:123410  TotalLoss:0.00192  PostnetLoss:0.00094  DecoderLoss:0.00099  StopLoss:0.06464  GradNorm:0.00284  GradNormST:0.01174  AvgTextLen:29.3  AvgSpecLen:102.4  StepTime:0.55  LR:0.000100\n",
      "   | > Step:47/68  GlobalStep:123420  TotalLoss:0.00207  PostnetLoss:0.00101  DecoderLoss:0.00106  StopLoss:0.03008  GradNorm:0.00280  GradNormST:0.01122  AvgTextLen:36.7  AvgSpecLen:126.9  StepTime:0.70  LR:0.000100\n",
      "   | > Step:57/68  GlobalStep:123430  TotalLoss:0.00225  PostnetLoss:0.00109  DecoderLoss:0.00115  StopLoss:0.03408  GradNorm:0.00295  GradNormST:0.00787  AvgTextLen:47.1  AvgSpecLen:161.0  StepTime:0.84  LR:0.000100\n",
      "   | > Step:67/68  GlobalStep:123440  TotalLoss:0.00260  PostnetLoss:0.00126  DecoderLoss:0.00134  StopLoss:0.03453  GradNorm:0.00265  GradNormST:0.01404  AvgTextLen:80.6  AvgSpecLen:246.5  StepTime:1.22  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:123441  AvgTotalLoss:0.04898  AvgPostnetLoss:0.00090  AvgDecoderLoss:0.00095  AvgStopLoss:0.04713  EpochTime:41.63  AvgStepTime:0.60\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.07606   PostnetLoss: 0.00537   DecoderLoss:0.00565  StopLoss: 0.06504  \n",
      "   | > TotalLoss: 0.08110   PostnetLoss: 0.00893   DecoderLoss:0.00932  StopLoss: 0.06284  \n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00090   Validation Loss: 0.00798\n",
      "\n",
      " > Epoch 789/1000\n",
      "   | > Step:8/68  GlobalStep:123450  TotalLoss:0.00141  PostnetLoss:0.00069  DecoderLoss:0.00072  StopLoss:0.04647  GradNorm:0.00441  GradNormST:0.01209  AvgTextLen:8.3  AvgSpecLen:44.0  StepTime:0.30  LR:0.000100\n",
      "   | > Step:18/68  GlobalStep:123460  TotalLoss:0.00154  PostnetLoss:0.00075  DecoderLoss:0.00079  StopLoss:0.04499  GradNorm:0.00301  GradNormST:0.01374  AvgTextLen:15.2  AvgSpecLen:64.5  StepTime:0.45  LR:0.000100\n",
      "   | > Step:28/68  GlobalStep:123470  TotalLoss:0.00178  PostnetLoss:0.00087  DecoderLoss:0.00091  StopLoss:0.03900  GradNorm:0.00306  GradNormST:0.01183  AvgTextLen:22.1  AvgSpecLen:85.4  StepTime:0.57  LR:0.000100\n",
      "   | > Step:38/68  GlobalStep:123480  TotalLoss:0.00188  PostnetLoss:0.00091  DecoderLoss:0.00096  StopLoss:0.04444  GradNorm:0.00281  GradNormST:0.00951  AvgTextLen:29.4  AvgSpecLen:104.1  StepTime:0.65  LR:0.000100\n",
      "   | > Step:48/68  GlobalStep:123490  TotalLoss:0.00206  PostnetLoss:0.00100  DecoderLoss:0.00105  StopLoss:0.05203  GradNorm:0.00270  GradNormST:0.01640  AvgTextLen:37.5  AvgSpecLen:123.8  StepTime:0.79  LR:0.000100\n",
      "   | > Step:58/68  GlobalStep:123500  TotalLoss:0.00225  PostnetLoss:0.00110  DecoderLoss:0.00116  StopLoss:0.04449  GradNorm:0.00279  GradNormST:0.01399  AvgTextLen:47.8  AvgSpecLen:158.0  StepTime:0.82  LR:0.000100\n",
      "   | > Step:68/68  GlobalStep:123510  TotalLoss:0.00260  PostnetLoss:0.00126  DecoderLoss:0.00134  StopLoss:0.04429  GradNorm:0.00295  GradNormST:0.02304  AvgTextLen:103.0  AvgSpecLen:308.9  StepTime:1.86  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:123510  AvgTotalLoss:0.04864  AvgPostnetLoss:0.00090  AvgDecoderLoss:0.00095  AvgStopLoss:0.04679  EpochTime:42.20  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.07680   PostnetLoss: 0.00541   DecoderLoss:0.00569  StopLoss: 0.06569  \n",
      "   | > TotalLoss: 0.07972   PostnetLoss: 0.00892   DecoderLoss:0.00931  StopLoss: 0.06149  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00090   Validation Loss: 0.00802\n",
      "\n",
      " > Epoch 790/1000\n",
      "   | > Step:9/68  GlobalStep:123520  TotalLoss:0.00138  PostnetLoss:0.00067  DecoderLoss:0.00070  StopLoss:0.04023  GradNorm:0.00345  GradNormST:0.01417  AvgTextLen:9.0  AvgSpecLen:42.1  StepTime:0.32  LR:0.000100\n",
      "   | > Step:19/68  GlobalStep:123530  TotalLoss:0.00155  PostnetLoss:0.00076  DecoderLoss:0.00079  StopLoss:0.06279  GradNorm:0.00315  GradNormST:0.02002  AvgTextLen:15.5  AvgSpecLen:63.2  StepTime:0.42  LR:0.000100\n",
      "   | > Step:29/68  GlobalStep:123540  TotalLoss:0.00171  PostnetLoss:0.00084  DecoderLoss:0.00087  StopLoss:0.05582  GradNorm:0.00289  GradNormST:0.01264  AvgTextLen:21.8  AvgSpecLen:83.4  StepTime:0.59  LR:0.000100\n",
      "   | > Step:39/68  GlobalStep:123550  TotalLoss:0.00195  PostnetLoss:0.00095  DecoderLoss:0.00100  StopLoss:0.03786  GradNorm:0.00271  GradNormST:0.01238  AvgTextLen:30.2  AvgSpecLen:110.4  StepTime:0.68  LR:0.000100\n",
      "   | > Step:49/68  GlobalStep:123560  TotalLoss:0.00205  PostnetLoss:0.00100  DecoderLoss:0.00105  StopLoss:0.05590  GradNorm:0.00261  GradNormST:0.01747  AvgTextLen:38.2  AvgSpecLen:127.7  StepTime:0.77  LR:0.000100\n",
      "   | > Step:59/68  GlobalStep:123570  TotalLoss:0.00224  PostnetLoss:0.00109  DecoderLoss:0.00115  StopLoss:0.05673  GradNorm:0.00270  GradNormST:0.01741  AvgTextLen:50.5  AvgSpecLen:155.9  StepTime:0.88  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:123579  AvgTotalLoss:0.05150  AvgPostnetLoss:0.00090  AvgDecoderLoss:0.00095  AvgStopLoss:0.04965  EpochTime:42.95  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.07739   PostnetLoss: 0.00541   DecoderLoss:0.00568  StopLoss: 0.06630  \n",
      "   | > TotalLoss: 0.08069   PostnetLoss: 0.00894   DecoderLoss:0.00932  StopLoss: 0.06242  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00090   Validation Loss: 0.00803\n",
      "\n",
      " > Epoch 791/1000\n",
      "   | > Step:0/68  GlobalStep:123580  TotalLoss:0.00136  PostnetLoss:0.00065  DecoderLoss:0.00071  StopLoss:0.05876  GradNorm:0.00567  GradNormST:0.01966  AvgTextLen:3.0  AvgSpecLen:25.3  StepTime:0.47  LR:0.000100\n",
      "   | > Step:10/68  GlobalStep:123590  TotalLoss:0.00144  PostnetLoss:0.00070  DecoderLoss:0.00074  StopLoss:0.04448  GradNorm:0.00338  GradNormST:0.01803  AvgTextLen:9.8  AvgSpecLen:48.6  StepTime:0.47  LR:0.000100\n",
      "   | > Step:20/68  GlobalStep:123600  TotalLoss:0.00162  PostnetLoss:0.00079  DecoderLoss:0.00082  StopLoss:0.04017  GradNorm:0.00318  GradNormST:0.01860  AvgTextLen:16.4  AvgSpecLen:64.8  StepTime:0.49  LR:0.000100\n",
      "   | > Step:30/68  GlobalStep:123610  TotalLoss:0.00177  PostnetLoss:0.00086  DecoderLoss:0.00090  StopLoss:0.05241  GradNorm:0.00305  GradNormST:0.01437  AvgTextLen:22.9  AvgSpecLen:85.8  StepTime:0.61  LR:0.000100\n",
      "   | > Step:40/68  GlobalStep:123620  TotalLoss:0.00193  PostnetLoss:0.00094  DecoderLoss:0.00099  StopLoss:0.05209  GradNorm:0.00279  GradNormST:0.01288  AvgTextLen:30.9  AvgSpecLen:105.3  StepTime:0.59  LR:0.000100\n",
      "   | > Step:50/68  GlobalStep:123630  TotalLoss:0.00208  PostnetLoss:0.00101  DecoderLoss:0.00107  StopLoss:0.04771  GradNorm:0.00262  GradNormST:0.01630  AvgTextLen:38.8  AvgSpecLen:132.0  StepTime:0.82  LR:0.000100\n",
      "   | > Step:60/68  GlobalStep:123640  TotalLoss:0.00229  PostnetLoss:0.00111  DecoderLoss:0.00118  StopLoss:0.03019  GradNorm:0.00284  GradNormST:0.00855  AvgTextLen:52.0  AvgSpecLen:172.7  StepTime:0.95  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:123648  AvgTotalLoss:0.05072  AvgPostnetLoss:0.00090  AvgDecoderLoss:0.00095  AvgStopLoss:0.04887  EpochTime:42.40  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.07506   PostnetLoss: 0.00552   DecoderLoss:0.00579  StopLoss: 0.06375  \n",
      "   | > TotalLoss: 0.07827   PostnetLoss: 0.00872   DecoderLoss:0.00911  StopLoss: 0.06044  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00090   Validation Loss: 0.00795\n",
      "\n",
      " > Epoch 792/1000\n",
      "   | > Step:1/68  GlobalStep:123650  TotalLoss:0.00135  PostnetLoss:0.00066  DecoderLoss:0.00070  StopLoss:0.04820  GradNorm:0.00495  GradNormST:0.01740  AvgTextLen:3.7  AvgSpecLen:28.6  StepTime:0.42  LR:0.000100\n",
      "   | > Step:11/68  GlobalStep:123660  TotalLoss:0.00141  PostnetLoss:0.00069  DecoderLoss:0.00072  StopLoss:0.04860  GradNorm:0.00319  GradNormST:0.02117  AvgTextLen:10.3  AvgSpecLen:49.4  StepTime:0.34  LR:0.000100\n",
      "   | > Step:21/68  GlobalStep:123670  TotalLoss:0.00160  PostnetLoss:0.00078  DecoderLoss:0.00082  StopLoss:0.04986  GradNorm:0.00296  GradNormST:0.02134  AvgTextLen:16.6  AvgSpecLen:66.3  StepTime:0.34  LR:0.000100\n",
      "   | > Step:31/68  GlobalStep:123680  TotalLoss:0.00177  PostnetLoss:0.00087  DecoderLoss:0.00091  StopLoss:0.05254  GradNorm:0.00301  GradNormST:0.01712  AvgTextLen:24.1  AvgSpecLen:82.7  StepTime:0.55  LR:0.000100\n",
      "   | > Step:41/68  GlobalStep:123690  TotalLoss:0.00197  PostnetLoss:0.00096  DecoderLoss:0.00101  StopLoss:0.03286  GradNorm:0.00281  GradNormST:0.01415  AvgTextLen:32.3  AvgSpecLen:109.4  StepTime:0.63  LR:0.000100\n",
      "   | > Step:51/68  GlobalStep:123700  TotalLoss:0.00206  PostnetLoss:0.00100  DecoderLoss:0.00106  StopLoss:0.04614  GradNorm:0.00268  GradNormST:0.01290  AvgTextLen:39.2  AvgSpecLen:126.9  StepTime:0.79  LR:0.000100\n",
      "   | > Step:61/68  GlobalStep:123710  TotalLoss:0.00227  PostnetLoss:0.00111  DecoderLoss:0.00117  StopLoss:0.05266  GradNorm:0.00273  GradNormST:0.01922  AvgTextLen:54.6  AvgSpecLen:165.5  StepTime:0.90  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:123717  AvgTotalLoss:0.05058  AvgPostnetLoss:0.00090  AvgDecoderLoss:0.00095  AvgStopLoss:0.04874  EpochTime:41.83  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.07586   PostnetLoss: 0.00549   DecoderLoss:0.00577  StopLoss: 0.06460  \n",
      "   | > TotalLoss: 0.07935   PostnetLoss: 0.00888   DecoderLoss:0.00926  StopLoss: 0.06121  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00090   Validation Loss: 0.00807\n",
      "\n",
      " > Epoch 793/1000\n",
      "   | > Step:2/68  GlobalStep:123720  TotalLoss:0.00121  PostnetLoss:0.00059  DecoderLoss:0.00062  StopLoss:0.04022  GradNorm:0.00381  GradNormST:0.01298  AvgTextLen:4.0  AvgSpecLen:31.3  StepTime:0.44  LR:0.000100\n",
      "   | > Step:12/68  GlobalStep:123730  TotalLoss:0.00143  PostnetLoss:0.00070  DecoderLoss:0.00073  StopLoss:0.07728  GradNorm:0.00321  GradNormST:0.03846  AvgTextLen:11.0  AvgSpecLen:49.3  StepTime:0.47  LR:0.000100\n",
      "   | > Step:22/68  GlobalStep:123740  TotalLoss:0.00162  PostnetLoss:0.00079  DecoderLoss:0.00083  StopLoss:0.05611  GradNorm:0.00315  GradNormST:0.01913  AvgTextLen:18.0  AvgSpecLen:72.0  StepTime:0.48  LR:0.000100\n",
      "   | > Step:32/68  GlobalStep:123750  TotalLoss:0.00178  PostnetLoss:0.00087  DecoderLoss:0.00091  StopLoss:0.06533  GradNorm:0.00298  GradNormST:0.01452  AvgTextLen:24.4  AvgSpecLen:91.5  StepTime:0.54  LR:0.000100\n",
      "   | > Step:42/68  GlobalStep:123760  TotalLoss:0.00196  PostnetLoss:0.00095  DecoderLoss:0.00100  StopLoss:0.04132  GradNorm:0.00328  GradNormST:0.01336  AvgTextLen:33.1  AvgSpecLen:109.7  StepTime:0.63  LR:0.000100\n",
      "   | > Step:52/68  GlobalStep:123770  TotalLoss:0.00217  PostnetLoss:0.00106  DecoderLoss:0.00111  StopLoss:0.07404  GradNorm:0.00291  GradNormST:0.01736  AvgTextLen:42.1  AvgSpecLen:137.5  StepTime:0.80  LR:0.000100\n",
      "   | > Step:62/68  GlobalStep:123780  TotalLoss:0.00234  PostnetLoss:0.00114  DecoderLoss:0.00120  StopLoss:0.03497  GradNorm:0.00254  GradNormST:0.00967  AvgTextLen:54.0  AvgSpecLen:174.7  StepTime:0.80  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:123786  AvgTotalLoss:0.04947  AvgPostnetLoss:0.00090  AvgDecoderLoss:0.00094  AvgStopLoss:0.04762  EpochTime:42.73  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.07399   PostnetLoss: 0.00541   DecoderLoss:0.00570  StopLoss: 0.06288  \n",
      "   | > TotalLoss: 0.07986   PostnetLoss: 0.00902   DecoderLoss:0.00942  StopLoss: 0.06142  \n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00090   Validation Loss: 0.00815\n",
      "\n",
      " > Epoch 794/1000\n",
      "   | > Step:3/68  GlobalStep:123790  TotalLoss:0.00126  PostnetLoss:0.00061  DecoderLoss:0.00064  StopLoss:0.05974  GradNorm:0.00363  GradNormST:0.02620  AvgTextLen:4.7  AvgSpecLen:29.7  StepTime:0.30  LR:0.000100\n",
      "   | > Step:13/68  GlobalStep:123800  TotalLoss:0.00145  PostnetLoss:0.00071  DecoderLoss:0.00074  StopLoss:0.06425  GradNorm:0.00314  GradNormST:0.02048  AvgTextLen:11.5  AvgSpecLen:55.2  StepTime:0.42  LR:0.000100\n",
      "   | > Step:23/68  GlobalStep:123810  TotalLoss:0.00167  PostnetLoss:0.00082  DecoderLoss:0.00086  StopLoss:0.03964  GradNorm:0.00308  GradNormST:0.01066  AvgTextLen:18.3  AvgSpecLen:72.2  StepTime:0.43  LR:0.000100\n",
      "   | > Step:33/68  GlobalStep:123820  TotalLoss:0.00185  PostnetLoss:0.00090  DecoderLoss:0.00094  StopLoss:0.04867  GradNorm:0.00320  GradNormST:0.01155  AvgTextLen:25.9  AvgSpecLen:88.9  StepTime:0.44  LR:0.000100\n",
      "   | > Step:43/68  GlobalStep:123830  TotalLoss:0.00196  PostnetLoss:0.00095  DecoderLoss:0.00100  StopLoss:0.05679  GradNorm:0.00271  GradNormST:0.01591  AvgTextLen:33.4  AvgSpecLen:115.7  StepTime:0.60  LR:0.000100\n",
      "   | > Step:53/68  GlobalStep:123840  TotalLoss:0.00211  PostnetLoss:0.00103  DecoderLoss:0.00108  StopLoss:0.04228  GradNorm:0.00297  GradNormST:0.01311  AvgTextLen:42.4  AvgSpecLen:137.6  StepTime:0.79  LR:0.000100\n",
      "   | > Step:63/68  GlobalStep:123850  TotalLoss:0.00236  PostnetLoss:0.00115  DecoderLoss:0.00121  StopLoss:0.03613  GradNorm:0.00246  GradNormST:0.01535  AvgTextLen:58.6  AvgSpecLen:185.1  StepTime:0.91  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:123855  AvgTotalLoss:0.04935  AvgPostnetLoss:0.00090  AvgDecoderLoss:0.00094  AvgStopLoss:0.04751  EpochTime:42.12  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.07589   PostnetLoss: 0.00541   DecoderLoss:0.00568  StopLoss: 0.06480  \n",
      "   | > TotalLoss: 0.08247   PostnetLoss: 0.00885   DecoderLoss:0.00923  StopLoss: 0.06439  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00090   Validation Loss: 0.00803\n",
      "\n",
      " > Epoch 795/1000\n",
      "   | > Step:4/68  GlobalStep:123860  TotalLoss:0.00128  PostnetLoss:0.00063  DecoderLoss:0.00066  StopLoss:0.07667  GradNorm:0.00385  GradNormST:0.03282  AvgTextLen:4.8  AvgSpecLen:31.1  StepTime:0.27  LR:0.000100\n",
      "   | > Step:14/68  GlobalStep:123870  TotalLoss:0.00148  PostnetLoss:0.00072  DecoderLoss:0.00076  StopLoss:0.04651  GradNorm:0.00361  GradNormST:0.01320  AvgTextLen:12.2  AvgSpecLen:52.0  StepTime:0.26  LR:0.000100\n",
      "   | > Step:24/68  GlobalStep:123880  TotalLoss:0.00173  PostnetLoss:0.00084  DecoderLoss:0.00088  StopLoss:0.05378  GradNorm:0.00303  GradNormST:0.01691  AvgTextLen:19.2  AvgSpecLen:79.0  StepTime:0.47  LR:0.000100\n",
      "   | > Step:34/68  GlobalStep:123890  TotalLoss:0.00186  PostnetLoss:0.00091  DecoderLoss:0.00095  StopLoss:0.03668  GradNorm:0.00303  GradNormST:0.01084  AvgTextLen:26.2  AvgSpecLen:95.9  StepTime:0.63  LR:0.000100\n",
      "   | > Step:44/68  GlobalStep:123900  TotalLoss:0.00199  PostnetLoss:0.00097  DecoderLoss:0.00102  StopLoss:0.04181  GradNorm:0.00274  GradNormST:0.01598  AvgTextLen:33.8  AvgSpecLen:116.3  StepTime:0.65  LR:0.000100\n",
      "   | > Step:54/68  GlobalStep:123910  TotalLoss:0.00216  PostnetLoss:0.00105  DecoderLoss:0.00111  StopLoss:0.04668  GradNorm:0.00290  GradNormST:0.01243  AvgTextLen:42.2  AvgSpecLen:144.9  StepTime:0.69  LR:0.000100\n",
      "   | > Step:64/68  GlobalStep:123920  TotalLoss:0.00227  PostnetLoss:0.00110  DecoderLoss:0.00117  StopLoss:0.04085  GradNorm:0.00277  GradNormST:0.01478  AvgTextLen:60.6  AvgSpecLen:177.6  StepTime:0.81  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:123924  AvgTotalLoss:0.04876  AvgPostnetLoss:0.00090  AvgDecoderLoss:0.00094  AvgStopLoss:0.04692  EpochTime:42.27  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.07598   PostnetLoss: 0.00544   DecoderLoss:0.00571  StopLoss: 0.06483  \n",
      "   | > TotalLoss: 0.07914   PostnetLoss: 0.00878   DecoderLoss:0.00916  StopLoss: 0.06120  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00090   Validation Loss: 0.00802\n",
      "\n",
      " > Epoch 796/1000\n",
      "   | > Step:5/68  GlobalStep:123930  TotalLoss:0.00132  PostnetLoss:0.00064  DecoderLoss:0.00068  StopLoss:0.04570  GradNorm:0.00345  GradNormST:0.01394  AvgTextLen:5.9  AvgSpecLen:33.2  StepTime:0.37  LR:0.000100\n",
      "   | > Step:15/68  GlobalStep:123940  TotalLoss:0.00156  PostnetLoss:0.00076  DecoderLoss:0.00080  StopLoss:0.05543  GradNorm:0.00321  GradNormST:0.03315  AvgTextLen:12.8  AvgSpecLen:59.1  StepTime:0.54  LR:0.000100\n",
      "   | > Step:25/68  GlobalStep:123950  TotalLoss:0.00170  PostnetLoss:0.00083  DecoderLoss:0.00087  StopLoss:0.05700  GradNorm:0.00324  GradNormST:0.01257  AvgTextLen:19.8  AvgSpecLen:73.3  StepTime:0.45  LR:0.000100\n",
      "   | > Step:35/68  GlobalStep:123960  TotalLoss:0.00187  PostnetLoss:0.00091  DecoderLoss:0.00096  StopLoss:0.03924  GradNorm:0.00278  GradNormST:0.01243  AvgTextLen:27.1  AvgSpecLen:104.7  StepTime:0.63  LR:0.000100\n",
      "   | > Step:45/68  GlobalStep:123970  TotalLoss:0.00196  PostnetLoss:0.00095  DecoderLoss:0.00100  StopLoss:0.06237  GradNorm:0.00272  GradNormST:0.01979  AvgTextLen:34.4  AvgSpecLen:118.6  StepTime:0.65  LR:0.000100\n",
      "   | > Step:55/68  GlobalStep:123980  TotalLoss:0.00222  PostnetLoss:0.00108  DecoderLoss:0.00114  StopLoss:0.03891  GradNorm:0.00300  GradNormST:0.01014  AvgTextLen:43.9  AvgSpecLen:145.2  StepTime:0.67  LR:0.000100\n",
      "   | > Step:65/68  GlobalStep:123990  TotalLoss:0.00244  PostnetLoss:0.00118  DecoderLoss:0.00125  StopLoss:0.04551  GradNorm:0.00320  GradNormST:0.01628  AvgTextLen:64.2  AvgSpecLen:207.4  StepTime:1.18  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:123993  AvgTotalLoss:0.04940  AvgPostnetLoss:0.00090  AvgDecoderLoss:0.00094  AvgStopLoss:0.04756  EpochTime:42.03  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.07730   PostnetLoss: 0.00520   DecoderLoss:0.00549  StopLoss: 0.06661  \n",
      "   | > TotalLoss: 0.08002   PostnetLoss: 0.00887   DecoderLoss:0.00929  StopLoss: 0.06185  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00090   Validation Loss: 0.00801\n",
      "\n",
      " > Epoch 797/1000\n",
      "   | > Step:6/68  GlobalStep:124000  TotalLoss:0.00127  PostnetLoss:0.00062  DecoderLoss:0.00065  StopLoss:0.04419  GradNorm:0.00347  GradNormST:0.02296  AvgTextLen:6.0  AvgSpecLen:34.8  StepTime:0.32  LR:0.000100\n",
      " | | > Checkpoint saving : /vm/TTS/models/cartman_models/mozilla-no-loc-fattn-stopnet-sigmoid-loss_masking-August-15-2019_11+26AM-de6aeb9/checkpoint_124000.pth.tar\n",
      "warning: audio amplitude out of range, auto clipped.\n",
      "   | > Step:16/68  GlobalStep:124010  TotalLoss:0.00156  PostnetLoss:0.00076  DecoderLoss:0.00080  StopLoss:0.03767  GradNorm:0.00344  GradNormST:0.01251  AvgTextLen:13.8  AvgSpecLen:60.6  StepTime:0.39  LR:0.000100\n",
      "   | > Step:26/68  GlobalStep:124020  TotalLoss:0.00172  PostnetLoss:0.00084  DecoderLoss:0.00088  StopLoss:0.04648  GradNorm:0.00304  GradNormST:0.01486  AvgTextLen:20.3  AvgSpecLen:77.5  StepTime:0.38  LR:0.000100\n",
      "   | > Step:36/68  GlobalStep:124030  TotalLoss:0.00184  PostnetLoss:0.00090  DecoderLoss:0.00094  StopLoss:0.05822  GradNorm:0.00287  GradNormST:0.01074  AvgTextLen:28.5  AvgSpecLen:100.2  StepTime:0.61  LR:0.000100\n",
      "   | > Step:46/68  GlobalStep:124040  TotalLoss:0.00200  PostnetLoss:0.00097  DecoderLoss:0.00102  StopLoss:0.05069  GradNorm:0.00280  GradNormST:0.02086  AvgTextLen:35.4  AvgSpecLen:122.6  StepTime:0.78  LR:0.000100\n",
      "   | > Step:56/68  GlobalStep:124050  TotalLoss:0.00219  PostnetLoss:0.00107  DecoderLoss:0.00113  StopLoss:0.04700  GradNorm:0.00298  GradNormST:0.02116  AvgTextLen:45.8  AvgSpecLen:145.4  StepTime:0.83  LR:0.000100\n",
      "   | > Step:66/68  GlobalStep:124060  TotalLoss:0.00254  PostnetLoss:0.00123  DecoderLoss:0.00131  StopLoss:0.03868  GradNorm:0.00252  GradNormST:0.01366  AvgTextLen:71.2  AvgSpecLen:220.8  StepTime:1.03  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:124062  AvgTotalLoss:0.04931  AvgPostnetLoss:0.00089  AvgDecoderLoss:0.00094  AvgStopLoss:0.04748  EpochTime:42.49  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.07532   PostnetLoss: 0.00552   DecoderLoss:0.00581  StopLoss: 0.06399  \n",
      "   | > TotalLoss: 0.07909   PostnetLoss: 0.00880   DecoderLoss:0.00918  StopLoss: 0.06110  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00089   Validation Loss: 0.00803\n",
      "\n",
      " > Epoch 798/1000\n",
      "   | > Step:7/68  GlobalStep:124070  TotalLoss:0.00136  PostnetLoss:0.00066  DecoderLoss:0.00069  StopLoss:0.04873  GradNorm:0.00360  GradNormST:0.02267  AvgTextLen:7.4  AvgSpecLen:44.6  StepTime:0.40  LR:0.000100\n",
      "   | > Step:17/68  GlobalStep:124080  TotalLoss:0.00154  PostnetLoss:0.00075  DecoderLoss:0.00079  StopLoss:0.06100  GradNorm:0.00305  GradNormST:0.01775  AvgTextLen:14.7  AvgSpecLen:66.0  StepTime:0.47  LR:0.000100\n",
      "   | > Step:27/68  GlobalStep:124090  TotalLoss:0.00176  PostnetLoss:0.00086  DecoderLoss:0.00090  StopLoss:0.04976  GradNorm:0.00296  GradNormST:0.01292  AvgTextLen:21.0  AvgSpecLen:80.2  StepTime:0.64  LR:0.000100\n",
      "   | > Step:37/68  GlobalStep:124100  TotalLoss:0.00190  PostnetLoss:0.00093  DecoderLoss:0.00097  StopLoss:0.04767  GradNorm:0.00301  GradNormST:0.01139  AvgTextLen:29.3  AvgSpecLen:102.4  StepTime:0.63  LR:0.000100\n",
      "   | > Step:47/68  GlobalStep:124110  TotalLoss:0.00206  PostnetLoss:0.00100  DecoderLoss:0.00106  StopLoss:0.03695  GradNorm:0.00293  GradNormST:0.01029  AvgTextLen:36.7  AvgSpecLen:126.9  StepTime:0.70  LR:0.000100\n",
      "   | > Step:57/68  GlobalStep:124120  TotalLoss:0.00222  PostnetLoss:0.00108  DecoderLoss:0.00114  StopLoss:0.03430  GradNorm:0.00306  GradNormST:0.00783  AvgTextLen:47.1  AvgSpecLen:161.0  StepTime:0.90  LR:0.000100\n",
      "   | > Step:67/68  GlobalStep:124130  TotalLoss:0.00257  PostnetLoss:0.00124  DecoderLoss:0.00132  StopLoss:0.03476  GradNorm:0.00236  GradNormST:0.01234  AvgTextLen:80.6  AvgSpecLen:246.5  StepTime:1.22  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:124131  AvgTotalLoss:0.04589  AvgPostnetLoss:0.00090  AvgDecoderLoss:0.00094  AvgStopLoss:0.04406  EpochTime:42.68  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.07429   PostnetLoss: 0.00546   DecoderLoss:0.00574  StopLoss: 0.06310  \n",
      "   | > TotalLoss: 0.08033   PostnetLoss: 0.00896   DecoderLoss:0.00934  StopLoss: 0.06203  \n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00090   Validation Loss: 0.00807\n",
      "\n",
      " > Epoch 799/1000\n",
      "   | > Step:8/68  GlobalStep:124140  TotalLoss:0.00139  PostnetLoss:0.00068  DecoderLoss:0.00071  StopLoss:0.04349  GradNorm:0.00404  GradNormST:0.01207  AvgTextLen:8.3  AvgSpecLen:44.0  StepTime:0.33  LR:0.000100\n",
      "   | > Step:18/68  GlobalStep:124150  TotalLoss:0.00154  PostnetLoss:0.00075  DecoderLoss:0.00079  StopLoss:0.04601  GradNorm:0.00335  GradNormST:0.01198  AvgTextLen:15.2  AvgSpecLen:64.5  StepTime:0.36  LR:0.000100\n",
      "   | > Step:28/68  GlobalStep:124160  TotalLoss:0.00174  PostnetLoss:0.00085  DecoderLoss:0.00089  StopLoss:0.04042  GradNorm:0.00285  GradNormST:0.01348  AvgTextLen:22.1  AvgSpecLen:85.4  StepTime:0.58  LR:0.000100\n",
      "   | > Step:38/68  GlobalStep:124170  TotalLoss:0.00185  PostnetLoss:0.00090  DecoderLoss:0.00095  StopLoss:0.03813  GradNorm:0.00284  GradNormST:0.00938  AvgTextLen:29.4  AvgSpecLen:104.1  StepTime:0.59  LR:0.000100\n",
      "   | > Step:48/68  GlobalStep:124180  TotalLoss:0.00206  PostnetLoss:0.00100  DecoderLoss:0.00106  StopLoss:0.03485  GradNorm:0.00268  GradNormST:0.00821  AvgTextLen:37.5  AvgSpecLen:123.8  StepTime:0.55  LR:0.000100\n",
      "   | > Step:58/68  GlobalStep:124190  TotalLoss:0.00223  PostnetLoss:0.00109  DecoderLoss:0.00114  StopLoss:0.04224  GradNorm:0.00292  GradNormST:0.01097  AvgTextLen:47.8  AvgSpecLen:158.0  StepTime:0.81  LR:0.000100\n",
      "   | > Step:68/68  GlobalStep:124200  TotalLoss:0.00258  PostnetLoss:0.00125  DecoderLoss:0.00133  StopLoss:0.06913  GradNorm:0.00290  GradNormST:0.04016  AvgTextLen:103.0  AvgSpecLen:308.9  StepTime:1.86  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:124200  AvgTotalLoss:0.04739  AvgPostnetLoss:0.00090  AvgDecoderLoss:0.00094  AvgStopLoss:0.04555  EpochTime:42.18  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.07475   PostnetLoss: 0.00538   DecoderLoss:0.00566  StopLoss: 0.06371  \n",
      "   | > TotalLoss: 0.08070   PostnetLoss: 0.00891   DecoderLoss:0.00932  StopLoss: 0.06247  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00090   Validation Loss: 0.00809\n",
      "\n",
      " > Epoch 800/1000\n",
      "   | > Step:9/68  GlobalStep:124210  TotalLoss:0.00136  PostnetLoss:0.00067  DecoderLoss:0.00070  StopLoss:0.04495  GradNorm:0.00375  GradNormST:0.01536  AvgTextLen:9.0  AvgSpecLen:42.1  StepTime:0.39  LR:0.000100\n",
      "   | > Step:19/68  GlobalStep:124220  TotalLoss:0.00157  PostnetLoss:0.00077  DecoderLoss:0.00080  StopLoss:0.06272  GradNorm:0.00332  GradNormST:0.01674  AvgTextLen:15.5  AvgSpecLen:63.2  StepTime:0.33  LR:0.000100\n",
      "   | > Step:29/68  GlobalStep:124230  TotalLoss:0.00174  PostnetLoss:0.00085  DecoderLoss:0.00089  StopLoss:0.05505  GradNorm:0.00313  GradNormST:0.01799  AvgTextLen:21.8  AvgSpecLen:83.4  StepTime:0.56  LR:0.000100\n",
      "   | > Step:39/68  GlobalStep:124240  TotalLoss:0.00196  PostnetLoss:0.00096  DecoderLoss:0.00100  StopLoss:0.04555  GradNorm:0.00282  GradNormST:0.01690  AvgTextLen:30.2  AvgSpecLen:110.4  StepTime:0.69  LR:0.000100\n",
      "   | > Step:49/68  GlobalStep:124250  TotalLoss:0.00207  PostnetLoss:0.00101  DecoderLoss:0.00106  StopLoss:0.03367  GradNorm:0.00274  GradNormST:0.00674  AvgTextLen:38.2  AvgSpecLen:127.7  StepTime:0.72  LR:0.000100\n",
      "   | > Step:59/68  GlobalStep:124260  TotalLoss:0.00220  PostnetLoss:0.00107  DecoderLoss:0.00113  StopLoss:0.04550  GradNorm:0.00266  GradNormST:0.01401  AvgTextLen:50.5  AvgSpecLen:155.9  StepTime:0.71  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:124269  AvgTotalLoss:0.04613  AvgPostnetLoss:0.00089  AvgDecoderLoss:0.00094  AvgStopLoss:0.04430  EpochTime:42.13  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.07671   PostnetLoss: 0.00553   DecoderLoss:0.00582  StopLoss: 0.06537  \n",
      "   | > TotalLoss: 0.08477   PostnetLoss: 0.00912   DecoderLoss:0.00950  StopLoss: 0.06614  \n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00089   Validation Loss: 0.00832\n",
      "\n",
      " > Epoch 801/1000\n",
      "   | > Step:0/68  GlobalStep:124270  TotalLoss:0.00135  PostnetLoss:0.00065  DecoderLoss:0.00071  StopLoss:0.05607  GradNorm:0.00590  GradNormST:0.02766  AvgTextLen:3.0  AvgSpecLen:25.3  StepTime:0.49  LR:0.000100\n",
      "   | > Step:10/68  GlobalStep:124280  TotalLoss:0.00144  PostnetLoss:0.00070  DecoderLoss:0.00074  StopLoss:0.04187  GradNorm:0.00358  GradNormST:0.01651  AvgTextLen:9.8  AvgSpecLen:48.6  StepTime:0.53  LR:0.000100\n",
      "   | > Step:20/68  GlobalStep:124290  TotalLoss:0.00173  PostnetLoss:0.00085  DecoderLoss:0.00088  StopLoss:0.04099  GradNorm:0.00394  GradNormST:0.01319  AvgTextLen:16.4  AvgSpecLen:64.8  StepTime:0.49  LR:0.000100\n",
      "   | > Step:30/68  GlobalStep:124300  TotalLoss:0.00182  PostnetLoss:0.00089  DecoderLoss:0.00093  StopLoss:0.03873  GradNorm:0.00303  GradNormST:0.01430  AvgTextLen:22.9  AvgSpecLen:85.8  StepTime:0.55  LR:0.000100\n",
      "   | > Step:40/68  GlobalStep:124310  TotalLoss:0.00195  PostnetLoss:0.00095  DecoderLoss:0.00100  StopLoss:0.03843  GradNorm:0.00284  GradNormST:0.01265  AvgTextLen:30.9  AvgSpecLen:105.3  StepTime:0.65  LR:0.000100\n",
      "   | > Step:50/68  GlobalStep:124320  TotalLoss:0.00213  PostnetLoss:0.00104  DecoderLoss:0.00109  StopLoss:0.05998  GradNorm:0.00300  GradNormST:0.02104  AvgTextLen:38.8  AvgSpecLen:132.0  StepTime:0.81  LR:0.000100\n",
      "   | > Step:60/68  GlobalStep:124330  TotalLoss:0.00232  PostnetLoss:0.00113  DecoderLoss:0.00119  StopLoss:0.03054  GradNorm:0.00289  GradNormST:0.00723  AvgTextLen:52.0  AvgSpecLen:172.7  StepTime:0.79  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:124338  AvgTotalLoss:0.04733  AvgPostnetLoss:0.00092  AvgDecoderLoss:0.00097  AvgStopLoss:0.04544  EpochTime:41.58  AvgStepTime:0.60\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.08413   PostnetLoss: 0.00536   DecoderLoss:0.00565  StopLoss: 0.07312  \n",
      "   | > TotalLoss: 0.08626   PostnetLoss: 0.00904   DecoderLoss:0.00944  StopLoss: 0.06778  \n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00092   Validation Loss: 0.00825\n",
      "\n",
      " > Epoch 802/1000\n",
      "   | > Step:1/68  GlobalStep:124340  TotalLoss:0.00132  PostnetLoss:0.00064  DecoderLoss:0.00068  StopLoss:0.05009  GradNorm:0.00497  GradNormST:0.01709  AvgTextLen:3.7  AvgSpecLen:28.6  StepTime:0.34  LR:0.000100\n",
      "   | > Step:11/68  GlobalStep:124350  TotalLoss:0.00144  PostnetLoss:0.00070  DecoderLoss:0.00074  StopLoss:0.03434  GradNorm:0.00351  GradNormST:0.01131  AvgTextLen:10.3  AvgSpecLen:49.4  StepTime:0.38  LR:0.000100\n",
      "   | > Step:21/68  GlobalStep:124360  TotalLoss:0.00163  PostnetLoss:0.00080  DecoderLoss:0.00083  StopLoss:0.05145  GradNorm:0.00355  GradNormST:0.01435  AvgTextLen:16.6  AvgSpecLen:66.3  StepTime:0.42  LR:0.000100\n",
      "   | > Step:31/68  GlobalStep:124370  TotalLoss:0.00177  PostnetLoss:0.00087  DecoderLoss:0.00090  StopLoss:0.04711  GradNorm:0.00341  GradNormST:0.01401  AvgTextLen:24.1  AvgSpecLen:82.7  StepTime:0.56  LR:0.000100\n",
      "   | > Step:41/68  GlobalStep:124380  TotalLoss:0.00200  PostnetLoss:0.00098  DecoderLoss:0.00102  StopLoss:0.03386  GradNorm:0.00295  GradNormST:0.01087  AvgTextLen:32.3  AvgSpecLen:109.4  StepTime:0.52  LR:0.000100\n",
      "   | > Step:51/68  GlobalStep:124390  TotalLoss:0.00208  PostnetLoss:0.00101  DecoderLoss:0.00107  StopLoss:0.03335  GradNorm:0.00276  GradNormST:0.00811  AvgTextLen:39.2  AvgSpecLen:126.9  StepTime:0.74  LR:0.000100\n",
      "   | > Step:61/68  GlobalStep:124400  TotalLoss:0.00231  PostnetLoss:0.00112  DecoderLoss:0.00119  StopLoss:0.03588  GradNorm:0.00277  GradNormST:0.00876  AvgTextLen:54.6  AvgSpecLen:165.5  StepTime:0.90  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:124407  AvgTotalLoss:0.04709  AvgPostnetLoss:0.00091  AvgDecoderLoss:0.00096  AvgStopLoss:0.04522  EpochTime:41.95  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.07883   PostnetLoss: 0.00548   DecoderLoss:0.00577  StopLoss: 0.06758  \n",
      "   | > TotalLoss: 0.08554   PostnetLoss: 0.00919   DecoderLoss:0.00958  StopLoss: 0.06677  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00091   Validation Loss: 0.00833\n",
      "\n",
      " > Epoch 803/1000\n",
      "   | > Step:2/68  GlobalStep:124410  TotalLoss:0.00122  PostnetLoss:0.00059  DecoderLoss:0.00063  StopLoss:0.03599  GradNorm:0.00422  GradNormST:0.01713  AvgTextLen:4.0  AvgSpecLen:31.3  StepTime:0.38  LR:0.000100\n",
      "   | > Step:12/68  GlobalStep:124420  TotalLoss:0.00143  PostnetLoss:0.00070  DecoderLoss:0.00073  StopLoss:0.05008  GradNorm:0.00360  GradNormST:0.01315  AvgTextLen:11.0  AvgSpecLen:49.3  StepTime:0.41  LR:0.000100\n",
      "   | > Step:22/68  GlobalStep:124430  TotalLoss:0.00169  PostnetLoss:0.00083  DecoderLoss:0.00087  StopLoss:0.06338  GradNorm:0.00377  GradNormST:0.01753  AvgTextLen:18.0  AvgSpecLen:72.0  StepTime:0.46  LR:0.000100\n",
      "   | > Step:32/68  GlobalStep:124440  TotalLoss:0.00184  PostnetLoss:0.00090  DecoderLoss:0.00094  StopLoss:0.06736  GradNorm:0.00334  GradNormST:0.02849  AvgTextLen:24.4  AvgSpecLen:91.5  StepTime:0.56  LR:0.000100\n",
      "   | > Step:42/68  GlobalStep:124450  TotalLoss:0.00200  PostnetLoss:0.00097  DecoderLoss:0.00103  StopLoss:0.04726  GradNorm:0.00332  GradNormST:0.01411  AvgTextLen:33.1  AvgSpecLen:109.7  StepTime:0.58  LR:0.000100\n",
      "   | > Step:52/68  GlobalStep:124460  TotalLoss:0.00215  PostnetLoss:0.00105  DecoderLoss:0.00110  StopLoss:0.05954  GradNorm:0.00288  GradNormST:0.01509  AvgTextLen:42.1  AvgSpecLen:137.5  StepTime:0.77  LR:0.000100\n",
      "   | > Step:62/68  GlobalStep:124470  TotalLoss:0.00235  PostnetLoss:0.00114  DecoderLoss:0.00121  StopLoss:0.03966  GradNorm:0.00252  GradNormST:0.00997  AvgTextLen:54.0  AvgSpecLen:174.7  StepTime:0.75  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:124476  AvgTotalLoss:0.04783  AvgPostnetLoss:0.00090  AvgDecoderLoss:0.00095  AvgStopLoss:0.04598  EpochTime:42.29  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.07878   PostnetLoss: 0.00547   DecoderLoss:0.00575  StopLoss: 0.06756  \n",
      "   | > TotalLoss: 0.08227   PostnetLoss: 0.00908   DecoderLoss:0.00948  StopLoss: 0.06371  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00090   Validation Loss: 0.00829\n",
      "\n",
      " > Epoch 804/1000\n",
      "   | > Step:3/68  GlobalStep:124480  TotalLoss:0.00126  PostnetLoss:0.00061  DecoderLoss:0.00064  StopLoss:0.07044  GradNorm:0.00358  GradNormST:0.02363  AvgTextLen:4.7  AvgSpecLen:29.7  StepTime:0.31  LR:0.000100\n",
      "   | > Step:13/68  GlobalStep:124490  TotalLoss:0.00143  PostnetLoss:0.00070  DecoderLoss:0.00073  StopLoss:0.04991  GradNorm:0.00318  GradNormST:0.01985  AvgTextLen:11.5  AvgSpecLen:55.2  StepTime:0.33  LR:0.000100\n",
      "   | > Step:23/68  GlobalStep:124500  TotalLoss:0.00170  PostnetLoss:0.00083  DecoderLoss:0.00087  StopLoss:0.03673  GradNorm:0.00364  GradNormST:0.00944  AvgTextLen:18.3  AvgSpecLen:72.2  StepTime:0.38  LR:0.000100\n",
      "   | > Step:33/68  GlobalStep:124510  TotalLoss:0.00185  PostnetLoss:0.00091  DecoderLoss:0.00095  StopLoss:0.04270  GradNorm:0.00359  GradNormST:0.01155  AvgTextLen:25.9  AvgSpecLen:88.9  StepTime:0.57  LR:0.000100\n",
      "   | > Step:43/68  GlobalStep:124520  TotalLoss:0.00200  PostnetLoss:0.00097  DecoderLoss:0.00102  StopLoss:0.03709  GradNorm:0.00331  GradNormST:0.00899  AvgTextLen:33.4  AvgSpecLen:115.7  StepTime:0.55  LR:0.000100\n",
      "   | > Step:53/68  GlobalStep:124530  TotalLoss:0.00209  PostnetLoss:0.00101  DecoderLoss:0.00107  StopLoss:0.03549  GradNorm:0.00262  GradNormST:0.00943  AvgTextLen:42.4  AvgSpecLen:137.6  StepTime:0.81  LR:0.000100\n",
      "   | > Step:63/68  GlobalStep:124540  TotalLoss:0.00237  PostnetLoss:0.00115  DecoderLoss:0.00122  StopLoss:0.03969  GradNorm:0.00263  GradNormST:0.01435  AvgTextLen:58.6  AvgSpecLen:185.1  StepTime:0.90  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:124545  AvgTotalLoss:0.04728  AvgPostnetLoss:0.00090  AvgDecoderLoss:0.00095  AvgStopLoss:0.04543  EpochTime:42.38  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.07625   PostnetLoss: 0.00550   DecoderLoss:0.00578  StopLoss: 0.06497  \n",
      "   | > TotalLoss: 0.08697   PostnetLoss: 0.00927   DecoderLoss:0.00966  StopLoss: 0.06803  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00090   Validation Loss: 0.00833\n",
      "\n",
      " > Epoch 805/1000\n",
      "   | > Step:4/68  GlobalStep:124550  TotalLoss:0.00126  PostnetLoss:0.00061  DecoderLoss:0.00064  StopLoss:0.06937  GradNorm:0.00380  GradNormST:0.02383  AvgTextLen:4.8  AvgSpecLen:31.1  StepTime:0.24  LR:0.000100\n",
      "   | > Step:14/68  GlobalStep:124560  TotalLoss:0.00150  PostnetLoss:0.00073  DecoderLoss:0.00077  StopLoss:0.04780  GradNorm:0.00331  GradNormST:0.01672  AvgTextLen:12.2  AvgSpecLen:52.0  StepTime:0.34  LR:0.000100\n",
      "   | > Step:24/68  GlobalStep:124570  TotalLoss:0.00173  PostnetLoss:0.00085  DecoderLoss:0.00089  StopLoss:0.05238  GradNorm:0.00340  GradNormST:0.01299  AvgTextLen:19.2  AvgSpecLen:79.0  StepTime:0.50  LR:0.000100\n",
      "   | > Step:34/68  GlobalStep:124580  TotalLoss:0.00187  PostnetLoss:0.00092  DecoderLoss:0.00096  StopLoss:0.03336  GradNorm:0.00326  GradNormST:0.01068  AvgTextLen:26.2  AvgSpecLen:95.9  StepTime:0.62  LR:0.000100\n",
      "   | > Step:44/68  GlobalStep:124590  TotalLoss:0.00204  PostnetLoss:0.00099  DecoderLoss:0.00104  StopLoss:0.03603  GradNorm:0.00306  GradNormST:0.01096  AvgTextLen:33.8  AvgSpecLen:116.3  StepTime:0.55  LR:0.000100\n",
      "   | > Step:54/68  GlobalStep:124600  TotalLoss:0.00214  PostnetLoss:0.00104  DecoderLoss:0.00110  StopLoss:0.03968  GradNorm:0.00265  GradNormST:0.01036  AvgTextLen:42.2  AvgSpecLen:144.9  StepTime:0.86  LR:0.000100\n",
      "   | > Step:64/68  GlobalStep:124610  TotalLoss:0.00226  PostnetLoss:0.00110  DecoderLoss:0.00116  StopLoss:0.03943  GradNorm:0.00268  GradNormST:0.01338  AvgTextLen:60.6  AvgSpecLen:177.6  StepTime:0.81  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:124614  AvgTotalLoss:0.05073  AvgPostnetLoss:0.00090  AvgDecoderLoss:0.00094  AvgStopLoss:0.04889  EpochTime:42.49  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.07478   PostnetLoss: 0.00544   DecoderLoss:0.00572  StopLoss: 0.06361  \n",
      "   | > TotalLoss: 0.08193   PostnetLoss: 0.00906   DecoderLoss:0.00945  StopLoss: 0.06342  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00090   Validation Loss: 0.00827\n",
      "\n",
      " > Epoch 806/1000\n",
      "   | > Step:5/68  GlobalStep:124620  TotalLoss:0.00130  PostnetLoss:0.00063  DecoderLoss:0.00067  StopLoss:0.05546  GradNorm:0.00328  GradNormST:0.02668  AvgTextLen:5.9  AvgSpecLen:33.2  StepTime:0.27  LR:0.000100\n",
      "   | > Step:15/68  GlobalStep:124630  TotalLoss:0.00156  PostnetLoss:0.00076  DecoderLoss:0.00080  StopLoss:0.07132  GradNorm:0.00338  GradNormST:0.03182  AvgTextLen:12.8  AvgSpecLen:59.1  StepTime:0.54  LR:0.000100\n",
      "   | > Step:25/68  GlobalStep:124640  TotalLoss:0.00168  PostnetLoss:0.00082  DecoderLoss:0.00086  StopLoss:0.03543  GradNorm:0.00379  GradNormST:0.01066  AvgTextLen:19.8  AvgSpecLen:73.3  StepTime:0.48  LR:0.000100\n",
      "   | > Step:35/68  GlobalStep:124650  TotalLoss:0.00188  PostnetLoss:0.00092  DecoderLoss:0.00097  StopLoss:0.05266  GradNorm:0.00349  GradNormST:0.01937  AvgTextLen:27.1  AvgSpecLen:104.7  StepTime:0.74  LR:0.000100\n",
      "   | > Step:45/68  GlobalStep:124660  TotalLoss:0.00196  PostnetLoss:0.00096  DecoderLoss:0.00101  StopLoss:0.04884  GradNorm:0.00322  GradNormST:0.01119  AvgTextLen:34.4  AvgSpecLen:118.6  StepTime:0.54  LR:0.000100\n",
      "   | > Step:55/68  GlobalStep:124670  TotalLoss:0.00221  PostnetLoss:0.00108  DecoderLoss:0.00114  StopLoss:0.03801  GradNorm:0.00296  GradNormST:0.01010  AvgTextLen:43.9  AvgSpecLen:145.2  StepTime:0.86  LR:0.000100\n",
      "   | > Step:65/68  GlobalStep:124680  TotalLoss:0.00242  PostnetLoss:0.00117  DecoderLoss:0.00124  StopLoss:0.05808  GradNorm:0.00250  GradNormST:0.02046  AvgTextLen:64.2  AvgSpecLen:207.4  StepTime:1.19  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:124683  AvgTotalLoss:0.04911  AvgPostnetLoss:0.00090  AvgDecoderLoss:0.00094  AvgStopLoss:0.04727  EpochTime:42.08  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.07544   PostnetLoss: 0.00550   DecoderLoss:0.00578  StopLoss: 0.06417  \n",
      "   | > TotalLoss: 0.08015   PostnetLoss: 0.00906   DecoderLoss:0.00944  StopLoss: 0.06166  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00090   Validation Loss: 0.00818\n",
      "\n",
      " > Epoch 807/1000\n",
      "   | > Step:6/68  GlobalStep:124690  TotalLoss:0.00130  PostnetLoss:0.00064  DecoderLoss:0.00066  StopLoss:0.03634  GradNorm:0.00396  GradNormST:0.01610  AvgTextLen:6.0  AvgSpecLen:34.8  StepTime:0.36  LR:0.000100\n",
      "   | > Step:16/68  GlobalStep:124700  TotalLoss:0.00155  PostnetLoss:0.00076  DecoderLoss:0.00079  StopLoss:0.03464  GradNorm:0.00328  GradNormST:0.01165  AvgTextLen:13.8  AvgSpecLen:60.6  StepTime:0.36  LR:0.000100\n",
      "   | > Step:26/68  GlobalStep:124710  TotalLoss:0.00171  PostnetLoss:0.00084  DecoderLoss:0.00088  StopLoss:0.05869  GradNorm:0.00332  GradNormST:0.01670  AvgTextLen:20.3  AvgSpecLen:77.5  StepTime:0.42  LR:0.000100\n",
      "   | > Step:36/68  GlobalStep:124720  TotalLoss:0.00185  PostnetLoss:0.00091  DecoderLoss:0.00095  StopLoss:0.04002  GradNorm:0.00358  GradNormST:0.01256  AvgTextLen:28.5  AvgSpecLen:100.2  StepTime:0.60  LR:0.000100\n",
      "   | > Step:46/68  GlobalStep:124730  TotalLoss:0.00202  PostnetLoss:0.00099  DecoderLoss:0.00103  StopLoss:0.04796  GradNorm:0.00371  GradNormST:0.01817  AvgTextLen:35.4  AvgSpecLen:122.6  StepTime:0.78  LR:0.000100\n",
      "   | > Step:56/68  GlobalStep:124740  TotalLoss:0.00218  PostnetLoss:0.00106  DecoderLoss:0.00112  StopLoss:0.03647  GradNorm:0.00310  GradNormST:0.01581  AvgTextLen:45.8  AvgSpecLen:145.4  StepTime:0.95  LR:0.000100\n",
      "   | > Step:66/68  GlobalStep:124750  TotalLoss:0.00256  PostnetLoss:0.00124  DecoderLoss:0.00132  StopLoss:0.03394  GradNorm:0.00285  GradNormST:0.01379  AvgTextLen:71.2  AvgSpecLen:220.8  StepTime:1.04  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:124752  AvgTotalLoss:0.04924  AvgPostnetLoss:0.00089  AvgDecoderLoss:0.00094  AvgStopLoss:0.04740  EpochTime:41.86  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.07757   PostnetLoss: 0.00548   DecoderLoss:0.00575  StopLoss: 0.06634  \n",
      "   | > TotalLoss: 0.08352   PostnetLoss: 0.00924   DecoderLoss:0.00963  StopLoss: 0.06464  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00089   Validation Loss: 0.00836\n",
      "\n",
      " > Epoch 808/1000\n",
      "   | > Step:7/68  GlobalStep:124760  TotalLoss:0.00138  PostnetLoss:0.00067  DecoderLoss:0.00071  StopLoss:0.03724  GradNorm:0.00353  GradNormST:0.01360  AvgTextLen:7.4  AvgSpecLen:44.6  StepTime:0.40  LR:0.000100\n",
      "   | > Step:17/68  GlobalStep:124770  TotalLoss:0.00156  PostnetLoss:0.00076  DecoderLoss:0.00080  StopLoss:0.05592  GradNorm:0.00319  GradNormST:0.01407  AvgTextLen:14.7  AvgSpecLen:66.0  StepTime:0.42  LR:0.000100\n",
      "   | > Step:27/68  GlobalStep:124780  TotalLoss:0.00172  PostnetLoss:0.00084  DecoderLoss:0.00088  StopLoss:0.04739  GradNorm:0.00299  GradNormST:0.01401  AvgTextLen:21.0  AvgSpecLen:80.2  StepTime:0.63  LR:0.000100\n",
      "   | > Step:37/68  GlobalStep:124790  TotalLoss:0.00190  PostnetLoss:0.00093  DecoderLoss:0.00097  StopLoss:0.05120  GradNorm:0.00309  GradNormST:0.01586  AvgTextLen:29.3  AvgSpecLen:102.4  StepTime:0.62  LR:0.000100\n",
      "   | > Step:47/68  GlobalStep:124800  TotalLoss:0.00206  PostnetLoss:0.00100  DecoderLoss:0.00106  StopLoss:0.03144  GradNorm:0.00335  GradNormST:0.00970  AvgTextLen:36.7  AvgSpecLen:126.9  StepTime:0.70  LR:0.000100\n",
      "   | > Step:57/68  GlobalStep:124810  TotalLoss:0.00225  PostnetLoss:0.00110  DecoderLoss:0.00116  StopLoss:0.03835  GradNorm:0.00341  GradNormST:0.00826  AvgTextLen:47.1  AvgSpecLen:161.0  StepTime:0.89  LR:0.000100\n",
      "   | > Step:67/68  GlobalStep:124820  TotalLoss:0.00256  PostnetLoss:0.00124  DecoderLoss:0.00132  StopLoss:0.04237  GradNorm:0.00243  GradNormST:0.01233  AvgTextLen:80.6  AvgSpecLen:246.5  StepTime:1.23  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:124821  AvgTotalLoss:0.04362  AvgPostnetLoss:0.00089  AvgDecoderLoss:0.00094  AvgStopLoss:0.04178  EpochTime:42.13  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.07452   PostnetLoss: 0.00553   DecoderLoss:0.00580  StopLoss: 0.06319  \n",
      "   | > TotalLoss: 0.08418   PostnetLoss: 0.00920   DecoderLoss:0.00959  StopLoss: 0.06539  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00089   Validation Loss: 0.00836\n",
      "\n",
      " > Epoch 809/1000\n",
      "   | > Step:8/68  GlobalStep:124830  TotalLoss:0.00136  PostnetLoss:0.00066  DecoderLoss:0.00069  StopLoss:0.04422  GradNorm:0.00334  GradNormST:0.01378  AvgTextLen:8.3  AvgSpecLen:44.0  StepTime:0.33  LR:0.000100\n",
      "   | > Step:18/68  GlobalStep:124840  TotalLoss:0.00152  PostnetLoss:0.00074  DecoderLoss:0.00078  StopLoss:0.04665  GradNorm:0.00295  GradNormST:0.01445  AvgTextLen:15.2  AvgSpecLen:64.5  StepTime:0.43  LR:0.000100\n",
      "   | > Step:28/68  GlobalStep:124850  TotalLoss:0.00175  PostnetLoss:0.00085  DecoderLoss:0.00089  StopLoss:0.04732  GradNorm:0.00295  GradNormST:0.01061  AvgTextLen:22.1  AvgSpecLen:85.4  StepTime:0.55  LR:0.000100\n",
      "   | > Step:38/68  GlobalStep:124860  TotalLoss:0.00187  PostnetLoss:0.00091  DecoderLoss:0.00096  StopLoss:0.04075  GradNorm:0.00329  GradNormST:0.00937  AvgTextLen:29.4  AvgSpecLen:104.1  StepTime:0.66  LR:0.000100\n",
      "   | > Step:48/68  GlobalStep:124870  TotalLoss:0.00205  PostnetLoss:0.00100  DecoderLoss:0.00105  StopLoss:0.03743  GradNorm:0.00302  GradNormST:0.01279  AvgTextLen:37.5  AvgSpecLen:123.8  StepTime:0.72  LR:0.000100\n",
      "   | > Step:58/68  GlobalStep:124880  TotalLoss:0.00227  PostnetLoss:0.00111  DecoderLoss:0.00116  StopLoss:0.04956  GradNorm:0.00357  GradNormST:0.01500  AvgTextLen:47.8  AvgSpecLen:158.0  StepTime:0.86  LR:0.000100\n",
      "   | > Step:68/68  GlobalStep:124890  TotalLoss:0.00265  PostnetLoss:0.00129  DecoderLoss:0.00137  StopLoss:0.03190  GradNorm:0.00352  GradNormST:0.01248  AvgTextLen:103.0  AvgSpecLen:308.9  StepTime:1.87  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:124890  AvgTotalLoss:0.04765  AvgPostnetLoss:0.00089  AvgDecoderLoss:0.00094  AvgStopLoss:0.04582  EpochTime:43.19  AvgStepTime:0.63\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.07551   PostnetLoss: 0.00544   DecoderLoss:0.00571  StopLoss: 0.06436  \n",
      "   | > TotalLoss: 0.08547   PostnetLoss: 0.00920   DecoderLoss:0.00959  StopLoss: 0.06668  \n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00089   Validation Loss: 0.00835\n",
      "\n",
      " > Epoch 810/1000\n",
      "   | > Step:9/68  GlobalStep:124900  TotalLoss:0.00134  PostnetLoss:0.00066  DecoderLoss:0.00069  StopLoss:0.03678  GradNorm:0.00321  GradNormST:0.01347  AvgTextLen:9.0  AvgSpecLen:42.1  StepTime:0.42  LR:0.000100\n",
      "   | > Step:19/68  GlobalStep:124910  TotalLoss:0.00156  PostnetLoss:0.00076  DecoderLoss:0.00080  StopLoss:0.05634  GradNorm:0.00327  GradNormST:0.01982  AvgTextLen:15.5  AvgSpecLen:63.2  StepTime:0.41  LR:0.000100\n",
      "   | > Step:29/68  GlobalStep:124920  TotalLoss:0.00170  PostnetLoss:0.00083  DecoderLoss:0.00087  StopLoss:0.05163  GradNorm:0.00273  GradNormST:0.01462  AvgTextLen:21.8  AvgSpecLen:83.4  StepTime:0.59  LR:0.000100\n",
      "   | > Step:39/68  GlobalStep:124930  TotalLoss:0.00192  PostnetLoss:0.00094  DecoderLoss:0.00098  StopLoss:0.03859  GradNorm:0.00287  GradNormST:0.01034  AvgTextLen:30.2  AvgSpecLen:110.4  StepTime:0.67  LR:0.000100\n",
      "   | > Step:49/68  GlobalStep:124940  TotalLoss:0.00207  PostnetLoss:0.00101  DecoderLoss:0.00106  StopLoss:0.03240  GradNorm:0.00334  GradNormST:0.00787  AvgTextLen:38.2  AvgSpecLen:127.7  StepTime:0.76  LR:0.000100\n",
      "   | > Step:59/68  GlobalStep:124950  TotalLoss:0.00221  PostnetLoss:0.00108  DecoderLoss:0.00114  StopLoss:0.04525  GradNorm:0.00298  GradNormST:0.01789  AvgTextLen:50.5  AvgSpecLen:155.9  StepTime:0.88  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:124959  AvgTotalLoss:0.04570  AvgPostnetLoss:0.00089  AvgDecoderLoss:0.00094  AvgStopLoss:0.04387  EpochTime:43.25  AvgStepTime:0.63\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.07738   PostnetLoss: 0.00544   DecoderLoss:0.00572  StopLoss: 0.06623  \n",
      "   | > TotalLoss: 0.08164   PostnetLoss: 0.00925   DecoderLoss:0.00965  StopLoss: 0.06274  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00089   Validation Loss: 0.00836\n",
      "\n",
      " > Epoch 811/1000\n",
      "   | > Step:0/68  GlobalStep:124960  TotalLoss:0.00136  PostnetLoss:0.00065  DecoderLoss:0.00071  StopLoss:0.05815  GradNorm:0.00496  GradNormST:0.02833  AvgTextLen:3.0  AvgSpecLen:25.3  StepTime:0.46  LR:0.000100\n",
      "   | > Step:10/68  GlobalStep:124970  TotalLoss:0.00144  PostnetLoss:0.00070  DecoderLoss:0.00074  StopLoss:0.03137  GradNorm:0.00355  GradNormST:0.01336  AvgTextLen:9.8  AvgSpecLen:48.6  StepTime:0.62  LR:0.000100\n",
      "   | > Step:20/68  GlobalStep:124980  TotalLoss:0.00163  PostnetLoss:0.00080  DecoderLoss:0.00083  StopLoss:0.03529  GradNorm:0.00341  GradNormST:0.01334  AvgTextLen:16.4  AvgSpecLen:64.8  StepTime:0.40  LR:0.000100\n",
      "   | > Step:30/68  GlobalStep:124990  TotalLoss:0.00177  PostnetLoss:0.00086  DecoderLoss:0.00090  StopLoss:0.04776  GradNorm:0.00295  GradNormST:0.01345  AvgTextLen:22.9  AvgSpecLen:85.8  StepTime:0.60  LR:0.000100\n",
      "   | > Step:40/68  GlobalStep:125000  TotalLoss:0.00189  PostnetLoss:0.00092  DecoderLoss:0.00097  StopLoss:0.04420  GradNorm:0.00315  GradNormST:0.01280  AvgTextLen:30.9  AvgSpecLen:105.3  StepTime:0.54  LR:0.000100\n",
      " | | > Checkpoint saving : /vm/TTS/models/cartman_models/mozilla-no-loc-fattn-stopnet-sigmoid-loss_masking-August-15-2019_11+26AM-de6aeb9/checkpoint_125000.pth.tar\n",
      "warning: audio amplitude out of range, auto clipped.\n",
      "   | > Step:50/68  GlobalStep:125010  TotalLoss:0.00206  PostnetLoss:0.00100  DecoderLoss:0.00106  StopLoss:0.04228  GradNorm:0.00297  GradNormST:0.01228  AvgTextLen:38.8  AvgSpecLen:132.0  StepTime:0.67  LR:0.000100\n",
      "   | > Step:60/68  GlobalStep:125020  TotalLoss:0.00230  PostnetLoss:0.00112  DecoderLoss:0.00118  StopLoss:0.02989  GradNorm:0.00355  GradNormST:0.00926  AvgTextLen:52.0  AvgSpecLen:172.7  StepTime:0.78  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:125028  AvgTotalLoss:0.04407  AvgPostnetLoss:0.00089  AvgDecoderLoss:0.00094  AvgStopLoss:0.04224  EpochTime:42.10  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.07655   PostnetLoss: 0.00555   DecoderLoss:0.00583  StopLoss: 0.06516  \n",
      "   | > TotalLoss: 0.08231   PostnetLoss: 0.00944   DecoderLoss:0.00983  StopLoss: 0.06304  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00089   Validation Loss: 0.00855\n",
      "\n",
      " > Epoch 812/1000\n",
      "   | > Step:1/68  GlobalStep:125030  TotalLoss:0.00130  PostnetLoss:0.00063  DecoderLoss:0.00067  StopLoss:0.04593  GradNorm:0.00478  GradNormST:0.01860  AvgTextLen:3.7  AvgSpecLen:28.6  StepTime:0.22  LR:0.000100\n",
      "   | > Step:11/68  GlobalStep:125040  TotalLoss:0.00139  PostnetLoss:0.00068  DecoderLoss:0.00071  StopLoss:0.03929  GradNorm:0.00305  GradNormST:0.02214  AvgTextLen:10.3  AvgSpecLen:49.4  StepTime:0.31  LR:0.000100\n",
      "   | > Step:21/68  GlobalStep:125050  TotalLoss:0.00159  PostnetLoss:0.00078  DecoderLoss:0.00081  StopLoss:0.04135  GradNorm:0.00309  GradNormST:0.01718  AvgTextLen:16.6  AvgSpecLen:66.3  StepTime:0.44  LR:0.000100\n",
      "   | > Step:31/68  GlobalStep:125060  TotalLoss:0.00175  PostnetLoss:0.00086  DecoderLoss:0.00089  StopLoss:0.04419  GradNorm:0.00301  GradNormST:0.01109  AvgTextLen:24.1  AvgSpecLen:82.7  StepTime:0.56  LR:0.000100\n",
      "   | > Step:41/68  GlobalStep:125070  TotalLoss:0.00195  PostnetLoss:0.00095  DecoderLoss:0.00100  StopLoss:0.03553  GradNorm:0.00300  GradNormST:0.01135  AvgTextLen:32.3  AvgSpecLen:109.4  StepTime:0.76  LR:0.000100\n",
      "   | > Step:51/68  GlobalStep:125080  TotalLoss:0.00204  PostnetLoss:0.00099  DecoderLoss:0.00105  StopLoss:0.03641  GradNorm:0.00315  GradNormST:0.00710  AvgTextLen:39.2  AvgSpecLen:126.9  StepTime:0.72  LR:0.000100\n",
      "   | > Step:61/68  GlobalStep:125090  TotalLoss:0.00227  PostnetLoss:0.00110  DecoderLoss:0.00117  StopLoss:0.04054  GradNorm:0.00291  GradNormST:0.01002  AvgTextLen:54.6  AvgSpecLen:165.5  StepTime:0.90  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:125097  AvgTotalLoss:0.04484  AvgPostnetLoss:0.00089  AvgDecoderLoss:0.00094  AvgStopLoss:0.04301  EpochTime:42.73  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.08028   PostnetLoss: 0.00554   DecoderLoss:0.00582  StopLoss: 0.06891  \n",
      "   | > TotalLoss: 0.08177   PostnetLoss: 0.00932   DecoderLoss:0.00973  StopLoss: 0.06272  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00089   Validation Loss: 0.00842\n",
      "\n",
      " > Epoch 813/1000\n",
      "   | > Step:2/68  GlobalStep:125100  TotalLoss:0.00120  PostnetLoss:0.00058  DecoderLoss:0.00062  StopLoss:0.03942  GradNorm:0.00389  GradNormST:0.01453  AvgTextLen:4.0  AvgSpecLen:31.3  StepTime:0.27  LR:0.000100\n",
      "   | > Step:12/68  GlobalStep:125110  TotalLoss:0.00139  PostnetLoss:0.00068  DecoderLoss:0.00071  StopLoss:0.03672  GradNorm:0.00324  GradNormST:0.01181  AvgTextLen:11.0  AvgSpecLen:49.3  StepTime:0.44  LR:0.000100\n",
      "   | > Step:22/68  GlobalStep:125120  TotalLoss:0.00164  PostnetLoss:0.00080  DecoderLoss:0.00084  StopLoss:0.05739  GradNorm:0.00341  GradNormST:0.01748  AvgTextLen:18.0  AvgSpecLen:72.0  StepTime:0.46  LR:0.000100\n",
      "   | > Step:32/68  GlobalStep:125130  TotalLoss:0.00179  PostnetLoss:0.00087  DecoderLoss:0.00091  StopLoss:0.05629  GradNorm:0.00292  GradNormST:0.02417  AvgTextLen:24.4  AvgSpecLen:91.5  StepTime:0.59  LR:0.000100\n",
      "   | > Step:42/68  GlobalStep:125140  TotalLoss:0.00192  PostnetLoss:0.00094  DecoderLoss:0.00099  StopLoss:0.04212  GradNorm:0.00267  GradNormST:0.01587  AvgTextLen:33.1  AvgSpecLen:109.7  StepTime:0.60  LR:0.000100\n",
      "   | > Step:52/68  GlobalStep:125150  TotalLoss:0.00213  PostnetLoss:0.00104  DecoderLoss:0.00109  StopLoss:0.04899  GradNorm:0.00298  GradNormST:0.01140  AvgTextLen:42.1  AvgSpecLen:137.5  StepTime:0.86  LR:0.000100\n",
      "   | > Step:62/68  GlobalStep:125160  TotalLoss:0.00231  PostnetLoss:0.00113  DecoderLoss:0.00119  StopLoss:0.04311  GradNorm:0.00315  GradNormST:0.01244  AvgTextLen:54.0  AvgSpecLen:174.7  StepTime:0.76  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:125166  AvgTotalLoss:0.04607  AvgPostnetLoss:0.00089  AvgDecoderLoss:0.00094  AvgStopLoss:0.04425  EpochTime:42.88  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.07619   PostnetLoss: 0.00554   DecoderLoss:0.00582  StopLoss: 0.06483  \n",
      "   | > TotalLoss: 0.08503   PostnetLoss: 0.00955   DecoderLoss:0.00993  StopLoss: 0.06554  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00089   Validation Loss: 0.00854\n",
      "\n",
      " > Epoch 814/1000\n",
      "   | > Step:3/68  GlobalStep:125170  TotalLoss:0.00123  PostnetLoss:0.00060  DecoderLoss:0.00063  StopLoss:0.05230  GradNorm:0.00373  GradNormST:0.01621  AvgTextLen:4.7  AvgSpecLen:29.7  StepTime:0.34  LR:0.000100\n",
      "   | > Step:13/68  GlobalStep:125180  TotalLoss:0.00142  PostnetLoss:0.00070  DecoderLoss:0.00072  StopLoss:0.04417  GradNorm:0.00316  GradNormST:0.02002  AvgTextLen:11.5  AvgSpecLen:55.2  StepTime:0.42  LR:0.000100\n",
      "   | > Step:23/68  GlobalStep:125190  TotalLoss:0.00168  PostnetLoss:0.00082  DecoderLoss:0.00086  StopLoss:0.04458  GradNorm:0.00333  GradNormST:0.01134  AvgTextLen:18.3  AvgSpecLen:72.2  StepTime:0.45  LR:0.000100\n",
      "   | > Step:33/68  GlobalStep:125200  TotalLoss:0.00183  PostnetLoss:0.00090  DecoderLoss:0.00094  StopLoss:0.05589  GradNorm:0.00317  GradNormST:0.01515  AvgTextLen:25.9  AvgSpecLen:88.9  StepTime:0.55  LR:0.000100\n",
      "   | > Step:43/68  GlobalStep:125210  TotalLoss:0.00194  PostnetLoss:0.00094  DecoderLoss:0.00099  StopLoss:0.04381  GradNorm:0.00279  GradNormST:0.00996  AvgTextLen:33.4  AvgSpecLen:115.7  StepTime:0.61  LR:0.000100\n",
      "   | > Step:53/68  GlobalStep:125220  TotalLoss:0.00210  PostnetLoss:0.00102  DecoderLoss:0.00108  StopLoss:0.03186  GradNorm:0.00303  GradNormST:0.00791  AvgTextLen:42.4  AvgSpecLen:137.6  StepTime:0.63  LR:0.000100\n",
      "   | > Step:63/68  GlobalStep:125230  TotalLoss:0.00235  PostnetLoss:0.00114  DecoderLoss:0.00121  StopLoss:0.03485  GradNorm:0.00281  GradNormST:0.01285  AvgTextLen:58.6  AvgSpecLen:185.1  StepTime:0.90  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:125235  AvgTotalLoss:0.04588  AvgPostnetLoss:0.00089  AvgDecoderLoss:0.00094  AvgStopLoss:0.04405  EpochTime:42.61  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.07997   PostnetLoss: 0.00564   DecoderLoss:0.00592  StopLoss: 0.06842  \n",
      "   | > TotalLoss: 0.08362   PostnetLoss: 0.00940   DecoderLoss:0.00980  StopLoss: 0.06441  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00089   Validation Loss: 0.00854\n",
      "\n",
      " > Epoch 815/1000\n",
      "   | > Step:4/68  GlobalStep:125240  TotalLoss:0.00125  PostnetLoss:0.00061  DecoderLoss:0.00064  StopLoss:0.06692  GradNorm:0.00356  GradNormST:0.02219  AvgTextLen:4.8  AvgSpecLen:31.1  StepTime:0.26  LR:0.000100\n",
      "   | > Step:14/68  GlobalStep:125250  TotalLoss:0.00146  PostnetLoss:0.00071  DecoderLoss:0.00075  StopLoss:0.04343  GradNorm:0.00317  GradNormST:0.01517  AvgTextLen:12.2  AvgSpecLen:52.0  StepTime:0.34  LR:0.000100\n",
      "   | > Step:24/68  GlobalStep:125260  TotalLoss:0.00172  PostnetLoss:0.00084  DecoderLoss:0.00088  StopLoss:0.04923  GradNorm:0.00329  GradNormST:0.01072  AvgTextLen:19.2  AvgSpecLen:79.0  StepTime:0.39  LR:0.000100\n",
      "   | > Step:34/68  GlobalStep:125270  TotalLoss:0.00187  PostnetLoss:0.00091  DecoderLoss:0.00096  StopLoss:0.02845  GradNorm:0.00292  GradNormST:0.00831  AvgTextLen:26.2  AvgSpecLen:95.9  StepTime:0.59  LR:0.000100\n",
      "   | > Step:44/68  GlobalStep:125280  TotalLoss:0.00200  PostnetLoss:0.00098  DecoderLoss:0.00103  StopLoss:0.03175  GradNorm:0.00288  GradNormST:0.01269  AvgTextLen:33.8  AvgSpecLen:116.3  StepTime:0.57  LR:0.000100\n",
      "   | > Step:54/68  GlobalStep:125290  TotalLoss:0.00213  PostnetLoss:0.00104  DecoderLoss:0.00110  StopLoss:0.04369  GradNorm:0.00279  GradNormST:0.01446  AvgTextLen:42.2  AvgSpecLen:144.9  StepTime:0.87  LR:0.000100\n",
      "   | > Step:64/68  GlobalStep:125300  TotalLoss:0.00222  PostnetLoss:0.00108  DecoderLoss:0.00114  StopLoss:0.03062  GradNorm:0.00251  GradNormST:0.00641  AvgTextLen:60.6  AvgSpecLen:177.6  StepTime:0.81  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:125304  AvgTotalLoss:0.04606  AvgPostnetLoss:0.00089  AvgDecoderLoss:0.00094  AvgStopLoss:0.04424  EpochTime:42.06  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.07983   PostnetLoss: 0.00547   DecoderLoss:0.00575  StopLoss: 0.06861  \n",
      "   | > TotalLoss: 0.08114   PostnetLoss: 0.00943   DecoderLoss:0.00981  StopLoss: 0.06190  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00089   Validation Loss: 0.00854\n",
      "\n",
      " > Epoch 816/1000\n",
      "   | > Step:5/68  GlobalStep:125310  TotalLoss:0.00131  PostnetLoss:0.00064  DecoderLoss:0.00067  StopLoss:0.05840  GradNorm:0.00349  GradNormST:0.02458  AvgTextLen:5.9  AvgSpecLen:33.2  StepTime:0.34  LR:0.000100\n",
      "   | > Step:15/68  GlobalStep:125320  TotalLoss:0.00152  PostnetLoss:0.00074  DecoderLoss:0.00077  StopLoss:0.04531  GradNorm:0.00323  GradNormST:0.01808  AvgTextLen:12.8  AvgSpecLen:59.1  StepTime:0.54  LR:0.000100\n",
      "   | > Step:25/68  GlobalStep:125330  TotalLoss:0.00169  PostnetLoss:0.00082  DecoderLoss:0.00086  StopLoss:0.04128  GradNorm:0.00376  GradNormST:0.01024  AvgTextLen:19.8  AvgSpecLen:73.3  StepTime:0.47  LR:0.000100\n",
      "   | > Step:35/68  GlobalStep:125340  TotalLoss:0.00185  PostnetLoss:0.00090  DecoderLoss:0.00095  StopLoss:0.03885  GradNorm:0.00297  GradNormST:0.01353  AvgTextLen:27.1  AvgSpecLen:104.7  StepTime:0.81  LR:0.000100\n",
      "   | > Step:45/68  GlobalStep:125350  TotalLoss:0.00193  PostnetLoss:0.00094  DecoderLoss:0.00099  StopLoss:0.05411  GradNorm:0.00269  GradNormST:0.01306  AvgTextLen:34.4  AvgSpecLen:118.6  StepTime:0.64  LR:0.000100\n",
      "   | > Step:55/68  GlobalStep:125360  TotalLoss:0.00215  PostnetLoss:0.00105  DecoderLoss:0.00110  StopLoss:0.04301  GradNorm:0.00255  GradNormST:0.01007  AvgTextLen:43.9  AvgSpecLen:145.2  StepTime:0.69  LR:0.000100\n",
      "   | > Step:65/68  GlobalStep:125370  TotalLoss:0.00242  PostnetLoss:0.00118  DecoderLoss:0.00125  StopLoss:0.05655  GradNorm:0.00253  GradNormST:0.02054  AvgTextLen:64.2  AvgSpecLen:207.4  StepTime:1.23  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:125373  AvgTotalLoss:0.04841  AvgPostnetLoss:0.00089  AvgDecoderLoss:0.00093  AvgStopLoss:0.04659  EpochTime:42.33  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.08128   PostnetLoss: 0.00554   DecoderLoss:0.00582  StopLoss: 0.06992  \n",
      "   | > TotalLoss: 0.08223   PostnetLoss: 0.00951   DecoderLoss:0.00989  StopLoss: 0.06282  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00089   Validation Loss: 0.00855\n",
      "\n",
      " > Epoch 817/1000\n",
      "   | > Step:6/68  GlobalStep:125380  TotalLoss:0.00130  PostnetLoss:0.00063  DecoderLoss:0.00066  StopLoss:0.04192  GradNorm:0.00385  GradNormST:0.01470  AvgTextLen:6.0  AvgSpecLen:34.8  StepTime:0.37  LR:0.000100\n",
      "   | > Step:16/68  GlobalStep:125390  TotalLoss:0.00159  PostnetLoss:0.00078  DecoderLoss:0.00082  StopLoss:0.04815  GradNorm:0.00333  GradNormST:0.01419  AvgTextLen:13.8  AvgSpecLen:60.6  StepTime:0.37  LR:0.000100\n",
      "   | > Step:26/68  GlobalStep:125400  TotalLoss:0.00175  PostnetLoss:0.00086  DecoderLoss:0.00090  StopLoss:0.04637  GradNorm:0.00357  GradNormST:0.01252  AvgTextLen:20.3  AvgSpecLen:77.5  StepTime:0.50  LR:0.000100\n",
      "   | > Step:36/68  GlobalStep:125410  TotalLoss:0.00184  PostnetLoss:0.00090  DecoderLoss:0.00094  StopLoss:0.04678  GradNorm:0.00315  GradNormST:0.01029  AvgTextLen:28.5  AvgSpecLen:100.2  StepTime:0.67  LR:0.000100\n",
      "   | > Step:46/68  GlobalStep:125420  TotalLoss:0.00200  PostnetLoss:0.00097  DecoderLoss:0.00102  StopLoss:0.06343  GradNorm:0.00310  GradNormST:0.02566  AvgTextLen:35.4  AvgSpecLen:122.6  StepTime:0.84  LR:0.000100\n",
      "   | > Step:56/68  GlobalStep:125430  TotalLoss:0.00216  PostnetLoss:0.00105  DecoderLoss:0.00111  StopLoss:0.04341  GradNorm:0.00265  GradNormST:0.02050  AvgTextLen:45.8  AvgSpecLen:145.4  StepTime:1.04  LR:0.000100\n",
      "   | > Step:66/68  GlobalStep:125440  TotalLoss:0.00253  PostnetLoss:0.00122  DecoderLoss:0.00130  StopLoss:0.06737  GradNorm:0.00254  GradNormST:0.02650  AvgTextLen:71.2  AvgSpecLen:220.8  StepTime:1.05  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:125442  AvgTotalLoss:0.05342  AvgPostnetLoss:0.00089  AvgDecoderLoss:0.00094  AvgStopLoss:0.05159  EpochTime:42.89  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.07998   PostnetLoss: 0.00556   DecoderLoss:0.00583  StopLoss: 0.06858  \n",
      "   | > TotalLoss: 0.08682   PostnetLoss: 0.00934   DecoderLoss:0.00972  StopLoss: 0.06776  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00089   Validation Loss: 0.00848\n",
      "\n",
      " > Epoch 818/1000\n",
      "   | > Step:7/68  GlobalStep:125450  TotalLoss:0.00136  PostnetLoss:0.00067  DecoderLoss:0.00069  StopLoss:0.04888  GradNorm:0.00348  GradNormST:0.02704  AvgTextLen:7.4  AvgSpecLen:44.6  StepTime:0.38  LR:0.000100\n",
      "   | > Step:17/68  GlobalStep:125460  TotalLoss:0.00152  PostnetLoss:0.00074  DecoderLoss:0.00078  StopLoss:0.06699  GradNorm:0.00305  GradNormST:0.01634  AvgTextLen:14.7  AvgSpecLen:66.0  StepTime:0.43  LR:0.000100\n",
      "   | > Step:27/68  GlobalStep:125470  TotalLoss:0.00175  PostnetLoss:0.00086  DecoderLoss:0.00089  StopLoss:0.06233  GradNorm:0.00301  GradNormST:0.02054  AvgTextLen:21.0  AvgSpecLen:80.2  StepTime:0.65  LR:0.000100\n",
      "   | > Step:37/68  GlobalStep:125480  TotalLoss:0.00189  PostnetLoss:0.00092  DecoderLoss:0.00097  StopLoss:0.04973  GradNorm:0.00314  GradNormST:0.01467  AvgTextLen:29.3  AvgSpecLen:102.4  StepTime:0.48  LR:0.000100\n",
      "   | > Step:47/68  GlobalStep:125490  TotalLoss:0.00203  PostnetLoss:0.00099  DecoderLoss:0.00104  StopLoss:0.03775  GradNorm:0.00336  GradNormST:0.01030  AvgTextLen:36.7  AvgSpecLen:126.9  StepTime:0.74  LR:0.000100\n",
      "   | > Step:57/68  GlobalStep:125500  TotalLoss:0.00223  PostnetLoss:0.00109  DecoderLoss:0.00115  StopLoss:0.04107  GradNorm:0.00280  GradNormST:0.00979  AvgTextLen:47.1  AvgSpecLen:161.0  StepTime:0.83  LR:0.000100\n",
      "   | > Step:67/68  GlobalStep:125510  TotalLoss:0.00254  PostnetLoss:0.00123  DecoderLoss:0.00131  StopLoss:0.04893  GradNorm:0.00268  GradNormST:0.02320  AvgTextLen:80.6  AvgSpecLen:246.5  StepTime:1.22  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:125511  AvgTotalLoss:0.05382  AvgPostnetLoss:0.00089  AvgDecoderLoss:0.00094  AvgStopLoss:0.05199  EpochTime:42.09  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.08024   PostnetLoss: 0.00546   DecoderLoss:0.00573  StopLoss: 0.06904  \n",
      "   | > TotalLoss: 0.08407   PostnetLoss: 0.00924   DecoderLoss:0.00963  StopLoss: 0.06519  \n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00089   Validation Loss: 0.00842\n",
      "\n",
      " > Epoch 819/1000\n",
      "   | > Step:8/68  GlobalStep:125520  TotalLoss:0.00137  PostnetLoss:0.00067  DecoderLoss:0.00070  StopLoss:0.04248  GradNorm:0.00355  GradNormST:0.01236  AvgTextLen:8.3  AvgSpecLen:44.0  StepTime:0.26  LR:0.000100\n",
      "   | > Step:18/68  GlobalStep:125530  TotalLoss:0.00153  PostnetLoss:0.00075  DecoderLoss:0.00078  StopLoss:0.04293  GradNorm:0.00342  GradNormST:0.01204  AvgTextLen:15.2  AvgSpecLen:64.5  StepTime:0.43  LR:0.000100\n",
      "   | > Step:28/68  GlobalStep:125540  TotalLoss:0.00175  PostnetLoss:0.00085  DecoderLoss:0.00089  StopLoss:0.05140  GradNorm:0.00294  GradNormST:0.02080  AvgTextLen:22.1  AvgSpecLen:85.4  StepTime:0.45  LR:0.000100\n",
      "   | > Step:38/68  GlobalStep:125550  TotalLoss:0.00189  PostnetLoss:0.00092  DecoderLoss:0.00097  StopLoss:0.03494  GradNorm:0.00321  GradNormST:0.01052  AvgTextLen:29.4  AvgSpecLen:104.1  StepTime:0.62  LR:0.000100\n",
      "   | > Step:48/68  GlobalStep:125560  TotalLoss:0.00203  PostnetLoss:0.00099  DecoderLoss:0.00104  StopLoss:0.04787  GradNorm:0.00314  GradNormST:0.01462  AvgTextLen:37.5  AvgSpecLen:123.8  StepTime:0.72  LR:0.000100\n",
      "   | > Step:58/68  GlobalStep:125570  TotalLoss:0.00223  PostnetLoss:0.00109  DecoderLoss:0.00114  StopLoss:0.06123  GradNorm:0.00350  GradNormST:0.01837  AvgTextLen:47.8  AvgSpecLen:158.0  StepTime:0.85  LR:0.000100\n",
      "   | > Step:68/68  GlobalStep:125580  TotalLoss:0.00256  PostnetLoss:0.00124  DecoderLoss:0.00132  StopLoss:0.04068  GradNorm:0.00329  GradNormST:0.02818  AvgTextLen:103.0  AvgSpecLen:308.9  StepTime:1.86  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:125580  AvgTotalLoss:0.04999  AvgPostnetLoss:0.00089  AvgDecoderLoss:0.00094  AvgStopLoss:0.04816  EpochTime:42.84  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.07964   PostnetLoss: 0.00553   DecoderLoss:0.00580  StopLoss: 0.06830  \n",
      "   | > TotalLoss: 0.08414   PostnetLoss: 0.00913   DecoderLoss:0.00950  StopLoss: 0.06551  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00089   Validation Loss: 0.00832\n",
      "\n",
      " > Epoch 820/1000\n",
      "   | > Step:9/68  GlobalStep:125590  TotalLoss:0.00134  PostnetLoss:0.00066  DecoderLoss:0.00069  StopLoss:0.04183  GradNorm:0.00360  GradNormST:0.01889  AvgTextLen:9.0  AvgSpecLen:42.1  StepTime:0.41  LR:0.000100\n",
      "   | > Step:19/68  GlobalStep:125600  TotalLoss:0.00155  PostnetLoss:0.00076  DecoderLoss:0.00079  StopLoss:0.06313  GradNorm:0.00313  GradNormST:0.01693  AvgTextLen:15.5  AvgSpecLen:63.2  StepTime:0.34  LR:0.000100\n",
      "   | > Step:29/68  GlobalStep:125610  TotalLoss:0.00171  PostnetLoss:0.00084  DecoderLoss:0.00088  StopLoss:0.05253  GradNorm:0.00284  GradNormST:0.01552  AvgTextLen:21.8  AvgSpecLen:83.4  StepTime:0.56  LR:0.000100\n",
      "   | > Step:39/68  GlobalStep:125620  TotalLoss:0.00193  PostnetLoss:0.00094  DecoderLoss:0.00099  StopLoss:0.05081  GradNorm:0.00299  GradNormST:0.01681  AvgTextLen:30.2  AvgSpecLen:110.4  StepTime:0.55  LR:0.000100\n",
      "   | > Step:49/68  GlobalStep:125630  TotalLoss:0.00208  PostnetLoss:0.00101  DecoderLoss:0.00107  StopLoss:0.04703  GradNorm:0.00326  GradNormST:0.00907  AvgTextLen:38.2  AvgSpecLen:127.7  StepTime:0.59  LR:0.000100\n",
      "   | > Step:59/68  GlobalStep:125640  TotalLoss:0.00220  PostnetLoss:0.00107  DecoderLoss:0.00113  StopLoss:0.05307  GradNorm:0.00307  GradNormST:0.02734  AvgTextLen:50.5  AvgSpecLen:155.9  StepTime:0.91  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:125649  AvgTotalLoss:0.05441  AvgPostnetLoss:0.00089  AvgDecoderLoss:0.00094  AvgStopLoss:0.05258  EpochTime:42.33  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.07944   PostnetLoss: 0.00538   DecoderLoss:0.00565  StopLoss: 0.06841  \n",
      "   | > TotalLoss: 0.08332   PostnetLoss: 0.00909   DecoderLoss:0.00946  StopLoss: 0.06477  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00089   Validation Loss: 0.00830\n",
      "\n",
      " > Epoch 821/1000\n",
      "   | > Step:0/68  GlobalStep:125650  TotalLoss:0.00136  PostnetLoss:0.00065  DecoderLoss:0.00071  StopLoss:0.06617  GradNorm:0.00616  GradNormST:0.02780  AvgTextLen:3.0  AvgSpecLen:25.3  StepTime:0.43  LR:0.000100\n",
      "   | > Step:10/68  GlobalStep:125660  TotalLoss:0.00141  PostnetLoss:0.00069  DecoderLoss:0.00072  StopLoss:0.04579  GradNorm:0.00332  GradNormST:0.01717  AvgTextLen:9.8  AvgSpecLen:48.6  StepTime:0.50  LR:0.000100\n",
      "   | > Step:20/68  GlobalStep:125670  TotalLoss:0.00160  PostnetLoss:0.00078  DecoderLoss:0.00082  StopLoss:0.03255  GradNorm:0.00321  GradNormST:0.01268  AvgTextLen:16.4  AvgSpecLen:64.8  StepTime:0.42  LR:0.000100\n",
      "   | > Step:30/68  GlobalStep:125680  TotalLoss:0.00174  PostnetLoss:0.00085  DecoderLoss:0.00089  StopLoss:0.05686  GradNorm:0.00287  GradNormST:0.01355  AvgTextLen:22.9  AvgSpecLen:85.8  StepTime:0.60  LR:0.000100\n",
      "   | > Step:40/68  GlobalStep:125690  TotalLoss:0.00191  PostnetLoss:0.00093  DecoderLoss:0.00097  StopLoss:0.04963  GradNorm:0.00293  GradNormST:0.01281  AvgTextLen:30.9  AvgSpecLen:105.3  StepTime:0.63  LR:0.000100\n",
      "   | > Step:50/68  GlobalStep:125700  TotalLoss:0.00209  PostnetLoss:0.00102  DecoderLoss:0.00107  StopLoss:0.05039  GradNorm:0.00298  GradNormST:0.01868  AvgTextLen:38.8  AvgSpecLen:132.0  StepTime:0.80  LR:0.000100\n",
      "   | > Step:60/68  GlobalStep:125710  TotalLoss:0.00225  PostnetLoss:0.00110  DecoderLoss:0.00115  StopLoss:0.04188  GradNorm:0.00286  GradNormST:0.01057  AvgTextLen:52.0  AvgSpecLen:172.7  StepTime:0.79  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:125718  AvgTotalLoss:0.05335  AvgPostnetLoss:0.00089  AvgDecoderLoss:0.00093  AvgStopLoss:0.05153  EpochTime:41.81  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.07945   PostnetLoss: 0.00542   DecoderLoss:0.00569  StopLoss: 0.06835  \n",
      "   | > TotalLoss: 0.08095   PostnetLoss: 0.00911   DecoderLoss:0.00948  StopLoss: 0.06236  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00089   Validation Loss: 0.00832\n",
      "\n",
      " > Epoch 822/1000\n",
      "   | > Step:1/68  GlobalStep:125720  TotalLoss:0.00129  PostnetLoss:0.00063  DecoderLoss:0.00066  StopLoss:0.03447  GradNorm:0.00506  GradNormST:0.01563  AvgTextLen:3.7  AvgSpecLen:28.6  StepTime:0.26  LR:0.000100\n",
      "   | > Step:11/68  GlobalStep:125730  TotalLoss:0.00139  PostnetLoss:0.00068  DecoderLoss:0.00071  StopLoss:0.03372  GradNorm:0.00348  GradNormST:0.01301  AvgTextLen:10.3  AvgSpecLen:49.4  StepTime:0.39  LR:0.000100\n",
      "   | > Step:21/68  GlobalStep:125740  TotalLoss:0.00161  PostnetLoss:0.00079  DecoderLoss:0.00082  StopLoss:0.04907  GradNorm:0.00305  GradNormST:0.01603  AvgTextLen:16.6  AvgSpecLen:66.3  StepTime:0.43  LR:0.000100\n",
      "   | > Step:31/68  GlobalStep:125750  TotalLoss:0.00175  PostnetLoss:0.00085  DecoderLoss:0.00089  StopLoss:0.04954  GradNorm:0.00293  GradNormST:0.01888  AvgTextLen:24.1  AvgSpecLen:82.7  StepTime:0.55  LR:0.000100\n",
      "   | > Step:41/68  GlobalStep:125760  TotalLoss:0.00197  PostnetLoss:0.00096  DecoderLoss:0.00101  StopLoss:0.03434  GradNorm:0.00285  GradNormST:0.00900  AvgTextLen:32.3  AvgSpecLen:109.4  StepTime:0.62  LR:0.000100\n",
      "   | > Step:51/68  GlobalStep:125770  TotalLoss:0.00205  PostnetLoss:0.00100  DecoderLoss:0.00105  StopLoss:0.06223  GradNorm:0.00272  GradNormST:0.02101  AvgTextLen:39.2  AvgSpecLen:126.9  StepTime:0.62  LR:0.000100\n",
      "   | > Step:61/68  GlobalStep:125780  TotalLoss:0.00224  PostnetLoss:0.00109  DecoderLoss:0.00115  StopLoss:0.04908  GradNorm:0.00303  GradNormST:0.01175  AvgTextLen:54.6  AvgSpecLen:165.5  StepTime:0.91  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:125787  AvgTotalLoss:0.05162  AvgPostnetLoss:0.00089  AvgDecoderLoss:0.00093  AvgStopLoss:0.04980  EpochTime:41.58  AvgStepTime:0.60\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.08298   PostnetLoss: 0.00546   DecoderLoss:0.00573  StopLoss: 0.07179  \n",
      "   | > TotalLoss: 0.08955   PostnetLoss: 0.00965   DecoderLoss:0.01002  StopLoss: 0.06988  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00089   Validation Loss: 0.00866\n",
      "\n",
      " > Epoch 823/1000\n",
      "   | > Step:2/68  GlobalStep:125790  TotalLoss:0.00125  PostnetLoss:0.00061  DecoderLoss:0.00064  StopLoss:0.04198  GradNorm:0.00405  GradNormST:0.02188  AvgTextLen:4.0  AvgSpecLen:31.3  StepTime:0.44  LR:0.000100\n",
      "   | > Step:12/68  GlobalStep:125800  TotalLoss:0.00141  PostnetLoss:0.00069  DecoderLoss:0.00072  StopLoss:0.05258  GradNorm:0.00335  GradNormST:0.02059  AvgTextLen:11.0  AvgSpecLen:49.3  StepTime:0.42  LR:0.000100\n",
      "   | > Step:22/68  GlobalStep:125810  TotalLoss:0.00164  PostnetLoss:0.00080  DecoderLoss:0.00084  StopLoss:0.05737  GradNorm:0.00340  GradNormST:0.02285  AvgTextLen:18.0  AvgSpecLen:72.0  StepTime:0.47  LR:0.000100\n",
      "   | > Step:32/68  GlobalStep:125820  TotalLoss:0.00178  PostnetLoss:0.00087  DecoderLoss:0.00091  StopLoss:0.06070  GradNorm:0.00297  GradNormST:0.01813  AvgTextLen:24.4  AvgSpecLen:91.5  StepTime:0.56  LR:0.000100\n",
      "   | > Step:42/68  GlobalStep:125830  TotalLoss:0.00190  PostnetLoss:0.00093  DecoderLoss:0.00097  StopLoss:0.04854  GradNorm:0.00272  GradNormST:0.01943  AvgTextLen:33.1  AvgSpecLen:109.7  StepTime:0.55  LR:0.000100\n",
      "   | > Step:52/68  GlobalStep:125840  TotalLoss:0.00211  PostnetLoss:0.00103  DecoderLoss:0.00108  StopLoss:0.05734  GradNorm:0.00341  GradNormST:0.01249  AvgTextLen:42.1  AvgSpecLen:137.5  StepTime:0.78  LR:0.000100\n",
      "   | > Step:62/68  GlobalStep:125850  TotalLoss:0.00231  PostnetLoss:0.00112  DecoderLoss:0.00119  StopLoss:0.05129  GradNorm:0.00291  GradNormST:0.01501  AvgTextLen:54.0  AvgSpecLen:174.7  StepTime:0.75  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:125856  AvgTotalLoss:0.05018  AvgPostnetLoss:0.00089  AvgDecoderLoss:0.00093  AvgStopLoss:0.04836  EpochTime:42.90  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.08316   PostnetLoss: 0.00567   DecoderLoss:0.00595  StopLoss: 0.07155  \n",
      "   | > TotalLoss: 0.08990   PostnetLoss: 0.00972   DecoderLoss:0.01010  StopLoss: 0.07008  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00089   Validation Loss: 0.00875\n",
      "\n",
      " > Epoch 824/1000\n",
      "   | > Step:3/68  GlobalStep:125860  TotalLoss:0.00127  PostnetLoss:0.00062  DecoderLoss:0.00065  StopLoss:0.09304  GradNorm:0.00379  GradNormST:0.03237  AvgTextLen:4.7  AvgSpecLen:29.7  StepTime:0.29  LR:0.000100\n",
      "   | > Step:13/68  GlobalStep:125870  TotalLoss:0.00143  PostnetLoss:0.00070  DecoderLoss:0.00073  StopLoss:0.07071  GradNorm:0.00355  GradNormST:0.02213  AvgTextLen:11.5  AvgSpecLen:55.2  StepTime:0.43  LR:0.000100\n",
      "   | > Step:23/68  GlobalStep:125880  TotalLoss:0.00167  PostnetLoss:0.00082  DecoderLoss:0.00085  StopLoss:0.04614  GradNorm:0.00306  GradNormST:0.01355  AvgTextLen:18.3  AvgSpecLen:72.2  StepTime:0.42  LR:0.000100\n",
      "   | > Step:33/68  GlobalStep:125890  TotalLoss:0.00181  PostnetLoss:0.00089  DecoderLoss:0.00093  StopLoss:0.05071  GradNorm:0.00307  GradNormST:0.01313  AvgTextLen:25.9  AvgSpecLen:88.9  StepTime:0.46  LR:0.000100\n",
      "   | > Step:43/68  GlobalStep:125900  TotalLoss:0.00195  PostnetLoss:0.00095  DecoderLoss:0.00100  StopLoss:0.05523  GradNorm:0.00298  GradNormST:0.01387  AvgTextLen:33.4  AvgSpecLen:115.7  StepTime:0.62  LR:0.000100\n",
      "   | > Step:53/68  GlobalStep:125910  TotalLoss:0.00207  PostnetLoss:0.00101  DecoderLoss:0.00106  StopLoss:0.04589  GradNorm:0.00313  GradNormST:0.01278  AvgTextLen:42.4  AvgSpecLen:137.6  StepTime:0.76  LR:0.000100\n",
      "   | > Step:63/68  GlobalStep:125920  TotalLoss:0.00236  PostnetLoss:0.00115  DecoderLoss:0.00121  StopLoss:0.04078  GradNorm:0.00303  GradNormST:0.01696  AvgTextLen:58.6  AvgSpecLen:185.1  StepTime:0.90  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:125925  AvgTotalLoss:0.05014  AvgPostnetLoss:0.00089  AvgDecoderLoss:0.00093  AvgStopLoss:0.04832  EpochTime:42.04  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.08245   PostnetLoss: 0.00553   DecoderLoss:0.00581  StopLoss: 0.07111  \n",
      "   | > TotalLoss: 0.08514   PostnetLoss: 0.00960   DecoderLoss:0.00997  StopLoss: 0.06557  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00089   Validation Loss: 0.00863\n",
      "\n",
      " > Epoch 825/1000\n",
      "   | > Step:4/68  GlobalStep:125930  TotalLoss:0.00128  PostnetLoss:0.00063  DecoderLoss:0.00066  StopLoss:0.06624  GradNorm:0.00362  GradNormST:0.02741  AvgTextLen:4.8  AvgSpecLen:31.1  StepTime:0.27  LR:0.000100\n",
      "   | > Step:14/68  GlobalStep:125940  TotalLoss:0.00147  PostnetLoss:0.00072  DecoderLoss:0.00075  StopLoss:0.03924  GradNorm:0.00350  GradNormST:0.01584  AvgTextLen:12.2  AvgSpecLen:52.0  StepTime:0.26  LR:0.000100\n",
      "   | > Step:24/68  GlobalStep:125950  TotalLoss:0.00172  PostnetLoss:0.00084  DecoderLoss:0.00088  StopLoss:0.04488  GradNorm:0.00307  GradNormST:0.01104  AvgTextLen:19.2  AvgSpecLen:79.0  StepTime:0.53  LR:0.000100\n",
      "   | > Step:34/68  GlobalStep:125960  TotalLoss:0.00184  PostnetLoss:0.00090  DecoderLoss:0.00094  StopLoss:0.03339  GradNorm:0.00283  GradNormST:0.00894  AvgTextLen:26.2  AvgSpecLen:95.9  StepTime:0.64  LR:0.000100\n",
      "   | > Step:44/68  GlobalStep:125970  TotalLoss:0.00199  PostnetLoss:0.00097  DecoderLoss:0.00102  StopLoss:0.03822  GradNorm:0.00282  GradNormST:0.01180  AvgTextLen:33.8  AvgSpecLen:116.3  StepTime:0.67  LR:0.000100\n",
      "   | > Step:54/68  GlobalStep:125980  TotalLoss:0.00212  PostnetLoss:0.00103  DecoderLoss:0.00109  StopLoss:0.04829  GradNorm:0.00310  GradNormST:0.01291  AvgTextLen:42.2  AvgSpecLen:144.9  StepTime:0.80  LR:0.000100\n",
      "   | > Step:64/68  GlobalStep:125990  TotalLoss:0.00225  PostnetLoss:0.00109  DecoderLoss:0.00116  StopLoss:0.05417  GradNorm:0.00291  GradNormST:0.01763  AvgTextLen:60.6  AvgSpecLen:177.6  StepTime:0.80  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:125994  AvgTotalLoss:0.05176  AvgPostnetLoss:0.00089  AvgDecoderLoss:0.00093  AvgStopLoss:0.04994  EpochTime:41.81  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.08209   PostnetLoss: 0.00561   DecoderLoss:0.00587  StopLoss: 0.07061  \n",
      "   | > TotalLoss: 0.08787   PostnetLoss: 0.00962   DecoderLoss:0.00999  StopLoss: 0.06826  \n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00089   Validation Loss: 0.00874\n",
      "\n",
      " > Epoch 826/1000\n",
      "   | > Step:5/68  GlobalStep:126000  TotalLoss:0.00129  PostnetLoss:0.00063  DecoderLoss:0.00066  StopLoss:0.07536  GradNorm:0.00382  GradNormST:0.03034  AvgTextLen:5.9  AvgSpecLen:33.2  StepTime:0.31  LR:0.000100\n",
      " | | > Checkpoint saving : /vm/TTS/models/cartman_models/mozilla-no-loc-fattn-stopnet-sigmoid-loss_masking-August-15-2019_11+26AM-de6aeb9/checkpoint_126000.pth.tar\n",
      "warning: audio amplitude out of range, auto clipped.\n",
      "   | > Step:15/68  GlobalStep:126010  TotalLoss:0.00151  PostnetLoss:0.00074  DecoderLoss:0.00077  StopLoss:0.05960  GradNorm:0.00324  GradNormST:0.02582  AvgTextLen:12.8  AvgSpecLen:59.1  StepTime:0.46  LR:0.000100\n",
      "   | > Step:25/68  GlobalStep:126020  TotalLoss:0.00167  PostnetLoss:0.00081  DecoderLoss:0.00085  StopLoss:0.05080  GradNorm:0.00320  GradNormST:0.01285  AvgTextLen:19.8  AvgSpecLen:73.3  StepTime:0.48  LR:0.000100\n",
      "   | > Step:35/68  GlobalStep:126030  TotalLoss:0.00186  PostnetLoss:0.00091  DecoderLoss:0.00095  StopLoss:0.07025  GradNorm:0.00271  GradNormST:0.02973  AvgTextLen:27.1  AvgSpecLen:104.7  StepTime:0.80  LR:0.000100\n",
      "   | > Step:45/68  GlobalStep:126040  TotalLoss:0.00196  PostnetLoss:0.00095  DecoderLoss:0.00100  StopLoss:0.05726  GradNorm:0.00280  GradNormST:0.01883  AvgTextLen:34.4  AvgSpecLen:118.6  StepTime:0.62  LR:0.000100\n",
      "   | > Step:55/68  GlobalStep:126050  TotalLoss:0.00217  PostnetLoss:0.00106  DecoderLoss:0.00111  StopLoss:0.04327  GradNorm:0.00295  GradNormST:0.00903  AvgTextLen:43.9  AvgSpecLen:145.2  StepTime:0.78  LR:0.000100\n",
      "   | > Step:65/68  GlobalStep:126060  TotalLoss:0.00240  PostnetLoss:0.00117  DecoderLoss:0.00123  StopLoss:0.05576  GradNorm:0.00266  GradNormST:0.02106  AvgTextLen:64.2  AvgSpecLen:207.4  StepTime:1.19  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:126063  AvgTotalLoss:0.05183  AvgPostnetLoss:0.00089  AvgDecoderLoss:0.00093  AvgStopLoss:0.05002  EpochTime:42.61  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.08204   PostnetLoss: 0.00557   DecoderLoss:0.00584  StopLoss: 0.07063  \n",
      "   | > TotalLoss: 0.08840   PostnetLoss: 0.00987   DecoderLoss:0.01024  StopLoss: 0.06830  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00089   Validation Loss: 0.00877\n",
      "\n",
      " > Epoch 827/1000\n",
      "   | > Step:6/68  GlobalStep:126070  TotalLoss:0.00128  PostnetLoss:0.00063  DecoderLoss:0.00066  StopLoss:0.03622  GradNorm:0.00406  GradNormST:0.01151  AvgTextLen:6.0  AvgSpecLen:34.8  StepTime:0.38  LR:0.000100\n",
      "   | > Step:16/68  GlobalStep:126080  TotalLoss:0.00154  PostnetLoss:0.00075  DecoderLoss:0.00079  StopLoss:0.04557  GradNorm:0.00322  GradNormST:0.01580  AvgTextLen:13.8  AvgSpecLen:60.6  StepTime:0.39  LR:0.000100\n",
      "   | > Step:26/68  GlobalStep:126090  TotalLoss:0.00171  PostnetLoss:0.00083  DecoderLoss:0.00087  StopLoss:0.04586  GradNorm:0.00309  GradNormST:0.01312  AvgTextLen:20.3  AvgSpecLen:77.5  StepTime:0.50  LR:0.000100\n",
      "   | > Step:36/68  GlobalStep:126100  TotalLoss:0.00182  PostnetLoss:0.00089  DecoderLoss:0.00093  StopLoss:0.04803  GradNorm:0.00279  GradNormST:0.01243  AvgTextLen:28.5  AvgSpecLen:100.2  StepTime:0.61  LR:0.000100\n",
      "   | > Step:46/68  GlobalStep:126110  TotalLoss:0.00199  PostnetLoss:0.00097  DecoderLoss:0.00102  StopLoss:0.04854  GradNorm:0.00269  GradNormST:0.01480  AvgTextLen:35.4  AvgSpecLen:122.6  StepTime:0.77  LR:0.000100\n",
      "   | > Step:56/68  GlobalStep:126120  TotalLoss:0.00217  PostnetLoss:0.00106  DecoderLoss:0.00111  StopLoss:0.05070  GradNorm:0.00286  GradNormST:0.02030  AvgTextLen:45.8  AvgSpecLen:145.4  StepTime:0.96  LR:0.000100\n",
      "   | > Step:66/68  GlobalStep:126130  TotalLoss:0.00254  PostnetLoss:0.00123  DecoderLoss:0.00131  StopLoss:0.05301  GradNorm:0.00281  GradNormST:0.02187  AvgTextLen:71.2  AvgSpecLen:220.8  StepTime:1.03  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:126132  AvgTotalLoss:0.05044  AvgPostnetLoss:0.00089  AvgDecoderLoss:0.00093  AvgStopLoss:0.04862  EpochTime:42.18  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.07882   PostnetLoss: 0.00562   DecoderLoss:0.00589  StopLoss: 0.06731  \n",
      "   | > TotalLoss: 0.09030   PostnetLoss: 0.00986   DecoderLoss:0.01024  StopLoss: 0.07021  \n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00089   Validation Loss: 0.00888\n",
      "\n",
      " > Epoch 828/1000\n",
      "   | > Step:7/68  GlobalStep:126140  TotalLoss:0.00136  PostnetLoss:0.00067  DecoderLoss:0.00070  StopLoss:0.03609  GradNorm:0.00353  GradNormST:0.01977  AvgTextLen:7.4  AvgSpecLen:44.6  StepTime:0.30  LR:0.000100\n",
      "   | > Step:17/68  GlobalStep:126150  TotalLoss:0.00151  PostnetLoss:0.00074  DecoderLoss:0.00077  StopLoss:0.06569  GradNorm:0.00310  GradNormST:0.01725  AvgTextLen:14.7  AvgSpecLen:66.0  StepTime:0.42  LR:0.000100\n",
      "   | > Step:27/68  GlobalStep:126160  TotalLoss:0.00174  PostnetLoss:0.00085  DecoderLoss:0.00089  StopLoss:0.07548  GradNorm:0.00325  GradNormST:0.02873  AvgTextLen:21.0  AvgSpecLen:80.2  StepTime:0.63  LR:0.000100\n",
      "   | > Step:37/68  GlobalStep:126170  TotalLoss:0.00189  PostnetLoss:0.00092  DecoderLoss:0.00097  StopLoss:0.05044  GradNorm:0.00284  GradNormST:0.01737  AvgTextLen:29.3  AvgSpecLen:102.4  StepTime:0.51  LR:0.000100\n",
      "   | > Step:47/68  GlobalStep:126180  TotalLoss:0.00207  PostnetLoss:0.00101  DecoderLoss:0.00106  StopLoss:0.03978  GradNorm:0.00272  GradNormST:0.01195  AvgTextLen:36.7  AvgSpecLen:126.9  StepTime:0.74  LR:0.000100\n",
      "   | > Step:57/68  GlobalStep:126190  TotalLoss:0.00220  PostnetLoss:0.00107  DecoderLoss:0.00113  StopLoss:0.03811  GradNorm:0.00275  GradNormST:0.01012  AvgTextLen:47.1  AvgSpecLen:161.0  StepTime:0.89  LR:0.000100\n",
      "   | > Step:67/68  GlobalStep:126200  TotalLoss:0.00255  PostnetLoss:0.00124  DecoderLoss:0.00131  StopLoss:0.03744  GradNorm:0.00304  GradNormST:0.01575  AvgTextLen:80.6  AvgSpecLen:246.5  StepTime:1.21  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:126201  AvgTotalLoss:0.04958  AvgPostnetLoss:0.00089  AvgDecoderLoss:0.00093  AvgStopLoss:0.04777  EpochTime:41.64  AvgStepTime:0.60\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.08089   PostnetLoss: 0.00565   DecoderLoss:0.00592  StopLoss: 0.06932  \n",
      "   | > TotalLoss: 0.08996   PostnetLoss: 0.00974   DecoderLoss:0.01011  StopLoss: 0.07012  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00089   Validation Loss: 0.00877\n",
      "\n",
      " > Epoch 829/1000\n",
      "   | > Step:8/68  GlobalStep:126210  TotalLoss:0.00137  PostnetLoss:0.00067  DecoderLoss:0.00070  StopLoss:0.04500  GradNorm:0.00384  GradNormST:0.01158  AvgTextLen:8.3  AvgSpecLen:44.0  StepTime:0.32  LR:0.000100\n",
      "   | > Step:18/68  GlobalStep:126220  TotalLoss:0.00155  PostnetLoss:0.00076  DecoderLoss:0.00079  StopLoss:0.04556  GradNorm:0.00355  GradNormST:0.01063  AvgTextLen:15.2  AvgSpecLen:64.5  StepTime:0.41  LR:0.000100\n",
      "   | > Step:28/68  GlobalStep:126230  TotalLoss:0.00173  PostnetLoss:0.00084  DecoderLoss:0.00088  StopLoss:0.05193  GradNorm:0.00327  GradNormST:0.01340  AvgTextLen:22.1  AvgSpecLen:85.4  StepTime:0.55  LR:0.000100\n",
      "   | > Step:38/68  GlobalStep:126240  TotalLoss:0.00186  PostnetLoss:0.00091  DecoderLoss:0.00095  StopLoss:0.04251  GradNorm:0.00298  GradNormST:0.01167  AvgTextLen:29.4  AvgSpecLen:104.1  StepTime:0.59  LR:0.000100\n",
      "   | > Step:48/68  GlobalStep:126250  TotalLoss:0.00202  PostnetLoss:0.00099  DecoderLoss:0.00103  StopLoss:0.03896  GradNorm:0.00276  GradNormST:0.01040  AvgTextLen:37.5  AvgSpecLen:123.8  StepTime:0.67  LR:0.000100\n",
      "   | > Step:58/68  GlobalStep:126260  TotalLoss:0.00221  PostnetLoss:0.00108  DecoderLoss:0.00113  StopLoss:0.05180  GradNorm:0.00291  GradNormST:0.01385  AvgTextLen:47.8  AvgSpecLen:158.0  StepTime:0.70  LR:0.000100\n",
      "   | > Step:68/68  GlobalStep:126270  TotalLoss:0.00253  PostnetLoss:0.00123  DecoderLoss:0.00130  StopLoss:0.04580  GradNorm:0.00301  GradNormST:0.02999  AvgTextLen:103.0  AvgSpecLen:308.9  StepTime:1.88  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:126270  AvgTotalLoss:0.05196  AvgPostnetLoss:0.00089  AvgDecoderLoss:0.00093  AvgStopLoss:0.05014  EpochTime:42.23  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.08443   PostnetLoss: 0.00567   DecoderLoss:0.00595  StopLoss: 0.07281  \n",
      "   | > TotalLoss: 0.09638   PostnetLoss: 0.01025   DecoderLoss:0.01063  StopLoss: 0.07550  \n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00089   Validation Loss: 0.00912\n",
      "\n",
      " > Epoch 830/1000\n",
      "   | > Step:9/68  GlobalStep:126280  TotalLoss:0.00136  PostnetLoss:0.00066  DecoderLoss:0.00069  StopLoss:0.03769  GradNorm:0.00350  GradNormST:0.01480  AvgTextLen:9.0  AvgSpecLen:42.1  StepTime:0.36  LR:0.000100\n",
      "   | > Step:19/68  GlobalStep:126290  TotalLoss:0.00153  PostnetLoss:0.00075  DecoderLoss:0.00078  StopLoss:0.06591  GradNorm:0.00320  GradNormST:0.02132  AvgTextLen:15.5  AvgSpecLen:63.2  StepTime:0.41  LR:0.000100\n",
      "   | > Step:29/68  GlobalStep:126300  TotalLoss:0.00174  PostnetLoss:0.00085  DecoderLoss:0.00089  StopLoss:0.06354  GradNorm:0.00362  GradNormST:0.01801  AvgTextLen:21.8  AvgSpecLen:83.4  StepTime:0.58  LR:0.000100\n",
      "   | > Step:39/68  GlobalStep:126310  TotalLoss:0.00192  PostnetLoss:0.00094  DecoderLoss:0.00098  StopLoss:0.04114  GradNorm:0.00299  GradNormST:0.01169  AvgTextLen:30.2  AvgSpecLen:110.4  StepTime:0.62  LR:0.000100\n",
      "   | > Step:49/68  GlobalStep:126320  TotalLoss:0.00203  PostnetLoss:0.00099  DecoderLoss:0.00104  StopLoss:0.04160  GradNorm:0.00260  GradNormST:0.00909  AvgTextLen:38.2  AvgSpecLen:127.7  StepTime:0.73  LR:0.000100\n",
      "   | > Step:59/68  GlobalStep:126330  TotalLoss:0.00218  PostnetLoss:0.00106  DecoderLoss:0.00112  StopLoss:0.04077  GradNorm:0.00257  GradNormST:0.01530  AvgTextLen:50.5  AvgSpecLen:155.9  StepTime:0.83  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:126339  AvgTotalLoss:0.05025  AvgPostnetLoss:0.00089  AvgDecoderLoss:0.00093  AvgStopLoss:0.04843  EpochTime:42.25  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.08021   PostnetLoss: 0.00559   DecoderLoss:0.00587  StopLoss: 0.06874  \n",
      "   | > TotalLoss: 0.08970   PostnetLoss: 0.00951   DecoderLoss:0.00989  StopLoss: 0.07029  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00089   Validation Loss: 0.00854\n",
      "\n",
      " > Epoch 831/1000\n",
      "   | > Step:0/68  GlobalStep:126340  TotalLoss:0.00142  PostnetLoss:0.00068  DecoderLoss:0.00074  StopLoss:0.06814  GradNorm:0.00762  GradNormST:0.02718  AvgTextLen:3.0  AvgSpecLen:25.3  StepTime:0.49  LR:0.000100\n",
      "   | > Step:10/68  GlobalStep:126350  TotalLoss:0.00143  PostnetLoss:0.00070  DecoderLoss:0.00073  StopLoss:0.05476  GradNorm:0.00391  GradNormST:0.02350  AvgTextLen:9.8  AvgSpecLen:48.6  StepTime:0.55  LR:0.000100\n",
      "   | > Step:20/68  GlobalStep:126360  TotalLoss:0.00162  PostnetLoss:0.00079  DecoderLoss:0.00083  StopLoss:0.03404  GradNorm:0.00374  GradNormST:0.01272  AvgTextLen:16.4  AvgSpecLen:64.8  StepTime:0.57  LR:0.000100\n",
      "   | > Step:30/68  GlobalStep:126370  TotalLoss:0.00177  PostnetLoss:0.00087  DecoderLoss:0.00090  StopLoss:0.04292  GradNorm:0.00323  GradNormST:0.01142  AvgTextLen:22.9  AvgSpecLen:85.8  StepTime:0.57  LR:0.000100\n",
      "   | > Step:40/68  GlobalStep:126380  TotalLoss:0.00188  PostnetLoss:0.00092  DecoderLoss:0.00096  StopLoss:0.04647  GradNorm:0.00315  GradNormST:0.01213  AvgTextLen:30.9  AvgSpecLen:105.3  StepTime:0.61  LR:0.000100\n",
      "   | > Step:50/68  GlobalStep:126390  TotalLoss:0.00205  PostnetLoss:0.00100  DecoderLoss:0.00105  StopLoss:0.04050  GradNorm:0.00275  GradNormST:0.01262  AvgTextLen:38.8  AvgSpecLen:132.0  StepTime:0.85  LR:0.000100\n",
      "   | > Step:60/68  GlobalStep:126400  TotalLoss:0.00227  PostnetLoss:0.00110  DecoderLoss:0.00116  StopLoss:0.04085  GradNorm:0.00269  GradNormST:0.01160  AvgTextLen:52.0  AvgSpecLen:172.7  StepTime:0.91  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:126408  AvgTotalLoss:0.04942  AvgPostnetLoss:0.00089  AvgDecoderLoss:0.00093  AvgStopLoss:0.04760  EpochTime:42.59  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.08282   PostnetLoss: 0.00555   DecoderLoss:0.00582  StopLoss: 0.07146  \n",
      "   | > TotalLoss: 0.09108   PostnetLoss: 0.00973   DecoderLoss:0.01009  StopLoss: 0.07127  \n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00089   Validation Loss: 0.00879\n",
      "\n",
      " > Epoch 832/1000\n",
      "   | > Step:1/68  GlobalStep:126410  TotalLoss:0.00129  PostnetLoss:0.00063  DecoderLoss:0.00066  StopLoss:0.03921  GradNorm:0.00457  GradNormST:0.01680  AvgTextLen:3.7  AvgSpecLen:28.6  StepTime:0.47  LR:0.000100\n",
      "   | > Step:11/68  GlobalStep:126420  TotalLoss:0.00140  PostnetLoss:0.00069  DecoderLoss:0.00072  StopLoss:0.03755  GradNorm:0.00347  GradNormST:0.01591  AvgTextLen:10.3  AvgSpecLen:49.4  StepTime:0.39  LR:0.000100\n",
      "   | > Step:21/68  GlobalStep:126430  TotalLoss:0.00158  PostnetLoss:0.00077  DecoderLoss:0.00081  StopLoss:0.04458  GradNorm:0.00347  GradNormST:0.01663  AvgTextLen:16.6  AvgSpecLen:66.3  StepTime:0.45  LR:0.000100\n",
      "   | > Step:31/68  GlobalStep:126440  TotalLoss:0.00174  PostnetLoss:0.00085  DecoderLoss:0.00089  StopLoss:0.04325  GradNorm:0.00310  GradNormST:0.01224  AvgTextLen:24.1  AvgSpecLen:82.7  StepTime:0.54  LR:0.000100\n",
      "   | > Step:41/68  GlobalStep:126450  TotalLoss:0.00198  PostnetLoss:0.00097  DecoderLoss:0.00101  StopLoss:0.02842  GradNorm:0.00367  GradNormST:0.00875  AvgTextLen:32.3  AvgSpecLen:109.4  StepTime:0.64  LR:0.000100\n",
      "   | > Step:51/68  GlobalStep:126460  TotalLoss:0.00205  PostnetLoss:0.00100  DecoderLoss:0.00105  StopLoss:0.04533  GradNorm:0.00285  GradNormST:0.01289  AvgTextLen:39.2  AvgSpecLen:126.9  StepTime:0.74  LR:0.000100\n",
      "   | > Step:61/68  GlobalStep:126470  TotalLoss:0.00223  PostnetLoss:0.00108  DecoderLoss:0.00114  StopLoss:0.04523  GradNorm:0.00248  GradNormST:0.01030  AvgTextLen:54.6  AvgSpecLen:165.5  StepTime:0.90  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:126477  AvgTotalLoss:0.04822  AvgPostnetLoss:0.00089  AvgDecoderLoss:0.00093  AvgStopLoss:0.04641  EpochTime:42.06  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.07982   PostnetLoss: 0.00553   DecoderLoss:0.00580  StopLoss: 0.06849  \n",
      "   | > TotalLoss: 0.09159   PostnetLoss: 0.00987   DecoderLoss:0.01026  StopLoss: 0.07146  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00089   Validation Loss: 0.00892\n",
      "\n",
      " > Epoch 833/1000\n",
      "   | > Step:2/68  GlobalStep:126480  TotalLoss:0.00121  PostnetLoss:0.00059  DecoderLoss:0.00062  StopLoss:0.03894  GradNorm:0.00388  GradNormST:0.01861  AvgTextLen:4.0  AvgSpecLen:31.3  StepTime:0.45  LR:0.000100\n",
      "   | > Step:12/68  GlobalStep:126490  TotalLoss:0.00139  PostnetLoss:0.00068  DecoderLoss:0.00071  StopLoss:0.05141  GradNorm:0.00342  GradNormST:0.01921  AvgTextLen:11.0  AvgSpecLen:49.3  StepTime:0.34  LR:0.000100\n",
      "   | > Step:22/68  GlobalStep:126500  TotalLoss:0.00164  PostnetLoss:0.00080  DecoderLoss:0.00084  StopLoss:0.05622  GradNorm:0.00306  GradNormST:0.01499  AvgTextLen:18.0  AvgSpecLen:72.0  StepTime:0.48  LR:0.000100\n",
      "   | > Step:32/68  GlobalStep:126510  TotalLoss:0.00178  PostnetLoss:0.00087  DecoderLoss:0.00091  StopLoss:0.05129  GradNorm:0.00312  GradNormST:0.01358  AvgTextLen:24.4  AvgSpecLen:91.5  StepTime:0.46  LR:0.000100\n",
      "   | > Step:42/68  GlobalStep:126520  TotalLoss:0.00194  PostnetLoss:0.00095  DecoderLoss:0.00100  StopLoss:0.05428  GradNorm:0.00327  GradNormST:0.02023  AvgTextLen:33.1  AvgSpecLen:109.7  StepTime:0.60  LR:0.000100\n",
      "   | > Step:52/68  GlobalStep:126530  TotalLoss:0.00213  PostnetLoss:0.00104  DecoderLoss:0.00109  StopLoss:0.05355  GradNorm:0.00328  GradNormST:0.01320  AvgTextLen:42.1  AvgSpecLen:137.5  StepTime:0.81  LR:0.000100\n",
      "   | > Step:62/68  GlobalStep:126540  TotalLoss:0.00227  PostnetLoss:0.00110  DecoderLoss:0.00117  StopLoss:0.04691  GradNorm:0.00275  GradNormST:0.01026  AvgTextLen:54.0  AvgSpecLen:174.7  StepTime:0.75  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:126546  AvgTotalLoss:0.04784  AvgPostnetLoss:0.00088  AvgDecoderLoss:0.00093  AvgStopLoss:0.04603  EpochTime:41.92  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.07805   PostnetLoss: 0.00556   DecoderLoss:0.00583  StopLoss: 0.06666  \n",
      "   | > TotalLoss: 0.09046   PostnetLoss: 0.00995   DecoderLoss:0.01032  StopLoss: 0.07020  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00088   Validation Loss: 0.00887\n",
      "\n",
      " > Epoch 834/1000\n",
      "   | > Step:3/68  GlobalStep:126550  TotalLoss:0.00122  PostnetLoss:0.00060  DecoderLoss:0.00063  StopLoss:0.07693  GradNorm:0.00370  GradNormST:0.02557  AvgTextLen:4.7  AvgSpecLen:29.7  StepTime:0.31  LR:0.000100\n",
      "   | > Step:13/68  GlobalStep:126560  TotalLoss:0.00140  PostnetLoss:0.00069  DecoderLoss:0.00072  StopLoss:0.05292  GradNorm:0.00306  GradNormST:0.01994  AvgTextLen:11.5  AvgSpecLen:55.2  StepTime:0.44  LR:0.000100\n",
      "   | > Step:23/68  GlobalStep:126570  TotalLoss:0.00166  PostnetLoss:0.00081  DecoderLoss:0.00085  StopLoss:0.04767  GradNorm:0.00299  GradNormST:0.01231  AvgTextLen:18.3  AvgSpecLen:72.2  StepTime:0.44  LR:0.000100\n",
      "   | > Step:33/68  GlobalStep:126580  TotalLoss:0.00182  PostnetLoss:0.00089  DecoderLoss:0.00093  StopLoss:0.04262  GradNorm:0.00305  GradNormST:0.01322  AvgTextLen:25.9  AvgSpecLen:88.9  StepTime:0.54  LR:0.000100\n",
      "   | > Step:43/68  GlobalStep:126590  TotalLoss:0.00196  PostnetLoss:0.00096  DecoderLoss:0.00101  StopLoss:0.04843  GradNorm:0.00304  GradNormST:0.01144  AvgTextLen:33.4  AvgSpecLen:115.7  StepTime:0.52  LR:0.000100\n",
      "   | > Step:53/68  GlobalStep:126600  TotalLoss:0.00207  PostnetLoss:0.00101  DecoderLoss:0.00106  StopLoss:0.03918  GradNorm:0.00318  GradNormST:0.00919  AvgTextLen:42.4  AvgSpecLen:137.6  StepTime:0.76  LR:0.000100\n",
      "   | > Step:63/68  GlobalStep:126610  TotalLoss:0.00235  PostnetLoss:0.00114  DecoderLoss:0.00120  StopLoss:0.04246  GradNorm:0.00280  GradNormST:0.01791  AvgTextLen:58.6  AvgSpecLen:185.1  StepTime:0.91  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:126615  AvgTotalLoss:0.04718  AvgPostnetLoss:0.00088  AvgDecoderLoss:0.00093  AvgStopLoss:0.04537  EpochTime:43.06  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.07987   PostnetLoss: 0.00558   DecoderLoss:0.00585  StopLoss: 0.06843  \n",
      "   | > TotalLoss: 0.08511   PostnetLoss: 0.00980   DecoderLoss:0.01018  StopLoss: 0.06513  \n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00088   Validation Loss: 0.00884\n",
      "\n",
      " > Epoch 835/1000\n",
      "   | > Step:4/68  GlobalStep:126620  TotalLoss:0.00124  PostnetLoss:0.00060  DecoderLoss:0.00064  StopLoss:0.06183  GradNorm:0.00373  GradNormST:0.02492  AvgTextLen:4.8  AvgSpecLen:31.1  StepTime:0.26  LR:0.000100\n",
      "   | > Step:14/68  GlobalStep:126630  TotalLoss:0.00148  PostnetLoss:0.00072  DecoderLoss:0.00075  StopLoss:0.04012  GradNorm:0.00335  GradNormST:0.01429  AvgTextLen:12.2  AvgSpecLen:52.0  StepTime:0.35  LR:0.000100\n",
      "   | > Step:24/68  GlobalStep:126640  TotalLoss:0.00172  PostnetLoss:0.00084  DecoderLoss:0.00088  StopLoss:0.04302  GradNorm:0.00296  GradNormST:0.01291  AvgTextLen:19.2  AvgSpecLen:79.0  StepTime:0.52  LR:0.000100\n",
      "   | > Step:34/68  GlobalStep:126650  TotalLoss:0.00186  PostnetLoss:0.00091  DecoderLoss:0.00095  StopLoss:0.03548  GradNorm:0.00287  GradNormST:0.00939  AvgTextLen:26.2  AvgSpecLen:95.9  StepTime:0.48  LR:0.000100\n",
      "   | > Step:44/68  GlobalStep:126660  TotalLoss:0.00198  PostnetLoss:0.00097  DecoderLoss:0.00101  StopLoss:0.04186  GradNorm:0.00294  GradNormST:0.01032  AvgTextLen:33.8  AvgSpecLen:116.3  StepTime:0.71  LR:0.000100\n",
      "   | > Step:54/68  GlobalStep:126670  TotalLoss:0.00211  PostnetLoss:0.00103  DecoderLoss:0.00108  StopLoss:0.04145  GradNorm:0.00296  GradNormST:0.01446  AvgTextLen:42.2  AvgSpecLen:144.9  StepTime:0.87  LR:0.000100\n",
      "   | > Step:64/68  GlobalStep:126680  TotalLoss:0.00222  PostnetLoss:0.00108  DecoderLoss:0.00114  StopLoss:0.03540  GradNorm:0.00254  GradNormST:0.00745  AvgTextLen:60.6  AvgSpecLen:177.6  StepTime:0.80  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:126684  AvgTotalLoss:0.04673  AvgPostnetLoss:0.00088  AvgDecoderLoss:0.00093  AvgStopLoss:0.04492  EpochTime:42.34  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.07802   PostnetLoss: 0.00557   DecoderLoss:0.00585  StopLoss: 0.06660  \n",
      "   | > TotalLoss: 0.08696   PostnetLoss: 0.00993   DecoderLoss:0.01032  StopLoss: 0.06671  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00088   Validation Loss: 0.00892\n",
      "\n",
      " > Epoch 836/1000\n",
      "   | > Step:5/68  GlobalStep:126690  TotalLoss:0.00130  PostnetLoss:0.00063  DecoderLoss:0.00067  StopLoss:0.04256  GradNorm:0.00352  GradNormST:0.01415  AvgTextLen:5.9  AvgSpecLen:33.2  StepTime:0.34  LR:0.000100\n",
      "   | > Step:15/68  GlobalStep:126700  TotalLoss:0.00152  PostnetLoss:0.00074  DecoderLoss:0.00078  StopLoss:0.04913  GradNorm:0.00339  GradNormST:0.01852  AvgTextLen:12.8  AvgSpecLen:59.1  StepTime:0.57  LR:0.000100\n",
      "   | > Step:25/68  GlobalStep:126710  TotalLoss:0.00169  PostnetLoss:0.00082  DecoderLoss:0.00086  StopLoss:0.05927  GradNorm:0.00304  GradNormST:0.01613  AvgTextLen:19.8  AvgSpecLen:73.3  StepTime:0.36  LR:0.000100\n",
      "   | > Step:35/68  GlobalStep:126720  TotalLoss:0.00186  PostnetLoss:0.00091  DecoderLoss:0.00095  StopLoss:0.04135  GradNorm:0.00275  GradNormST:0.01411  AvgTextLen:27.1  AvgSpecLen:104.7  StepTime:0.63  LR:0.000100\n",
      "   | > Step:45/68  GlobalStep:126730  TotalLoss:0.00192  PostnetLoss:0.00094  DecoderLoss:0.00099  StopLoss:0.06003  GradNorm:0.00276  GradNormST:0.01643  AvgTextLen:34.4  AvgSpecLen:118.6  StepTime:0.66  LR:0.000100\n",
      "   | > Step:55/68  GlobalStep:126740  TotalLoss:0.00216  PostnetLoss:0.00105  DecoderLoss:0.00111  StopLoss:0.03787  GradNorm:0.00300  GradNormST:0.00941  AvgTextLen:43.9  AvgSpecLen:145.2  StepTime:0.87  LR:0.000100\n",
      "   | > Step:65/68  GlobalStep:126750  TotalLoss:0.00240  PostnetLoss:0.00117  DecoderLoss:0.00124  StopLoss:0.03598  GradNorm:0.00260  GradNormST:0.01024  AvgTextLen:64.2  AvgSpecLen:207.4  StepTime:1.18  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:126753  AvgTotalLoss:0.04800  AvgPostnetLoss:0.00089  AvgDecoderLoss:0.00093  AvgStopLoss:0.04619  EpochTime:41.62  AvgStepTime:0.60\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.08028   PostnetLoss: 0.00563   DecoderLoss:0.00590  StopLoss: 0.06874  \n",
      "   | > TotalLoss: 0.08625   PostnetLoss: 0.00992   DecoderLoss:0.01030  StopLoss: 0.06603  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00089   Validation Loss: 0.00893\n",
      "\n",
      " > Epoch 837/1000\n",
      "   | > Step:6/68  GlobalStep:126760  TotalLoss:0.00128  PostnetLoss:0.00063  DecoderLoss:0.00065  StopLoss:0.04539  GradNorm:0.00351  GradNormST:0.01175  AvgTextLen:6.0  AvgSpecLen:34.8  StepTime:0.31  LR:0.000100\n",
      "   | > Step:16/68  GlobalStep:126770  TotalLoss:0.00158  PostnetLoss:0.00077  DecoderLoss:0.00080  StopLoss:0.05510  GradNorm:0.00350  GradNormST:0.01846  AvgTextLen:13.8  AvgSpecLen:60.6  StepTime:0.36  LR:0.000100\n",
      "   | > Step:26/68  GlobalStep:126780  TotalLoss:0.00172  PostnetLoss:0.00084  DecoderLoss:0.00088  StopLoss:0.04509  GradNorm:0.00316  GradNormST:0.01390  AvgTextLen:20.3  AvgSpecLen:77.5  StepTime:0.48  LR:0.000100\n",
      "   | > Step:36/68  GlobalStep:126790  TotalLoss:0.00186  PostnetLoss:0.00091  DecoderLoss:0.00096  StopLoss:0.05104  GradNorm:0.00281  GradNormST:0.00998  AvgTextLen:28.5  AvgSpecLen:100.2  StepTime:0.58  LR:0.000100\n",
      "   | > Step:46/68  GlobalStep:126800  TotalLoss:0.00197  PostnetLoss:0.00096  DecoderLoss:0.00101  StopLoss:0.04482  GradNorm:0.00288  GradNormST:0.01549  AvgTextLen:35.4  AvgSpecLen:122.6  StepTime:0.79  LR:0.000100\n",
      "   | > Step:56/68  GlobalStep:126810  TotalLoss:0.00217  PostnetLoss:0.00106  DecoderLoss:0.00111  StopLoss:0.04608  GradNorm:0.00297  GradNormST:0.01573  AvgTextLen:45.8  AvgSpecLen:145.4  StepTime:0.95  LR:0.000100\n",
      "   | > Step:66/68  GlobalStep:126820  TotalLoss:0.00255  PostnetLoss:0.00124  DecoderLoss:0.00131  StopLoss:0.03780  GradNorm:0.00259  GradNormST:0.00964  AvgTextLen:71.2  AvgSpecLen:220.8  StepTime:1.03  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:126822  AvgTotalLoss:0.05229  AvgPostnetLoss:0.00089  AvgDecoderLoss:0.00094  AvgStopLoss:0.05046  EpochTime:42.06  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.07988   PostnetLoss: 0.00561   DecoderLoss:0.00589  StopLoss: 0.06837  \n",
      "   | > TotalLoss: 0.07994   PostnetLoss: 0.00934   DecoderLoss:0.00971  StopLoss: 0.06089  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00089   Validation Loss: 0.00840\n",
      "\n",
      " > Epoch 838/1000\n",
      "   | > Step:7/68  GlobalStep:126830  TotalLoss:0.00136  PostnetLoss:0.00066  DecoderLoss:0.00069  StopLoss:0.04152  GradNorm:0.00382  GradNormST:0.01801  AvgTextLen:7.4  AvgSpecLen:44.6  StepTime:0.39  LR:0.000100\n",
      "   | > Step:17/68  GlobalStep:126840  TotalLoss:0.00155  PostnetLoss:0.00076  DecoderLoss:0.00079  StopLoss:0.05579  GradNorm:0.00368  GradNormST:0.01475  AvgTextLen:14.7  AvgSpecLen:66.0  StepTime:0.43  LR:0.000100\n",
      "   | > Step:27/68  GlobalStep:126850  TotalLoss:0.00175  PostnetLoss:0.00086  DecoderLoss:0.00089  StopLoss:0.05190  GradNorm:0.00344  GradNormST:0.01977  AvgTextLen:21.0  AvgSpecLen:80.2  StepTime:0.57  LR:0.000100\n",
      "   | > Step:37/68  GlobalStep:126860  TotalLoss:0.00188  PostnetLoss:0.00092  DecoderLoss:0.00097  StopLoss:0.04514  GradNorm:0.00291  GradNormST:0.01513  AvgTextLen:29.3  AvgSpecLen:102.4  StepTime:0.60  LR:0.000100\n",
      "   | > Step:47/68  GlobalStep:126870  TotalLoss:0.00204  PostnetLoss:0.00100  DecoderLoss:0.00105  StopLoss:0.04079  GradNorm:0.00290  GradNormST:0.01293  AvgTextLen:36.7  AvgSpecLen:126.9  StepTime:0.70  LR:0.000100\n",
      "   | > Step:57/68  GlobalStep:126880  TotalLoss:0.00224  PostnetLoss:0.00109  DecoderLoss:0.00115  StopLoss:0.03525  GradNorm:0.00311  GradNormST:0.00870  AvgTextLen:47.1  AvgSpecLen:161.0  StepTime:0.81  LR:0.000100\n",
      "   | > Step:67/68  GlobalStep:126890  TotalLoss:0.00256  PostnetLoss:0.00124  DecoderLoss:0.00132  StopLoss:0.02590  GradNorm:0.00262  GradNormST:0.01199  AvgTextLen:80.6  AvgSpecLen:246.5  StepTime:1.21  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:126891  AvgTotalLoss:0.04615  AvgPostnetLoss:0.00089  AvgDecoderLoss:0.00094  AvgStopLoss:0.04432  EpochTime:42.89  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.08142   PostnetLoss: 0.00565   DecoderLoss:0.00592  StopLoss: 0.06986  \n",
      "   | > TotalLoss: 0.08439   PostnetLoss: 0.00945   DecoderLoss:0.00982  StopLoss: 0.06512  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00089   Validation Loss: 0.00853\n",
      "\n",
      " > Epoch 839/1000\n",
      "   | > Step:8/68  GlobalStep:126900  TotalLoss:0.00136  PostnetLoss:0.00067  DecoderLoss:0.00069  StopLoss:0.03881  GradNorm:0.00383  GradNormST:0.01324  AvgTextLen:8.3  AvgSpecLen:44.0  StepTime:0.24  LR:0.000100\n",
      "   | > Step:18/68  GlobalStep:126910  TotalLoss:0.00151  PostnetLoss:0.00074  DecoderLoss:0.00077  StopLoss:0.04367  GradNorm:0.00310  GradNormST:0.01244  AvgTextLen:15.2  AvgSpecLen:64.5  StepTime:0.41  LR:0.000100\n",
      "   | > Step:28/68  GlobalStep:126920  TotalLoss:0.00174  PostnetLoss:0.00085  DecoderLoss:0.00089  StopLoss:0.04687  GradNorm:0.00283  GradNormST:0.01490  AvgTextLen:22.1  AvgSpecLen:85.4  StepTime:0.53  LR:0.000100\n",
      "   | > Step:38/68  GlobalStep:126930  TotalLoss:0.00185  PostnetLoss:0.00090  DecoderLoss:0.00095  StopLoss:0.03452  GradNorm:0.00316  GradNormST:0.01405  AvgTextLen:29.4  AvgSpecLen:104.1  StepTime:0.61  LR:0.000100\n",
      "   | > Step:48/68  GlobalStep:126940  TotalLoss:0.00202  PostnetLoss:0.00099  DecoderLoss:0.00103  StopLoss:0.02861  GradNorm:0.00283  GradNormST:0.01095  AvgTextLen:37.5  AvgSpecLen:123.8  StepTime:0.59  LR:0.000100\n",
      "   | > Step:58/68  GlobalStep:126950  TotalLoss:0.00220  PostnetLoss:0.00107  DecoderLoss:0.00113  StopLoss:0.04559  GradNorm:0.00309  GradNormST:0.01258  AvgTextLen:47.8  AvgSpecLen:158.0  StepTime:0.82  LR:0.000100\n",
      "   | > Step:68/68  GlobalStep:126960  TotalLoss:0.00255  PostnetLoss:0.00124  DecoderLoss:0.00131  StopLoss:0.02738  GradNorm:0.00285  GradNormST:0.01025  AvgTextLen:103.0  AvgSpecLen:308.9  StepTime:1.87  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:126960  AvgTotalLoss:0.04633  AvgPostnetLoss:0.00089  AvgDecoderLoss:0.00093  AvgStopLoss:0.04451  EpochTime:41.60  AvgStepTime:0.60\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.08335   PostnetLoss: 0.00566   DecoderLoss:0.00593  StopLoss: 0.07176  \n",
      "   | > TotalLoss: 0.08784   PostnetLoss: 0.00966   DecoderLoss:0.01001  StopLoss: 0.06817  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00089   Validation Loss: 0.00877\n",
      "\n",
      " > Epoch 840/1000\n",
      "   | > Step:9/68  GlobalStep:126970  TotalLoss:0.00135  PostnetLoss:0.00066  DecoderLoss:0.00069  StopLoss:0.04083  GradNorm:0.00349  GradNormST:0.02145  AvgTextLen:9.0  AvgSpecLen:42.1  StepTime:0.43  LR:0.000100\n",
      "   | > Step:19/68  GlobalStep:126980  TotalLoss:0.00153  PostnetLoss:0.00075  DecoderLoss:0.00078  StopLoss:0.06582  GradNorm:0.00331  GradNormST:0.02367  AvgTextLen:15.5  AvgSpecLen:63.2  StepTime:0.41  LR:0.000100\n",
      "   | > Step:29/68  GlobalStep:126990  TotalLoss:0.00169  PostnetLoss:0.00083  DecoderLoss:0.00086  StopLoss:0.04807  GradNorm:0.00296  GradNormST:0.01080  AvgTextLen:21.8  AvgSpecLen:83.4  StepTime:0.51  LR:0.000100\n",
      "   | > Step:39/68  GlobalStep:127000  TotalLoss:0.00190  PostnetLoss:0.00093  DecoderLoss:0.00097  StopLoss:0.03407  GradNorm:0.00277  GradNormST:0.00963  AvgTextLen:30.2  AvgSpecLen:110.4  StepTime:0.62  LR:0.000100\n",
      " | | > Checkpoint saving : /vm/TTS/models/cartman_models/mozilla-no-loc-fattn-stopnet-sigmoid-loss_masking-August-15-2019_11+26AM-de6aeb9/checkpoint_127000.pth.tar\n",
      "   | > Step:49/68  GlobalStep:127010  TotalLoss:0.00206  PostnetLoss:0.00100  DecoderLoss:0.00106  StopLoss:0.04078  GradNorm:0.00288  GradNormST:0.00725  AvgTextLen:38.2  AvgSpecLen:127.7  StepTime:0.77  LR:0.000100\n",
      "   | > Step:59/68  GlobalStep:127020  TotalLoss:0.00220  PostnetLoss:0.00107  DecoderLoss:0.00113  StopLoss:0.03557  GradNorm:0.00274  GradNormST:0.00936  AvgTextLen:50.5  AvgSpecLen:155.9  StepTime:0.77  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:127029  AvgTotalLoss:0.04638  AvgPostnetLoss:0.00089  AvgDecoderLoss:0.00093  AvgStopLoss:0.04457  EpochTime:42.79  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.08281   PostnetLoss: 0.00571   DecoderLoss:0.00598  StopLoss: 0.07112  \n",
      "   | > TotalLoss: 0.08171   PostnetLoss: 0.00954   DecoderLoss:0.00989  StopLoss: 0.06228  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00089   Validation Loss: 0.00860\n",
      "\n",
      " > Epoch 841/1000\n",
      "   | > Step:0/68  GlobalStep:127030  TotalLoss:0.00132  PostnetLoss:0.00063  DecoderLoss:0.00068  StopLoss:0.07080  GradNorm:0.00563  GradNormST:0.02831  AvgTextLen:3.0  AvgSpecLen:25.3  StepTime:0.56  LR:0.000100\n",
      "   | > Step:10/68  GlobalStep:127040  TotalLoss:0.00140  PostnetLoss:0.00068  DecoderLoss:0.00072  StopLoss:0.03980  GradNorm:0.00337  GradNormST:0.01608  AvgTextLen:9.8  AvgSpecLen:48.6  StepTime:0.55  LR:0.000100\n",
      "   | > Step:20/68  GlobalStep:127050  TotalLoss:0.00162  PostnetLoss:0.00079  DecoderLoss:0.00083  StopLoss:0.02858  GradNorm:0.00363  GradNormST:0.01100  AvgTextLen:16.4  AvgSpecLen:64.8  StepTime:0.39  LR:0.000100\n",
      "   | > Step:30/68  GlobalStep:127060  TotalLoss:0.00174  PostnetLoss:0.00085  DecoderLoss:0.00089  StopLoss:0.05307  GradNorm:0.00312  GradNormST:0.01411  AvgTextLen:22.9  AvgSpecLen:85.8  StepTime:0.56  LR:0.000100\n",
      "   | > Step:40/68  GlobalStep:127070  TotalLoss:0.00190  PostnetLoss:0.00093  DecoderLoss:0.00097  StopLoss:0.04041  GradNorm:0.00290  GradNormST:0.01094  AvgTextLen:30.9  AvgSpecLen:105.3  StepTime:0.62  LR:0.000100\n",
      "   | > Step:50/68  GlobalStep:127080  TotalLoss:0.00203  PostnetLoss:0.00099  DecoderLoss:0.00104  StopLoss:0.04084  GradNorm:0.00294  GradNormST:0.01006  AvgTextLen:38.8  AvgSpecLen:132.0  StepTime:0.66  LR:0.000100\n",
      "   | > Step:60/68  GlobalStep:127090  TotalLoss:0.00224  PostnetLoss:0.00109  DecoderLoss:0.00115  StopLoss:0.02886  GradNorm:0.00286  GradNormST:0.00891  AvgTextLen:52.0  AvgSpecLen:172.7  StepTime:0.78  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:127098  AvgTotalLoss:0.04769  AvgPostnetLoss:0.00088  AvgDecoderLoss:0.00093  AvgStopLoss:0.04588  EpochTime:42.22  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.08273   PostnetLoss: 0.00565   DecoderLoss:0.00592  StopLoss: 0.07115  \n",
      "   | > TotalLoss: 0.08199   PostnetLoss: 0.00968   DecoderLoss:0.01004  StopLoss: 0.06228  \n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00088   Validation Loss: 0.00861\n",
      "\n",
      " > Epoch 842/1000\n",
      "   | > Step:1/68  GlobalStep:127100  TotalLoss:0.00133  PostnetLoss:0.00064  DecoderLoss:0.00068  StopLoss:0.03565  GradNorm:0.00484  GradNormST:0.01908  AvgTextLen:3.7  AvgSpecLen:28.6  StepTime:0.35  LR:0.000100\n",
      "   | > Step:11/68  GlobalStep:127110  TotalLoss:0.00140  PostnetLoss:0.00069  DecoderLoss:0.00071  StopLoss:0.05562  GradNorm:0.00355  GradNormST:0.02834  AvgTextLen:10.3  AvgSpecLen:49.4  StepTime:0.32  LR:0.000100\n",
      "   | > Step:21/68  GlobalStep:127120  TotalLoss:0.00158  PostnetLoss:0.00077  DecoderLoss:0.00080  StopLoss:0.04451  GradNorm:0.00326  GradNormST:0.01655  AvgTextLen:16.6  AvgSpecLen:66.3  StepTime:0.39  LR:0.000100\n",
      "   | > Step:31/68  GlobalStep:127130  TotalLoss:0.00174  PostnetLoss:0.00085  DecoderLoss:0.00089  StopLoss:0.04914  GradNorm:0.00322  GradNormST:0.01414  AvgTextLen:24.1  AvgSpecLen:82.7  StepTime:0.55  LR:0.000100\n",
      "   | > Step:41/68  GlobalStep:127140  TotalLoss:0.00195  PostnetLoss:0.00095  DecoderLoss:0.00100  StopLoss:0.03504  GradNorm:0.00338  GradNormST:0.01128  AvgTextLen:32.3  AvgSpecLen:109.4  StepTime:0.62  LR:0.000100\n",
      "   | > Step:51/68  GlobalStep:127150  TotalLoss:0.00205  PostnetLoss:0.00100  DecoderLoss:0.00105  StopLoss:0.05784  GradNorm:0.00279  GradNormST:0.01684  AvgTextLen:39.2  AvgSpecLen:126.9  StepTime:0.74  LR:0.000100\n",
      "   | > Step:61/68  GlobalStep:127160  TotalLoss:0.00223  PostnetLoss:0.00108  DecoderLoss:0.00114  StopLoss:0.04510  GradNorm:0.00270  GradNormST:0.00995  AvgTextLen:54.6  AvgSpecLen:165.5  StepTime:0.96  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:127167  AvgTotalLoss:0.04561  AvgPostnetLoss:0.00088  AvgDecoderLoss:0.00093  AvgStopLoss:0.04380  EpochTime:41.82  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.08055   PostnetLoss: 0.00559   DecoderLoss:0.00586  StopLoss: 0.06909  \n",
      "   | > TotalLoss: 0.09090   PostnetLoss: 0.00985   DecoderLoss:0.01023  StopLoss: 0.07082  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00088   Validation Loss: 0.00896\n",
      "\n",
      " > Epoch 843/1000\n",
      "   | > Step:2/68  GlobalStep:127170  TotalLoss:0.00119  PostnetLoss:0.00058  DecoderLoss:0.00061  StopLoss:0.03350  GradNorm:0.00380  GradNormST:0.01434  AvgTextLen:4.0  AvgSpecLen:31.3  StepTime:0.40  LR:0.000100\n",
      "   | > Step:12/68  GlobalStep:127180  TotalLoss:0.00137  PostnetLoss:0.00067  DecoderLoss:0.00070  StopLoss:0.04508  GradNorm:0.00302  GradNormST:0.01650  AvgTextLen:11.0  AvgSpecLen:49.3  StepTime:0.34  LR:0.000100\n",
      "   | > Step:22/68  GlobalStep:127190  TotalLoss:0.00165  PostnetLoss:0.00081  DecoderLoss:0.00084  StopLoss:0.04943  GradNorm:0.00360  GradNormST:0.01065  AvgTextLen:18.0  AvgSpecLen:72.0  StepTime:0.36  LR:0.000100\n",
      "   | > Step:32/68  GlobalStep:127200  TotalLoss:0.00177  PostnetLoss:0.00087  DecoderLoss:0.00091  StopLoss:0.05041  GradNorm:0.00343  GradNormST:0.01168  AvgTextLen:24.4  AvgSpecLen:91.5  StepTime:0.55  LR:0.000100\n",
      "   | > Step:42/68  GlobalStep:127210  TotalLoss:0.00193  PostnetLoss:0.00094  DecoderLoss:0.00099  StopLoss:0.04607  GradNorm:0.00322  GradNormST:0.02196  AvgTextLen:33.1  AvgSpecLen:109.7  StepTime:0.61  LR:0.000100\n",
      "   | > Step:52/68  GlobalStep:127220  TotalLoss:0.00212  PostnetLoss:0.00104  DecoderLoss:0.00109  StopLoss:0.05651  GradNorm:0.00301  GradNormST:0.01082  AvgTextLen:42.1  AvgSpecLen:137.5  StepTime:0.80  LR:0.000100\n",
      "   | > Step:62/68  GlobalStep:127230  TotalLoss:0.00229  PostnetLoss:0.00112  DecoderLoss:0.00118  StopLoss:0.04221  GradNorm:0.00258  GradNormST:0.00909  AvgTextLen:54.0  AvgSpecLen:174.7  StepTime:0.75  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:127236  AvgTotalLoss:0.04586  AvgPostnetLoss:0.00088  AvgDecoderLoss:0.00093  AvgStopLoss:0.04405  EpochTime:42.40  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.08395   PostnetLoss: 0.00573   DecoderLoss:0.00601  StopLoss: 0.07220  \n",
      "   | > TotalLoss: 0.08956   PostnetLoss: 0.00975   DecoderLoss:0.01013  StopLoss: 0.06969  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00088   Validation Loss: 0.00882\n",
      "\n",
      " > Epoch 844/1000\n",
      "   | > Step:3/68  GlobalStep:127240  TotalLoss:0.00123  PostnetLoss:0.00060  DecoderLoss:0.00063  StopLoss:0.04629  GradNorm:0.00372  GradNormST:0.01322  AvgTextLen:4.7  AvgSpecLen:29.7  StepTime:0.31  LR:0.000100\n",
      "   | > Step:13/68  GlobalStep:127250  TotalLoss:0.00141  PostnetLoss:0.00069  DecoderLoss:0.00072  StopLoss:0.05529  GradNorm:0.00322  GradNormST:0.02460  AvgTextLen:11.5  AvgSpecLen:55.2  StepTime:0.39  LR:0.000100\n",
      "   | > Step:23/68  GlobalStep:127260  TotalLoss:0.00166  PostnetLoss:0.00081  DecoderLoss:0.00085  StopLoss:0.03410  GradNorm:0.00342  GradNormST:0.01002  AvgTextLen:18.3  AvgSpecLen:72.2  StepTime:0.43  LR:0.000100\n",
      "   | > Step:33/68  GlobalStep:127270  TotalLoss:0.00183  PostnetLoss:0.00089  DecoderLoss:0.00093  StopLoss:0.04879  GradNorm:0.00372  GradNormST:0.01286  AvgTextLen:25.9  AvgSpecLen:88.9  StepTime:0.52  LR:0.000100\n",
      "   | > Step:43/68  GlobalStep:127280  TotalLoss:0.00196  PostnetLoss:0.00096  DecoderLoss:0.00100  StopLoss:0.04069  GradNorm:0.00347  GradNormST:0.01333  AvgTextLen:33.4  AvgSpecLen:115.7  StepTime:0.61  LR:0.000100\n",
      "   | > Step:53/68  GlobalStep:127290  TotalLoss:0.00208  PostnetLoss:0.00101  DecoderLoss:0.00107  StopLoss:0.03337  GradNorm:0.00327  GradNormST:0.00766  AvgTextLen:42.4  AvgSpecLen:137.6  StepTime:0.64  LR:0.000100\n",
      "   | > Step:63/68  GlobalStep:127300  TotalLoss:0.00235  PostnetLoss:0.00114  DecoderLoss:0.00121  StopLoss:0.03032  GradNorm:0.00257  GradNormST:0.00944  AvgTextLen:58.6  AvgSpecLen:185.1  StepTime:0.91  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:127305  AvgTotalLoss:0.04561  AvgPostnetLoss:0.00088  AvgDecoderLoss:0.00093  AvgStopLoss:0.04379  EpochTime:42.34  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.07874   PostnetLoss: 0.00562   DecoderLoss:0.00589  StopLoss: 0.06724  \n",
      "   | > TotalLoss: 0.08650   PostnetLoss: 0.00996   DecoderLoss:0.01034  StopLoss: 0.06620  \n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00088   Validation Loss: 0.00887\n",
      "\n",
      " > Epoch 845/1000\n",
      "   | > Step:4/68  GlobalStep:127310  TotalLoss:0.00124  PostnetLoss:0.00060  DecoderLoss:0.00064  StopLoss:0.06158  GradNorm:0.00365  GradNormST:0.02944  AvgTextLen:4.8  AvgSpecLen:31.1  StepTime:0.27  LR:0.000100\n",
      "   | > Step:14/68  GlobalStep:127320  TotalLoss:0.00147  PostnetLoss:0.00072  DecoderLoss:0.00075  StopLoss:0.04879  GradNorm:0.00333  GradNormST:0.01493  AvgTextLen:12.2  AvgSpecLen:52.0  StepTime:0.34  LR:0.000100\n",
      "   | > Step:24/68  GlobalStep:127330  TotalLoss:0.00170  PostnetLoss:0.00083  DecoderLoss:0.00087  StopLoss:0.06227  GradNorm:0.00316  GradNormST:0.01573  AvgTextLen:19.2  AvgSpecLen:79.0  StepTime:0.53  LR:0.000100\n",
      "   | > Step:34/68  GlobalStep:127340  TotalLoss:0.00183  PostnetLoss:0.00090  DecoderLoss:0.00094  StopLoss:0.03408  GradNorm:0.00341  GradNormST:0.01151  AvgTextLen:26.2  AvgSpecLen:95.9  StepTime:0.48  LR:0.000100\n",
      "   | > Step:44/68  GlobalStep:127350  TotalLoss:0.00200  PostnetLoss:0.00098  DecoderLoss:0.00102  StopLoss:0.03369  GradNorm:0.00405  GradNormST:0.00915  AvgTextLen:33.8  AvgSpecLen:116.3  StepTime:0.54  LR:0.000100\n",
      "   | > Step:54/68  GlobalStep:127360  TotalLoss:0.00215  PostnetLoss:0.00105  DecoderLoss:0.00110  StopLoss:0.04542  GradNorm:0.00358  GradNormST:0.01466  AvgTextLen:42.2  AvgSpecLen:144.9  StepTime:0.81  LR:0.000100\n",
      "   | > Step:64/68  GlobalStep:127370  TotalLoss:0.00224  PostnetLoss:0.00109  DecoderLoss:0.00115  StopLoss:0.04135  GradNorm:0.00272  GradNormST:0.00941  AvgTextLen:60.6  AvgSpecLen:177.6  StepTime:0.81  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:127374  AvgTotalLoss:0.04856  AvgPostnetLoss:0.00088  AvgDecoderLoss:0.00093  AvgStopLoss:0.04675  EpochTime:42.24  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.07966   PostnetLoss: 0.00564   DecoderLoss:0.00592  StopLoss: 0.06811  \n",
      "   | > TotalLoss: 0.08359   PostnetLoss: 0.01023   DecoderLoss:0.01060  StopLoss: 0.06277  \n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00088   Validation Loss: 0.00916\n",
      "\n",
      " > Epoch 846/1000\n",
      "   | > Step:5/68  GlobalStep:127380  TotalLoss:0.00130  PostnetLoss:0.00063  DecoderLoss:0.00067  StopLoss:0.04390  GradNorm:0.00361  GradNormST:0.01715  AvgTextLen:5.9  AvgSpecLen:33.2  StepTime:0.31  LR:0.000100\n",
      "   | > Step:15/68  GlobalStep:127390  TotalLoss:0.00154  PostnetLoss:0.00075  DecoderLoss:0.00079  StopLoss:0.06391  GradNorm:0.00324  GradNormST:0.02681  AvgTextLen:12.8  AvgSpecLen:59.1  StepTime:0.54  LR:0.000100\n",
      "   | > Step:25/68  GlobalStep:127400  TotalLoss:0.00165  PostnetLoss:0.00081  DecoderLoss:0.00084  StopLoss:0.04074  GradNorm:0.00308  GradNormST:0.01520  AvgTextLen:19.8  AvgSpecLen:73.3  StepTime:0.49  LR:0.000100\n",
      "   | > Step:35/68  GlobalStep:127410  TotalLoss:0.00188  PostnetLoss:0.00092  DecoderLoss:0.00096  StopLoss:0.04989  GradNorm:0.00331  GradNormST:0.02842  AvgTextLen:27.1  AvgSpecLen:104.7  StepTime:0.74  LR:0.000100\n",
      "   | > Step:45/68  GlobalStep:127420  TotalLoss:0.00195  PostnetLoss:0.00095  DecoderLoss:0.00100  StopLoss:0.05029  GradNorm:0.00379  GradNormST:0.01590  AvgTextLen:34.4  AvgSpecLen:118.6  StepTime:0.68  LR:0.000100\n",
      "   | > Step:55/68  GlobalStep:127430  TotalLoss:0.00219  PostnetLoss:0.00107  DecoderLoss:0.00112  StopLoss:0.03280  GradNorm:0.00326  GradNormST:0.00771  AvgTextLen:43.9  AvgSpecLen:145.2  StepTime:0.86  LR:0.000100\n",
      "   | > Step:65/68  GlobalStep:127440  TotalLoss:0.00240  PostnetLoss:0.00117  DecoderLoss:0.00123  StopLoss:0.05400  GradNorm:0.00352  GradNormST:0.02251  AvgTextLen:64.2  AvgSpecLen:207.4  StepTime:1.19  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:127443  AvgTotalLoss:0.04644  AvgPostnetLoss:0.00088  AvgDecoderLoss:0.00093  AvgStopLoss:0.04463  EpochTime:42.78  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.07789   PostnetLoss: 0.00568   DecoderLoss:0.00596  StopLoss: 0.06626  \n",
      "   | > TotalLoss: 0.08369   PostnetLoss: 0.00976   DecoderLoss:0.01013  StopLoss: 0.06380  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00088   Validation Loss: 0.00892\n",
      "\n",
      " > Epoch 847/1000\n",
      "   | > Step:6/68  GlobalStep:127450  TotalLoss:0.00128  PostnetLoss:0.00062  DecoderLoss:0.00065  StopLoss:0.04332  GradNorm:0.00372  GradNormST:0.01837  AvgTextLen:6.0  AvgSpecLen:34.8  StepTime:0.40  LR:0.000100\n",
      "   | > Step:16/68  GlobalStep:127460  TotalLoss:0.00155  PostnetLoss:0.00076  DecoderLoss:0.00079  StopLoss:0.03327  GradNorm:0.00333  GradNormST:0.01374  AvgTextLen:13.8  AvgSpecLen:60.6  StepTime:0.39  LR:0.000100\n",
      "   | > Step:26/68  GlobalStep:127470  TotalLoss:0.00170  PostnetLoss:0.00083  DecoderLoss:0.00087  StopLoss:0.04562  GradNorm:0.00299  GradNormST:0.01275  AvgTextLen:20.3  AvgSpecLen:77.5  StepTime:0.47  LR:0.000100\n",
      "   | > Step:36/68  GlobalStep:127480  TotalLoss:0.00184  PostnetLoss:0.00090  DecoderLoss:0.00094  StopLoss:0.05502  GradNorm:0.00291  GradNormST:0.00981  AvgTextLen:28.5  AvgSpecLen:100.2  StepTime:0.56  LR:0.000100\n",
      "   | > Step:46/68  GlobalStep:127490  TotalLoss:0.00200  PostnetLoss:0.00098  DecoderLoss:0.00102  StopLoss:0.03538  GradNorm:0.00384  GradNormST:0.00979  AvgTextLen:35.4  AvgSpecLen:122.6  StepTime:0.81  LR:0.000100\n",
      "   | > Step:56/68  GlobalStep:127500  TotalLoss:0.00218  PostnetLoss:0.00106  DecoderLoss:0.00112  StopLoss:0.04944  GradNorm:0.00381  GradNormST:0.02340  AvgTextLen:45.8  AvgSpecLen:145.4  StepTime:1.04  LR:0.000100\n",
      "   | > Step:66/68  GlobalStep:127510  TotalLoss:0.00252  PostnetLoss:0.00122  DecoderLoss:0.00130  StopLoss:0.03538  GradNorm:0.00268  GradNormST:0.01371  AvgTextLen:71.2  AvgSpecLen:220.8  StepTime:1.05  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:127512  AvgTotalLoss:0.04493  AvgPostnetLoss:0.00088  AvgDecoderLoss:0.00093  AvgStopLoss:0.04311  EpochTime:41.53  AvgStepTime:0.60\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.08042   PostnetLoss: 0.00553   DecoderLoss:0.00579  StopLoss: 0.06910  \n",
      "   | > TotalLoss: 0.08438   PostnetLoss: 0.01003   DecoderLoss:0.01038  StopLoss: 0.06396  \n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00088   Validation Loss: 0.00907\n",
      "\n",
      " > Epoch 848/1000\n",
      "   | > Step:7/68  GlobalStep:127520  TotalLoss:0.00134  PostnetLoss:0.00066  DecoderLoss:0.00068  StopLoss:0.04800  GradNorm:0.00341  GradNormST:0.02374  AvgTextLen:7.4  AvgSpecLen:44.6  StepTime:0.40  LR:0.000100\n",
      "   | > Step:17/68  GlobalStep:127530  TotalLoss:0.00150  PostnetLoss:0.00073  DecoderLoss:0.00077  StopLoss:0.05856  GradNorm:0.00333  GradNormST:0.01325  AvgTextLen:14.7  AvgSpecLen:66.0  StepTime:0.42  LR:0.000100\n",
      "   | > Step:27/68  GlobalStep:127540  TotalLoss:0.00174  PostnetLoss:0.00085  DecoderLoss:0.00089  StopLoss:0.06001  GradNorm:0.00314  GradNormST:0.01362  AvgTextLen:21.0  AvgSpecLen:80.2  StepTime:0.68  LR:0.000100\n",
      "   | > Step:37/68  GlobalStep:127550  TotalLoss:0.00185  PostnetLoss:0.00090  DecoderLoss:0.00095  StopLoss:0.04646  GradNorm:0.00290  GradNormST:0.01622  AvgTextLen:29.3  AvgSpecLen:102.4  StepTime:0.58  LR:0.000100\n",
      "   | > Step:47/68  GlobalStep:127560  TotalLoss:0.00204  PostnetLoss:0.00099  DecoderLoss:0.00105  StopLoss:0.03618  GradNorm:0.00359  GradNormST:0.01084  AvgTextLen:36.7  AvgSpecLen:126.9  StepTime:0.75  LR:0.000100\n",
      "   | > Step:57/68  GlobalStep:127570  TotalLoss:0.00224  PostnetLoss:0.00109  DecoderLoss:0.00115  StopLoss:0.03565  GradNorm:0.00355  GradNormST:0.00802  AvgTextLen:47.1  AvgSpecLen:161.0  StepTime:0.84  LR:0.000100\n",
      "   | > Step:67/68  GlobalStep:127580  TotalLoss:0.00253  PostnetLoss:0.00123  DecoderLoss:0.00130  StopLoss:0.03279  GradNorm:0.00299  GradNormST:0.01577  AvgTextLen:80.6  AvgSpecLen:246.5  StepTime:1.21  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:127581  AvgTotalLoss:0.04412  AvgPostnetLoss:0.00089  AvgDecoderLoss:0.00093  AvgStopLoss:0.04231  EpochTime:42.48  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.07783   PostnetLoss: 0.00568   DecoderLoss:0.00595  StopLoss: 0.06620  \n",
      "   | > TotalLoss: 0.08280   PostnetLoss: 0.00996   DecoderLoss:0.01032  StopLoss: 0.06253  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00089   Validation Loss: 0.00900\n",
      "\n",
      " > Epoch 849/1000\n",
      "   | > Step:8/68  GlobalStep:127590  TotalLoss:0.00138  PostnetLoss:0.00068  DecoderLoss:0.00071  StopLoss:0.05062  GradNorm:0.00424  GradNormST:0.01779  AvgTextLen:8.3  AvgSpecLen:44.0  StepTime:0.24  LR:0.000100\n",
      "   | > Step:18/68  GlobalStep:127600  TotalLoss:0.00152  PostnetLoss:0.00074  DecoderLoss:0.00078  StopLoss:0.03762  GradNorm:0.00318  GradNormST:0.00966  AvgTextLen:15.2  AvgSpecLen:64.5  StepTime:0.41  LR:0.000100\n",
      "   | > Step:28/68  GlobalStep:127610  TotalLoss:0.00173  PostnetLoss:0.00084  DecoderLoss:0.00088  StopLoss:0.04140  GradNorm:0.00332  GradNormST:0.01230  AvgTextLen:22.1  AvgSpecLen:85.4  StepTime:0.47  LR:0.000100\n",
      "   | > Step:38/68  GlobalStep:127620  TotalLoss:0.00185  PostnetLoss:0.00090  DecoderLoss:0.00095  StopLoss:0.04183  GradNorm:0.00318  GradNormST:0.01102  AvgTextLen:29.4  AvgSpecLen:104.1  StepTime:0.50  LR:0.000100\n",
      "   | > Step:48/68  GlobalStep:127630  TotalLoss:0.00203  PostnetLoss:0.00099  DecoderLoss:0.00104  StopLoss:0.03344  GradNorm:0.00379  GradNormST:0.00731  AvgTextLen:37.5  AvgSpecLen:123.8  StepTime:0.55  LR:0.000100\n",
      "   | > Step:58/68  GlobalStep:127640  TotalLoss:0.00224  PostnetLoss:0.00109  DecoderLoss:0.00115  StopLoss:0.05274  GradNorm:0.00414  GradNormST:0.01435  AvgTextLen:47.8  AvgSpecLen:158.0  StepTime:0.72  LR:0.000100\n",
      "   | > Step:68/68  GlobalStep:127650  TotalLoss:0.00251  PostnetLoss:0.00122  DecoderLoss:0.00129  StopLoss:0.02415  GradNorm:0.00270  GradNormST:0.00893  AvgTextLen:103.0  AvgSpecLen:308.9  StepTime:1.87  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:127650  AvgTotalLoss:0.04491  AvgPostnetLoss:0.00089  AvgDecoderLoss:0.00093  AvgStopLoss:0.04309  EpochTime:41.01  AvgStepTime:0.59\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.07769   PostnetLoss: 0.00564   DecoderLoss:0.00590  StopLoss: 0.06615  \n",
      "   | > TotalLoss: 0.08374   PostnetLoss: 0.01006   DecoderLoss:0.01040  StopLoss: 0.06327  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00089   Validation Loss: 0.00916\n",
      "\n",
      " > Epoch 850/1000\n",
      "   | > Step:9/68  GlobalStep:127660  TotalLoss:0.00134  PostnetLoss:0.00065  DecoderLoss:0.00068  StopLoss:0.03865  GradNorm:0.00353  GradNormST:0.01662  AvgTextLen:9.0  AvgSpecLen:42.1  StepTime:0.42  LR:0.000100\n",
      "   | > Step:19/68  GlobalStep:127670  TotalLoss:0.00151  PostnetLoss:0.00074  DecoderLoss:0.00077  StopLoss:0.05162  GradNorm:0.00328  GradNormST:0.01719  AvgTextLen:15.5  AvgSpecLen:63.2  StepTime:0.43  LR:0.000100\n",
      "   | > Step:29/68  GlobalStep:127680  TotalLoss:0.00170  PostnetLoss:0.00083  DecoderLoss:0.00087  StopLoss:0.04920  GradNorm:0.00293  GradNormST:0.01104  AvgTextLen:21.8  AvgSpecLen:83.4  StepTime:0.45  LR:0.000100\n",
      "   | > Step:39/68  GlobalStep:127690  TotalLoss:0.00190  PostnetLoss:0.00093  DecoderLoss:0.00097  StopLoss:0.03410  GradNorm:0.00304  GradNormST:0.00974  AvgTextLen:30.2  AvgSpecLen:110.4  StepTime:0.63  LR:0.000100\n",
      "   | > Step:49/68  GlobalStep:127700  TotalLoss:0.00205  PostnetLoss:0.00100  DecoderLoss:0.00105  StopLoss:0.04175  GradNorm:0.00330  GradNormST:0.00833  AvgTextLen:38.2  AvgSpecLen:127.7  StepTime:0.70  LR:0.000100\n",
      "   | > Step:59/68  GlobalStep:127710  TotalLoss:0.00220  PostnetLoss:0.00107  DecoderLoss:0.00113  StopLoss:0.04295  GradNorm:0.00365  GradNormST:0.01601  AvgTextLen:50.5  AvgSpecLen:155.9  StepTime:0.87  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:127719  AvgTotalLoss:0.04549  AvgPostnetLoss:0.00088  AvgDecoderLoss:0.00093  AvgStopLoss:0.04369  EpochTime:42.27  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.07833   PostnetLoss: 0.00575   DecoderLoss:0.00602  StopLoss: 0.06656  \n",
      "   | > TotalLoss: 0.08819   PostnetLoss: 0.00992   DecoderLoss:0.01027  StopLoss: 0.06800  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00088   Validation Loss: 0.00902\n",
      "\n",
      " > Epoch 851/1000\n",
      "   | > Step:0/68  GlobalStep:127720  TotalLoss:0.00132  PostnetLoss:0.00064  DecoderLoss:0.00069  StopLoss:0.07165  GradNorm:0.00625  GradNormST:0.03277  AvgTextLen:3.0  AvgSpecLen:25.3  StepTime:0.46  LR:0.000100\n",
      "   | > Step:10/68  GlobalStep:127730  TotalLoss:0.00141  PostnetLoss:0.00069  DecoderLoss:0.00072  StopLoss:0.03544  GradNorm:0.00427  GradNormST:0.01331  AvgTextLen:9.8  AvgSpecLen:48.6  StepTime:0.46  LR:0.000100\n",
      "   | > Step:20/68  GlobalStep:127740  TotalLoss:0.00162  PostnetLoss:0.00079  DecoderLoss:0.00083  StopLoss:0.03497  GradNorm:0.00340  GradNormST:0.01193  AvgTextLen:16.4  AvgSpecLen:64.8  StepTime:0.48  LR:0.000100\n",
      "   | > Step:30/68  GlobalStep:127750  TotalLoss:0.00172  PostnetLoss:0.00084  DecoderLoss:0.00088  StopLoss:0.05070  GradNorm:0.00288  GradNormST:0.01046  AvgTextLen:22.9  AvgSpecLen:85.8  StepTime:0.60  LR:0.000100\n",
      "   | > Step:40/68  GlobalStep:127760  TotalLoss:0.00189  PostnetLoss:0.00092  DecoderLoss:0.00097  StopLoss:0.04351  GradNorm:0.00315  GradNormST:0.01058  AvgTextLen:30.9  AvgSpecLen:105.3  StepTime:0.64  LR:0.000100\n",
      "   | > Step:50/68  GlobalStep:127770  TotalLoss:0.00205  PostnetLoss:0.00100  DecoderLoss:0.00105  StopLoss:0.03399  GradNorm:0.00360  GradNormST:0.00919  AvgTextLen:38.8  AvgSpecLen:132.0  StepTime:0.78  LR:0.000100\n",
      "   | > Step:60/68  GlobalStep:127780  TotalLoss:0.00225  PostnetLoss:0.00110  DecoderLoss:0.00115  StopLoss:0.03301  GradNorm:0.00383  GradNormST:0.00971  AvgTextLen:52.0  AvgSpecLen:172.7  StepTime:0.99  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:127788  AvgTotalLoss:0.04380  AvgPostnetLoss:0.00088  AvgDecoderLoss:0.00093  AvgStopLoss:0.04199  EpochTime:43.22  AvgStepTime:0.63\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.07842   PostnetLoss: 0.00569   DecoderLoss:0.00595  StopLoss: 0.06678  \n",
      "   | > TotalLoss: 0.08820   PostnetLoss: 0.01014   DecoderLoss:0.01049  StopLoss: 0.06757  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00088   Validation Loss: 0.00917\n",
      "\n",
      " > Epoch 852/1000\n",
      "   | > Step:1/68  GlobalStep:127790  TotalLoss:0.00127  PostnetLoss:0.00062  DecoderLoss:0.00065  StopLoss:0.04000  GradNorm:0.00520  GradNormST:0.01605  AvgTextLen:3.7  AvgSpecLen:28.6  StepTime:0.24  LR:0.000100\n",
      "   | > Step:11/68  GlobalStep:127800  TotalLoss:0.00138  PostnetLoss:0.00068  DecoderLoss:0.00071  StopLoss:0.04027  GradNorm:0.00331  GradNormST:0.02248  AvgTextLen:10.3  AvgSpecLen:49.4  StepTime:0.32  LR:0.000100\n",
      "   | > Step:21/68  GlobalStep:127810  TotalLoss:0.00162  PostnetLoss:0.00079  DecoderLoss:0.00082  StopLoss:0.04620  GradNorm:0.00333  GradNormST:0.01400  AvgTextLen:16.6  AvgSpecLen:66.3  StepTime:0.34  LR:0.000100\n",
      "   | > Step:31/68  GlobalStep:127820  TotalLoss:0.00174  PostnetLoss:0.00085  DecoderLoss:0.00089  StopLoss:0.03704  GradNorm:0.00298  GradNormST:0.01100  AvgTextLen:24.1  AvgSpecLen:82.7  StepTime:0.57  LR:0.000100\n",
      "   | > Step:41/68  GlobalStep:127830  TotalLoss:0.00194  PostnetLoss:0.00095  DecoderLoss:0.00100  StopLoss:0.02886  GradNorm:0.00275  GradNormST:0.00920  AvgTextLen:32.3  AvgSpecLen:109.4  StepTime:0.67  LR:0.000100\n",
      "   | > Step:51/68  GlobalStep:127840  TotalLoss:0.00204  PostnetLoss:0.00100  DecoderLoss:0.00105  StopLoss:0.04633  GradNorm:0.00303  GradNormST:0.01063  AvgTextLen:39.2  AvgSpecLen:126.9  StepTime:0.72  LR:0.000100\n",
      "   | > Step:61/68  GlobalStep:127850  TotalLoss:0.00224  PostnetLoss:0.00109  DecoderLoss:0.00115  StopLoss:0.03305  GradNorm:0.00384  GradNormST:0.01063  AvgTextLen:54.6  AvgSpecLen:165.5  StepTime:0.90  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:127857  AvgTotalLoss:0.04432  AvgPostnetLoss:0.00089  AvgDecoderLoss:0.00093  AvgStopLoss:0.04250  EpochTime:42.29  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.08008   PostnetLoss: 0.00572   DecoderLoss:0.00600  StopLoss: 0.06836  \n",
      "   | > TotalLoss: 0.08616   PostnetLoss: 0.00976   DecoderLoss:0.01014  StopLoss: 0.06627  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00089   Validation Loss: 0.00881\n",
      "\n",
      " > Epoch 853/1000\n",
      "   | > Step:2/68  GlobalStep:127860  TotalLoss:0.00118  PostnetLoss:0.00057  DecoderLoss:0.00060  StopLoss:0.04096  GradNorm:0.00428  GradNormST:0.01590  AvgTextLen:4.0  AvgSpecLen:31.3  StepTime:0.51  LR:0.000100\n",
      "   | > Step:12/68  GlobalStep:127870  TotalLoss:0.00141  PostnetLoss:0.00069  DecoderLoss:0.00072  StopLoss:0.04137  GradNorm:0.00333  GradNormST:0.01247  AvgTextLen:11.0  AvgSpecLen:49.3  StepTime:0.33  LR:0.000100\n",
      "   | > Step:22/68  GlobalStep:127880  TotalLoss:0.00162  PostnetLoss:0.00079  DecoderLoss:0.00083  StopLoss:0.04659  GradNorm:0.00328  GradNormST:0.01062  AvgTextLen:18.0  AvgSpecLen:72.0  StepTime:0.40  LR:0.000100\n",
      "   | > Step:32/68  GlobalStep:127890  TotalLoss:0.00175  PostnetLoss:0.00086  DecoderLoss:0.00090  StopLoss:0.05610  GradNorm:0.00277  GradNormST:0.01820  AvgTextLen:24.4  AvgSpecLen:91.5  StepTime:0.46  LR:0.000100\n",
      "   | > Step:42/68  GlobalStep:127900  TotalLoss:0.00190  PostnetLoss:0.00092  DecoderLoss:0.00097  StopLoss:0.04073  GradNorm:0.00273  GradNormST:0.01960  AvgTextLen:33.1  AvgSpecLen:109.7  StepTime:0.60  LR:0.000100\n",
      "   | > Step:52/68  GlobalStep:127910  TotalLoss:0.00210  PostnetLoss:0.00102  DecoderLoss:0.00107  StopLoss:0.05194  GradNorm:0.00330  GradNormST:0.01121  AvgTextLen:42.1  AvgSpecLen:137.5  StepTime:0.78  LR:0.000100\n",
      "   | > Step:62/68  GlobalStep:127920  TotalLoss:0.00230  PostnetLoss:0.00112  DecoderLoss:0.00118  StopLoss:0.03740  GradNorm:0.00353  GradNormST:0.01375  AvgTextLen:54.0  AvgSpecLen:174.7  StepTime:0.76  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:127926  AvgTotalLoss:0.04351  AvgPostnetLoss:0.00088  AvgDecoderLoss:0.00093  AvgStopLoss:0.04169  EpochTime:42.77  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.07870   PostnetLoss: 0.00577   DecoderLoss:0.00604  StopLoss: 0.06689  \n",
      "   | > TotalLoss: 0.09002   PostnetLoss: 0.01030   DecoderLoss:0.01067  StopLoss: 0.06905  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00088   Validation Loss: 0.00919\n",
      "\n",
      " > Epoch 854/1000\n",
      "   | > Step:3/68  GlobalStep:127930  TotalLoss:0.00124  PostnetLoss:0.00061  DecoderLoss:0.00063  StopLoss:0.05672  GradNorm:0.00373  GradNormST:0.01586  AvgTextLen:4.7  AvgSpecLen:29.7  StepTime:0.28  LR:0.000100\n",
      "   | > Step:13/68  GlobalStep:127940  TotalLoss:0.00141  PostnetLoss:0.00069  DecoderLoss:0.00072  StopLoss:0.04715  GradNorm:0.00320  GradNormST:0.02393  AvgTextLen:11.5  AvgSpecLen:55.2  StepTime:0.33  LR:0.000100\n",
      "   | > Step:23/68  GlobalStep:127950  TotalLoss:0.00169  PostnetLoss:0.00083  DecoderLoss:0.00086  StopLoss:0.04137  GradNorm:0.00348  GradNormST:0.01008  AvgTextLen:18.3  AvgSpecLen:72.2  StepTime:0.36  LR:0.000100\n",
      "   | > Step:33/68  GlobalStep:127960  TotalLoss:0.00183  PostnetLoss:0.00089  DecoderLoss:0.00094  StopLoss:0.04039  GradNorm:0.00310  GradNormST:0.01408  AvgTextLen:25.9  AvgSpecLen:88.9  StepTime:0.44  LR:0.000100\n",
      "   | > Step:43/68  GlobalStep:127970  TotalLoss:0.00194  PostnetLoss:0.00095  DecoderLoss:0.00099  StopLoss:0.03624  GradNorm:0.00281  GradNormST:0.01240  AvgTextLen:33.4  AvgSpecLen:115.7  StepTime:0.55  LR:0.000100\n",
      "   | > Step:53/68  GlobalStep:127980  TotalLoss:0.00206  PostnetLoss:0.00100  DecoderLoss:0.00106  StopLoss:0.02976  GradNorm:0.00310  GradNormST:0.00708  AvgTextLen:42.4  AvgSpecLen:137.6  StepTime:0.63  LR:0.000100\n",
      "   | > Step:63/68  GlobalStep:127990  TotalLoss:0.00233  PostnetLoss:0.00113  DecoderLoss:0.00119  StopLoss:0.03739  GradNorm:0.00276  GradNormST:0.01701  AvgTextLen:58.6  AvgSpecLen:185.1  StepTime:0.90  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:127995  AvgTotalLoss:0.04313  AvgPostnetLoss:0.00088  AvgDecoderLoss:0.00093  AvgStopLoss:0.04132  EpochTime:42.18  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.07409   PostnetLoss: 0.00562   DecoderLoss:0.00589  StopLoss: 0.06258  \n",
      "   | > TotalLoss: 0.08358   PostnetLoss: 0.00961   DecoderLoss:0.00998  StopLoss: 0.06399  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00088   Validation Loss: 0.00875\n",
      "\n",
      " > Epoch 855/1000\n",
      "   | > Step:4/68  GlobalStep:128000  TotalLoss:0.00125  PostnetLoss:0.00061  DecoderLoss:0.00064  StopLoss:0.06239  GradNorm:0.00393  GradNormST:0.02024  AvgTextLen:4.8  AvgSpecLen:31.1  StepTime:0.28  LR:0.000100\n",
      " | | > Checkpoint saving : /vm/TTS/models/cartman_models/mozilla-no-loc-fattn-stopnet-sigmoid-loss_masking-August-15-2019_11+26AM-de6aeb9/checkpoint_128000.pth.tar\n",
      "   | > Step:14/68  GlobalStep:128010  TotalLoss:0.00144  PostnetLoss:0.00071  DecoderLoss:0.00074  StopLoss:0.03996  GradNorm:0.00332  GradNormST:0.01283  AvgTextLen:12.2  AvgSpecLen:52.0  StepTime:0.26  LR:0.000100\n",
      "   | > Step:24/68  GlobalStep:128020  TotalLoss:0.00172  PostnetLoss:0.00084  DecoderLoss:0.00088  StopLoss:0.04049  GradNorm:0.00311  GradNormST:0.01159  AvgTextLen:19.2  AvgSpecLen:79.0  StepTime:0.51  LR:0.000100\n",
      "   | > Step:34/68  GlobalStep:128030  TotalLoss:0.00182  PostnetLoss:0.00089  DecoderLoss:0.00093  StopLoss:0.03165  GradNorm:0.00283  GradNormST:0.00899  AvgTextLen:26.2  AvgSpecLen:95.9  StepTime:0.57  LR:0.000100\n",
      "   | > Step:44/68  GlobalStep:128040  TotalLoss:0.00198  PostnetLoss:0.00097  DecoderLoss:0.00102  StopLoss:0.02819  GradNorm:0.00284  GradNormST:0.00961  AvgTextLen:33.8  AvgSpecLen:116.3  StepTime:0.67  LR:0.000100\n",
      "   | > Step:54/68  GlobalStep:128050  TotalLoss:0.00211  PostnetLoss:0.00103  DecoderLoss:0.00109  StopLoss:0.04796  GradNorm:0.00302  GradNormST:0.01389  AvgTextLen:42.2  AvgSpecLen:144.9  StepTime:0.79  LR:0.000100\n",
      "   | > Step:64/68  GlobalStep:128060  TotalLoss:0.00221  PostnetLoss:0.00107  DecoderLoss:0.00113  StopLoss:0.03366  GradNorm:0.00271  GradNormST:0.00709  AvgTextLen:60.6  AvgSpecLen:177.6  StepTime:0.81  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:128064  AvgTotalLoss:0.04464  AvgPostnetLoss:0.00088  AvgDecoderLoss:0.00093  AvgStopLoss:0.04283  EpochTime:41.79  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.07343   PostnetLoss: 0.00575   DecoderLoss:0.00604  StopLoss: 0.06163  \n",
      "   | > TotalLoss: 0.08062   PostnetLoss: 0.00930   DecoderLoss:0.00966  StopLoss: 0.06167  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00088   Validation Loss: 0.00856\n",
      "\n",
      " > Epoch 856/1000\n",
      "   | > Step:5/68  GlobalStep:128070  TotalLoss:0.00130  PostnetLoss:0.00063  DecoderLoss:0.00067  StopLoss:0.04022  GradNorm:0.00399  GradNormST:0.01109  AvgTextLen:5.9  AvgSpecLen:33.2  StepTime:0.29  LR:0.000100\n",
      "   | > Step:15/68  GlobalStep:128080  TotalLoss:0.00154  PostnetLoss:0.00075  DecoderLoss:0.00078  StopLoss:0.04530  GradNorm:0.00325  GradNormST:0.01515  AvgTextLen:12.8  AvgSpecLen:59.1  StepTime:0.52  LR:0.000100\n",
      "   | > Step:25/68  GlobalStep:128090  TotalLoss:0.00166  PostnetLoss:0.00081  DecoderLoss:0.00085  StopLoss:0.03459  GradNorm:0.00315  GradNormST:0.01079  AvgTextLen:19.8  AvgSpecLen:73.3  StepTime:0.36  LR:0.000100\n",
      "   | > Step:35/68  GlobalStep:128100  TotalLoss:0.00185  PostnetLoss:0.00090  DecoderLoss:0.00095  StopLoss:0.03882  GradNorm:0.00288  GradNormST:0.01599  AvgTextLen:27.1  AvgSpecLen:104.7  StepTime:0.65  LR:0.000100\n",
      "   | > Step:45/68  GlobalStep:128110  TotalLoss:0.00193  PostnetLoss:0.00094  DecoderLoss:0.00099  StopLoss:0.05474  GradNorm:0.00286  GradNormST:0.01524  AvgTextLen:34.4  AvgSpecLen:118.6  StepTime:0.57  LR:0.000100\n",
      "   | > Step:55/68  GlobalStep:128120  TotalLoss:0.00216  PostnetLoss:0.00105  DecoderLoss:0.00111  StopLoss:0.03828  GradNorm:0.00278  GradNormST:0.00792  AvgTextLen:43.9  AvgSpecLen:145.2  StepTime:0.67  LR:0.000100\n",
      "   | > Step:65/68  GlobalStep:128130  TotalLoss:0.00238  PostnetLoss:0.00116  DecoderLoss:0.00122  StopLoss:0.03844  GradNorm:0.00262  GradNormST:0.00847  AvgTextLen:64.2  AvgSpecLen:207.4  StepTime:1.21  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:128133  AvgTotalLoss:0.04397  AvgPostnetLoss:0.00089  AvgDecoderLoss:0.00093  AvgStopLoss:0.04216  EpochTime:42.14  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.07756   PostnetLoss: 0.00564   DecoderLoss:0.00592  StopLoss: 0.06600  \n",
      "   | > TotalLoss: 0.08721   PostnetLoss: 0.00981   DecoderLoss:0.01019  StopLoss: 0.06720  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00089   Validation Loss: 0.00895\n",
      "\n",
      " > Epoch 857/1000\n",
      "   | > Step:6/68  GlobalStep:128140  TotalLoss:0.00137  PostnetLoss:0.00067  DecoderLoss:0.00069  StopLoss:0.03283  GradNorm:0.00561  GradNormST:0.01691  AvgTextLen:6.0  AvgSpecLen:34.8  StepTime:0.38  LR:0.000100\n",
      "   | > Step:16/68  GlobalStep:128150  TotalLoss:0.00155  PostnetLoss:0.00076  DecoderLoss:0.00079  StopLoss:0.03987  GradNorm:0.00380  GradNormST:0.01492  AvgTextLen:13.8  AvgSpecLen:60.6  StepTime:0.36  LR:0.000100\n",
      "   | > Step:26/68  GlobalStep:128160  TotalLoss:0.00172  PostnetLoss:0.00084  DecoderLoss:0.00088  StopLoss:0.03929  GradNorm:0.00382  GradNormST:0.01252  AvgTextLen:20.3  AvgSpecLen:77.5  StepTime:0.38  LR:0.000100\n",
      "   | > Step:36/68  GlobalStep:128170  TotalLoss:0.00184  PostnetLoss:0.00090  DecoderLoss:0.00094  StopLoss:0.04584  GradNorm:0.00289  GradNormST:0.01316  AvgTextLen:28.5  AvgSpecLen:100.2  StepTime:0.69  LR:0.000100\n",
      "   | > Step:46/68  GlobalStep:128180  TotalLoss:0.00198  PostnetLoss:0.00096  DecoderLoss:0.00101  StopLoss:0.04102  GradNorm:0.00266  GradNormST:0.00983  AvgTextLen:35.4  AvgSpecLen:122.6  StepTime:0.79  LR:0.000100\n",
      "   | > Step:56/68  GlobalStep:128190  TotalLoss:0.00215  PostnetLoss:0.00105  DecoderLoss:0.00110  StopLoss:0.05130  GradNorm:0.00265  GradNormST:0.02463  AvgTextLen:45.8  AvgSpecLen:145.4  StepTime:0.97  LR:0.000100\n",
      "   | > Step:66/68  GlobalStep:128200  TotalLoss:0.00249  PostnetLoss:0.00121  DecoderLoss:0.00128  StopLoss:0.04289  GradNorm:0.00271  GradNormST:0.01651  AvgTextLen:71.2  AvgSpecLen:220.8  StepTime:1.07  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:128202  AvgTotalLoss:0.04470  AvgPostnetLoss:0.00089  AvgDecoderLoss:0.00093  AvgStopLoss:0.04288  EpochTime:42.27  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.07828   PostnetLoss: 0.00577   DecoderLoss:0.00604  StopLoss: 0.06647  \n",
      "   | > TotalLoss: 0.08114   PostnetLoss: 0.00963   DecoderLoss:0.00999  StopLoss: 0.06152  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00089   Validation Loss: 0.00885\n",
      "\n",
      " > Epoch 858/1000\n",
      "   | > Step:7/68  GlobalStep:128210  TotalLoss:0.00138  PostnetLoss:0.00068  DecoderLoss:0.00071  StopLoss:0.03852  GradNorm:0.00436  GradNormST:0.02451  AvgTextLen:7.4  AvgSpecLen:44.6  StepTime:0.40  LR:0.000100\n",
      "   | > Step:17/68  GlobalStep:128220  TotalLoss:0.00155  PostnetLoss:0.00076  DecoderLoss:0.00079  StopLoss:0.05223  GradNorm:0.00329  GradNormST:0.01233  AvgTextLen:14.7  AvgSpecLen:66.0  StepTime:0.42  LR:0.000100\n",
      "   | > Step:27/68  GlobalStep:128230  TotalLoss:0.00174  PostnetLoss:0.00085  DecoderLoss:0.00089  StopLoss:0.06484  GradNorm:0.00335  GradNormST:0.02059  AvgTextLen:21.0  AvgSpecLen:80.2  StepTime:0.64  LR:0.000100\n",
      "   | > Step:37/68  GlobalStep:128240  TotalLoss:0.00187  PostnetLoss:0.00092  DecoderLoss:0.00096  StopLoss:0.04157  GradNorm:0.00309  GradNormST:0.01084  AvgTextLen:29.3  AvgSpecLen:102.4  StepTime:0.62  LR:0.000100\n",
      "   | > Step:47/68  GlobalStep:128250  TotalLoss:0.00204  PostnetLoss:0.00099  DecoderLoss:0.00105  StopLoss:0.03824  GradNorm:0.00283  GradNormST:0.00953  AvgTextLen:36.7  AvgSpecLen:126.9  StepTime:0.71  LR:0.000100\n",
      "   | > Step:57/68  GlobalStep:128260  TotalLoss:0.00219  PostnetLoss:0.00107  DecoderLoss:0.00112  StopLoss:0.03042  GradNorm:0.00270  GradNormST:0.00632  AvgTextLen:47.1  AvgSpecLen:161.0  StepTime:0.84  LR:0.000100\n",
      "   | > Step:67/68  GlobalStep:128270  TotalLoss:0.00251  PostnetLoss:0.00122  DecoderLoss:0.00129  StopLoss:0.03692  GradNorm:0.00307  GradNormST:0.01553  AvgTextLen:80.6  AvgSpecLen:246.5  StepTime:1.22  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:128271  AvgTotalLoss:0.04604  AvgPostnetLoss:0.00088  AvgDecoderLoss:0.00093  AvgStopLoss:0.04424  EpochTime:42.38  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.07548   PostnetLoss: 0.00582   DecoderLoss:0.00610  StopLoss: 0.06357  \n",
      "   | > TotalLoss: 0.07693   PostnetLoss: 0.00907   DecoderLoss:0.00943  StopLoss: 0.05844  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00088   Validation Loss: 0.00838\n",
      "\n",
      " > Epoch 859/1000\n",
      "   | > Step:8/68  GlobalStep:128280  TotalLoss:0.00140  PostnetLoss:0.00069  DecoderLoss:0.00071  StopLoss:0.04873  GradNorm:0.00420  GradNormST:0.01305  AvgTextLen:8.3  AvgSpecLen:44.0  StepTime:0.24  LR:0.000100\n",
      "   | > Step:18/68  GlobalStep:128290  TotalLoss:0.00152  PostnetLoss:0.00074  DecoderLoss:0.00078  StopLoss:0.04412  GradNorm:0.00358  GradNormST:0.01311  AvgTextLen:15.2  AvgSpecLen:64.5  StepTime:0.44  LR:0.000100\n",
      "   | > Step:28/68  GlobalStep:128300  TotalLoss:0.00174  PostnetLoss:0.00085  DecoderLoss:0.00089  StopLoss:0.03903  GradNorm:0.00316  GradNormST:0.01094  AvgTextLen:22.1  AvgSpecLen:85.4  StepTime:0.56  LR:0.000100\n",
      "   | > Step:38/68  GlobalStep:128310  TotalLoss:0.00187  PostnetLoss:0.00091  DecoderLoss:0.00096  StopLoss:0.04082  GradNorm:0.00307  GradNormST:0.00925  AvgTextLen:29.4  AvgSpecLen:104.1  StepTime:0.65  LR:0.000100\n",
      "   | > Step:48/68  GlobalStep:128320  TotalLoss:0.00203  PostnetLoss:0.00099  DecoderLoss:0.00104  StopLoss:0.04473  GradNorm:0.00361  GradNormST:0.01057  AvgTextLen:37.5  AvgSpecLen:123.8  StepTime:0.72  LR:0.000100\n",
      "   | > Step:58/68  GlobalStep:128330  TotalLoss:0.00220  PostnetLoss:0.00107  DecoderLoss:0.00113  StopLoss:0.05792  GradNorm:0.00285  GradNormST:0.01862  AvgTextLen:47.8  AvgSpecLen:158.0  StepTime:0.90  LR:0.000100\n",
      "   | > Step:68/68  GlobalStep:128340  TotalLoss:0.00251  PostnetLoss:0.00122  DecoderLoss:0.00129  StopLoss:0.03891  GradNorm:0.00341  GradNormST:0.02621  AvgTextLen:103.0  AvgSpecLen:308.9  StepTime:1.87  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:128340  AvgTotalLoss:0.04698  AvgPostnetLoss:0.00088  AvgDecoderLoss:0.00093  AvgStopLoss:0.04517  EpochTime:43.04  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.07507   PostnetLoss: 0.00574   DecoderLoss:0.00601  StopLoss: 0.06332  \n",
      "   | > TotalLoss: 0.08557   PostnetLoss: 0.00955   DecoderLoss:0.00992  StopLoss: 0.06611  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00088   Validation Loss: 0.00869\n",
      "\n",
      " > Epoch 860/1000\n",
      "   | > Step:9/68  GlobalStep:128350  TotalLoss:0.00136  PostnetLoss:0.00067  DecoderLoss:0.00070  StopLoss:0.04846  GradNorm:0.00396  GradNormST:0.02483  AvgTextLen:9.0  AvgSpecLen:42.1  StepTime:0.41  LR:0.000100\n",
      "   | > Step:19/68  GlobalStep:128360  TotalLoss:0.00155  PostnetLoss:0.00076  DecoderLoss:0.00079  StopLoss:0.04935  GradNorm:0.00320  GradNormST:0.01866  AvgTextLen:15.5  AvgSpecLen:63.2  StepTime:0.43  LR:0.000100\n",
      "   | > Step:29/68  GlobalStep:128370  TotalLoss:0.00169  PostnetLoss:0.00082  DecoderLoss:0.00086  StopLoss:0.04661  GradNorm:0.00296  GradNormST:0.00887  AvgTextLen:21.8  AvgSpecLen:83.4  StepTime:0.57  LR:0.000100\n",
      "   | > Step:39/68  GlobalStep:128380  TotalLoss:0.00189  PostnetLoss:0.00092  DecoderLoss:0.00097  StopLoss:0.03395  GradNorm:0.00290  GradNormST:0.00994  AvgTextLen:30.2  AvgSpecLen:110.4  StepTime:0.53  LR:0.000100\n",
      "   | > Step:49/68  GlobalStep:128390  TotalLoss:0.00206  PostnetLoss:0.00101  DecoderLoss:0.00106  StopLoss:0.04272  GradNorm:0.00337  GradNormST:0.00974  AvgTextLen:38.2  AvgSpecLen:127.7  StepTime:0.73  LR:0.000100\n",
      "   | > Step:59/68  GlobalStep:128400  TotalLoss:0.00220  PostnetLoss:0.00107  DecoderLoss:0.00113  StopLoss:0.03830  GradNorm:0.00308  GradNormST:0.01320  AvgTextLen:50.5  AvgSpecLen:155.9  StepTime:0.89  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:128409  AvgTotalLoss:0.04570  AvgPostnetLoss:0.00089  AvgDecoderLoss:0.00093  AvgStopLoss:0.04388  EpochTime:43.92  AvgStepTime:0.64\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.07269   PostnetLoss: 0.00575   DecoderLoss:0.00603  StopLoss: 0.06091  \n",
      "   | > TotalLoss: 0.08269   PostnetLoss: 0.00952   DecoderLoss:0.00987  StopLoss: 0.06330  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00089   Validation Loss: 0.00870\n",
      "\n",
      " > Epoch 861/1000\n",
      "   | > Step:0/68  GlobalStep:128410  TotalLoss:0.00141  PostnetLoss:0.00068  DecoderLoss:0.00073  StopLoss:0.06855  GradNorm:0.00669  GradNormST:0.02648  AvgTextLen:3.0  AvgSpecLen:25.3  StepTime:0.59  LR:0.000100\n",
      "   | > Step:10/68  GlobalStep:128420  TotalLoss:0.00142  PostnetLoss:0.00070  DecoderLoss:0.00073  StopLoss:0.04592  GradNorm:0.00409  GradNormST:0.02645  AvgTextLen:9.8  AvgSpecLen:48.6  StepTime:0.52  LR:0.000100\n",
      "   | > Step:20/68  GlobalStep:128430  TotalLoss:0.00160  PostnetLoss:0.00079  DecoderLoss:0.00082  StopLoss:0.03947  GradNorm:0.00332  GradNormST:0.01326  AvgTextLen:16.4  AvgSpecLen:64.8  StepTime:0.52  LR:0.000100\n",
      "   | > Step:30/68  GlobalStep:128440  TotalLoss:0.00175  PostnetLoss:0.00086  DecoderLoss:0.00089  StopLoss:0.03789  GradNorm:0.00317  GradNormST:0.00772  AvgTextLen:22.9  AvgSpecLen:85.8  StepTime:0.58  LR:0.000100\n",
      "   | > Step:40/68  GlobalStep:128450  TotalLoss:0.00189  PostnetLoss:0.00092  DecoderLoss:0.00097  StopLoss:0.05269  GradNorm:0.00341  GradNormST:0.01459  AvgTextLen:30.9  AvgSpecLen:105.3  StepTime:0.65  LR:0.000100\n",
      "   | > Step:50/68  GlobalStep:128460  TotalLoss:0.00205  PostnetLoss:0.00100  DecoderLoss:0.00105  StopLoss:0.02784  GradNorm:0.00327  GradNormST:0.00658  AvgTextLen:38.8  AvgSpecLen:132.0  StepTime:0.80  LR:0.000100\n",
      "   | > Step:60/68  GlobalStep:128470  TotalLoss:0.00223  PostnetLoss:0.00109  DecoderLoss:0.00114  StopLoss:0.03589  GradNorm:0.00275  GradNormST:0.00921  AvgTextLen:52.0  AvgSpecLen:172.7  StepTime:0.90  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:128478  AvgTotalLoss:0.04531  AvgPostnetLoss:0.00088  AvgDecoderLoss:0.00093  AvgStopLoss:0.04350  EpochTime:42.29  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.07125   PostnetLoss: 0.00585   DecoderLoss:0.00611  StopLoss: 0.05928  \n",
      "   | > TotalLoss: 0.07464   PostnetLoss: 0.00893   DecoderLoss:0.00929  StopLoss: 0.05643  \n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00088   Validation Loss: 0.00830\n",
      "\n",
      " > Epoch 862/1000\n",
      "   | > Step:1/68  GlobalStep:128480  TotalLoss:0.00135  PostnetLoss:0.00065  DecoderLoss:0.00069  StopLoss:0.05138  GradNorm:0.00645  GradNormST:0.01850  AvgTextLen:3.7  AvgSpecLen:28.6  StepTime:0.39  LR:0.000100\n",
      "   | > Step:11/68  GlobalStep:128490  TotalLoss:0.00142  PostnetLoss:0.00070  DecoderLoss:0.00072  StopLoss:0.04497  GradNorm:0.00493  GradNormST:0.02754  AvgTextLen:10.3  AvgSpecLen:49.4  StepTime:0.39  LR:0.000100\n",
      "   | > Step:21/68  GlobalStep:128500  TotalLoss:0.00160  PostnetLoss:0.00078  DecoderLoss:0.00082  StopLoss:0.04312  GradNorm:0.00367  GradNormST:0.01684  AvgTextLen:16.6  AvgSpecLen:66.3  StepTime:0.41  LR:0.000100\n",
      "   | > Step:31/68  GlobalStep:128510  TotalLoss:0.00174  PostnetLoss:0.00085  DecoderLoss:0.00089  StopLoss:0.04407  GradNorm:0.00298  GradNormST:0.01379  AvgTextLen:24.1  AvgSpecLen:82.7  StepTime:0.52  LR:0.000100\n",
      "   | > Step:41/68  GlobalStep:128520  TotalLoss:0.00196  PostnetLoss:0.00096  DecoderLoss:0.00100  StopLoss:0.04157  GradNorm:0.00299  GradNormST:0.01394  AvgTextLen:32.3  AvgSpecLen:109.4  StepTime:0.62  LR:0.000100\n",
      "   | > Step:51/68  GlobalStep:128530  TotalLoss:0.00204  PostnetLoss:0.00100  DecoderLoss:0.00105  StopLoss:0.04219  GradNorm:0.00387  GradNormST:0.01028  AvgTextLen:39.2  AvgSpecLen:126.9  StepTime:0.61  LR:0.000100\n",
      "   | > Step:61/68  GlobalStep:128540  TotalLoss:0.00224  PostnetLoss:0.00109  DecoderLoss:0.00115  StopLoss:0.03452  GradNorm:0.00339  GradNormST:0.00959  AvgTextLen:54.6  AvgSpecLen:165.5  StepTime:0.90  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:128547  AvgTotalLoss:0.04509  AvgPostnetLoss:0.00089  AvgDecoderLoss:0.00093  AvgStopLoss:0.04327  EpochTime:42.22  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.07367   PostnetLoss: 0.00582   DecoderLoss:0.00608  StopLoss: 0.06177  \n",
      "   | > TotalLoss: 0.07787   PostnetLoss: 0.00911   DecoderLoss:0.00946  StopLoss: 0.05930  \n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00089   Validation Loss: 0.00850\n",
      "\n",
      " > Epoch 863/1000\n",
      "   | > Step:2/68  GlobalStep:128550  TotalLoss:0.00122  PostnetLoss:0.00060  DecoderLoss:0.00063  StopLoss:0.03929  GradNorm:0.00413  GradNormST:0.01506  AvgTextLen:4.0  AvgSpecLen:31.3  StepTime:0.35  LR:0.000100\n",
      "   | > Step:12/68  GlobalStep:128560  TotalLoss:0.00138  PostnetLoss:0.00068  DecoderLoss:0.00071  StopLoss:0.06913  GradNorm:0.00340  GradNormST:0.02598  AvgTextLen:11.0  AvgSpecLen:49.3  StepTime:0.44  LR:0.000100\n",
      "   | > Step:22/68  GlobalStep:128570  TotalLoss:0.00164  PostnetLoss:0.00080  DecoderLoss:0.00084  StopLoss:0.04632  GradNorm:0.00387  GradNormST:0.01375  AvgTextLen:18.0  AvgSpecLen:72.0  StepTime:0.43  LR:0.000100\n",
      "   | > Step:32/68  GlobalStep:128580  TotalLoss:0.00173  PostnetLoss:0.00085  DecoderLoss:0.00089  StopLoss:0.05701  GradNorm:0.00315  GradNormST:0.01574  AvgTextLen:24.4  AvgSpecLen:91.5  StepTime:0.55  LR:0.000100\n",
      "   | > Step:42/68  GlobalStep:128590  TotalLoss:0.00192  PostnetLoss:0.00093  DecoderLoss:0.00098  StopLoss:0.03988  GradNorm:0.00278  GradNormST:0.01843  AvgTextLen:33.1  AvgSpecLen:109.7  StepTime:0.58  LR:0.000100\n",
      "   | > Step:52/68  GlobalStep:128600  TotalLoss:0.00212  PostnetLoss:0.00104  DecoderLoss:0.00109  StopLoss:0.06766  GradNorm:0.00381  GradNormST:0.01596  AvgTextLen:42.1  AvgSpecLen:137.5  StepTime:0.67  LR:0.000100\n",
      "   | > Step:62/68  GlobalStep:128610  TotalLoss:0.00228  PostnetLoss:0.00111  DecoderLoss:0.00117  StopLoss:0.03306  GradNorm:0.00316  GradNormST:0.01037  AvgTextLen:54.0  AvgSpecLen:174.7  StepTime:0.76  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:128616  AvgTotalLoss:0.04280  AvgPostnetLoss:0.00088  AvgDecoderLoss:0.00093  AvgStopLoss:0.04099  EpochTime:42.56  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.06947   PostnetLoss: 0.00561   DecoderLoss:0.00587  StopLoss: 0.05799  \n",
      "   | > TotalLoss: 0.07900   PostnetLoss: 0.00911   DecoderLoss:0.00947  StopLoss: 0.06042  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00088   Validation Loss: 0.00845\n",
      "\n",
      " > Epoch 864/1000\n",
      "   | > Step:3/68  GlobalStep:128620  TotalLoss:0.00124  PostnetLoss:0.00061  DecoderLoss:0.00064  StopLoss:0.05560  GradNorm:0.00428  GradNormST:0.01728  AvgTextLen:4.7  AvgSpecLen:29.7  StepTime:0.31  LR:0.000100\n",
      "   | > Step:13/68  GlobalStep:128630  TotalLoss:0.00140  PostnetLoss:0.00069  DecoderLoss:0.00072  StopLoss:0.05117  GradNorm:0.00319  GradNormST:0.01835  AvgTextLen:11.5  AvgSpecLen:55.2  StepTime:0.33  LR:0.000100\n",
      "   | > Step:23/68  GlobalStep:128640  TotalLoss:0.00169  PostnetLoss:0.00083  DecoderLoss:0.00086  StopLoss:0.04010  GradNorm:0.00359  GradNormST:0.01342  AvgTextLen:18.3  AvgSpecLen:72.2  StepTime:0.45  LR:0.000100\n",
      "   | > Step:33/68  GlobalStep:128650  TotalLoss:0.00181  PostnetLoss:0.00088  DecoderLoss:0.00093  StopLoss:0.06990  GradNorm:0.00306  GradNormST:0.02201  AvgTextLen:25.9  AvgSpecLen:88.9  StepTime:0.47  LR:0.000100\n",
      "   | > Step:43/68  GlobalStep:128660  TotalLoss:0.00193  PostnetLoss:0.00094  DecoderLoss:0.00099  StopLoss:0.03888  GradNorm:0.00295  GradNormST:0.01168  AvgTextLen:33.4  AvgSpecLen:115.7  StepTime:0.51  LR:0.000100\n",
      "   | > Step:53/68  GlobalStep:128670  TotalLoss:0.00204  PostnetLoss:0.00099  DecoderLoss:0.00104  StopLoss:0.02865  GradNorm:0.00296  GradNormST:0.00708  AvgTextLen:42.4  AvgSpecLen:137.6  StepTime:0.66  LR:0.000100\n",
      "   | > Step:63/68  GlobalStep:128680  TotalLoss:0.00234  PostnetLoss:0.00114  DecoderLoss:0.00120  StopLoss:0.03696  GradNorm:0.00306  GradNormST:0.01227  AvgTextLen:58.6  AvgSpecLen:185.1  StepTime:0.90  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:128685  AvgTotalLoss:0.04405  AvgPostnetLoss:0.00088  AvgDecoderLoss:0.00093  AvgStopLoss:0.04223  EpochTime:42.67  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.07319   PostnetLoss: 0.00563   DecoderLoss:0.00590  StopLoss: 0.06166  \n",
      "   | > TotalLoss: 0.07427   PostnetLoss: 0.00900   DecoderLoss:0.00934  StopLoss: 0.05594  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00088   Validation Loss: 0.00840\n",
      "\n",
      " > Epoch 865/1000\n",
      "   | > Step:4/68  GlobalStep:128690  TotalLoss:0.00126  PostnetLoss:0.00062  DecoderLoss:0.00064  StopLoss:0.05808  GradNorm:0.00418  GradNormST:0.01925  AvgTextLen:4.8  AvgSpecLen:31.1  StepTime:0.26  LR:0.000100\n",
      "   | > Step:14/68  GlobalStep:128700  TotalLoss:0.00145  PostnetLoss:0.00071  DecoderLoss:0.00074  StopLoss:0.03731  GradNorm:0.00338  GradNormST:0.01326  AvgTextLen:12.2  AvgSpecLen:52.0  StepTime:0.33  LR:0.000100\n",
      "   | > Step:24/68  GlobalStep:128710  TotalLoss:0.00173  PostnetLoss:0.00085  DecoderLoss:0.00088  StopLoss:0.03906  GradNorm:0.00373  GradNormST:0.01664  AvgTextLen:19.2  AvgSpecLen:79.0  StepTime:0.50  LR:0.000100\n",
      "   | > Step:34/68  GlobalStep:128720  TotalLoss:0.00185  PostnetLoss:0.00090  DecoderLoss:0.00095  StopLoss:0.04289  GradNorm:0.00317  GradNormST:0.01380  AvgTextLen:26.2  AvgSpecLen:95.9  StepTime:0.58  LR:0.000100\n",
      "   | > Step:44/68  GlobalStep:128730  TotalLoss:0.00196  PostnetLoss:0.00096  DecoderLoss:0.00100  StopLoss:0.03668  GradNorm:0.00288  GradNormST:0.01048  AvgTextLen:33.8  AvgSpecLen:116.3  StepTime:0.64  LR:0.000100\n",
      "   | > Step:54/68  GlobalStep:128740  TotalLoss:0.00210  PostnetLoss:0.00102  DecoderLoss:0.00108  StopLoss:0.03962  GradNorm:0.00309  GradNormST:0.01047  AvgTextLen:42.2  AvgSpecLen:144.9  StepTime:0.86  LR:0.000100\n",
      "   | > Step:64/68  GlobalStep:128750  TotalLoss:0.00220  PostnetLoss:0.00107  DecoderLoss:0.00113  StopLoss:0.04675  GradNorm:0.00274  GradNormST:0.01503  AvgTextLen:60.6  AvgSpecLen:177.6  StepTime:0.81  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:128754  AvgTotalLoss:0.04585  AvgPostnetLoss:0.00088  AvgDecoderLoss:0.00093  AvgStopLoss:0.04404  EpochTime:41.66  AvgStepTime:0.60\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.07512   PostnetLoss: 0.00578   DecoderLoss:0.00604  StopLoss: 0.06330  \n",
      "   | > TotalLoss: 0.07703   PostnetLoss: 0.00908   DecoderLoss:0.00944  StopLoss: 0.05850  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00088   Validation Loss: 0.00837\n",
      "\n",
      " > Epoch 866/1000\n",
      "   | > Step:5/68  GlobalStep:128760  TotalLoss:0.00129  PostnetLoss:0.00062  DecoderLoss:0.00066  StopLoss:0.05385  GradNorm:0.00362  GradNormST:0.02203  AvgTextLen:5.9  AvgSpecLen:33.2  StepTime:0.34  LR:0.000100\n",
      "   | > Step:15/68  GlobalStep:128770  TotalLoss:0.00154  PostnetLoss:0.00075  DecoderLoss:0.00078  StopLoss:0.05343  GradNorm:0.00420  GradNormST:0.01866  AvgTextLen:12.8  AvgSpecLen:59.1  StepTime:0.52  LR:0.000100\n",
      "   | > Step:25/68  GlobalStep:128780  TotalLoss:0.00166  PostnetLoss:0.00081  DecoderLoss:0.00085  StopLoss:0.03965  GradNorm:0.00385  GradNormST:0.01504  AvgTextLen:19.8  AvgSpecLen:73.3  StepTime:0.48  LR:0.000100\n",
      "   | > Step:35/68  GlobalStep:128790  TotalLoss:0.00184  PostnetLoss:0.00090  DecoderLoss:0.00094  StopLoss:0.05450  GradNorm:0.00322  GradNormST:0.02104  AvgTextLen:27.1  AvgSpecLen:104.7  StepTime:0.75  LR:0.000100\n",
      "   | > Step:45/68  GlobalStep:128800  TotalLoss:0.00191  PostnetLoss:0.00093  DecoderLoss:0.00098  StopLoss:0.04482  GradNorm:0.00269  GradNormST:0.01512  AvgTextLen:34.4  AvgSpecLen:118.6  StepTime:0.65  LR:0.000100\n",
      "   | > Step:55/68  GlobalStep:128810  TotalLoss:0.00215  PostnetLoss:0.00105  DecoderLoss:0.00110  StopLoss:0.03243  GradNorm:0.00304  GradNormST:0.00852  AvgTextLen:43.9  AvgSpecLen:145.2  StepTime:0.85  LR:0.000100\n",
      "   | > Step:65/68  GlobalStep:128820  TotalLoss:0.00236  PostnetLoss:0.00115  DecoderLoss:0.00121  StopLoss:0.04071  GradNorm:0.00265  GradNormST:0.01244  AvgTextLen:64.2  AvgSpecLen:207.4  StepTime:1.17  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:128823  AvgTotalLoss:0.04389  AvgPostnetLoss:0.00088  AvgDecoderLoss:0.00093  AvgStopLoss:0.04208  EpochTime:42.92  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.07709   PostnetLoss: 0.00573   DecoderLoss:0.00600  StopLoss: 0.06536  \n",
      "   | > TotalLoss: 0.08387   PostnetLoss: 0.00959   DecoderLoss:0.00994  StopLoss: 0.06434  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00088   Validation Loss: 0.00877\n",
      "\n",
      " > Epoch 867/1000\n",
      "   | > Step:6/68  GlobalStep:128830  TotalLoss:0.00130  PostnetLoss:0.00063  DecoderLoss:0.00066  StopLoss:0.05047  GradNorm:0.00397  GradNormST:0.02282  AvgTextLen:6.0  AvgSpecLen:34.8  StepTime:0.37  LR:0.000100\n",
      "   | > Step:16/68  GlobalStep:128840  TotalLoss:0.00153  PostnetLoss:0.00075  DecoderLoss:0.00078  StopLoss:0.03505  GradNorm:0.00326  GradNormST:0.01219  AvgTextLen:13.8  AvgSpecLen:60.6  StepTime:0.32  LR:0.000100\n",
      "   | > Step:26/68  GlobalStep:128850  TotalLoss:0.00169  PostnetLoss:0.00083  DecoderLoss:0.00086  StopLoss:0.03770  GradNorm:0.00356  GradNormST:0.01476  AvgTextLen:20.3  AvgSpecLen:77.5  StepTime:0.38  LR:0.000100\n",
      "   | > Step:36/68  GlobalStep:128860  TotalLoss:0.00183  PostnetLoss:0.00089  DecoderLoss:0.00093  StopLoss:0.03594  GradNorm:0.00341  GradNormST:0.00879  AvgTextLen:28.5  AvgSpecLen:100.2  StepTime:0.56  LR:0.000100\n",
      "   | > Step:46/68  GlobalStep:128870  TotalLoss:0.00200  PostnetLoss:0.00098  DecoderLoss:0.00102  StopLoss:0.03252  GradNorm:0.00334  GradNormST:0.01106  AvgTextLen:35.4  AvgSpecLen:122.6  StepTime:0.79  LR:0.000100\n",
      "   | > Step:56/68  GlobalStep:128880  TotalLoss:0.00215  PostnetLoss:0.00105  DecoderLoss:0.00110  StopLoss:0.03173  GradNorm:0.00294  GradNormST:0.01203  AvgTextLen:45.8  AvgSpecLen:145.4  StepTime:1.05  LR:0.000100\n",
      "   | > Step:66/68  GlobalStep:128890  TotalLoss:0.00250  PostnetLoss:0.00121  DecoderLoss:0.00128  StopLoss:0.04546  GradNorm:0.00309  GradNormST:0.01775  AvgTextLen:71.2  AvgSpecLen:220.8  StepTime:1.10  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:128892  AvgTotalLoss:0.04380  AvgPostnetLoss:0.00088  AvgDecoderLoss:0.00093  AvgStopLoss:0.04199  EpochTime:42.29  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.07287   PostnetLoss: 0.00571   DecoderLoss:0.00598  StopLoss: 0.06118  \n",
      "   | > TotalLoss: 0.09049   PostnetLoss: 0.00983   DecoderLoss:0.01018  StopLoss: 0.07049  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00088   Validation Loss: 0.00890\n",
      "\n",
      " > Epoch 868/1000\n",
      "   | > Step:7/68  GlobalStep:128900  TotalLoss:0.00137  PostnetLoss:0.00067  DecoderLoss:0.00070  StopLoss:0.04874  GradNorm:0.00367  GradNormST:0.02694  AvgTextLen:7.4  AvgSpecLen:44.6  StepTime:0.40  LR:0.000100\n",
      "   | > Step:17/68  GlobalStep:128910  TotalLoss:0.00153  PostnetLoss:0.00075  DecoderLoss:0.00078  StopLoss:0.05998  GradNorm:0.00325  GradNormST:0.01300  AvgTextLen:14.7  AvgSpecLen:66.0  StepTime:0.44  LR:0.000100\n",
      "   | > Step:27/68  GlobalStep:128920  TotalLoss:0.00174  PostnetLoss:0.00086  DecoderLoss:0.00089  StopLoss:0.05773  GradNorm:0.00378  GradNormST:0.01955  AvgTextLen:21.0  AvgSpecLen:80.2  StepTime:0.56  LR:0.000100\n",
      "   | > Step:37/68  GlobalStep:128930  TotalLoss:0.00189  PostnetLoss:0.00092  DecoderLoss:0.00096  StopLoss:0.04265  GradNorm:0.00316  GradNormST:0.01190  AvgTextLen:29.3  AvgSpecLen:102.4  StepTime:0.59  LR:0.000100\n",
      "   | > Step:47/68  GlobalStep:128940  TotalLoss:0.00204  PostnetLoss:0.00099  DecoderLoss:0.00104  StopLoss:0.02868  GradNorm:0.00327  GradNormST:0.00899  AvgTextLen:36.7  AvgSpecLen:126.9  StepTime:0.75  LR:0.000100\n",
      "   | > Step:57/68  GlobalStep:128950  TotalLoss:0.00219  PostnetLoss:0.00107  DecoderLoss:0.00112  StopLoss:0.02544  GradNorm:0.00288  GradNormST:0.00617  AvgTextLen:47.1  AvgSpecLen:161.0  StepTime:0.84  LR:0.000100\n",
      "   | > Step:67/68  GlobalStep:128960  TotalLoss:0.00252  PostnetLoss:0.00122  DecoderLoss:0.00130  StopLoss:0.03535  GradNorm:0.00294  GradNormST:0.01341  AvgTextLen:80.6  AvgSpecLen:246.5  StepTime:1.21  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:128961  AvgTotalLoss:0.04306  AvgPostnetLoss:0.00088  AvgDecoderLoss:0.00093  AvgStopLoss:0.04125  EpochTime:43.25  AvgStepTime:0.63\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.07376   PostnetLoss: 0.00587   DecoderLoss:0.00614  StopLoss: 0.06176  \n",
      "   | > TotalLoss: 0.07783   PostnetLoss: 0.00909   DecoderLoss:0.00944  StopLoss: 0.05930  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00088   Validation Loss: 0.00842\n",
      "\n",
      " > Epoch 869/1000\n",
      "   | > Step:8/68  GlobalStep:128970  TotalLoss:0.00142  PostnetLoss:0.00070  DecoderLoss:0.00072  StopLoss:0.04078  GradNorm:0.00509  GradNormST:0.01262  AvgTextLen:8.3  AvgSpecLen:44.0  StepTime:0.30  LR:0.000100\n",
      "   | > Step:18/68  GlobalStep:128980  TotalLoss:0.00152  PostnetLoss:0.00074  DecoderLoss:0.00078  StopLoss:0.04183  GradNorm:0.00338  GradNormST:0.01048  AvgTextLen:15.2  AvgSpecLen:64.5  StepTime:0.41  LR:0.000100\n",
      "   | > Step:28/68  GlobalStep:128990  TotalLoss:0.00175  PostnetLoss:0.00086  DecoderLoss:0.00089  StopLoss:0.03706  GradNorm:0.00342  GradNormST:0.01139  AvgTextLen:22.1  AvgSpecLen:85.4  StepTime:0.53  LR:0.000100\n",
      "   | > Step:38/68  GlobalStep:129000  TotalLoss:0.00186  PostnetLoss:0.00091  DecoderLoss:0.00095  StopLoss:0.03714  GradNorm:0.00323  GradNormST:0.00850  AvgTextLen:29.4  AvgSpecLen:104.1  StepTime:0.65  LR:0.000100\n",
      " | | > Checkpoint saving : /vm/TTS/models/cartman_models/mozilla-no-loc-fattn-stopnet-sigmoid-loss_masking-August-15-2019_11+26AM-de6aeb9/checkpoint_129000.pth.tar\n",
      "warning: audio amplitude out of range, auto clipped.\n",
      "   | > Step:48/68  GlobalStep:129010  TotalLoss:0.00203  PostnetLoss:0.00099  DecoderLoss:0.00104  StopLoss:0.04573  GradNorm:0.00334  GradNormST:0.01089  AvgTextLen:37.5  AvgSpecLen:123.8  StepTime:0.71  LR:0.000100\n",
      "   | > Step:58/68  GlobalStep:129020  TotalLoss:0.00217  PostnetLoss:0.00106  DecoderLoss:0.00111  StopLoss:0.03334  GradNorm:0.00301  GradNormST:0.00862  AvgTextLen:47.8  AvgSpecLen:158.0  StepTime:0.93  LR:0.000100\n",
      "   | > Step:68/68  GlobalStep:129030  TotalLoss:0.00249  PostnetLoss:0.00121  DecoderLoss:0.00128  StopLoss:0.03009  GradNorm:0.00291  GradNormST:0.01460  AvgTextLen:103.0  AvgSpecLen:308.9  StepTime:1.86  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:129030  AvgTotalLoss:0.04291  AvgPostnetLoss:0.00089  AvgDecoderLoss:0.00093  AvgStopLoss:0.04109  EpochTime:42.44  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.07622   PostnetLoss: 0.00565   DecoderLoss:0.00592  StopLoss: 0.06465  \n",
      "   | > TotalLoss: 0.08570   PostnetLoss: 0.00959   DecoderLoss:0.00994  StopLoss: 0.06617  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00089   Validation Loss: 0.00873\n",
      "\n",
      " > Epoch 870/1000\n",
      "   | > Step:9/68  GlobalStep:129040  TotalLoss:0.00138  PostnetLoss:0.00068  DecoderLoss:0.00070  StopLoss:0.03766  GradNorm:0.00412  GradNormST:0.01379  AvgTextLen:9.0  AvgSpecLen:42.1  StepTime:0.43  LR:0.000100\n",
      "   | > Step:19/68  GlobalStep:129050  TotalLoss:0.00155  PostnetLoss:0.00076  DecoderLoss:0.00079  StopLoss:0.05711  GradNorm:0.00373  GradNormST:0.01625  AvgTextLen:15.5  AvgSpecLen:63.2  StepTime:0.32  LR:0.000100\n",
      "   | > Step:29/68  GlobalStep:129060  TotalLoss:0.00172  PostnetLoss:0.00084  DecoderLoss:0.00088  StopLoss:0.04368  GradNorm:0.00370  GradNormST:0.01291  AvgTextLen:21.8  AvgSpecLen:83.4  StepTime:0.54  LR:0.000100\n",
      "   | > Step:39/68  GlobalStep:129070  TotalLoss:0.00192  PostnetLoss:0.00094  DecoderLoss:0.00099  StopLoss:0.04055  GradNorm:0.00363  GradNormST:0.01498  AvgTextLen:30.2  AvgSpecLen:110.4  StepTime:0.64  LR:0.000100\n",
      "   | > Step:49/68  GlobalStep:129080  TotalLoss:0.00204  PostnetLoss:0.00099  DecoderLoss:0.00104  StopLoss:0.03860  GradNorm:0.00342  GradNormST:0.01033  AvgTextLen:38.2  AvgSpecLen:127.7  StepTime:0.72  LR:0.000100\n",
      "   | > Step:59/68  GlobalStep:129090  TotalLoss:0.00220  PostnetLoss:0.00107  DecoderLoss:0.00113  StopLoss:0.03300  GradNorm:0.00310  GradNormST:0.01029  AvgTextLen:50.5  AvgSpecLen:155.9  StepTime:0.90  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:129099  AvgTotalLoss:0.04446  AvgPostnetLoss:0.00089  AvgDecoderLoss:0.00093  AvgStopLoss:0.04263  EpochTime:42.63  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.07394   PostnetLoss: 0.00562   DecoderLoss:0.00588  StopLoss: 0.06243  \n",
      "   | > TotalLoss: 0.08489   PostnetLoss: 0.00938   DecoderLoss:0.00973  StopLoss: 0.06578  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00089   Validation Loss: 0.00859\n",
      "\n",
      " > Epoch 871/1000\n",
      "   | > Step:0/68  GlobalStep:129100  TotalLoss:0.00134  PostnetLoss:0.00064  DecoderLoss:0.00069  StopLoss:0.05796  GradNorm:0.00604  GradNormST:0.02165  AvgTextLen:3.0  AvgSpecLen:25.3  StepTime:0.53  LR:0.000100\n",
      "   | > Step:10/68  GlobalStep:129110  TotalLoss:0.00140  PostnetLoss:0.00068  DecoderLoss:0.00072  StopLoss:0.03598  GradNorm:0.00355  GradNormST:0.01452  AvgTextLen:9.8  AvgSpecLen:48.6  StepTime:0.52  LR:0.000100\n",
      "   | > Step:20/68  GlobalStep:129120  TotalLoss:0.00162  PostnetLoss:0.00080  DecoderLoss:0.00083  StopLoss:0.03374  GradNorm:0.00412  GradNormST:0.01504  AvgTextLen:16.4  AvgSpecLen:64.8  StepTime:0.51  LR:0.000100\n",
      "   | > Step:30/68  GlobalStep:129130  TotalLoss:0.00178  PostnetLoss:0.00087  DecoderLoss:0.00091  StopLoss:0.03562  GradNorm:0.00441  GradNormST:0.01225  AvgTextLen:22.9  AvgSpecLen:85.8  StepTime:0.61  LR:0.000100\n",
      "   | > Step:40/68  GlobalStep:129140  TotalLoss:0.00191  PostnetLoss:0.00093  DecoderLoss:0.00098  StopLoss:0.05437  GradNorm:0.00431  GradNormST:0.01611  AvgTextLen:30.9  AvgSpecLen:105.3  StepTime:0.54  LR:0.000100\n",
      "   | > Step:50/68  GlobalStep:129150  TotalLoss:0.00207  PostnetLoss:0.00101  DecoderLoss:0.00106  StopLoss:0.03855  GradNorm:0.00403  GradNormST:0.00909  AvgTextLen:38.8  AvgSpecLen:132.0  StepTime:0.68  LR:0.000100\n",
      "   | > Step:60/68  GlobalStep:129160  TotalLoss:0.00224  PostnetLoss:0.00109  DecoderLoss:0.00115  StopLoss:0.03042  GradNorm:0.00347  GradNormST:0.00764  AvgTextLen:52.0  AvgSpecLen:172.7  StepTime:0.91  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:129168  AvgTotalLoss:0.04404  AvgPostnetLoss:0.00089  AvgDecoderLoss:0.00093  AvgStopLoss:0.04223  EpochTime:41.95  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.07783   PostnetLoss: 0.00568   DecoderLoss:0.00595  StopLoss: 0.06620  \n",
      "   | > TotalLoss: 0.08394   PostnetLoss: 0.00933   DecoderLoss:0.00968  StopLoss: 0.06493  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00089   Validation Loss: 0.00858\n",
      "\n",
      " > Epoch 872/1000\n",
      "   | > Step:1/68  GlobalStep:129170  TotalLoss:0.00132  PostnetLoss:0.00065  DecoderLoss:0.00068  StopLoss:0.03410  GradNorm:0.00571  GradNormST:0.01338  AvgTextLen:3.7  AvgSpecLen:28.6  StepTime:0.26  LR:0.000100\n",
      "   | > Step:11/68  GlobalStep:129180  TotalLoss:0.00138  PostnetLoss:0.00067  DecoderLoss:0.00070  StopLoss:0.05456  GradNorm:0.00345  GradNormST:0.02550  AvgTextLen:10.3  AvgSpecLen:49.4  StepTime:0.30  LR:0.000100\n",
      "   | > Step:21/68  GlobalStep:129190  TotalLoss:0.00158  PostnetLoss:0.00077  DecoderLoss:0.00081  StopLoss:0.04508  GradNorm:0.00361  GradNormST:0.01430  AvgTextLen:16.6  AvgSpecLen:66.3  StepTime:0.42  LR:0.000100\n",
      "   | > Step:31/68  GlobalStep:129200  TotalLoss:0.00175  PostnetLoss:0.00086  DecoderLoss:0.00089  StopLoss:0.04201  GradNorm:0.00397  GradNormST:0.01307  AvgTextLen:24.1  AvgSpecLen:82.7  StepTime:0.53  LR:0.000100\n",
      "   | > Step:41/68  GlobalStep:129210  TotalLoss:0.00196  PostnetLoss:0.00096  DecoderLoss:0.00100  StopLoss:0.02944  GradNorm:0.00392  GradNormST:0.01166  AvgTextLen:32.3  AvgSpecLen:109.4  StepTime:0.69  LR:0.000100\n",
      "   | > Step:51/68  GlobalStep:129220  TotalLoss:0.00204  PostnetLoss:0.00100  DecoderLoss:0.00105  StopLoss:0.03557  GradNorm:0.00406  GradNormST:0.00713  AvgTextLen:39.2  AvgSpecLen:126.9  StepTime:0.71  LR:0.000100\n",
      "   | > Step:61/68  GlobalStep:129230  TotalLoss:0.00224  PostnetLoss:0.00109  DecoderLoss:0.00115  StopLoss:0.03789  GradNorm:0.00370  GradNormST:0.00917  AvgTextLen:54.6  AvgSpecLen:165.5  StepTime:0.90  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:129237  AvgTotalLoss:0.04484  AvgPostnetLoss:0.00089  AvgDecoderLoss:0.00093  AvgStopLoss:0.04302  EpochTime:42.48  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.07216   PostnetLoss: 0.00554   DecoderLoss:0.00580  StopLoss: 0.06082  \n",
      "   | > TotalLoss: 0.08187   PostnetLoss: 0.00920   DecoderLoss:0.00955  StopLoss: 0.06312  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00089   Validation Loss: 0.00850\n",
      "\n",
      " > Epoch 873/1000\n",
      "   | > Step:2/68  GlobalStep:129240  TotalLoss:0.00126  PostnetLoss:0.00061  DecoderLoss:0.00064  StopLoss:0.04437  GradNorm:0.00483  GradNormST:0.01898  AvgTextLen:4.0  AvgSpecLen:31.3  StepTime:0.42  LR:0.000100\n",
      "   | > Step:12/68  GlobalStep:129250  TotalLoss:0.00141  PostnetLoss:0.00069  DecoderLoss:0.00072  StopLoss:0.04339  GradNorm:0.00324  GradNormST:0.01278  AvgTextLen:11.0  AvgSpecLen:49.3  StepTime:0.42  LR:0.000100\n",
      "   | > Step:22/68  GlobalStep:129260  TotalLoss:0.00162  PostnetLoss:0.00079  DecoderLoss:0.00083  StopLoss:0.05518  GradNorm:0.00376  GradNormST:0.01473  AvgTextLen:18.0  AvgSpecLen:72.0  StepTime:0.46  LR:0.000100\n",
      "   | > Step:32/68  GlobalStep:129270  TotalLoss:0.00177  PostnetLoss:0.00087  DecoderLoss:0.00090  StopLoss:0.05315  GradNorm:0.00430  GradNormST:0.02341  AvgTextLen:24.4  AvgSpecLen:91.5  StepTime:0.56  LR:0.000100\n",
      "   | > Step:42/68  GlobalStep:129280  TotalLoss:0.00195  PostnetLoss:0.00095  DecoderLoss:0.00099  StopLoss:0.04211  GradNorm:0.00443  GradNormST:0.01226  AvgTextLen:33.1  AvgSpecLen:109.7  StepTime:0.60  LR:0.000100\n",
      "   | > Step:52/68  GlobalStep:129290  TotalLoss:0.00218  PostnetLoss:0.00106  DecoderLoss:0.00111  StopLoss:0.05072  GradNorm:0.00535  GradNormST:0.01250  AvgTextLen:42.1  AvgSpecLen:137.5  StepTime:0.67  LR:0.000100\n",
      "   | > Step:62/68  GlobalStep:129300  TotalLoss:0.00230  PostnetLoss:0.00112  DecoderLoss:0.00118  StopLoss:0.03770  GradNorm:0.00359  GradNormST:0.01050  AvgTextLen:54.0  AvgSpecLen:174.7  StepTime:0.75  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:129306  AvgTotalLoss:0.04663  AvgPostnetLoss:0.00089  AvgDecoderLoss:0.00093  AvgStopLoss:0.04481  EpochTime:42.69  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.07378   PostnetLoss: 0.00572   DecoderLoss:0.00599  StopLoss: 0.06208  \n",
      "   | > TotalLoss: 0.08458   PostnetLoss: 0.00928   DecoderLoss:0.00964  StopLoss: 0.06566  \n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00089   Validation Loss: 0.00851\n",
      "\n",
      " > Epoch 874/1000\n",
      "   | > Step:3/68  GlobalStep:129310  TotalLoss:0.00126  PostnetLoss:0.00062  DecoderLoss:0.00064  StopLoss:0.05479  GradNorm:0.00377  GradNormST:0.01653  AvgTextLen:4.7  AvgSpecLen:29.7  StepTime:0.29  LR:0.000100\n",
      "   | > Step:13/68  GlobalStep:129320  TotalLoss:0.00138  PostnetLoss:0.00067  DecoderLoss:0.00071  StopLoss:0.04435  GradNorm:0.00302  GradNormST:0.02319  AvgTextLen:11.5  AvgSpecLen:55.2  StepTime:0.38  LR:0.000100\n",
      "   | > Step:23/68  GlobalStep:129330  TotalLoss:0.00167  PostnetLoss:0.00082  DecoderLoss:0.00085  StopLoss:0.03915  GradNorm:0.00341  GradNormST:0.01294  AvgTextLen:18.3  AvgSpecLen:72.2  StepTime:0.43  LR:0.000100\n",
      "   | > Step:33/68  GlobalStep:129340  TotalLoss:0.00185  PostnetLoss:0.00090  DecoderLoss:0.00094  StopLoss:0.04735  GradNorm:0.00460  GradNormST:0.01130  AvgTextLen:25.9  AvgSpecLen:88.9  StepTime:0.43  LR:0.000100\n",
      "   | > Step:43/68  GlobalStep:129350  TotalLoss:0.00197  PostnetLoss:0.00096  DecoderLoss:0.00101  StopLoss:0.04167  GradNorm:0.00410  GradNormST:0.00872  AvgTextLen:33.4  AvgSpecLen:115.7  StepTime:0.55  LR:0.000100\n",
      "   | > Step:53/68  GlobalStep:129360  TotalLoss:0.00213  PostnetLoss:0.00104  DecoderLoss:0.00109  StopLoss:0.03873  GradNorm:0.00473  GradNormST:0.00907  AvgTextLen:42.4  AvgSpecLen:137.6  StepTime:0.81  LR:0.000100\n",
      "   | > Step:63/68  GlobalStep:129370  TotalLoss:0.00241  PostnetLoss:0.00118  DecoderLoss:0.00124  StopLoss:0.03603  GradNorm:0.00519  GradNormST:0.01805  AvgTextLen:58.6  AvgSpecLen:185.1  StepTime:0.91  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:129375  AvgTotalLoss:0.04595  AvgPostnetLoss:0.00089  AvgDecoderLoss:0.00094  AvgStopLoss:0.04412  EpochTime:42.12  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.06922   PostnetLoss: 0.00556   DecoderLoss:0.00582  StopLoss: 0.05785  \n",
      "   | > TotalLoss: 0.08641   PostnetLoss: 0.00974   DecoderLoss:0.01011  StopLoss: 0.06656  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00089   Validation Loss: 0.00868\n",
      "\n",
      " > Epoch 875/1000\n",
      "   | > Step:4/68  GlobalStep:129380  TotalLoss:0.00126  PostnetLoss:0.00061  DecoderLoss:0.00064  StopLoss:0.06868  GradNorm:0.00382  GradNormST:0.02307  AvgTextLen:4.8  AvgSpecLen:31.1  StepTime:0.25  LR:0.000100\n",
      "   | > Step:14/68  GlobalStep:129390  TotalLoss:0.00146  PostnetLoss:0.00071  DecoderLoss:0.00074  StopLoss:0.04563  GradNorm:0.00318  GradNormST:0.01710  AvgTextLen:12.2  AvgSpecLen:52.0  StepTime:0.35  LR:0.000100\n",
      "   | > Step:24/68  GlobalStep:129400  TotalLoss:0.00172  PostnetLoss:0.00084  DecoderLoss:0.00088  StopLoss:0.04606  GradNorm:0.00332  GradNormST:0.01235  AvgTextLen:19.2  AvgSpecLen:79.0  StepTime:0.49  LR:0.000100\n",
      "   | > Step:34/68  GlobalStep:129410  TotalLoss:0.00185  PostnetLoss:0.00090  DecoderLoss:0.00095  StopLoss:0.03016  GradNorm:0.00398  GradNormST:0.00885  AvgTextLen:26.2  AvgSpecLen:95.9  StepTime:0.48  LR:0.000100\n",
      "   | > Step:44/68  GlobalStep:129420  TotalLoss:0.00203  PostnetLoss:0.00099  DecoderLoss:0.00104  StopLoss:0.02431  GradNorm:0.00488  GradNormST:0.00725  AvgTextLen:33.8  AvgSpecLen:116.3  StepTime:0.67  LR:0.000100\n",
      "   | > Step:54/68  GlobalStep:129430  TotalLoss:0.00214  PostnetLoss:0.00105  DecoderLoss:0.00110  StopLoss:0.05305  GradNorm:0.00446  GradNormST:0.01810  AvgTextLen:42.2  AvgSpecLen:144.9  StepTime:0.71  LR:0.000100\n",
      "   | > Step:64/68  GlobalStep:129440  TotalLoss:0.00225  PostnetLoss:0.00109  DecoderLoss:0.00115  StopLoss:0.03558  GradNorm:0.00375  GradNormST:0.01036  AvgTextLen:60.6  AvgSpecLen:177.6  StepTime:0.81  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:129444  AvgTotalLoss:0.04864  AvgPostnetLoss:0.00089  AvgDecoderLoss:0.00093  AvgStopLoss:0.04681  EpochTime:42.11  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.07015   PostnetLoss: 0.00584   DecoderLoss:0.00610  StopLoss: 0.05821  \n",
      "   | > TotalLoss: 0.08339   PostnetLoss: 0.00909   DecoderLoss:0.00942  StopLoss: 0.06488  \n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00089   Validation Loss: 0.00842\n",
      "\n",
      " > Epoch 876/1000\n",
      "   | > Step:5/68  GlobalStep:129450  TotalLoss:0.00129  PostnetLoss:0.00063  DecoderLoss:0.00066  StopLoss:0.04480  GradNorm:0.00406  GradNormST:0.01587  AvgTextLen:5.9  AvgSpecLen:33.2  StepTime:0.36  LR:0.000100\n",
      "   | > Step:15/68  GlobalStep:129460  TotalLoss:0.00150  PostnetLoss:0.00074  DecoderLoss:0.00077  StopLoss:0.04888  GradNorm:0.00318  GradNormST:0.01718  AvgTextLen:12.8  AvgSpecLen:59.1  StepTime:0.44  LR:0.000100\n",
      "   | > Step:25/68  GlobalStep:129470  TotalLoss:0.00167  PostnetLoss:0.00082  DecoderLoss:0.00085  StopLoss:0.04629  GradNorm:0.00333  GradNormST:0.01353  AvgTextLen:19.8  AvgSpecLen:73.3  StepTime:0.36  LR:0.000100\n",
      "   | > Step:35/68  GlobalStep:129480  TotalLoss:0.00187  PostnetLoss:0.00091  DecoderLoss:0.00096  StopLoss:0.04251  GradNorm:0.00354  GradNormST:0.01737  AvgTextLen:27.1  AvgSpecLen:104.7  StepTime:0.63  LR:0.000100\n",
      "   | > Step:45/68  GlobalStep:129490  TotalLoss:0.00196  PostnetLoss:0.00096  DecoderLoss:0.00100  StopLoss:0.04445  GradNorm:0.00392  GradNormST:0.01356  AvgTextLen:34.4  AvgSpecLen:118.6  StepTime:0.59  LR:0.000100\n",
      "   | > Step:55/68  GlobalStep:129500  TotalLoss:0.00227  PostnetLoss:0.00111  DecoderLoss:0.00116  StopLoss:0.03238  GradNorm:0.00610  GradNormST:0.01157  AvgTextLen:43.9  AvgSpecLen:145.2  StepTime:0.80  LR:0.000100\n",
      "   | > Step:65/68  GlobalStep:129510  TotalLoss:0.00244  PostnetLoss:0.00119  DecoderLoss:0.00125  StopLoss:0.05024  GradNorm:0.00514  GradNormST:0.02237  AvgTextLen:64.2  AvgSpecLen:207.4  StepTime:1.19  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:129513  AvgTotalLoss:0.04498  AvgPostnetLoss:0.00089  AvgDecoderLoss:0.00094  AvgStopLoss:0.04315  EpochTime:42.07  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.07345   PostnetLoss: 0.00591   DecoderLoss:0.00618  StopLoss: 0.06136  \n",
      "   | > TotalLoss: 0.08940   PostnetLoss: 0.00928   DecoderLoss:0.00963  StopLoss: 0.07048  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00089   Validation Loss: 0.00851\n",
      "\n",
      " > Epoch 877/1000\n",
      "   | > Step:6/68  GlobalStep:129520  TotalLoss:0.00128  PostnetLoss:0.00063  DecoderLoss:0.00066  StopLoss:0.04933  GradNorm:0.00358  GradNormST:0.03424  AvgTextLen:6.0  AvgSpecLen:34.8  StepTime:0.38  LR:0.000100\n",
      "   | > Step:16/68  GlobalStep:129530  TotalLoss:0.00152  PostnetLoss:0.00074  DecoderLoss:0.00078  StopLoss:0.03883  GradNorm:0.00325  GradNormST:0.01035  AvgTextLen:13.8  AvgSpecLen:60.6  StepTime:0.36  LR:0.000100\n",
      "   | > Step:26/68  GlobalStep:129540  TotalLoss:0.00169  PostnetLoss:0.00083  DecoderLoss:0.00086  StopLoss:0.05066  GradNorm:0.00307  GradNormST:0.01463  AvgTextLen:20.3  AvgSpecLen:77.5  StepTime:0.41  LR:0.000100\n",
      "   | > Step:36/68  GlobalStep:129550  TotalLoss:0.00184  PostnetLoss:0.00090  DecoderLoss:0.00094  StopLoss:0.04644  GradNorm:0.00342  GradNormST:0.01073  AvgTextLen:28.5  AvgSpecLen:100.2  StepTime:0.61  LR:0.000100\n",
      "   | > Step:46/68  GlobalStep:129560  TotalLoss:0.00201  PostnetLoss:0.00098  DecoderLoss:0.00103  StopLoss:0.05031  GradNorm:0.00482  GradNormST:0.02010  AvgTextLen:35.4  AvgSpecLen:122.6  StepTime:0.68  LR:0.000100\n",
      "   | > Step:56/68  GlobalStep:129570  TotalLoss:0.00223  PostnetLoss:0.00109  DecoderLoss:0.00114  StopLoss:0.05016  GradNorm:0.00549  GradNormST:0.01806  AvgTextLen:45.8  AvgSpecLen:145.4  StepTime:0.96  LR:0.000100\n",
      "   | > Step:66/68  GlobalStep:129580  TotalLoss:0.00254  PostnetLoss:0.00123  DecoderLoss:0.00130  StopLoss:0.03569  GradNorm:0.00382  GradNormST:0.01351  AvgTextLen:71.2  AvgSpecLen:220.8  StepTime:1.03  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:129582  AvgTotalLoss:0.04740  AvgPostnetLoss:0.00089  AvgDecoderLoss:0.00094  AvgStopLoss:0.04558  EpochTime:42.16  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.07014   PostnetLoss: 0.00576   DecoderLoss:0.00603  StopLoss: 0.05834  \n",
      "   | > TotalLoss: 0.08660   PostnetLoss: 0.00910   DecoderLoss:0.00945  StopLoss: 0.06806  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00089   Validation Loss: 0.00837\n",
      "\n",
      " > Epoch 878/1000\n",
      "   | > Step:7/68  GlobalStep:129590  TotalLoss:0.00135  PostnetLoss:0.00066  DecoderLoss:0.00069  StopLoss:0.02882  GradNorm:0.00390  GradNormST:0.01078  AvgTextLen:7.4  AvgSpecLen:44.6  StepTime:0.30  LR:0.000100\n",
      "   | > Step:17/68  GlobalStep:129600  TotalLoss:0.00151  PostnetLoss:0.00074  DecoderLoss:0.00077  StopLoss:0.06247  GradNorm:0.00315  GradNormST:0.01721  AvgTextLen:14.7  AvgSpecLen:66.0  StepTime:0.43  LR:0.000100\n",
      "   | > Step:27/68  GlobalStep:129610  TotalLoss:0.00173  PostnetLoss:0.00085  DecoderLoss:0.00089  StopLoss:0.05524  GradNorm:0.00298  GradNormST:0.01582  AvgTextLen:21.0  AvgSpecLen:80.2  StepTime:0.53  LR:0.000100\n",
      "   | > Step:37/68  GlobalStep:129620  TotalLoss:0.00186  PostnetLoss:0.00091  DecoderLoss:0.00095  StopLoss:0.04600  GradNorm:0.00325  GradNormST:0.01634  AvgTextLen:29.3  AvgSpecLen:102.4  StepTime:0.59  LR:0.000100\n",
      "   | > Step:47/68  GlobalStep:129630  TotalLoss:0.00207  PostnetLoss:0.00101  DecoderLoss:0.00106  StopLoss:0.04292  GradNorm:0.00411  GradNormST:0.01113  AvgTextLen:36.7  AvgSpecLen:126.9  StepTime:0.60  LR:0.000100\n",
      "   | > Step:57/68  GlobalStep:129640  TotalLoss:0.00231  PostnetLoss:0.00113  DecoderLoss:0.00119  StopLoss:0.02608  GradNorm:0.00666  GradNormST:0.00693  AvgTextLen:47.1  AvgSpecLen:161.0  StepTime:0.70  LR:0.000100\n",
      "   | > Step:67/68  GlobalStep:129650  TotalLoss:0.00255  PostnetLoss:0.00124  DecoderLoss:0.00131  StopLoss:0.04051  GradNorm:0.00350  GradNormST:0.01835  AvgTextLen:80.6  AvgSpecLen:246.5  StepTime:1.24  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:129651  AvgTotalLoss:0.04669  AvgPostnetLoss:0.00089  AvgDecoderLoss:0.00093  AvgStopLoss:0.04487  EpochTime:41.69  AvgStepTime:0.60\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.06984   PostnetLoss: 0.00588   DecoderLoss:0.00615  StopLoss: 0.05781  \n",
      "   | > TotalLoss: 0.08312   PostnetLoss: 0.00919   DecoderLoss:0.00954  StopLoss: 0.06438  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00089   Validation Loss: 0.00841\n",
      "\n",
      " > Epoch 879/1000\n",
      "   | > Step:8/68  GlobalStep:129660  TotalLoss:0.00140  PostnetLoss:0.00069  DecoderLoss:0.00071  StopLoss:0.05359  GradNorm:0.00435  GradNormST:0.01325  AvgTextLen:8.3  AvgSpecLen:44.0  StepTime:0.32  LR:0.000100\n",
      "   | > Step:18/68  GlobalStep:129670  TotalLoss:0.00152  PostnetLoss:0.00074  DecoderLoss:0.00078  StopLoss:0.03779  GradNorm:0.00361  GradNormST:0.01129  AvgTextLen:15.2  AvgSpecLen:64.5  StepTime:0.42  LR:0.000100\n",
      "   | > Step:28/68  GlobalStep:129680  TotalLoss:0.00173  PostnetLoss:0.00085  DecoderLoss:0.00088  StopLoss:0.04327  GradNorm:0.00299  GradNormST:0.01267  AvgTextLen:22.1  AvgSpecLen:85.4  StepTime:0.44  LR:0.000100\n",
      "   | > Step:38/68  GlobalStep:129690  TotalLoss:0.00187  PostnetLoss:0.00091  DecoderLoss:0.00096  StopLoss:0.02977  GradNorm:0.00334  GradNormST:0.00950  AvgTextLen:29.4  AvgSpecLen:104.1  StepTime:0.49  LR:0.000100\n",
      "   | > Step:48/68  GlobalStep:129700  TotalLoss:0.00202  PostnetLoss:0.00098  DecoderLoss:0.00103  StopLoss:0.04219  GradNorm:0.00355  GradNormST:0.01091  AvgTextLen:37.5  AvgSpecLen:123.8  StepTime:0.78  LR:0.000100\n",
      "   | > Step:58/68  GlobalStep:129710  TotalLoss:0.00228  PostnetLoss:0.00111  DecoderLoss:0.00117  StopLoss:0.04928  GradNorm:0.00557  GradNormST:0.01498  AvgTextLen:47.8  AvgSpecLen:158.0  StepTime:0.87  LR:0.000100\n",
      "   | > Step:68/68  GlobalStep:129720  TotalLoss:0.00253  PostnetLoss:0.00123  DecoderLoss:0.00130  StopLoss:0.05400  GradNorm:0.00489  GradNormST:0.03056  AvgTextLen:103.0  AvgSpecLen:308.9  StepTime:1.86  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:129720  AvgTotalLoss:0.04493  AvgPostnetLoss:0.00089  AvgDecoderLoss:0.00094  AvgStopLoss:0.04310  EpochTime:41.75  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.07187   PostnetLoss: 0.00595   DecoderLoss:0.00622  StopLoss: 0.05969  \n",
      "   | > TotalLoss: 0.08223   PostnetLoss: 0.00903   DecoderLoss:0.00938  StopLoss: 0.06381  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00089   Validation Loss: 0.00835\n",
      "\n",
      " > Epoch 880/1000\n",
      "   | > Step:9/68  GlobalStep:129730  TotalLoss:0.00135  PostnetLoss:0.00066  DecoderLoss:0.00069  StopLoss:0.03948  GradNorm:0.00366  GradNormST:0.01703  AvgTextLen:9.0  AvgSpecLen:42.1  StepTime:0.41  LR:0.000100\n",
      "   | > Step:19/68  GlobalStep:129740  TotalLoss:0.00153  PostnetLoss:0.00075  DecoderLoss:0.00078  StopLoss:0.06063  GradNorm:0.00342  GradNormST:0.01782  AvgTextLen:15.5  AvgSpecLen:63.2  StepTime:0.38  LR:0.000100\n",
      "   | > Step:29/68  GlobalStep:129750  TotalLoss:0.00169  PostnetLoss:0.00083  DecoderLoss:0.00086  StopLoss:0.05550  GradNorm:0.00311  GradNormST:0.01443  AvgTextLen:21.8  AvgSpecLen:83.4  StepTime:0.59  LR:0.000100\n",
      "   | > Step:39/68  GlobalStep:129760  TotalLoss:0.00189  PostnetLoss:0.00092  DecoderLoss:0.00097  StopLoss:0.04011  GradNorm:0.00290  GradNormST:0.01029  AvgTextLen:30.2  AvgSpecLen:110.4  StepTime:0.57  LR:0.000100\n",
      "   | > Step:49/68  GlobalStep:129770  TotalLoss:0.00207  PostnetLoss:0.00101  DecoderLoss:0.00106  StopLoss:0.03495  GradNorm:0.00391  GradNormST:0.00740  AvgTextLen:38.2  AvgSpecLen:127.7  StepTime:0.72  LR:0.000100\n",
      "   | > Step:59/68  GlobalStep:129780  TotalLoss:0.00219  PostnetLoss:0.00107  DecoderLoss:0.00112  StopLoss:0.03445  GradNorm:0.00348  GradNormST:0.00953  AvgTextLen:50.5  AvgSpecLen:155.9  StepTime:0.71  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:129789  AvgTotalLoss:0.04861  AvgPostnetLoss:0.00089  AvgDecoderLoss:0.00094  AvgStopLoss:0.04678  EpochTime:42.67  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.07543   PostnetLoss: 0.00604   DecoderLoss:0.00631  StopLoss: 0.06308  \n",
      "   | > TotalLoss: 0.07854   PostnetLoss: 0.00872   DecoderLoss:0.00907  StopLoss: 0.06075  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00089   Validation Loss: 0.00823\n",
      "\n",
      " > Epoch 881/1000\n",
      "   | > Step:0/68  GlobalStep:129790  TotalLoss:0.00136  PostnetLoss:0.00066  DecoderLoss:0.00071  StopLoss:0.06261  GradNorm:0.00718  GradNormST:0.02559  AvgTextLen:3.0  AvgSpecLen:25.3  StepTime:0.41  LR:0.000100\n",
      "   | > Step:10/68  GlobalStep:129800  TotalLoss:0.00143  PostnetLoss:0.00070  DecoderLoss:0.00073  StopLoss:0.05273  GradNorm:0.00352  GradNormST:0.03490  AvgTextLen:9.8  AvgSpecLen:48.6  StepTime:0.61  LR:0.000100\n",
      "   | > Step:20/68  GlobalStep:129810  TotalLoss:0.00161  PostnetLoss:0.00079  DecoderLoss:0.00082  StopLoss:0.04024  GradNorm:0.00358  GradNormST:0.01627  AvgTextLen:16.4  AvgSpecLen:64.8  StepTime:0.52  LR:0.000100\n",
      "   | > Step:30/68  GlobalStep:129820  TotalLoss:0.00175  PostnetLoss:0.00086  DecoderLoss:0.00089  StopLoss:0.04699  GradNorm:0.00303  GradNormST:0.01430  AvgTextLen:22.9  AvgSpecLen:85.8  StepTime:0.60  LR:0.000100\n",
      "   | > Step:40/68  GlobalStep:129830  TotalLoss:0.00188  PostnetLoss:0.00092  DecoderLoss:0.00096  StopLoss:0.04428  GradNorm:0.00293  GradNormST:0.01425  AvgTextLen:30.9  AvgSpecLen:105.3  StepTime:0.59  LR:0.000100\n",
      "   | > Step:50/68  GlobalStep:129840  TotalLoss:0.00206  PostnetLoss:0.00100  DecoderLoss:0.00105  StopLoss:0.03555  GradNorm:0.00366  GradNormST:0.00775  AvgTextLen:38.8  AvgSpecLen:132.0  StepTime:0.79  LR:0.000100\n",
      "   | > Step:60/68  GlobalStep:129850  TotalLoss:0.00232  PostnetLoss:0.00113  DecoderLoss:0.00119  StopLoss:0.03422  GradNorm:0.00556  GradNormST:0.00719  AvgTextLen:52.0  AvgSpecLen:172.7  StepTime:0.96  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:129858  AvgTotalLoss:0.04637  AvgPostnetLoss:0.00089  AvgDecoderLoss:0.00094  AvgStopLoss:0.04454  EpochTime:42.19  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.07425   PostnetLoss: 0.00591   DecoderLoss:0.00619  StopLoss: 0.06216  \n",
      "   | > TotalLoss: 0.08434   PostnetLoss: 0.00928   DecoderLoss:0.00964  StopLoss: 0.06542  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00089   Validation Loss: 0.00864\n",
      "\n",
      " > Epoch 882/1000\n",
      "   | > Step:1/68  GlobalStep:129860  TotalLoss:0.00134  PostnetLoss:0.00065  DecoderLoss:0.00069  StopLoss:0.03339  GradNorm:0.00651  GradNormST:0.01244  AvgTextLen:3.7  AvgSpecLen:28.6  StepTime:0.32  LR:0.000100\n",
      "   | > Step:11/68  GlobalStep:129870  TotalLoss:0.00141  PostnetLoss:0.00069  DecoderLoss:0.00072  StopLoss:0.03257  GradNorm:0.00445  GradNormST:0.01097  AvgTextLen:10.3  AvgSpecLen:49.4  StepTime:0.40  LR:0.000100\n",
      "   | > Step:21/68  GlobalStep:129880  TotalLoss:0.00163  PostnetLoss:0.00080  DecoderLoss:0.00083  StopLoss:0.04135  GradNorm:0.00407  GradNormST:0.01803  AvgTextLen:16.6  AvgSpecLen:66.3  StepTime:0.37  LR:0.000100\n",
      "   | > Step:31/68  GlobalStep:129890  TotalLoss:0.00175  PostnetLoss:0.00086  DecoderLoss:0.00089  StopLoss:0.03748  GradNorm:0.00333  GradNormST:0.01293  AvgTextLen:24.1  AvgSpecLen:82.7  StepTime:0.54  LR:0.000100\n",
      "   | > Step:41/68  GlobalStep:129900  TotalLoss:0.00196  PostnetLoss:0.00096  DecoderLoss:0.00100  StopLoss:0.02504  GradNorm:0.00289  GradNormST:0.00901  AvgTextLen:32.3  AvgSpecLen:109.4  StepTime:0.68  LR:0.000100\n",
      "   | > Step:51/68  GlobalStep:129910  TotalLoss:0.00203  PostnetLoss:0.00099  DecoderLoss:0.00104  StopLoss:0.04575  GradNorm:0.00312  GradNormST:0.01124  AvgTextLen:39.2  AvgSpecLen:126.9  StepTime:0.75  LR:0.000100\n",
      "   | > Step:61/68  GlobalStep:129920  TotalLoss:0.00227  PostnetLoss:0.00110  DecoderLoss:0.00116  StopLoss:0.03398  GradNorm:0.00398  GradNormST:0.00894  AvgTextLen:54.6  AvgSpecLen:165.5  StepTime:0.90  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:129927  AvgTotalLoss:0.03988  AvgPostnetLoss:0.00090  AvgDecoderLoss:0.00094  AvgStopLoss:0.03804  EpochTime:42.63  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.06913   PostnetLoss: 0.00597   DecoderLoss:0.00623  StopLoss: 0.05693  \n",
      "   | > TotalLoss: 0.07922   PostnetLoss: 0.00906   DecoderLoss:0.00941  StopLoss: 0.06076  \n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00090   Validation Loss: 0.00843\n",
      "\n",
      " > Epoch 883/1000\n",
      "   | > Step:2/68  GlobalStep:129930  TotalLoss:0.00119  PostnetLoss:0.00058  DecoderLoss:0.00061  StopLoss:0.03425  GradNorm:0.00348  GradNormST:0.01279  AvgTextLen:4.0  AvgSpecLen:31.3  StepTime:0.52  LR:0.000100\n",
      "   | > Step:12/68  GlobalStep:129940  TotalLoss:0.00139  PostnetLoss:0.00068  DecoderLoss:0.00071  StopLoss:0.04532  GradNorm:0.00326  GradNormST:0.01266  AvgTextLen:11.0  AvgSpecLen:49.3  StepTime:0.34  LR:0.000100\n",
      "   | > Step:22/68  GlobalStep:129950  TotalLoss:0.00165  PostnetLoss:0.00081  DecoderLoss:0.00084  StopLoss:0.04238  GradNorm:0.00421  GradNormST:0.00919  AvgTextLen:18.0  AvgSpecLen:72.0  StepTime:0.48  LR:0.000100\n",
      "   | > Step:32/68  GlobalStep:129960  TotalLoss:0.00177  PostnetLoss:0.00086  DecoderLoss:0.00090  StopLoss:0.04725  GradNorm:0.00381  GradNormST:0.02638  AvgTextLen:24.4  AvgSpecLen:91.5  StepTime:0.59  LR:0.000100\n",
      "   | > Step:42/68  GlobalStep:129970  TotalLoss:0.00193  PostnetLoss:0.00094  DecoderLoss:0.00099  StopLoss:0.04079  GradNorm:0.00357  GradNormST:0.01008  AvgTextLen:33.1  AvgSpecLen:109.7  StepTime:0.64  LR:0.000100\n",
      "   | > Step:52/68  GlobalStep:129980  TotalLoss:0.00208  PostnetLoss:0.00102  DecoderLoss:0.00107  StopLoss:0.04214  GradNorm:0.00290  GradNormST:0.00894  AvgTextLen:42.1  AvgSpecLen:137.5  StepTime:0.68  LR:0.000100\n",
      "   | > Step:62/68  GlobalStep:129990  TotalLoss:0.00234  PostnetLoss:0.00114  DecoderLoss:0.00120  StopLoss:0.03354  GradNorm:0.00411  GradNormST:0.00716  AvgTextLen:54.0  AvgSpecLen:174.7  StepTime:0.75  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:129996  AvgTotalLoss:0.04044  AvgPostnetLoss:0.00089  AvgDecoderLoss:0.00093  AvgStopLoss:0.03862  EpochTime:42.21  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.07001   PostnetLoss: 0.00587   DecoderLoss:0.00613  StopLoss: 0.05801  \n",
      "   | > TotalLoss: 0.08049   PostnetLoss: 0.00916   DecoderLoss:0.00952  StopLoss: 0.06181  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00089   Validation Loss: 0.00839\n",
      "\n",
      " > Epoch 884/1000\n",
      "   | > Step:3/68  GlobalStep:130000  TotalLoss:0.00128  PostnetLoss:0.00062  DecoderLoss:0.00065  StopLoss:0.09141  GradNorm:0.00418  GradNormST:0.03593  AvgTextLen:4.7  AvgSpecLen:29.7  StepTime:0.31  LR:0.000100\n",
      " | | > Checkpoint saving : /vm/TTS/models/cartman_models/mozilla-no-loc-fattn-stopnet-sigmoid-loss_masking-August-15-2019_11+26AM-de6aeb9/checkpoint_130000.pth.tar\n",
      "   | > Step:13/68  GlobalStep:130010  TotalLoss:0.00142  PostnetLoss:0.00069  DecoderLoss:0.00072  StopLoss:0.04911  GradNorm:0.00315  GradNormST:0.01833  AvgTextLen:11.5  AvgSpecLen:55.2  StepTime:0.41  LR:0.000100\n",
      "   | > Step:23/68  GlobalStep:130020  TotalLoss:0.00169  PostnetLoss:0.00083  DecoderLoss:0.00086  StopLoss:0.05217  GradNorm:0.00387  GradNormST:0.01657  AvgTextLen:18.3  AvgSpecLen:72.2  StepTime:0.40  LR:0.000100\n",
      "   | > Step:33/68  GlobalStep:130030  TotalLoss:0.00185  PostnetLoss:0.00091  DecoderLoss:0.00095  StopLoss:0.03932  GradNorm:0.00332  GradNormST:0.01010  AvgTextLen:25.9  AvgSpecLen:88.9  StepTime:0.47  LR:0.000100\n",
      "   | > Step:43/68  GlobalStep:130040  TotalLoss:0.00194  PostnetLoss:0.00094  DecoderLoss:0.00099  StopLoss:0.04479  GradNorm:0.00278  GradNormST:0.01043  AvgTextLen:33.4  AvgSpecLen:115.7  StepTime:0.66  LR:0.000100\n",
      "   | > Step:53/68  GlobalStep:130050  TotalLoss:0.00205  PostnetLoss:0.00100  DecoderLoss:0.00105  StopLoss:0.03960  GradNorm:0.00270  GradNormST:0.01457  AvgTextLen:42.4  AvgSpecLen:137.6  StepTime:0.76  LR:0.000100\n",
      "   | > Step:63/68  GlobalStep:130060  TotalLoss:0.00237  PostnetLoss:0.00115  DecoderLoss:0.00122  StopLoss:0.03513  GradNorm:0.00360  GradNormST:0.01337  AvgTextLen:58.6  AvgSpecLen:185.1  StepTime:0.91  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:130065  AvgTotalLoss:0.04594  AvgPostnetLoss:0.00089  AvgDecoderLoss:0.00094  AvgStopLoss:0.04412  EpochTime:43.17  AvgStepTime:0.63\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.06996   PostnetLoss: 0.00599   DecoderLoss:0.00624  StopLoss: 0.05773  \n",
      "   | > TotalLoss: 0.08168   PostnetLoss: 0.00894   DecoderLoss:0.00930  StopLoss: 0.06343  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00089   Validation Loss: 0.00844\n",
      "\n",
      " > Epoch 885/1000\n",
      "   | > Step:4/68  GlobalStep:130070  TotalLoss:0.00128  PostnetLoss:0.00062  DecoderLoss:0.00066  StopLoss:0.06647  GradNorm:0.00376  GradNormST:0.02611  AvgTextLen:4.8  AvgSpecLen:31.1  StepTime:0.21  LR:0.000100\n",
      "   | > Step:14/68  GlobalStep:130080  TotalLoss:0.00149  PostnetLoss:0.00073  DecoderLoss:0.00076  StopLoss:0.03749  GradNorm:0.00356  GradNormST:0.01156  AvgTextLen:12.2  AvgSpecLen:52.0  StepTime:0.26  LR:0.000100\n",
      "   | > Step:24/68  GlobalStep:130090  TotalLoss:0.00173  PostnetLoss:0.00085  DecoderLoss:0.00089  StopLoss:0.04182  GradNorm:0.00379  GradNormST:0.01333  AvgTextLen:19.2  AvgSpecLen:79.0  StepTime:0.53  LR:0.000100\n",
      "   | > Step:34/68  GlobalStep:130100  TotalLoss:0.00189  PostnetLoss:0.00092  DecoderLoss:0.00097  StopLoss:0.02771  GradNorm:0.00313  GradNormST:0.00790  AvgTextLen:26.2  AvgSpecLen:95.9  StepTime:0.59  LR:0.000100\n",
      "   | > Step:44/68  GlobalStep:130110  TotalLoss:0.00200  PostnetLoss:0.00098  DecoderLoss:0.00102  StopLoss:0.03559  GradNorm:0.00329  GradNormST:0.01418  AvgTextLen:33.8  AvgSpecLen:116.3  StepTime:0.66  LR:0.000100\n",
      "   | > Step:54/68  GlobalStep:130120  TotalLoss:0.00210  PostnetLoss:0.00102  DecoderLoss:0.00108  StopLoss:0.04057  GradNorm:0.00274  GradNormST:0.01110  AvgTextLen:42.2  AvgSpecLen:144.9  StepTime:0.82  LR:0.000100\n",
      "   | > Step:64/68  GlobalStep:130130  TotalLoss:0.00225  PostnetLoss:0.00109  DecoderLoss:0.00115  StopLoss:0.04181  GradNorm:0.00321  GradNormST:0.01307  AvgTextLen:60.6  AvgSpecLen:177.6  StepTime:0.81  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:130134  AvgTotalLoss:0.04681  AvgPostnetLoss:0.00089  AvgDecoderLoss:0.00093  AvgStopLoss:0.04499  EpochTime:42.66  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.06975   PostnetLoss: 0.00566   DecoderLoss:0.00592  StopLoss: 0.05817  \n",
      "   | > TotalLoss: 0.08149   PostnetLoss: 0.00903   DecoderLoss:0.00940  StopLoss: 0.06306  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00089   Validation Loss: 0.00830\n",
      "\n",
      " > Epoch 886/1000\n",
      "   | > Step:5/68  GlobalStep:130140  TotalLoss:0.00134  PostnetLoss:0.00065  DecoderLoss:0.00069  StopLoss:0.03623  GradNorm:0.00423  GradNormST:0.01299  AvgTextLen:5.9  AvgSpecLen:33.2  StepTime:0.39  LR:0.000100\n",
      "   | > Step:15/68  GlobalStep:130150  TotalLoss:0.00155  PostnetLoss:0.00076  DecoderLoss:0.00079  StopLoss:0.04744  GradNorm:0.00355  GradNormST:0.01465  AvgTextLen:12.8  AvgSpecLen:59.1  StepTime:0.43  LR:0.000100\n",
      "   | > Step:25/68  GlobalStep:130160  TotalLoss:0.00168  PostnetLoss:0.00082  DecoderLoss:0.00086  StopLoss:0.03991  GradNorm:0.00380  GradNormST:0.00984  AvgTextLen:19.8  AvgSpecLen:73.3  StepTime:0.48  LR:0.000100\n",
      "   | > Step:35/68  GlobalStep:130170  TotalLoss:0.00188  PostnetLoss:0.00092  DecoderLoss:0.00096  StopLoss:0.04492  GradNorm:0.00329  GradNormST:0.01920  AvgTextLen:27.1  AvgSpecLen:104.7  StepTime:0.67  LR:0.000100\n",
      "   | > Step:45/68  GlobalStep:130180  TotalLoss:0.00194  PostnetLoss:0.00095  DecoderLoss:0.00099  StopLoss:0.04032  GradNorm:0.00319  GradNormST:0.01053  AvgTextLen:34.4  AvgSpecLen:118.6  StepTime:0.58  LR:0.000100\n",
      "   | > Step:55/68  GlobalStep:130190  TotalLoss:0.00217  PostnetLoss:0.00106  DecoderLoss:0.00111  StopLoss:0.04599  GradNorm:0.00269  GradNormST:0.01623  AvgTextLen:43.9  AvgSpecLen:145.2  StepTime:0.86  LR:0.000100\n",
      "   | > Step:65/68  GlobalStep:130200  TotalLoss:0.00238  PostnetLoss:0.00116  DecoderLoss:0.00122  StopLoss:0.10662  GradNorm:0.00277  GradNormST:0.05710  AvgTextLen:64.2  AvgSpecLen:207.4  StepTime:1.18  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:130203  AvgTotalLoss:0.04664  AvgPostnetLoss:0.00089  AvgDecoderLoss:0.00094  AvgStopLoss:0.04481  EpochTime:42.62  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.06992   PostnetLoss: 0.00552   DecoderLoss:0.00579  StopLoss: 0.05860  \n",
      "   | > TotalLoss: 0.07841   PostnetLoss: 0.00884   DecoderLoss:0.00921  StopLoss: 0.06036  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00089   Validation Loss: 0.00810\n",
      "\n",
      " > Epoch 887/1000\n",
      "   | > Step:6/68  GlobalStep:130210  TotalLoss:0.00133  PostnetLoss:0.00066  DecoderLoss:0.00068  StopLoss:0.04242  GradNorm:0.00428  GradNormST:0.02001  AvgTextLen:6.0  AvgSpecLen:34.8  StepTime:0.39  LR:0.000100\n",
      "   | > Step:16/68  GlobalStep:130220  TotalLoss:0.00159  PostnetLoss:0.00078  DecoderLoss:0.00081  StopLoss:0.02994  GradNorm:0.00375  GradNormST:0.01003  AvgTextLen:13.8  AvgSpecLen:60.6  StepTime:0.37  LR:0.000100\n",
      "   | > Step:26/68  GlobalStep:130230  TotalLoss:0.00173  PostnetLoss:0.00085  DecoderLoss:0.00088  StopLoss:0.03188  GradNorm:0.00399  GradNormST:0.00912  AvgTextLen:20.3  AvgSpecLen:77.5  StepTime:0.48  LR:0.000100\n",
      "   | > Step:36/68  GlobalStep:130240  TotalLoss:0.00187  PostnetLoss:0.00091  DecoderLoss:0.00096  StopLoss:0.03517  GradNorm:0.00347  GradNormST:0.00833  AvgTextLen:28.5  AvgSpecLen:100.2  StepTime:0.76  LR:0.000100\n",
      "   | > Step:46/68  GlobalStep:130250  TotalLoss:0.00197  PostnetLoss:0.00096  DecoderLoss:0.00101  StopLoss:0.04792  GradNorm:0.00364  GradNormST:0.02157  AvgTextLen:35.4  AvgSpecLen:122.6  StepTime:0.69  LR:0.000100\n",
      "   | > Step:56/68  GlobalStep:130260  TotalLoss:0.00216  PostnetLoss:0.00105  DecoderLoss:0.00111  StopLoss:0.04661  GradNorm:0.00303  GradNormST:0.01960  AvgTextLen:45.8  AvgSpecLen:145.4  StepTime:1.07  LR:0.000100\n",
      "   | > Step:66/68  GlobalStep:130270  TotalLoss:0.00249  PostnetLoss:0.00121  DecoderLoss:0.00128  StopLoss:0.03912  GradNorm:0.00291  GradNormST:0.01256  AvgTextLen:71.2  AvgSpecLen:220.8  StepTime:1.04  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:130272  AvgTotalLoss:0.04612  AvgPostnetLoss:0.00090  AvgDecoderLoss:0.00094  AvgStopLoss:0.04428  EpochTime:43.09  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.07359   PostnetLoss: 0.00576   DecoderLoss:0.00603  StopLoss: 0.06180  \n",
      "   | > TotalLoss: 0.08353   PostnetLoss: 0.00912   DecoderLoss:0.00944  StopLoss: 0.06497  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00090   Validation Loss: 0.00833\n",
      "\n",
      " > Epoch 888/1000\n",
      "   | > Step:7/68  GlobalStep:130280  TotalLoss:0.00140  PostnetLoss:0.00068  DecoderLoss:0.00071  StopLoss:0.03443  GradNorm:0.00551  GradNormST:0.01531  AvgTextLen:7.4  AvgSpecLen:44.6  StepTime:0.38  LR:0.000100\n",
      "   | > Step:17/68  GlobalStep:130290  TotalLoss:0.00156  PostnetLoss:0.00076  DecoderLoss:0.00080  StopLoss:0.05129  GradNorm:0.00346  GradNormST:0.01225  AvgTextLen:14.7  AvgSpecLen:66.0  StepTime:0.34  LR:0.000100\n",
      "   | > Step:27/68  GlobalStep:130300  TotalLoss:0.00174  PostnetLoss:0.00085  DecoderLoss:0.00088  StopLoss:0.06053  GradNorm:0.00362  GradNormST:0.02656  AvgTextLen:21.0  AvgSpecLen:80.2  StepTime:0.63  LR:0.000100\n",
      "   | > Step:37/68  GlobalStep:130310  TotalLoss:0.00189  PostnetLoss:0.00093  DecoderLoss:0.00097  StopLoss:0.04730  GradNorm:0.00414  GradNormST:0.01653  AvgTextLen:29.3  AvgSpecLen:102.4  StepTime:0.49  LR:0.000100\n",
      "   | > Step:47/68  GlobalStep:130320  TotalLoss:0.00205  PostnetLoss:0.00100  DecoderLoss:0.00105  StopLoss:0.02984  GradNorm:0.00425  GradNormST:0.00829  AvgTextLen:36.7  AvgSpecLen:126.9  StepTime:0.58  LR:0.000100\n",
      "   | > Step:57/68  GlobalStep:130330  TotalLoss:0.00226  PostnetLoss:0.00110  DecoderLoss:0.00116  StopLoss:0.02929  GradNorm:0.00392  GradNormST:0.00730  AvgTextLen:47.1  AvgSpecLen:161.0  StepTime:0.84  LR:0.000100\n",
      "   | > Step:67/68  GlobalStep:130340  TotalLoss:0.00252  PostnetLoss:0.00122  DecoderLoss:0.00130  StopLoss:0.03689  GradNorm:0.00311  GradNormST:0.01477  AvgTextLen:80.6  AvgSpecLen:246.5  StepTime:1.22  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:130341  AvgTotalLoss:0.04513  AvgPostnetLoss:0.00090  AvgDecoderLoss:0.00094  AvgStopLoss:0.04329  EpochTime:42.29  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.07652   PostnetLoss: 0.00592   DecoderLoss:0.00617  StopLoss: 0.06443  \n",
      "   | > TotalLoss: 0.08503   PostnetLoss: 0.00886   DecoderLoss:0.00918  StopLoss: 0.06699  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00090   Validation Loss: 0.00821\n",
      "\n",
      " > Epoch 889/1000\n",
      "   | > Step:8/68  GlobalStep:130350  TotalLoss:0.00147  PostnetLoss:0.00072  DecoderLoss:0.00075  StopLoss:0.04357  GradNorm:0.00666  GradNormST:0.02212  AvgTextLen:8.3  AvgSpecLen:44.0  StepTime:0.33  LR:0.000100\n",
      "   | > Step:18/68  GlobalStep:130360  TotalLoss:0.00153  PostnetLoss:0.00075  DecoderLoss:0.00078  StopLoss:0.04148  GradNorm:0.00401  GradNormST:0.01253  AvgTextLen:15.2  AvgSpecLen:64.5  StepTime:0.42  LR:0.000100\n",
      "   | > Step:28/68  GlobalStep:130370  TotalLoss:0.00175  PostnetLoss:0.00086  DecoderLoss:0.00090  StopLoss:0.04648  GradNorm:0.00320  GradNormST:0.01191  AvgTextLen:22.1  AvgSpecLen:85.4  StepTime:0.53  LR:0.000100\n",
      "   | > Step:38/68  GlobalStep:130380  TotalLoss:0.00189  PostnetLoss:0.00092  DecoderLoss:0.00097  StopLoss:0.03887  GradNorm:0.00390  GradNormST:0.01125  AvgTextLen:29.4  AvgSpecLen:104.1  StepTime:0.59  LR:0.000100\n",
      "   | > Step:48/68  GlobalStep:130390  TotalLoss:0.00206  PostnetLoss:0.00101  DecoderLoss:0.00106  StopLoss:0.03849  GradNorm:0.00435  GradNormST:0.01195  AvgTextLen:37.5  AvgSpecLen:123.8  StepTime:0.67  LR:0.000100\n",
      "   | > Step:58/68  GlobalStep:130400  TotalLoss:0.00217  PostnetLoss:0.00106  DecoderLoss:0.00111  StopLoss:0.04394  GradNorm:0.00352  GradNormST:0.01137  AvgTextLen:47.8  AvgSpecLen:158.0  StepTime:0.76  LR:0.000100\n",
      "   | > Step:68/68  GlobalStep:130410  TotalLoss:0.00250  PostnetLoss:0.00121  DecoderLoss:0.00129  StopLoss:0.04175  GradNorm:0.00409  GradNormST:0.02496  AvgTextLen:103.0  AvgSpecLen:308.9  StepTime:1.86  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:130410  AvgTotalLoss:0.04568  AvgPostnetLoss:0.00089  AvgDecoderLoss:0.00093  AvgStopLoss:0.04386  EpochTime:41.43  AvgStepTime:0.60\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.07407   PostnetLoss: 0.00573   DecoderLoss:0.00598  StopLoss: 0.06236  \n",
      "   | > TotalLoss: 0.09624   PostnetLoss: 0.00891   DecoderLoss:0.00924  StopLoss: 0.07808  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00089   Validation Loss: 0.00828\n",
      "\n",
      " > Epoch 890/1000\n",
      "   | > Step:9/68  GlobalStep:130420  TotalLoss:0.00135  PostnetLoss:0.00066  DecoderLoss:0.00069  StopLoss:0.03394  GradNorm:0.00390  GradNormST:0.01354  AvgTextLen:9.0  AvgSpecLen:42.1  StepTime:0.42  LR:0.000100\n",
      "   | > Step:19/68  GlobalStep:130430  TotalLoss:0.00154  PostnetLoss:0.00075  DecoderLoss:0.00079  StopLoss:0.04717  GradNorm:0.00399  GradNormST:0.01588  AvgTextLen:15.5  AvgSpecLen:63.2  StepTime:0.42  LR:0.000100\n",
      "   | > Step:29/68  GlobalStep:130440  TotalLoss:0.00170  PostnetLoss:0.00083  DecoderLoss:0.00087  StopLoss:0.05350  GradNorm:0.00318  GradNormST:0.01599  AvgTextLen:21.8  AvgSpecLen:83.4  StepTime:0.45  LR:0.000100\n",
      "   | > Step:39/68  GlobalStep:130450  TotalLoss:0.00191  PostnetLoss:0.00093  DecoderLoss:0.00098  StopLoss:0.03798  GradNorm:0.00296  GradNormST:0.01219  AvgTextLen:30.2  AvgSpecLen:110.4  StepTime:0.52  LR:0.000100\n",
      "   | > Step:49/68  GlobalStep:130460  TotalLoss:0.00202  PostnetLoss:0.00099  DecoderLoss:0.00104  StopLoss:0.04334  GradNorm:0.00378  GradNormST:0.01203  AvgTextLen:38.2  AvgSpecLen:127.7  StepTime:0.71  LR:0.000100\n",
      "   | > Step:59/68  GlobalStep:130470  TotalLoss:0.00218  PostnetLoss:0.00107  DecoderLoss:0.00112  StopLoss:0.05956  GradNorm:0.00369  GradNormST:0.02161  AvgTextLen:50.5  AvgSpecLen:155.9  StepTime:0.74  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:130479  AvgTotalLoss:0.05021  AvgPostnetLoss:0.00088  AvgDecoderLoss:0.00093  AvgStopLoss:0.04840  EpochTime:42.51  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.07417   PostnetLoss: 0.00574   DecoderLoss:0.00598  StopLoss: 0.06245  \n",
      "   | > TotalLoss: 0.10668   PostnetLoss: 0.00939   DecoderLoss:0.00971  StopLoss: 0.08758  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00088   Validation Loss: 0.00852\n",
      "\n",
      " > Epoch 891/1000\n",
      "   | > Step:0/68  GlobalStep:130480  TotalLoss:0.00137  PostnetLoss:0.00066  DecoderLoss:0.00071  StopLoss:0.06038  GradNorm:0.00724  GradNormST:0.03854  AvgTextLen:3.0  AvgSpecLen:25.3  StepTime:0.54  LR:0.000100\n",
      "   | > Step:10/68  GlobalStep:130490  TotalLoss:0.00143  PostnetLoss:0.00070  DecoderLoss:0.00073  StopLoss:0.06730  GradNorm:0.00342  GradNormST:0.05166  AvgTextLen:9.8  AvgSpecLen:48.6  StepTime:0.52  LR:0.000100\n",
      "   | > Step:20/68  GlobalStep:130500  TotalLoss:0.00162  PostnetLoss:0.00080  DecoderLoss:0.00083  StopLoss:0.04582  GradNorm:0.00364  GradNormST:0.01927  AvgTextLen:16.4  AvgSpecLen:64.8  StepTime:0.52  LR:0.000100\n",
      "   | > Step:30/68  GlobalStep:130510  TotalLoss:0.00174  PostnetLoss:0.00085  DecoderLoss:0.00089  StopLoss:0.05244  GradNorm:0.00315  GradNormST:0.02434  AvgTextLen:22.9  AvgSpecLen:85.8  StepTime:0.46  LR:0.000100\n",
      "   | > Step:40/68  GlobalStep:130520  TotalLoss:0.00189  PostnetLoss:0.00092  DecoderLoss:0.00096  StopLoss:0.05070  GradNorm:0.00289  GradNormST:0.01313  AvgTextLen:30.9  AvgSpecLen:105.3  StepTime:0.61  LR:0.000100\n",
      "   | > Step:50/68  GlobalStep:130530  TotalLoss:0.00206  PostnetLoss:0.00100  DecoderLoss:0.00106  StopLoss:0.03941  GradNorm:0.00305  GradNormST:0.01567  AvgTextLen:38.8  AvgSpecLen:132.0  StepTime:0.79  LR:0.000100\n",
      "   | > Step:60/68  GlobalStep:130540  TotalLoss:0.00224  PostnetLoss:0.00110  DecoderLoss:0.00115  StopLoss:0.03469  GradNorm:0.00356  GradNormST:0.00951  AvgTextLen:52.0  AvgSpecLen:172.7  StepTime:0.79  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:130548  AvgTotalLoss:0.05234  AvgPostnetLoss:0.00088  AvgDecoderLoss:0.00093  AvgStopLoss:0.05053  EpochTime:41.76  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.07004   PostnetLoss: 0.00579   DecoderLoss:0.00604  StopLoss: 0.05821  \n",
      "   | > TotalLoss: 0.18027   PostnetLoss: 0.00931   DecoderLoss:0.00963  StopLoss: 0.16133  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00088   Validation Loss: 0.00842\n",
      "\n",
      " > Epoch 892/1000\n",
      "   | > Step:1/68  GlobalStep:130550  TotalLoss:0.00135  PostnetLoss:0.00066  DecoderLoss:0.00070  StopLoss:0.03625  GradNorm:0.00539  GradNormST:0.01514  AvgTextLen:3.7  AvgSpecLen:28.6  StepTime:0.32  LR:0.000100\n",
      "   | > Step:11/68  GlobalStep:130560  TotalLoss:0.00139  PostnetLoss:0.00068  DecoderLoss:0.00071  StopLoss:0.04843  GradNorm:0.00383  GradNormST:0.02474  AvgTextLen:10.3  AvgSpecLen:49.4  StepTime:0.30  LR:0.000100\n",
      "   | > Step:21/68  GlobalStep:130570  TotalLoss:0.00159  PostnetLoss:0.00078  DecoderLoss:0.00081  StopLoss:0.04590  GradNorm:0.00332  GradNormST:0.01296  AvgTextLen:16.6  AvgSpecLen:66.3  StepTime:0.42  LR:0.000100\n",
      "   | > Step:31/68  GlobalStep:130580  TotalLoss:0.00172  PostnetLoss:0.00084  DecoderLoss:0.00088  StopLoss:0.04894  GradNorm:0.00305  GradNormST:0.01812  AvgTextLen:24.1  AvgSpecLen:82.7  StepTime:0.54  LR:0.000100\n",
      "   | > Step:41/68  GlobalStep:130590  TotalLoss:0.00195  PostnetLoss:0.00095  DecoderLoss:0.00100  StopLoss:0.03867  GradNorm:0.00311  GradNormST:0.01087  AvgTextLen:32.3  AvgSpecLen:109.4  StepTime:0.53  LR:0.000100\n",
      "   | > Step:51/68  GlobalStep:130600  TotalLoss:0.00202  PostnetLoss:0.00098  DecoderLoss:0.00103  StopLoss:0.06341  GradNorm:0.00315  GradNormST:0.02923  AvgTextLen:39.2  AvgSpecLen:126.9  StepTime:0.61  LR:0.000100\n",
      "   | > Step:61/68  GlobalStep:130610  TotalLoss:0.00220  PostnetLoss:0.00107  DecoderLoss:0.00113  StopLoss:0.05968  GradNorm:0.00311  GradNormST:0.01797  AvgTextLen:54.6  AvgSpecLen:165.5  StepTime:0.92  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:130617  AvgTotalLoss:0.04995  AvgPostnetLoss:0.00088  AvgDecoderLoss:0.00092  AvgStopLoss:0.04815  EpochTime:42.41  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.06763   PostnetLoss: 0.00563   DecoderLoss:0.00588  StopLoss: 0.05613  \n",
      "   | > TotalLoss: 0.12084   PostnetLoss: 0.00919   DecoderLoss:0.00950  StopLoss: 0.10216  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00088   Validation Loss: 0.00838\n",
      "\n",
      " > Epoch 893/1000\n",
      "   | > Step:2/68  GlobalStep:130620  TotalLoss:0.00116  PostnetLoss:0.00057  DecoderLoss:0.00059  StopLoss:0.03706  GradNorm:0.00409  GradNormST:0.01608  AvgTextLen:4.0  AvgSpecLen:31.3  StepTime:0.31  LR:0.000100\n",
      "   | > Step:12/68  GlobalStep:130630  TotalLoss:0.00138  PostnetLoss:0.00067  DecoderLoss:0.00070  StopLoss:0.03935  GradNorm:0.00341  GradNormST:0.01105  AvgTextLen:11.0  AvgSpecLen:49.3  StepTime:0.44  LR:0.000100\n",
      "   | > Step:22/68  GlobalStep:130640  TotalLoss:0.00162  PostnetLoss:0.00079  DecoderLoss:0.00083  StopLoss:0.05412  GradNorm:0.00342  GradNormST:0.01467  AvgTextLen:18.0  AvgSpecLen:72.0  StepTime:0.45  LR:0.000100\n",
      "   | > Step:32/68  GlobalStep:130650  TotalLoss:0.00177  PostnetLoss:0.00086  DecoderLoss:0.00090  StopLoss:0.06387  GradNorm:0.00304  GradNormST:0.02785  AvgTextLen:24.4  AvgSpecLen:91.5  StepTime:0.57  LR:0.000100\n",
      "   | > Step:42/68  GlobalStep:130660  TotalLoss:0.00191  PostnetLoss:0.00093  DecoderLoss:0.00098  StopLoss:0.04766  GradNorm:0.00286  GradNormST:0.02278  AvgTextLen:33.1  AvgSpecLen:109.7  StepTime:0.58  LR:0.000100\n",
      "   | > Step:52/68  GlobalStep:130670  TotalLoss:0.00208  PostnetLoss:0.00102  DecoderLoss:0.00107  StopLoss:0.05117  GradNorm:0.00373  GradNormST:0.01188  AvgTextLen:42.1  AvgSpecLen:137.5  StepTime:0.67  LR:0.000100\n",
      "   | > Step:62/68  GlobalStep:130680  TotalLoss:0.00228  PostnetLoss:0.00111  DecoderLoss:0.00117  StopLoss:0.04139  GradNorm:0.00297  GradNormST:0.01188  AvgTextLen:54.0  AvgSpecLen:174.7  StepTime:0.76  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:130686  AvgTotalLoss:0.04903  AvgPostnetLoss:0.00088  AvgDecoderLoss:0.00092  AvgStopLoss:0.04723  EpochTime:42.68  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.07278   PostnetLoss: 0.00588   DecoderLoss:0.00613  StopLoss: 0.06077  \n",
      "   | > TotalLoss: 0.14504   PostnetLoss: 0.00932   DecoderLoss:0.00963  StopLoss: 0.12609  \n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00088   Validation Loss: 0.00845\n",
      "\n",
      " > Epoch 894/1000\n",
      "   | > Step:3/68  GlobalStep:130690  TotalLoss:0.00120  PostnetLoss:0.00059  DecoderLoss:0.00061  StopLoss:0.08616  GradNorm:0.00362  GradNormST:0.02387  AvgTextLen:4.7  AvgSpecLen:29.7  StepTime:0.32  LR:0.000100\n",
      "   | > Step:13/68  GlobalStep:130700  TotalLoss:0.00141  PostnetLoss:0.00069  DecoderLoss:0.00072  StopLoss:0.05226  GradNorm:0.00391  GradNormST:0.02075  AvgTextLen:11.5  AvgSpecLen:55.2  StepTime:0.43  LR:0.000100\n",
      "   | > Step:23/68  GlobalStep:130710  TotalLoss:0.00165  PostnetLoss:0.00080  DecoderLoss:0.00084  StopLoss:0.04242  GradNorm:0.00326  GradNormST:0.01665  AvgTextLen:18.3  AvgSpecLen:72.2  StepTime:0.34  LR:0.000100\n",
      "   | > Step:33/68  GlobalStep:130720  TotalLoss:0.00181  PostnetLoss:0.00088  DecoderLoss:0.00092  StopLoss:0.05209  GradNorm:0.00301  GradNormST:0.01243  AvgTextLen:25.9  AvgSpecLen:88.9  StepTime:0.44  LR:0.000100\n",
      "   | > Step:43/68  GlobalStep:130730  TotalLoss:0.00192  PostnetLoss:0.00093  DecoderLoss:0.00098  StopLoss:0.04637  GradNorm:0.00286  GradNormST:0.01347  AvgTextLen:33.4  AvgSpecLen:115.7  StepTime:0.63  LR:0.000100\n",
      "   | > Step:53/68  GlobalStep:130740  TotalLoss:0.00205  PostnetLoss:0.00100  DecoderLoss:0.00105  StopLoss:0.03203  GradNorm:0.00352  GradNormST:0.00880  AvgTextLen:42.4  AvgSpecLen:137.6  StepTime:0.81  LR:0.000100\n",
      "   | > Step:63/68  GlobalStep:130750  TotalLoss:0.00230  PostnetLoss:0.00112  DecoderLoss:0.00118  StopLoss:0.05221  GradNorm:0.00301  GradNormST:0.02579  AvgTextLen:58.6  AvgSpecLen:185.1  StepTime:0.91  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:130755  AvgTotalLoss:0.04720  AvgPostnetLoss:0.00087  AvgDecoderLoss:0.00092  AvgStopLoss:0.04541  EpochTime:42.44  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.07338   PostnetLoss: 0.00584   DecoderLoss:0.00608  StopLoss: 0.06146  \n",
      "   | > TotalLoss: 0.09763   PostnetLoss: 0.00955   DecoderLoss:0.00986  StopLoss: 0.07822  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00087   Validation Loss: 0.00871\n",
      "\n",
      " > Epoch 895/1000\n",
      "   | > Step:4/68  GlobalStep:130760  TotalLoss:0.00124  PostnetLoss:0.00060  DecoderLoss:0.00064  StopLoss:0.05929  GradNorm:0.00364  GradNormST:0.02548  AvgTextLen:4.8  AvgSpecLen:31.1  StepTime:0.26  LR:0.000100\n",
      "   | > Step:14/68  GlobalStep:130770  TotalLoss:0.00145  PostnetLoss:0.00071  DecoderLoss:0.00074  StopLoss:0.03789  GradNorm:0.00377  GradNormST:0.01467  AvgTextLen:12.2  AvgSpecLen:52.0  StepTime:0.36  LR:0.000100\n",
      "   | > Step:24/68  GlobalStep:130780  TotalLoss:0.00171  PostnetLoss:0.00084  DecoderLoss:0.00087  StopLoss:0.04107  GradNorm:0.00358  GradNormST:0.01145  AvgTextLen:19.2  AvgSpecLen:79.0  StepTime:0.40  LR:0.000100\n",
      "   | > Step:34/68  GlobalStep:130790  TotalLoss:0.00180  PostnetLoss:0.00088  DecoderLoss:0.00092  StopLoss:0.03259  GradNorm:0.00294  GradNormST:0.01021  AvgTextLen:26.2  AvgSpecLen:95.9  StepTime:0.48  LR:0.000100\n",
      "   | > Step:44/68  GlobalStep:130800  TotalLoss:0.00196  PostnetLoss:0.00096  DecoderLoss:0.00100  StopLoss:0.03256  GradNorm:0.00286  GradNormST:0.01050  AvgTextLen:33.8  AvgSpecLen:116.3  StepTime:0.70  LR:0.000100\n",
      "   | > Step:54/68  GlobalStep:130810  TotalLoss:0.00210  PostnetLoss:0.00102  DecoderLoss:0.00108  StopLoss:0.03175  GradNorm:0.00310  GradNormST:0.00789  AvgTextLen:42.2  AvgSpecLen:144.9  StepTime:0.69  LR:0.000100\n",
      "   | > Step:64/68  GlobalStep:130820  TotalLoss:0.00220  PostnetLoss:0.00107  DecoderLoss:0.00113  StopLoss:0.05272  GradNorm:0.00279  GradNormST:0.02033  AvgTextLen:60.6  AvgSpecLen:177.6  StepTime:0.81  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:130824  AvgTotalLoss:0.04699  AvgPostnetLoss:0.00087  AvgDecoderLoss:0.00092  AvgStopLoss:0.04520  EpochTime:41.45  AvgStepTime:0.60\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.07132   PostnetLoss: 0.00575   DecoderLoss:0.00599  StopLoss: 0.05957  \n",
      "   | > TotalLoss: 0.08287   PostnetLoss: 0.00940   DecoderLoss:0.00970  StopLoss: 0.06377  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00087   Validation Loss: 0.00854\n",
      "\n",
      " > Epoch 896/1000\n",
      "   | > Step:5/68  GlobalStep:130830  TotalLoss:0.00126  PostnetLoss:0.00061  DecoderLoss:0.00065  StopLoss:0.03459  GradNorm:0.00314  GradNormST:0.01179  AvgTextLen:5.9  AvgSpecLen:33.2  StepTime:0.30  LR:0.000100\n",
      "   | > Step:15/68  GlobalStep:130840  TotalLoss:0.00151  PostnetLoss:0.00074  DecoderLoss:0.00077  StopLoss:0.02937  GradNorm:0.00353  GradNormST:0.01170  AvgTextLen:12.8  AvgSpecLen:59.1  StepTime:0.54  LR:0.000100\n",
      "   | > Step:25/68  GlobalStep:130850  TotalLoss:0.00165  PostnetLoss:0.00081  DecoderLoss:0.00085  StopLoss:0.04416  GradNorm:0.00336  GradNormST:0.01159  AvgTextLen:19.8  AvgSpecLen:73.3  StepTime:0.48  LR:0.000100\n",
      "   | > Step:35/68  GlobalStep:130860  TotalLoss:0.00182  PostnetLoss:0.00089  DecoderLoss:0.00093  StopLoss:0.03925  GradNorm:0.00298  GradNormST:0.01358  AvgTextLen:27.1  AvgSpecLen:104.7  StepTime:0.75  LR:0.000100\n",
      "   | > Step:45/68  GlobalStep:130870  TotalLoss:0.00194  PostnetLoss:0.00095  DecoderLoss:0.00099  StopLoss:0.04264  GradNorm:0.00271  GradNormST:0.01595  AvgTextLen:34.4  AvgSpecLen:118.6  StepTime:0.71  LR:0.000100\n",
      "   | > Step:55/68  GlobalStep:130880  TotalLoss:0.00214  PostnetLoss:0.00104  DecoderLoss:0.00110  StopLoss:0.03700  GradNorm:0.00287  GradNormST:0.01148  AvgTextLen:43.9  AvgSpecLen:145.2  StepTime:0.78  LR:0.000100\n",
      "   | > Step:65/68  GlobalStep:130890  TotalLoss:0.00235  PostnetLoss:0.00114  DecoderLoss:0.00121  StopLoss:0.08114  GradNorm:0.00258  GradNormST:0.04717  AvgTextLen:64.2  AvgSpecLen:207.4  StepTime:1.19  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:130893  AvgTotalLoss:0.04569  AvgPostnetLoss:0.00087  AvgDecoderLoss:0.00091  AvgStopLoss:0.04390  EpochTime:42.44  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.07185   PostnetLoss: 0.00581   DecoderLoss:0.00605  StopLoss: 0.05999  \n",
      "   | > TotalLoss: 0.11397   PostnetLoss: 0.00977   DecoderLoss:0.01007  StopLoss: 0.09413  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00087   Validation Loss: 0.00874\n",
      "\n",
      " > Epoch 897/1000\n",
      "   | > Step:6/68  GlobalStep:130900  TotalLoss:0.00125  PostnetLoss:0.00061  DecoderLoss:0.00064  StopLoss:0.03304  GradNorm:0.00339  GradNormST:0.01144  AvgTextLen:6.0  AvgSpecLen:34.8  StepTime:0.41  LR:0.000100\n",
      "   | > Step:16/68  GlobalStep:130910  TotalLoss:0.00154  PostnetLoss:0.00075  DecoderLoss:0.00079  StopLoss:0.04035  GradNorm:0.00357  GradNormST:0.01384  AvgTextLen:13.8  AvgSpecLen:60.6  StepTime:0.37  LR:0.000100\n",
      "   | > Step:26/68  GlobalStep:130920  TotalLoss:0.00170  PostnetLoss:0.00083  DecoderLoss:0.00087  StopLoss:0.04234  GradNorm:0.00381  GradNormST:0.01180  AvgTextLen:20.3  AvgSpecLen:77.5  StepTime:0.42  LR:0.000100\n",
      "   | > Step:36/68  GlobalStep:130930  TotalLoss:0.00178  PostnetLoss:0.00087  DecoderLoss:0.00091  StopLoss:0.04296  GradNorm:0.00323  GradNormST:0.01370  AvgTextLen:28.5  AvgSpecLen:100.2  StepTime:0.57  LR:0.000100\n",
      "   | > Step:46/68  GlobalStep:130940  TotalLoss:0.00192  PostnetLoss:0.00094  DecoderLoss:0.00098  StopLoss:0.04788  GradNorm:0.00281  GradNormST:0.01670  AvgTextLen:35.4  AvgSpecLen:122.6  StepTime:0.78  LR:0.000100\n",
      "   | > Step:56/68  GlobalStep:130950  TotalLoss:0.00210  PostnetLoss:0.00102  DecoderLoss:0.00108  StopLoss:0.04336  GradNorm:0.00281  GradNormST:0.01794  AvgTextLen:45.8  AvgSpecLen:145.4  StepTime:1.04  LR:0.000100\n",
      "   | > Step:66/68  GlobalStep:130960  TotalLoss:0.00245  PostnetLoss:0.00119  DecoderLoss:0.00126  StopLoss:0.05139  GradNorm:0.00245  GradNormST:0.02192  AvgTextLen:71.2  AvgSpecLen:220.8  StepTime:1.04  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:130962  AvgTotalLoss:0.04672  AvgPostnetLoss:0.00087  AvgDecoderLoss:0.00091  AvgStopLoss:0.04494  EpochTime:42.58  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.07424   PostnetLoss: 0.00582   DecoderLoss:0.00607  StopLoss: 0.06235  \n",
      "   | > TotalLoss: 0.12966   PostnetLoss: 0.00969   DecoderLoss:0.01001  StopLoss: 0.10995  \n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00087   Validation Loss: 0.00869\n",
      "\n",
      " > Epoch 898/1000\n",
      "   | > Step:7/68  GlobalStep:130970  TotalLoss:0.00134  PostnetLoss:0.00066  DecoderLoss:0.00069  StopLoss:0.02993  GradNorm:0.00344  GradNormST:0.01314  AvgTextLen:7.4  AvgSpecLen:44.6  StepTime:0.40  LR:0.000100\n",
      "   | > Step:17/68  GlobalStep:130980  TotalLoss:0.00150  PostnetLoss:0.00074  DecoderLoss:0.00077  StopLoss:0.04191  GradNorm:0.00330  GradNormST:0.01188  AvgTextLen:14.7  AvgSpecLen:66.0  StepTime:0.43  LR:0.000100\n",
      "   | > Step:27/68  GlobalStep:130990  TotalLoss:0.00174  PostnetLoss:0.00085  DecoderLoss:0.00089  StopLoss:0.06807  GradNorm:0.00375  GradNormST:0.01952  AvgTextLen:21.0  AvgSpecLen:80.2  StepTime:0.62  LR:0.000100\n",
      "   | > Step:37/68  GlobalStep:131000  TotalLoss:0.00185  PostnetLoss:0.00091  DecoderLoss:0.00095  StopLoss:0.04413  GradNorm:0.00313  GradNormST:0.01783  AvgTextLen:29.3  AvgSpecLen:102.4  StepTime:0.62  LR:0.000100\n",
      " | | > Checkpoint saving : /vm/TTS/models/cartman_models/mozilla-no-loc-fattn-stopnet-sigmoid-loss_masking-August-15-2019_11+26AM-de6aeb9/checkpoint_131000.pth.tar\n",
      "   | > Step:47/68  GlobalStep:131010  TotalLoss:0.00199  PostnetLoss:0.00097  DecoderLoss:0.00102  StopLoss:0.02713  GradNorm:0.00272  GradNormST:0.00904  AvgTextLen:36.7  AvgSpecLen:126.9  StepTime:0.70  LR:0.000100\n",
      "   | > Step:57/68  GlobalStep:131020  TotalLoss:0.00215  PostnetLoss:0.00105  DecoderLoss:0.00110  StopLoss:0.03866  GradNorm:0.00262  GradNormST:0.01093  AvgTextLen:47.1  AvgSpecLen:161.0  StepTime:0.88  LR:0.000100\n",
      "   | > Step:67/68  GlobalStep:131030  TotalLoss:0.00250  PostnetLoss:0.00121  DecoderLoss:0.00129  StopLoss:0.05759  GradNorm:0.00254  GradNormST:0.03216  AvgTextLen:80.6  AvgSpecLen:246.5  StepTime:1.21  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:131031  AvgTotalLoss:0.04520  AvgPostnetLoss:0.00087  AvgDecoderLoss:0.00091  AvgStopLoss:0.04342  EpochTime:42.64  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.07370   PostnetLoss: 0.00571   DecoderLoss:0.00595  StopLoss: 0.06204  \n",
      "   | > TotalLoss: 0.10217   PostnetLoss: 0.00979   DecoderLoss:0.01010  StopLoss: 0.08229  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00087   Validation Loss: 0.00880\n",
      "\n",
      " > Epoch 899/1000\n",
      "   | > Step:8/68  GlobalStep:131040  TotalLoss:0.00137  PostnetLoss:0.00067  DecoderLoss:0.00070  StopLoss:0.03573  GradNorm:0.00385  GradNormST:0.01081  AvgTextLen:8.3  AvgSpecLen:44.0  StepTime:0.32  LR:0.000100\n",
      "   | > Step:18/68  GlobalStep:131050  TotalLoss:0.00152  PostnetLoss:0.00074  DecoderLoss:0.00078  StopLoss:0.04346  GradNorm:0.00317  GradNormST:0.01608  AvgTextLen:15.2  AvgSpecLen:64.5  StepTime:0.43  LR:0.000100\n",
      "   | > Step:28/68  GlobalStep:131060  TotalLoss:0.00171  PostnetLoss:0.00084  DecoderLoss:0.00088  StopLoss:0.03652  GradNorm:0.00343  GradNormST:0.01189  AvgTextLen:22.1  AvgSpecLen:85.4  StepTime:0.57  LR:0.000100\n",
      "   | > Step:38/68  GlobalStep:131070  TotalLoss:0.00183  PostnetLoss:0.00089  DecoderLoss:0.00093  StopLoss:0.03500  GradNorm:0.00342  GradNormST:0.01124  AvgTextLen:29.4  AvgSpecLen:104.1  StepTime:0.64  LR:0.000100\n",
      "   | > Step:48/68  GlobalStep:131080  TotalLoss:0.00196  PostnetLoss:0.00096  DecoderLoss:0.00100  StopLoss:0.03058  GradNorm:0.00281  GradNormST:0.00838  AvgTextLen:37.5  AvgSpecLen:123.8  StepTime:0.67  LR:0.000100\n",
      "   | > Step:58/68  GlobalStep:131090  TotalLoss:0.00216  PostnetLoss:0.00105  DecoderLoss:0.00111  StopLoss:0.04988  GradNorm:0.00264  GradNormST:0.01424  AvgTextLen:47.8  AvgSpecLen:158.0  StepTime:0.92  LR:0.000100\n",
      "   | > Step:68/68  GlobalStep:131100  TotalLoss:0.00246  PostnetLoss:0.00120  DecoderLoss:0.00127  StopLoss:0.04859  GradNorm:0.00253  GradNormST:0.03255  AvgTextLen:103.0  AvgSpecLen:308.9  StepTime:1.87  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:131100  AvgTotalLoss:0.04637  AvgPostnetLoss:0.00087  AvgDecoderLoss:0.00092  AvgStopLoss:0.04458  EpochTime:43.40  AvgStepTime:0.63\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.07565   PostnetLoss: 0.00574   DecoderLoss:0.00599  StopLoss: 0.06392  \n",
      "   | > TotalLoss: 0.07678   PostnetLoss: 0.00953   DecoderLoss:0.00984  StopLoss: 0.05741  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00087   Validation Loss: 0.00869\n",
      "\n",
      " > Epoch 900/1000\n",
      "   | > Step:9/68  GlobalStep:131110  TotalLoss:0.00133  PostnetLoss:0.00065  DecoderLoss:0.00068  StopLoss:0.03694  GradNorm:0.00334  GradNormST:0.01458  AvgTextLen:9.0  AvgSpecLen:42.1  StepTime:0.42  LR:0.000100\n",
      "   | > Step:19/68  GlobalStep:131120  TotalLoss:0.00152  PostnetLoss:0.00074  DecoderLoss:0.00078  StopLoss:0.05279  GradNorm:0.00334  GradNormST:0.01530  AvgTextLen:15.5  AvgSpecLen:63.2  StepTime:0.43  LR:0.000100\n",
      "   | > Step:29/68  GlobalStep:131130  TotalLoss:0.00169  PostnetLoss:0.00082  DecoderLoss:0.00086  StopLoss:0.05107  GradNorm:0.00346  GradNormST:0.01206  AvgTextLen:21.8  AvgSpecLen:83.4  StepTime:0.48  LR:0.000100\n",
      "   | > Step:39/68  GlobalStep:131140  TotalLoss:0.00188  PostnetLoss:0.00092  DecoderLoss:0.00096  StopLoss:0.02952  GradNorm:0.00328  GradNormST:0.01102  AvgTextLen:30.2  AvgSpecLen:110.4  StepTime:0.75  LR:0.000100\n",
      "   | > Step:49/68  GlobalStep:131150  TotalLoss:0.00201  PostnetLoss:0.00098  DecoderLoss:0.00103  StopLoss:0.04033  GradNorm:0.00301  GradNormST:0.01006  AvgTextLen:38.2  AvgSpecLen:127.7  StepTime:0.68  LR:0.000100\n",
      "   | > Step:59/68  GlobalStep:131160  TotalLoss:0.00214  PostnetLoss:0.00105  DecoderLoss:0.00110  StopLoss:0.05279  GradNorm:0.00258  GradNormST:0.01734  AvgTextLen:50.5  AvgSpecLen:155.9  StepTime:0.84  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:131169  AvgTotalLoss:0.04643  AvgPostnetLoss:0.00087  AvgDecoderLoss:0.00091  AvgStopLoss:0.04465  EpochTime:42.19  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.07228   PostnetLoss: 0.00570   DecoderLoss:0.00594  StopLoss: 0.06064  \n",
      "   | > TotalLoss: 0.07777   PostnetLoss: 0.00952   DecoderLoss:0.00983  StopLoss: 0.05842  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00087   Validation Loss: 0.00857\n",
      "\n",
      " > Epoch 901/1000\n",
      "   | > Step:0/68  GlobalStep:131170  TotalLoss:0.00129  PostnetLoss:0.00062  DecoderLoss:0.00067  StopLoss:0.06943  GradNorm:0.00457  GradNormST:0.02177  AvgTextLen:3.0  AvgSpecLen:25.3  StepTime:0.63  LR:0.000100\n",
      "   | > Step:10/68  GlobalStep:131180  TotalLoss:0.00138  PostnetLoss:0.00067  DecoderLoss:0.00071  StopLoss:0.02573  GradNorm:0.00322  GradNormST:0.00945  AvgTextLen:9.8  AvgSpecLen:48.6  StepTime:0.53  LR:0.000100\n",
      "   | > Step:20/68  GlobalStep:131190  TotalLoss:0.00160  PostnetLoss:0.00078  DecoderLoss:0.00082  StopLoss:0.03320  GradNorm:0.00320  GradNormST:0.01053  AvgTextLen:16.4  AvgSpecLen:64.8  StepTime:0.39  LR:0.000100\n",
      "   | > Step:30/68  GlobalStep:131200  TotalLoss:0.00173  PostnetLoss:0.00085  DecoderLoss:0.00088  StopLoss:0.04442  GradNorm:0.00349  GradNormST:0.01024  AvgTextLen:22.9  AvgSpecLen:85.8  StepTime:0.46  LR:0.000100\n",
      "   | > Step:40/68  GlobalStep:131210  TotalLoss:0.00186  PostnetLoss:0.00091  DecoderLoss:0.00095  StopLoss:0.03898  GradNorm:0.00346  GradNormST:0.01360  AvgTextLen:30.9  AvgSpecLen:105.3  StepTime:0.60  LR:0.000100\n",
      "   | > Step:50/68  GlobalStep:131220  TotalLoss:0.00203  PostnetLoss:0.00099  DecoderLoss:0.00104  StopLoss:0.03671  GradNorm:0.00304  GradNormST:0.01295  AvgTextLen:38.8  AvgSpecLen:132.0  StepTime:0.67  LR:0.000100\n",
      "   | > Step:60/68  GlobalStep:131230  TotalLoss:0.00222  PostnetLoss:0.00108  DecoderLoss:0.00114  StopLoss:0.02949  GradNorm:0.00293  GradNormST:0.00765  AvgTextLen:52.0  AvgSpecLen:172.7  StepTime:1.00  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:131238  AvgTotalLoss:0.04613  AvgPostnetLoss:0.00087  AvgDecoderLoss:0.00091  AvgStopLoss:0.04435  EpochTime:42.30  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.07242   PostnetLoss: 0.00561   DecoderLoss:0.00586  StopLoss: 0.06094  \n",
      "   | > TotalLoss: 0.08926   PostnetLoss: 0.00957   DecoderLoss:0.00990  StopLoss: 0.06978  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00087   Validation Loss: 0.00868\n",
      "\n",
      " > Epoch 902/1000\n",
      "   | > Step:1/68  GlobalStep:131240  TotalLoss:0.00128  PostnetLoss:0.00062  DecoderLoss:0.00065  StopLoss:0.04068  GradNorm:0.00497  GradNormST:0.01535  AvgTextLen:3.7  AvgSpecLen:28.6  StepTime:0.33  LR:0.000100\n",
      "   | > Step:11/68  GlobalStep:131250  TotalLoss:0.00137  PostnetLoss:0.00067  DecoderLoss:0.00070  StopLoss:0.02623  GradNorm:0.00341  GradNormST:0.00862  AvgTextLen:10.3  AvgSpecLen:49.4  StepTime:0.40  LR:0.000100\n",
      "   | > Step:21/68  GlobalStep:131260  TotalLoss:0.00161  PostnetLoss:0.00079  DecoderLoss:0.00082  StopLoss:0.05226  GradNorm:0.00317  GradNormST:0.01705  AvgTextLen:16.6  AvgSpecLen:66.3  StepTime:0.43  LR:0.000100\n",
      "   | > Step:31/68  GlobalStep:131270  TotalLoss:0.00171  PostnetLoss:0.00083  DecoderLoss:0.00087  StopLoss:0.03992  GradNorm:0.00309  GradNormST:0.01266  AvgTextLen:24.1  AvgSpecLen:82.7  StepTime:0.43  LR:0.000100\n",
      "   | > Step:41/68  GlobalStep:131280  TotalLoss:0.00193  PostnetLoss:0.00095  DecoderLoss:0.00099  StopLoss:0.02308  GradNorm:0.00362  GradNormST:0.00733  AvgTextLen:32.3  AvgSpecLen:109.4  StepTime:0.67  LR:0.000100\n",
      "   | > Step:51/68  GlobalStep:131290  TotalLoss:0.00204  PostnetLoss:0.00099  DecoderLoss:0.00105  StopLoss:0.05518  GradNorm:0.00358  GradNormST:0.02116  AvgTextLen:39.2  AvgSpecLen:126.9  StepTime:0.62  LR:0.000100\n",
      "   | > Step:61/68  GlobalStep:131300  TotalLoss:0.00216  PostnetLoss:0.00105  DecoderLoss:0.00111  StopLoss:0.05874  GradNorm:0.00305  GradNormST:0.01647  AvgTextLen:54.6  AvgSpecLen:165.5  StepTime:0.90  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:131307  AvgTotalLoss:0.04839  AvgPostnetLoss:0.00087  AvgDecoderLoss:0.00091  AvgStopLoss:0.04660  EpochTime:41.84  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.07214   PostnetLoss: 0.00567   DecoderLoss:0.00592  StopLoss: 0.06055  \n",
      "   | > TotalLoss: 0.08248   PostnetLoss: 0.00978   DecoderLoss:0.01009  StopLoss: 0.06261  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00087   Validation Loss: 0.00873\n",
      "\n",
      " > Epoch 903/1000\n",
      "   | > Step:2/68  GlobalStep:131310  TotalLoss:0.00122  PostnetLoss:0.00059  DecoderLoss:0.00062  StopLoss:0.03690  GradNorm:0.00427  GradNormST:0.01229  AvgTextLen:4.0  AvgSpecLen:31.3  StepTime:0.52  LR:0.000100\n",
      "   | > Step:12/68  GlobalStep:131320  TotalLoss:0.00136  PostnetLoss:0.00066  DecoderLoss:0.00070  StopLoss:0.04151  GradNorm:0.00302  GradNormST:0.01326  AvgTextLen:11.0  AvgSpecLen:49.3  StepTime:0.42  LR:0.000100\n",
      "   | > Step:22/68  GlobalStep:131330  TotalLoss:0.00160  PostnetLoss:0.00078  DecoderLoss:0.00082  StopLoss:0.04291  GradNorm:0.00301  GradNormST:0.01309  AvgTextLen:18.0  AvgSpecLen:72.0  StepTime:0.42  LR:0.000100\n",
      "   | > Step:32/68  GlobalStep:131340  TotalLoss:0.00175  PostnetLoss:0.00085  DecoderLoss:0.00089  StopLoss:0.04841  GradNorm:0.00347  GradNormST:0.01340  AvgTextLen:24.4  AvgSpecLen:91.5  StepTime:0.59  LR:0.000100\n",
      "   | > Step:42/68  GlobalStep:131350  TotalLoss:0.00188  PostnetLoss:0.00092  DecoderLoss:0.00096  StopLoss:0.03584  GradNorm:0.00336  GradNormST:0.01873  AvgTextLen:33.1  AvgSpecLen:109.7  StepTime:0.52  LR:0.000100\n",
      "   | > Step:52/68  GlobalStep:131360  TotalLoss:0.00208  PostnetLoss:0.00101  DecoderLoss:0.00107  StopLoss:0.06740  GradNorm:0.00369  GradNormST:0.02341  AvgTextLen:42.1  AvgSpecLen:137.5  StepTime:0.77  LR:0.000100\n",
      "   | > Step:62/68  GlobalStep:131370  TotalLoss:0.00225  PostnetLoss:0.00110  DecoderLoss:0.00115  StopLoss:0.04733  GradNorm:0.00310  GradNormST:0.01792  AvgTextLen:54.0  AvgSpecLen:174.7  StepTime:0.75  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:131376  AvgTotalLoss:0.05082  AvgPostnetLoss:0.00087  AvgDecoderLoss:0.00091  AvgStopLoss:0.04903  EpochTime:42.72  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.07279   PostnetLoss: 0.00570   DecoderLoss:0.00595  StopLoss: 0.06114  \n",
      "   | > TotalLoss: 0.08442   PostnetLoss: 0.00981   DecoderLoss:0.01013  StopLoss: 0.06448  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00087   Validation Loss: 0.00878\n",
      "\n",
      " > Epoch 904/1000\n",
      "   | > Step:3/68  GlobalStep:131380  TotalLoss:0.00121  PostnetLoss:0.00059  DecoderLoss:0.00061  StopLoss:0.03938  GradNorm:0.00383  GradNormST:0.01143  AvgTextLen:4.7  AvgSpecLen:29.7  StepTime:0.31  LR:0.000100\n",
      "   | > Step:13/68  GlobalStep:131390  TotalLoss:0.00138  PostnetLoss:0.00068  DecoderLoss:0.00071  StopLoss:0.04215  GradNorm:0.00311  GradNormST:0.02082  AvgTextLen:11.5  AvgSpecLen:55.2  StepTime:0.32  LR:0.000100\n",
      "   | > Step:23/68  GlobalStep:131400  TotalLoss:0.00162  PostnetLoss:0.00079  DecoderLoss:0.00083  StopLoss:0.05133  GradNorm:0.00291  GradNormST:0.01639  AvgTextLen:18.3  AvgSpecLen:72.2  StepTime:0.34  LR:0.000100\n",
      "   | > Step:33/68  GlobalStep:131410  TotalLoss:0.00178  PostnetLoss:0.00087  DecoderLoss:0.00091  StopLoss:0.05812  GradNorm:0.00345  GradNormST:0.01634  AvgTextLen:25.9  AvgSpecLen:88.9  StepTime:0.57  LR:0.000100\n",
      "   | > Step:43/68  GlobalStep:131420  TotalLoss:0.00190  PostnetLoss:0.00093  DecoderLoss:0.00097  StopLoss:0.04889  GradNorm:0.00356  GradNormST:0.01332  AvgTextLen:33.4  AvgSpecLen:115.7  StepTime:0.66  LR:0.000100\n",
      "   | > Step:53/68  GlobalStep:131430  TotalLoss:0.00205  PostnetLoss:0.00100  DecoderLoss:0.00105  StopLoss:0.04081  GradNorm:0.00393  GradNormST:0.01034  AvgTextLen:42.4  AvgSpecLen:137.6  StepTime:0.67  LR:0.000100\n",
      "   | > Step:63/68  GlobalStep:131440  TotalLoss:0.00231  PostnetLoss:0.00112  DecoderLoss:0.00118  StopLoss:0.08188  GradNorm:0.00276  GradNormST:0.03923  AvgTextLen:58.6  AvgSpecLen:185.1  StepTime:0.91  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:131445  AvgTotalLoss:0.04841  AvgPostnetLoss:0.00087  AvgDecoderLoss:0.00091  AvgStopLoss:0.04663  EpochTime:42.52  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.07381   PostnetLoss: 0.00573   DecoderLoss:0.00599  StopLoss: 0.06210  \n",
      "   | > TotalLoss: 0.08948   PostnetLoss: 0.00998   DecoderLoss:0.01032  StopLoss: 0.06918  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00087   Validation Loss: 0.00895\n",
      "\n",
      " > Epoch 905/1000\n",
      "   | > Step:4/68  GlobalStep:131450  TotalLoss:0.00124  PostnetLoss:0.00060  DecoderLoss:0.00064  StopLoss:0.06940  GradNorm:0.00360  GradNormST:0.02320  AvgTextLen:4.8  AvgSpecLen:31.1  StepTime:0.27  LR:0.000100\n",
      "   | > Step:14/68  GlobalStep:131460  TotalLoss:0.00144  PostnetLoss:0.00070  DecoderLoss:0.00074  StopLoss:0.04144  GradNorm:0.00350  GradNormST:0.01881  AvgTextLen:12.2  AvgSpecLen:52.0  StepTime:0.33  LR:0.000100\n",
      "   | > Step:24/68  GlobalStep:131470  TotalLoss:0.00168  PostnetLoss:0.00082  DecoderLoss:0.00086  StopLoss:0.03642  GradNorm:0.00305  GradNormST:0.01202  AvgTextLen:19.2  AvgSpecLen:79.0  StepTime:0.51  LR:0.000100\n",
      "   | > Step:34/68  GlobalStep:131480  TotalLoss:0.00181  PostnetLoss:0.00088  DecoderLoss:0.00093  StopLoss:0.02335  GradNorm:0.00296  GradNormST:0.00833  AvgTextLen:26.2  AvgSpecLen:95.9  StepTime:0.52  LR:0.000100\n",
      "   | > Step:44/68  GlobalStep:131490  TotalLoss:0.00197  PostnetLoss:0.00096  DecoderLoss:0.00101  StopLoss:0.04127  GradNorm:0.00360  GradNormST:0.01050  AvgTextLen:33.8  AvgSpecLen:116.3  StepTime:0.53  LR:0.000100\n",
      "   | > Step:54/68  GlobalStep:131500  TotalLoss:0.00211  PostnetLoss:0.00103  DecoderLoss:0.00108  StopLoss:0.05878  GradNorm:0.00419  GradNormST:0.01728  AvgTextLen:42.2  AvgSpecLen:144.9  StepTime:0.81  LR:0.000100\n",
      "   | > Step:64/68  GlobalStep:131510  TotalLoss:0.00218  PostnetLoss:0.00106  DecoderLoss:0.00112  StopLoss:0.04394  GradNorm:0.00311  GradNormST:0.01308  AvgTextLen:60.6  AvgSpecLen:177.6  StepTime:0.80  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:131514  AvgTotalLoss:0.04902  AvgPostnetLoss:0.00087  AvgDecoderLoss:0.00091  AvgStopLoss:0.04723  EpochTime:42.35  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.07181   PostnetLoss: 0.00573   DecoderLoss:0.00600  StopLoss: 0.06009  \n",
      "   | > TotalLoss: 0.08724   PostnetLoss: 0.00958   DecoderLoss:0.00991  StopLoss: 0.06775  \n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00087   Validation Loss: 0.00868\n",
      "\n",
      " > Epoch 906/1000\n",
      "   | > Step:5/68  GlobalStep:131520  TotalLoss:0.00124  PostnetLoss:0.00060  DecoderLoss:0.00064  StopLoss:0.02952  GradNorm:0.00362  GradNormST:0.00917  AvgTextLen:5.9  AvgSpecLen:33.2  StepTime:0.30  LR:0.000100\n",
      "   | > Step:15/68  GlobalStep:131530  TotalLoss:0.00152  PostnetLoss:0.00074  DecoderLoss:0.00077  StopLoss:0.03281  GradNorm:0.00313  GradNormST:0.00931  AvgTextLen:12.8  AvgSpecLen:59.1  StepTime:0.51  LR:0.000100\n",
      "   | > Step:25/68  GlobalStep:131540  TotalLoss:0.00162  PostnetLoss:0.00079  DecoderLoss:0.00083  StopLoss:0.04141  GradNorm:0.00324  GradNormST:0.01130  AvgTextLen:19.8  AvgSpecLen:73.3  StepTime:0.45  LR:0.000100\n",
      "   | > Step:35/68  GlobalStep:131550  TotalLoss:0.00183  PostnetLoss:0.00090  DecoderLoss:0.00094  StopLoss:0.04320  GradNorm:0.00287  GradNormST:0.01377  AvgTextLen:27.1  AvgSpecLen:104.7  StepTime:0.74  LR:0.000100\n",
      "   | > Step:45/68  GlobalStep:131560  TotalLoss:0.00189  PostnetLoss:0.00092  DecoderLoss:0.00097  StopLoss:0.04372  GradNorm:0.00283  GradNormST:0.01473  AvgTextLen:34.4  AvgSpecLen:118.6  StepTime:0.57  LR:0.000100\n",
      "   | > Step:55/68  GlobalStep:131570  TotalLoss:0.00214  PostnetLoss:0.00104  DecoderLoss:0.00110  StopLoss:0.03578  GradNorm:0.00383  GradNormST:0.00818  AvgTextLen:43.9  AvgSpecLen:145.2  StepTime:0.66  LR:0.000100\n",
      "   | > Step:65/68  GlobalStep:131580  TotalLoss:0.00234  PostnetLoss:0.00114  DecoderLoss:0.00120  StopLoss:0.08773  GradNorm:0.00244  GradNormST:0.05179  AvgTextLen:64.2  AvgSpecLen:207.4  StepTime:1.18  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:131583  AvgTotalLoss:0.04666  AvgPostnetLoss:0.00087  AvgDecoderLoss:0.00091  AvgStopLoss:0.04487  EpochTime:42.23  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.07684   PostnetLoss: 0.00569   DecoderLoss:0.00595  StopLoss: 0.06520  \n",
      "   | > TotalLoss: 0.08983   PostnetLoss: 0.00999   DecoderLoss:0.01033  StopLoss: 0.06952  \n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00087   Validation Loss: 0.00890\n",
      "\n",
      " > Epoch 907/1000\n",
      "   | > Step:6/68  GlobalStep:131590  TotalLoss:0.00129  PostnetLoss:0.00063  DecoderLoss:0.00066  StopLoss:0.03529  GradNorm:0.00485  GradNormST:0.01489  AvgTextLen:6.0  AvgSpecLen:34.8  StepTime:0.43  LR:0.000100\n",
      "   | > Step:16/68  GlobalStep:131600  TotalLoss:0.00150  PostnetLoss:0.00073  DecoderLoss:0.00076  StopLoss:0.03466  GradNorm:0.00326  GradNormST:0.01440  AvgTextLen:13.8  AvgSpecLen:60.6  StepTime:0.36  LR:0.000100\n",
      "   | > Step:26/68  GlobalStep:131610  TotalLoss:0.00167  PostnetLoss:0.00082  DecoderLoss:0.00085  StopLoss:0.03281  GradNorm:0.00287  GradNormST:0.01149  AvgTextLen:20.3  AvgSpecLen:77.5  StepTime:0.47  LR:0.000100\n",
      "   | > Step:36/68  GlobalStep:131620  TotalLoss:0.00180  PostnetLoss:0.00088  DecoderLoss:0.00092  StopLoss:0.04362  GradNorm:0.00279  GradNormST:0.01084  AvgTextLen:28.5  AvgSpecLen:100.2  StepTime:0.61  LR:0.000100\n",
      "   | > Step:46/68  GlobalStep:131630  TotalLoss:0.00195  PostnetLoss:0.00095  DecoderLoss:0.00100  StopLoss:0.04536  GradNorm:0.00316  GradNormST:0.01420  AvgTextLen:35.4  AvgSpecLen:122.6  StepTime:0.69  LR:0.000100\n",
      "   | > Step:56/68  GlobalStep:131640  TotalLoss:0.00214  PostnetLoss:0.00104  DecoderLoss:0.00109  StopLoss:0.05292  GradNorm:0.00358  GradNormST:0.02207  AvgTextLen:45.8  AvgSpecLen:145.4  StepTime:1.02  LR:0.000100\n",
      "   | > Step:66/68  GlobalStep:131650  TotalLoss:0.00247  PostnetLoss:0.00120  DecoderLoss:0.00127  StopLoss:0.05610  GradNorm:0.00299  GradNormST:0.02435  AvgTextLen:71.2  AvgSpecLen:220.8  StepTime:1.04  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:131652  AvgTotalLoss:0.04713  AvgPostnetLoss:0.00087  AvgDecoderLoss:0.00091  AvgStopLoss:0.04535  EpochTime:42.64  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.07344   PostnetLoss: 0.00569   DecoderLoss:0.00594  StopLoss: 0.06181  \n",
      "   | > TotalLoss: 0.08625   PostnetLoss: 0.00989   DecoderLoss:0.01024  StopLoss: 0.06612  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00087   Validation Loss: 0.00902\n",
      "\n",
      " > Epoch 908/1000\n",
      "   | > Step:7/68  GlobalStep:131660  TotalLoss:0.00135  PostnetLoss:0.00066  DecoderLoss:0.00069  StopLoss:0.03261  GradNorm:0.00336  GradNormST:0.01359  AvgTextLen:7.4  AvgSpecLen:44.6  StepTime:0.40  LR:0.000100\n",
      "   | > Step:17/68  GlobalStep:131670  TotalLoss:0.00150  PostnetLoss:0.00074  DecoderLoss:0.00077  StopLoss:0.05826  GradNorm:0.00311  GradNormST:0.01842  AvgTextLen:14.7  AvgSpecLen:66.0  StepTime:0.44  LR:0.000100\n",
      "   | > Step:27/68  GlobalStep:131680  TotalLoss:0.00167  PostnetLoss:0.00082  DecoderLoss:0.00085  StopLoss:0.04589  GradNorm:0.00288  GradNormST:0.00906  AvgTextLen:21.0  AvgSpecLen:80.2  StepTime:0.64  LR:0.000100\n",
      "   | > Step:37/68  GlobalStep:131690  TotalLoss:0.00184  PostnetLoss:0.00090  DecoderLoss:0.00094  StopLoss:0.03859  GradNorm:0.00308  GradNormST:0.01191  AvgTextLen:29.3  AvgSpecLen:102.4  StepTime:0.59  LR:0.000100\n",
      "   | > Step:47/68  GlobalStep:131700  TotalLoss:0.00200  PostnetLoss:0.00098  DecoderLoss:0.00103  StopLoss:0.03935  GradNorm:0.00329  GradNormST:0.00948  AvgTextLen:36.7  AvgSpecLen:126.9  StepTime:0.73  LR:0.000100\n",
      "   | > Step:57/68  GlobalStep:131710  TotalLoss:0.00221  PostnetLoss:0.00108  DecoderLoss:0.00113  StopLoss:0.03111  GradNorm:0.00410  GradNormST:0.00825  AvgTextLen:47.1  AvgSpecLen:161.0  StepTime:0.84  LR:0.000100\n",
      "   | > Step:67/68  GlobalStep:131720  TotalLoss:0.00249  PostnetLoss:0.00121  DecoderLoss:0.00128  StopLoss:0.05550  GradNorm:0.00275  GradNormST:0.02918  AvgTextLen:80.6  AvgSpecLen:246.5  StepTime:1.23  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:131721  AvgTotalLoss:0.04485  AvgPostnetLoss:0.00087  AvgDecoderLoss:0.00091  AvgStopLoss:0.04307  EpochTime:42.34  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.07385   PostnetLoss: 0.00573   DecoderLoss:0.00598  StopLoss: 0.06214  \n",
      "   | > TotalLoss: 0.08986   PostnetLoss: 0.01024   DecoderLoss:0.01057  StopLoss: 0.06904  \n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00087   Validation Loss: 0.00916\n",
      "\n",
      " > Epoch 909/1000\n",
      "   | > Step:8/68  GlobalStep:131730  TotalLoss:0.00135  PostnetLoss:0.00066  DecoderLoss:0.00069  StopLoss:0.03905  GradNorm:0.00406  GradNormST:0.01088  AvgTextLen:8.3  AvgSpecLen:44.0  StepTime:0.27  LR:0.000100\n",
      "   | > Step:18/68  GlobalStep:131740  TotalLoss:0.00150  PostnetLoss:0.00073  DecoderLoss:0.00077  StopLoss:0.03475  GradNorm:0.00334  GradNormST:0.01031  AvgTextLen:15.2  AvgSpecLen:64.5  StepTime:0.38  LR:0.000100\n",
      "   | > Step:28/68  GlobalStep:131750  TotalLoss:0.00168  PostnetLoss:0.00082  DecoderLoss:0.00086  StopLoss:0.03493  GradNorm:0.00288  GradNormST:0.01436  AvgTextLen:22.1  AvgSpecLen:85.4  StepTime:0.45  LR:0.000100\n",
      "   | > Step:38/68  GlobalStep:131760  TotalLoss:0.00179  PostnetLoss:0.00087  DecoderLoss:0.00092  StopLoss:0.02958  GradNorm:0.00277  GradNormST:0.00787  AvgTextLen:29.4  AvgSpecLen:104.1  StepTime:0.65  LR:0.000100\n",
      "   | > Step:48/68  GlobalStep:131770  TotalLoss:0.00198  PostnetLoss:0.00097  DecoderLoss:0.00102  StopLoss:0.03673  GradNorm:0.00286  GradNormST:0.00866  AvgTextLen:37.5  AvgSpecLen:123.8  StepTime:0.77  LR:0.000100\n",
      "   | > Step:58/68  GlobalStep:131780  TotalLoss:0.00217  PostnetLoss:0.00106  DecoderLoss:0.00111  StopLoss:0.06290  GradNorm:0.00355  GradNormST:0.01493  AvgTextLen:47.8  AvgSpecLen:158.0  StepTime:0.95  LR:0.000100\n",
      "   | > Step:68/68  GlobalStep:131790  TotalLoss:0.00247  PostnetLoss:0.00120  DecoderLoss:0.00127  StopLoss:0.03886  GradNorm:0.00418  GradNormST:0.02719  AvgTextLen:103.0  AvgSpecLen:308.9  StepTime:1.87  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:131790  AvgTotalLoss:0.04519  AvgPostnetLoss:0.00087  AvgDecoderLoss:0.00091  AvgStopLoss:0.04341  EpochTime:43.12  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.07417   PostnetLoss: 0.00568   DecoderLoss:0.00593  StopLoss: 0.06256  \n",
      "   | > TotalLoss: 0.08726   PostnetLoss: 0.01002   DecoderLoss:0.01036  StopLoss: 0.06689  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00087   Validation Loss: 0.00904\n",
      "\n",
      " > Epoch 910/1000\n",
      "   | > Step:9/68  GlobalStep:131800  TotalLoss:0.00132  PostnetLoss:0.00065  DecoderLoss:0.00067  StopLoss:0.03217  GradNorm:0.00379  GradNormST:0.01512  AvgTextLen:9.0  AvgSpecLen:42.1  StepTime:0.33  LR:0.000100\n",
      "   | > Step:19/68  GlobalStep:131810  TotalLoss:0.00150  PostnetLoss:0.00073  DecoderLoss:0.00076  StopLoss:0.05999  GradNorm:0.00344  GradNormST:0.02180  AvgTextLen:15.5  AvgSpecLen:63.2  StepTime:0.41  LR:0.000100\n",
      "   | > Step:29/68  GlobalStep:131820  TotalLoss:0.00167  PostnetLoss:0.00082  DecoderLoss:0.00085  StopLoss:0.05225  GradNorm:0.00323  GradNormST:0.01089  AvgTextLen:21.8  AvgSpecLen:83.4  StepTime:0.53  LR:0.000100\n",
      "   | > Step:39/68  GlobalStep:131830  TotalLoss:0.00186  PostnetLoss:0.00091  DecoderLoss:0.00095  StopLoss:0.03317  GradNorm:0.00280  GradNormST:0.01043  AvgTextLen:30.2  AvgSpecLen:110.4  StepTime:0.67  LR:0.000100\n",
      "   | > Step:49/68  GlobalStep:131840  TotalLoss:0.00199  PostnetLoss:0.00097  DecoderLoss:0.00102  StopLoss:0.04003  GradNorm:0.00271  GradNormST:0.01090  AvgTextLen:38.2  AvgSpecLen:127.7  StepTime:0.60  LR:0.000100\n",
      "   | > Step:59/68  GlobalStep:131850  TotalLoss:0.00217  PostnetLoss:0.00105  DecoderLoss:0.00111  StopLoss:0.06315  GradNorm:0.00319  GradNormST:0.01980  AvgTextLen:50.5  AvgSpecLen:155.9  StepTime:0.91  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:131859  AvgTotalLoss:0.04673  AvgPostnetLoss:0.00087  AvgDecoderLoss:0.00091  AvgStopLoss:0.04495  EpochTime:42.07  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.07338   PostnetLoss: 0.00571   DecoderLoss:0.00597  StopLoss: 0.06170  \n",
      "   | > TotalLoss: 0.09857   PostnetLoss: 0.01043   DecoderLoss:0.01077  StopLoss: 0.07736  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00087   Validation Loss: 0.00932\n",
      "\n",
      " > Epoch 911/1000\n",
      "   | > Step:0/68  GlobalStep:131860  TotalLoss:0.00133  PostnetLoss:0.00064  DecoderLoss:0.00069  StopLoss:0.05000  GradNorm:0.00594  GradNormST:0.01945  AvgTextLen:3.0  AvgSpecLen:25.3  StepTime:0.54  LR:0.000100\n",
      "   | > Step:10/68  GlobalStep:131870  TotalLoss:0.00140  PostnetLoss:0.00069  DecoderLoss:0.00072  StopLoss:0.03376  GradNorm:0.00362  GradNormST:0.01160  AvgTextLen:9.8  AvgSpecLen:48.6  StepTime:0.52  LR:0.000100\n",
      "   | > Step:20/68  GlobalStep:131880  TotalLoss:0.00164  PostnetLoss:0.00081  DecoderLoss:0.00084  StopLoss:0.03578  GradNorm:0.00360  GradNormST:0.01331  AvgTextLen:16.4  AvgSpecLen:64.8  StepTime:0.49  LR:0.000100\n",
      "   | > Step:30/68  GlobalStep:131890  TotalLoss:0.00178  PostnetLoss:0.00087  DecoderLoss:0.00091  StopLoss:0.04532  GradNorm:0.00328  GradNormST:0.01160  AvgTextLen:22.9  AvgSpecLen:85.8  StepTime:0.60  LR:0.000100\n",
      "   | > Step:40/68  GlobalStep:131900  TotalLoss:0.00187  PostnetLoss:0.00091  DecoderLoss:0.00096  StopLoss:0.06771  GradNorm:0.00321  GradNormST:0.01486  AvgTextLen:30.9  AvgSpecLen:105.3  StepTime:0.59  LR:0.000100\n",
      "   | > Step:50/68  GlobalStep:131910  TotalLoss:0.00208  PostnetLoss:0.00101  DecoderLoss:0.00107  StopLoss:0.03796  GradNorm:0.00296  GradNormST:0.01088  AvgTextLen:38.8  AvgSpecLen:132.0  StepTime:0.88  LR:0.000100\n",
      "   | > Step:60/68  GlobalStep:131920  TotalLoss:0.00239  PostnetLoss:0.00117  DecoderLoss:0.00123  StopLoss:0.04049  GradNorm:0.00562  GradNormST:0.00980  AvgTextLen:52.0  AvgSpecLen:172.7  StepTime:1.00  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:131928  AvgTotalLoss:0.04977  AvgPostnetLoss:0.00091  AvgDecoderLoss:0.00095  AvgStopLoss:0.04792  EpochTime:42.65  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.07473   PostnetLoss: 0.00581   DecoderLoss:0.00609  StopLoss: 0.06284  \n",
      "   | > TotalLoss: 0.09367   PostnetLoss: 0.00977   DecoderLoss:0.01016  StopLoss: 0.07375  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00091   Validation Loss: 0.00886\n",
      "\n",
      " > Epoch 912/1000\n",
      "   | > Step:1/68  GlobalStep:131930  TotalLoss:0.00136  PostnetLoss:0.00066  DecoderLoss:0.00070  StopLoss:0.03193  GradNorm:0.00538  GradNormST:0.01202  AvgTextLen:3.7  AvgSpecLen:28.6  StepTime:0.43  LR:0.000100\n",
      "   | > Step:11/68  GlobalStep:131940  TotalLoss:0.00141  PostnetLoss:0.00069  DecoderLoss:0.00072  StopLoss:0.03053  GradNorm:0.00339  GradNormST:0.01355  AvgTextLen:10.3  AvgSpecLen:49.4  StepTime:0.30  LR:0.000100\n",
      "   | > Step:21/68  GlobalStep:131950  TotalLoss:0.00161  PostnetLoss:0.00079  DecoderLoss:0.00082  StopLoss:0.03818  GradNorm:0.00386  GradNormST:0.01626  AvgTextLen:16.6  AvgSpecLen:66.3  StepTime:0.46  LR:0.000100\n",
      "   | > Step:31/68  GlobalStep:131960  TotalLoss:0.00180  PostnetLoss:0.00088  DecoderLoss:0.00092  StopLoss:0.03962  GradNorm:0.00331  GradNormST:0.01226  AvgTextLen:24.1  AvgSpecLen:82.7  StepTime:0.53  LR:0.000100\n",
      "   | > Step:41/68  GlobalStep:131970  TotalLoss:0.00198  PostnetLoss:0.00097  DecoderLoss:0.00101  StopLoss:0.03366  GradNorm:0.00320  GradNormST:0.00943  AvgTextLen:32.3  AvgSpecLen:109.4  StepTime:0.64  LR:0.000100\n",
      "   | > Step:51/68  GlobalStep:131980  TotalLoss:0.00204  PostnetLoss:0.00099  DecoderLoss:0.00105  StopLoss:0.04409  GradNorm:0.00310  GradNormST:0.01318  AvgTextLen:39.2  AvgSpecLen:126.9  StepTime:0.79  LR:0.000100\n",
      "   | > Step:61/68  GlobalStep:131990  TotalLoss:0.00222  PostnetLoss:0.00108  DecoderLoss:0.00114  StopLoss:0.06275  GradNorm:0.00284  GradNormST:0.01574  AvgTextLen:54.6  AvgSpecLen:165.5  StepTime:0.90  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:131997  AvgTotalLoss:0.05089  AvgPostnetLoss:0.00090  AvgDecoderLoss:0.00094  AvgStopLoss:0.04905  EpochTime:43.31  AvgStepTime:0.63\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.07441   PostnetLoss: 0.00587   DecoderLoss:0.00613  StopLoss: 0.06240  \n",
      "   | > TotalLoss: 0.10285   PostnetLoss: 0.00994   DecoderLoss:0.01029  StopLoss: 0.08262  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00090   Validation Loss: 0.00893\n",
      "\n",
      " > Epoch 913/1000\n",
      "   | > Step:2/68  GlobalStep:132000  TotalLoss:0.00121  PostnetLoss:0.00059  DecoderLoss:0.00062  StopLoss:0.03115  GradNorm:0.00487  GradNormST:0.01847  AvgTextLen:4.0  AvgSpecLen:31.3  StepTime:0.41  LR:0.000100\n",
      " | | > Checkpoint saving : /vm/TTS/models/cartman_models/mozilla-no-loc-fattn-stopnet-sigmoid-loss_masking-August-15-2019_11+26AM-de6aeb9/checkpoint_132000.pth.tar\n",
      "   | > Step:12/68  GlobalStep:132010  TotalLoss:0.00145  PostnetLoss:0.00071  DecoderLoss:0.00074  StopLoss:0.04626  GradNorm:0.00359  GradNormST:0.01601  AvgTextLen:11.0  AvgSpecLen:49.3  StepTime:0.42  LR:0.000100\n",
      "   | > Step:22/68  GlobalStep:132020  TotalLoss:0.00167  PostnetLoss:0.00082  DecoderLoss:0.00086  StopLoss:0.05181  GradNorm:0.00348  GradNormST:0.01532  AvgTextLen:18.0  AvgSpecLen:72.0  StepTime:0.39  LR:0.000100\n",
      "   | > Step:32/68  GlobalStep:132030  TotalLoss:0.00179  PostnetLoss:0.00088  DecoderLoss:0.00091  StopLoss:0.06336  GradNorm:0.00335  GradNormST:0.01791  AvgTextLen:24.4  AvgSpecLen:91.5  StepTime:0.50  LR:0.000100\n",
      "   | > Step:42/68  GlobalStep:132040  TotalLoss:0.00191  PostnetLoss:0.00094  DecoderLoss:0.00098  StopLoss:0.04369  GradNorm:0.00390  GradNormST:0.02351  AvgTextLen:33.1  AvgSpecLen:109.7  StepTime:0.49  LR:0.000100\n",
      "   | > Step:52/68  GlobalStep:132050  TotalLoss:0.00209  PostnetLoss:0.00102  DecoderLoss:0.00107  StopLoss:0.05762  GradNorm:0.00286  GradNormST:0.01764  AvgTextLen:42.1  AvgSpecLen:137.5  StepTime:0.86  LR:0.000100\n",
      "   | > Step:62/68  GlobalStep:132060  TotalLoss:0.00227  PostnetLoss:0.00111  DecoderLoss:0.00117  StopLoss:0.05478  GradNorm:0.00332  GradNormST:0.02302  AvgTextLen:54.0  AvgSpecLen:174.7  StepTime:0.75  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:132066  AvgTotalLoss:0.04868  AvgPostnetLoss:0.00089  AvgDecoderLoss:0.00093  AvgStopLoss:0.04686  EpochTime:42.83  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.07069   PostnetLoss: 0.00568   DecoderLoss:0.00595  StopLoss: 0.05906  \n",
      "   | > TotalLoss: 0.09693   PostnetLoss: 0.01044   DecoderLoss:0.01080  StopLoss: 0.07569  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00089   Validation Loss: 0.00911\n",
      "\n",
      " > Epoch 914/1000\n",
      "   | > Step:3/68  GlobalStep:132070  TotalLoss:0.00125  PostnetLoss:0.00061  DecoderLoss:0.00064  StopLoss:0.04400  GradNorm:0.00475  GradNormST:0.01154  AvgTextLen:4.7  AvgSpecLen:29.7  StepTime:0.31  LR:0.000100\n",
      "   | > Step:13/68  GlobalStep:132080  TotalLoss:0.00141  PostnetLoss:0.00069  DecoderLoss:0.00072  StopLoss:0.04562  GradNorm:0.00352  GradNormST:0.01310  AvgTextLen:11.5  AvgSpecLen:55.2  StepTime:0.41  LR:0.000100\n",
      "   | > Step:23/68  GlobalStep:132090  TotalLoss:0.00164  PostnetLoss:0.00080  DecoderLoss:0.00084  StopLoss:0.04025  GradNorm:0.00344  GradNormST:0.01222  AvgTextLen:18.3  AvgSpecLen:72.2  StepTime:0.45  LR:0.000100\n",
      "   | > Step:33/68  GlobalStep:132100  TotalLoss:0.00181  PostnetLoss:0.00088  DecoderLoss:0.00092  StopLoss:0.04443  GradNorm:0.00321  GradNormST:0.01046  AvgTextLen:25.9  AvgSpecLen:88.9  StepTime:0.43  LR:0.000100\n",
      "   | > Step:43/68  GlobalStep:132110  TotalLoss:0.00191  PostnetLoss:0.00093  DecoderLoss:0.00098  StopLoss:0.05220  GradNorm:0.00316  GradNormST:0.01256  AvgTextLen:33.4  AvgSpecLen:115.7  StepTime:0.61  LR:0.000100\n",
      "   | > Step:53/68  GlobalStep:132120  TotalLoss:0.00206  PostnetLoss:0.00100  DecoderLoss:0.00105  StopLoss:0.04151  GradNorm:0.00290  GradNormST:0.00889  AvgTextLen:42.4  AvgSpecLen:137.6  StepTime:0.81  LR:0.000100\n",
      "   | > Step:63/68  GlobalStep:132130  TotalLoss:0.00230  PostnetLoss:0.00112  DecoderLoss:0.00118  StopLoss:0.04356  GradNorm:0.00272  GradNormST:0.02132  AvgTextLen:58.6  AvgSpecLen:185.1  StepTime:0.91  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:132135  AvgTotalLoss:0.04620  AvgPostnetLoss:0.00088  AvgDecoderLoss:0.00093  AvgStopLoss:0.04439  EpochTime:42.25  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.07244   PostnetLoss: 0.00574   DecoderLoss:0.00599  StopLoss: 0.06071  \n",
      "   | > TotalLoss: 0.10853   PostnetLoss: 0.01087   DecoderLoss:0.01120  StopLoss: 0.08646  \n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00088   Validation Loss: 0.00944\n",
      "\n",
      " > Epoch 915/1000\n",
      "   | > Step:4/68  GlobalStep:132140  TotalLoss:0.00126  PostnetLoss:0.00061  DecoderLoss:0.00064  StopLoss:0.07093  GradNorm:0.00362  GradNormST:0.02298  AvgTextLen:4.8  AvgSpecLen:31.1  StepTime:0.27  LR:0.000100\n",
      "   | > Step:14/68  GlobalStep:132150  TotalLoss:0.00148  PostnetLoss:0.00072  DecoderLoss:0.00075  StopLoss:0.04714  GradNorm:0.00344  GradNormST:0.01855  AvgTextLen:12.2  AvgSpecLen:52.0  StepTime:0.33  LR:0.000100\n",
      "   | > Step:24/68  GlobalStep:132160  TotalLoss:0.00170  PostnetLoss:0.00083  DecoderLoss:0.00087  StopLoss:0.03995  GradNorm:0.00331  GradNormST:0.01324  AvgTextLen:19.2  AvgSpecLen:79.0  StepTime:0.47  LR:0.000100\n",
      "   | > Step:34/68  GlobalStep:132170  TotalLoss:0.00181  PostnetLoss:0.00089  DecoderLoss:0.00093  StopLoss:0.03088  GradNorm:0.00297  GradNormST:0.00818  AvgTextLen:26.2  AvgSpecLen:95.9  StepTime:0.48  LR:0.000100\n",
      "   | > Step:44/68  GlobalStep:132180  TotalLoss:0.00199  PostnetLoss:0.00097  DecoderLoss:0.00102  StopLoss:0.03691  GradNorm:0.00330  GradNormST:0.01125  AvgTextLen:33.8  AvgSpecLen:116.3  StepTime:0.65  LR:0.000100\n",
      "   | > Step:54/68  GlobalStep:132190  TotalLoss:0.00208  PostnetLoss:0.00101  DecoderLoss:0.00107  StopLoss:0.03437  GradNorm:0.00282  GradNormST:0.00882  AvgTextLen:42.2  AvgSpecLen:144.9  StepTime:0.82  LR:0.000100\n",
      "   | > Step:64/68  GlobalStep:132200  TotalLoss:0.00218  PostnetLoss:0.00106  DecoderLoss:0.00112  StopLoss:0.05005  GradNorm:0.00245  GradNormST:0.01671  AvgTextLen:60.6  AvgSpecLen:177.6  StepTime:0.80  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:132204  AvgTotalLoss:0.04700  AvgPostnetLoss:0.00088  AvgDecoderLoss:0.00092  AvgStopLoss:0.04521  EpochTime:42.43  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.07136   PostnetLoss: 0.00573   DecoderLoss:0.00600  StopLoss: 0.05963  \n",
      "   | > TotalLoss: 0.08585   PostnetLoss: 0.00970   DecoderLoss:0.01006  StopLoss: 0.06610  \n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00088   Validation Loss: 0.00870\n",
      "\n",
      " > Epoch 916/1000\n",
      "   | > Step:5/68  GlobalStep:132210  TotalLoss:0.00138  PostnetLoss:0.00068  DecoderLoss:0.00071  StopLoss:0.03879  GradNorm:0.00517  GradNormST:0.01841  AvgTextLen:5.9  AvgSpecLen:33.2  StepTime:0.33  LR:0.000100\n",
      "   | > Step:15/68  GlobalStep:132220  TotalLoss:0.00154  PostnetLoss:0.00075  DecoderLoss:0.00079  StopLoss:0.04138  GradNorm:0.00399  GradNormST:0.01441  AvgTextLen:12.8  AvgSpecLen:59.1  StepTime:0.45  LR:0.000100\n",
      "   | > Step:25/68  GlobalStep:132230  TotalLoss:0.00164  PostnetLoss:0.00080  DecoderLoss:0.00084  StopLoss:0.03826  GradNorm:0.00336  GradNormST:0.00941  AvgTextLen:19.8  AvgSpecLen:73.3  StepTime:0.48  LR:0.000100\n",
      "   | > Step:35/68  GlobalStep:132240  TotalLoss:0.00182  PostnetLoss:0.00089  DecoderLoss:0.00093  StopLoss:0.05153  GradNorm:0.00287  GradNormST:0.01982  AvgTextLen:27.1  AvgSpecLen:104.7  StepTime:0.80  LR:0.000100\n",
      "   | > Step:45/68  GlobalStep:132250  TotalLoss:0.00191  PostnetLoss:0.00093  DecoderLoss:0.00098  StopLoss:0.05228  GradNorm:0.00293  GradNormST:0.01890  AvgTextLen:34.4  AvgSpecLen:118.6  StepTime:0.69  LR:0.000100\n",
      "   | > Step:55/68  GlobalStep:132260  TotalLoss:0.00212  PostnetLoss:0.00103  DecoderLoss:0.00109  StopLoss:0.03424  GradNorm:0.00265  GradNormST:0.00811  AvgTextLen:43.9  AvgSpecLen:145.2  StepTime:0.66  LR:0.000100\n",
      "   | > Step:65/68  GlobalStep:132270  TotalLoss:0.00234  PostnetLoss:0.00114  DecoderLoss:0.00120  StopLoss:0.06554  GradNorm:0.00287  GradNormST:0.02743  AvgTextLen:64.2  AvgSpecLen:207.4  StepTime:1.18  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:132273  AvgTotalLoss:0.04698  AvgPostnetLoss:0.00088  AvgDecoderLoss:0.00092  AvgStopLoss:0.04518  EpochTime:41.56  AvgStepTime:0.60\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.06800   PostnetLoss: 0.00569   DecoderLoss:0.00595  StopLoss: 0.05636  \n",
      "   | > TotalLoss: 0.08725   PostnetLoss: 0.00955   DecoderLoss:0.00988  StopLoss: 0.06782  \n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00088   Validation Loss: 0.00861\n",
      "\n",
      " > Epoch 917/1000\n",
      "   | > Step:6/68  GlobalStep:132280  TotalLoss:0.00130  PostnetLoss:0.00064  DecoderLoss:0.00066  StopLoss:0.04149  GradNorm:0.00404  GradNormST:0.01605  AvgTextLen:6.0  AvgSpecLen:34.8  StepTime:0.33  LR:0.000100\n",
      "   | > Step:16/68  GlobalStep:132290  TotalLoss:0.00151  PostnetLoss:0.00074  DecoderLoss:0.00077  StopLoss:0.03542  GradNorm:0.00337  GradNormST:0.01694  AvgTextLen:13.8  AvgSpecLen:60.6  StepTime:0.32  LR:0.000100\n",
      "   | > Step:26/68  GlobalStep:132300  TotalLoss:0.00168  PostnetLoss:0.00082  DecoderLoss:0.00086  StopLoss:0.03868  GradNorm:0.00368  GradNormST:0.01295  AvgTextLen:20.3  AvgSpecLen:77.5  StepTime:0.50  LR:0.000100\n",
      "   | > Step:36/68  GlobalStep:132310  TotalLoss:0.00182  PostnetLoss:0.00089  DecoderLoss:0.00093  StopLoss:0.05266  GradNorm:0.00309  GradNormST:0.01036  AvgTextLen:28.5  AvgSpecLen:100.2  StepTime:0.62  LR:0.000100\n",
      "   | > Step:46/68  GlobalStep:132320  TotalLoss:0.00195  PostnetLoss:0.00095  DecoderLoss:0.00100  StopLoss:0.04843  GradNorm:0.00422  GradNormST:0.01228  AvgTextLen:35.4  AvgSpecLen:122.6  StepTime:0.77  LR:0.000100\n",
      "   | > Step:56/68  GlobalStep:132330  TotalLoss:0.00211  PostnetLoss:0.00103  DecoderLoss:0.00108  StopLoss:0.04664  GradNorm:0.00284  GradNormST:0.01889  AvgTextLen:45.8  AvgSpecLen:145.4  StepTime:0.99  LR:0.000100\n",
      "   | > Step:66/68  GlobalStep:132340  TotalLoss:0.00248  PostnetLoss:0.00120  DecoderLoss:0.00128  StopLoss:0.05695  GradNorm:0.00285  GradNormST:0.02155  AvgTextLen:71.2  AvgSpecLen:220.8  StepTime:1.04  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:132342  AvgTotalLoss:0.04754  AvgPostnetLoss:0.00087  AvgDecoderLoss:0.00092  AvgStopLoss:0.04575  EpochTime:42.06  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.07037   PostnetLoss: 0.00565   DecoderLoss:0.00592  StopLoss: 0.05880  \n",
      "   | > TotalLoss: 0.09012   PostnetLoss: 0.00962   DecoderLoss:0.00997  StopLoss: 0.07053  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00087   Validation Loss: 0.00881\n",
      "\n",
      " > Epoch 918/1000\n",
      "   | > Step:7/68  GlobalStep:132350  TotalLoss:0.00138  PostnetLoss:0.00067  DecoderLoss:0.00070  StopLoss:0.03833  GradNorm:0.00461  GradNormST:0.02323  AvgTextLen:7.4  AvgSpecLen:44.6  StepTime:0.39  LR:0.000100\n",
      "   | > Step:17/68  GlobalStep:132360  TotalLoss:0.00150  PostnetLoss:0.00073  DecoderLoss:0.00076  StopLoss:0.05347  GradNorm:0.00334  GradNormST:0.01518  AvgTextLen:14.7  AvgSpecLen:66.0  StepTime:0.45  LR:0.000100\n",
      "   | > Step:27/68  GlobalStep:132370  TotalLoss:0.00169  PostnetLoss:0.00083  DecoderLoss:0.00086  StopLoss:0.06945  GradNorm:0.00306  GradNormST:0.01936  AvgTextLen:21.0  AvgSpecLen:80.2  StepTime:0.64  LR:0.000100\n",
      "   | > Step:37/68  GlobalStep:132380  TotalLoss:0.00184  PostnetLoss:0.00090  DecoderLoss:0.00094  StopLoss:0.04071  GradNorm:0.00311  GradNormST:0.01163  AvgTextLen:29.3  AvgSpecLen:102.4  StepTime:0.59  LR:0.000100\n",
      "   | > Step:47/68  GlobalStep:132390  TotalLoss:0.00204  PostnetLoss:0.00100  DecoderLoss:0.00105  StopLoss:0.03050  GradNorm:0.00373  GradNormST:0.00957  AvgTextLen:36.7  AvgSpecLen:126.9  StepTime:0.68  LR:0.000100\n",
      "   | > Step:57/68  GlobalStep:132400  TotalLoss:0.00215  PostnetLoss:0.00105  DecoderLoss:0.00110  StopLoss:0.04060  GradNorm:0.00314  GradNormST:0.01223  AvgTextLen:47.1  AvgSpecLen:161.0  StepTime:0.83  LR:0.000100\n",
      "   | > Step:67/68  GlobalStep:132410  TotalLoss:0.00248  PostnetLoss:0.00120  DecoderLoss:0.00127  StopLoss:0.05450  GradNorm:0.00276  GradNormST:0.02716  AvgTextLen:80.6  AvgSpecLen:246.5  StepTime:1.23  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:132411  AvgTotalLoss:0.04305  AvgPostnetLoss:0.00087  AvgDecoderLoss:0.00091  AvgStopLoss:0.04127  EpochTime:43.11  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.06917   PostnetLoss: 0.00564   DecoderLoss:0.00589  StopLoss: 0.05765  \n",
      "   | > TotalLoss: 0.08672   PostnetLoss: 0.00947   DecoderLoss:0.00981  StopLoss: 0.06745  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00087   Validation Loss: 0.00868\n",
      "\n",
      " > Epoch 919/1000\n",
      "   | > Step:8/68  GlobalStep:132420  TotalLoss:0.00136  PostnetLoss:0.00067  DecoderLoss:0.00069  StopLoss:0.04473  GradNorm:0.00412  GradNormST:0.01018  AvgTextLen:8.3  AvgSpecLen:44.0  StepTime:0.24  LR:0.000100\n",
      "   | > Step:18/68  GlobalStep:132430  TotalLoss:0.00151  PostnetLoss:0.00074  DecoderLoss:0.00077  StopLoss:0.03981  GradNorm:0.00351  GradNormST:0.01206  AvgTextLen:15.2  AvgSpecLen:64.5  StepTime:0.33  LR:0.000100\n",
      "   | > Step:28/68  GlobalStep:132440  TotalLoss:0.00168  PostnetLoss:0.00082  DecoderLoss:0.00086  StopLoss:0.04002  GradNorm:0.00295  GradNormST:0.01170  AvgTextLen:22.1  AvgSpecLen:85.4  StepTime:0.48  LR:0.000100\n",
      "   | > Step:38/68  GlobalStep:132450  TotalLoss:0.00179  PostnetLoss:0.00087  DecoderLoss:0.00092  StopLoss:0.02662  GradNorm:0.00315  GradNormST:0.00736  AvgTextLen:29.4  AvgSpecLen:104.1  StepTime:0.60  LR:0.000100\n",
      "   | > Step:48/68  GlobalStep:132460  TotalLoss:0.00198  PostnetLoss:0.00097  DecoderLoss:0.00101  StopLoss:0.03236  GradNorm:0.00341  GradNormST:0.00900  AvgTextLen:37.5  AvgSpecLen:123.8  StepTime:0.66  LR:0.000100\n",
      "   | > Step:58/68  GlobalStep:132470  TotalLoss:0.00217  PostnetLoss:0.00106  DecoderLoss:0.00111  StopLoss:0.05171  GradNorm:0.00380  GradNormST:0.01945  AvgTextLen:47.8  AvgSpecLen:158.0  StepTime:0.70  LR:0.000100\n",
      "   | > Step:68/68  GlobalStep:132480  TotalLoss:0.00244  PostnetLoss:0.00119  DecoderLoss:0.00125  StopLoss:0.02085  GradNorm:0.00283  GradNormST:0.00644  AvgTextLen:103.0  AvgSpecLen:308.9  StepTime:1.86  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:132480  AvgTotalLoss:0.04353  AvgPostnetLoss:0.00087  AvgDecoderLoss:0.00091  AvgStopLoss:0.04176  EpochTime:42.08  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.06582   PostnetLoss: 0.00558   DecoderLoss:0.00584  StopLoss: 0.05440  \n",
      "   | > TotalLoss: 0.08745   PostnetLoss: 0.00964   DecoderLoss:0.00998  StopLoss: 0.06784  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00087   Validation Loss: 0.00875\n",
      "\n",
      " > Epoch 920/1000\n",
      "   | > Step:9/68  GlobalStep:132490  TotalLoss:0.00132  PostnetLoss:0.00065  DecoderLoss:0.00068  StopLoss:0.03581  GradNorm:0.00363  GradNormST:0.01724  AvgTextLen:9.0  AvgSpecLen:42.1  StepTime:0.40  LR:0.000100\n",
      "   | > Step:19/68  GlobalStep:132500  TotalLoss:0.00151  PostnetLoss:0.00074  DecoderLoss:0.00077  StopLoss:0.05015  GradNorm:0.00344  GradNormST:0.01691  AvgTextLen:15.5  AvgSpecLen:63.2  StepTime:0.44  LR:0.000100\n",
      "   | > Step:29/68  GlobalStep:132510  TotalLoss:0.00167  PostnetLoss:0.00082  DecoderLoss:0.00086  StopLoss:0.04313  GradNorm:0.00287  GradNormST:0.00960  AvgTextLen:21.8  AvgSpecLen:83.4  StepTime:0.52  LR:0.000100\n",
      "   | > Step:39/68  GlobalStep:132520  TotalLoss:0.00183  PostnetLoss:0.00089  DecoderLoss:0.00094  StopLoss:0.03357  GradNorm:0.00261  GradNormST:0.01177  AvgTextLen:30.2  AvgSpecLen:110.4  StepTime:0.52  LR:0.000100\n",
      "   | > Step:49/68  GlobalStep:132530  TotalLoss:0.00202  PostnetLoss:0.00099  DecoderLoss:0.00103  StopLoss:0.03272  GradNorm:0.00373  GradNormST:0.01359  AvgTextLen:38.2  AvgSpecLen:127.7  StepTime:0.77  LR:0.000100\n",
      "   | > Step:59/68  GlobalStep:132540  TotalLoss:0.00216  PostnetLoss:0.00105  DecoderLoss:0.00111  StopLoss:0.07637  GradNorm:0.00304  GradNormST:0.02409  AvgTextLen:50.5  AvgSpecLen:155.9  StepTime:0.72  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:132549  AvgTotalLoss:0.04387  AvgPostnetLoss:0.00086  AvgDecoderLoss:0.00091  AvgStopLoss:0.04210  EpochTime:42.17  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.06794   PostnetLoss: 0.00558   DecoderLoss:0.00583  StopLoss: 0.05654  \n",
      "   | > TotalLoss: 0.08499   PostnetLoss: 0.00954   DecoderLoss:0.00988  StopLoss: 0.06557  \n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00086   Validation Loss: 0.00875\n",
      "\n",
      " > Epoch 921/1000\n",
      "   | > Step:0/68  GlobalStep:132550  TotalLoss:0.00130  PostnetLoss:0.00063  DecoderLoss:0.00067  StopLoss:0.06539  GradNorm:0.00523  GradNormST:0.03901  AvgTextLen:3.0  AvgSpecLen:25.3  StepTime:0.49  LR:0.000100\n",
      "   | > Step:10/68  GlobalStep:132560  TotalLoss:0.00140  PostnetLoss:0.00068  DecoderLoss:0.00072  StopLoss:0.02920  GradNorm:0.00384  GradNormST:0.01206  AvgTextLen:9.8  AvgSpecLen:48.6  StepTime:0.51  LR:0.000100\n",
      "   | > Step:20/68  GlobalStep:132570  TotalLoss:0.00158  PostnetLoss:0.00078  DecoderLoss:0.00081  StopLoss:0.03084  GradNorm:0.00370  GradNormST:0.01285  AvgTextLen:16.4  AvgSpecLen:64.8  StepTime:0.43  LR:0.000100\n",
      "   | > Step:30/68  GlobalStep:132580  TotalLoss:0.00171  PostnetLoss:0.00083  DecoderLoss:0.00087  StopLoss:0.04408  GradNorm:0.00309  GradNormST:0.01025  AvgTextLen:22.9  AvgSpecLen:85.8  StepTime:0.47  LR:0.000100\n",
      "   | > Step:40/68  GlobalStep:132590  TotalLoss:0.00184  PostnetLoss:0.00090  DecoderLoss:0.00094  StopLoss:0.04361  GradNorm:0.00288  GradNormST:0.01723  AvgTextLen:30.9  AvgSpecLen:105.3  StepTime:0.62  LR:0.000100\n",
      "   | > Step:50/68  GlobalStep:132600  TotalLoss:0.00201  PostnetLoss:0.00098  DecoderLoss:0.00103  StopLoss:0.03813  GradNorm:0.00327  GradNormST:0.01081  AvgTextLen:38.8  AvgSpecLen:132.0  StepTime:0.85  LR:0.000100\n",
      "   | > Step:60/68  GlobalStep:132610  TotalLoss:0.00224  PostnetLoss:0.00109  DecoderLoss:0.00115  StopLoss:0.03414  GradNorm:0.00360  GradNormST:0.00820  AvgTextLen:52.0  AvgSpecLen:172.7  StepTime:0.92  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:132618  AvgTotalLoss:0.04489  AvgPostnetLoss:0.00087  AvgDecoderLoss:0.00091  AvgStopLoss:0.04312  EpochTime:42.23  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.06699   PostnetLoss: 0.00556   DecoderLoss:0.00582  StopLoss: 0.05561  \n",
      "   | > TotalLoss: 0.08127   PostnetLoss: 0.00928   DecoderLoss:0.00961  StopLoss: 0.06237  \n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00087   Validation Loss: 0.00863\n",
      "\n",
      " > Epoch 922/1000\n",
      "   | > Step:1/68  GlobalStep:132620  TotalLoss:0.00128  PostnetLoss:0.00062  DecoderLoss:0.00066  StopLoss:0.04380  GradNorm:0.00600  GradNormST:0.01500  AvgTextLen:3.7  AvgSpecLen:28.6  StepTime:0.40  LR:0.000100\n",
      "   | > Step:11/68  GlobalStep:132630  TotalLoss:0.00139  PostnetLoss:0.00068  DecoderLoss:0.00071  StopLoss:0.03591  GradNorm:0.00374  GradNormST:0.01957  AvgTextLen:10.3  AvgSpecLen:49.4  StepTime:0.30  LR:0.000100\n",
      "   | > Step:21/68  GlobalStep:132640  TotalLoss:0.00157  PostnetLoss:0.00077  DecoderLoss:0.00080  StopLoss:0.04525  GradNorm:0.00375  GradNormST:0.01484  AvgTextLen:16.6  AvgSpecLen:66.3  StepTime:0.42  LR:0.000100\n",
      "   | > Step:31/68  GlobalStep:132650  TotalLoss:0.00169  PostnetLoss:0.00083  DecoderLoss:0.00087  StopLoss:0.05472  GradNorm:0.00298  GradNormST:0.02012  AvgTextLen:24.1  AvgSpecLen:82.7  StepTime:0.53  LR:0.000100\n",
      "   | > Step:41/68  GlobalStep:132660  TotalLoss:0.00188  PostnetLoss:0.00092  DecoderLoss:0.00096  StopLoss:0.03113  GradNorm:0.00282  GradNormST:0.01088  AvgTextLen:32.3  AvgSpecLen:109.4  StepTime:0.57  LR:0.000100\n",
      "   | > Step:51/68  GlobalStep:132670  TotalLoss:0.00200  PostnetLoss:0.00097  DecoderLoss:0.00102  StopLoss:0.03914  GradNorm:0.00307  GradNormST:0.01328  AvgTextLen:39.2  AvgSpecLen:126.9  StepTime:0.72  LR:0.000100\n",
      "   | > Step:61/68  GlobalStep:132680  TotalLoss:0.00216  PostnetLoss:0.00105  DecoderLoss:0.00111  StopLoss:0.04472  GradNorm:0.00322  GradNormST:0.01352  AvgTextLen:54.6  AvgSpecLen:165.5  StepTime:0.93  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:132687  AvgTotalLoss:0.04599  AvgPostnetLoss:0.00086  AvgDecoderLoss:0.00091  AvgStopLoss:0.04422  EpochTime:42.92  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.06738   PostnetLoss: 0.00551   DecoderLoss:0.00576  StopLoss: 0.05612  \n",
      "   | > TotalLoss: 0.08264   PostnetLoss: 0.00938   DecoderLoss:0.00972  StopLoss: 0.06354  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00086   Validation Loss: 0.00862\n",
      "\n",
      " > Epoch 923/1000\n",
      "   | > Step:2/68  GlobalStep:132690  TotalLoss:0.00120  PostnetLoss:0.00059  DecoderLoss:0.00061  StopLoss:0.03850  GradNorm:0.00442  GradNormST:0.01357  AvgTextLen:4.0  AvgSpecLen:31.3  StepTime:0.51  LR:0.000100\n",
      "   | > Step:12/68  GlobalStep:132700  TotalLoss:0.00137  PostnetLoss:0.00067  DecoderLoss:0.00070  StopLoss:0.04993  GradNorm:0.00394  GradNormST:0.01707  AvgTextLen:11.0  AvgSpecLen:49.3  StepTime:0.44  LR:0.000100\n",
      "   | > Step:22/68  GlobalStep:132710  TotalLoss:0.00160  PostnetLoss:0.00079  DecoderLoss:0.00082  StopLoss:0.04368  GradNorm:0.00386  GradNormST:0.01173  AvgTextLen:18.0  AvgSpecLen:72.0  StepTime:0.40  LR:0.000100\n",
      "   | > Step:32/68  GlobalStep:132720  TotalLoss:0.00173  PostnetLoss:0.00084  DecoderLoss:0.00088  StopLoss:0.05048  GradNorm:0.00325  GradNormST:0.01092  AvgTextLen:24.4  AvgSpecLen:91.5  StepTime:0.56  LR:0.000100\n",
      "   | > Step:42/68  GlobalStep:132730  TotalLoss:0.00187  PostnetLoss:0.00091  DecoderLoss:0.00095  StopLoss:0.03736  GradNorm:0.00304  GradNormST:0.01745  AvgTextLen:33.1  AvgSpecLen:109.7  StepTime:0.63  LR:0.000100\n",
      "   | > Step:52/68  GlobalStep:132740  TotalLoss:0.00204  PostnetLoss:0.00099  DecoderLoss:0.00104  StopLoss:0.05096  GradNorm:0.00283  GradNormST:0.01068  AvgTextLen:42.1  AvgSpecLen:137.5  StepTime:0.86  LR:0.000100\n",
      "   | > Step:62/68  GlobalStep:132750  TotalLoss:0.00224  PostnetLoss:0.00109  DecoderLoss:0.00115  StopLoss:0.03720  GradNorm:0.00279  GradNormST:0.01308  AvgTextLen:54.0  AvgSpecLen:174.7  StepTime:0.75  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:132756  AvgTotalLoss:0.04384  AvgPostnetLoss:0.00086  AvgDecoderLoss:0.00090  AvgStopLoss:0.04207  EpochTime:43.32  AvgStepTime:0.63\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.06631   PostnetLoss: 0.00545   DecoderLoss:0.00570  StopLoss: 0.05516  \n",
      "   | > TotalLoss: 0.08083   PostnetLoss: 0.00928   DecoderLoss:0.00960  StopLoss: 0.06196  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00086   Validation Loss: 0.00864\n",
      "\n",
      " > Epoch 924/1000\n",
      "   | > Step:3/68  GlobalStep:132760  TotalLoss:0.00120  PostnetLoss:0.00058  DecoderLoss:0.00061  StopLoss:0.05275  GradNorm:0.00392  GradNormST:0.01433  AvgTextLen:4.7  AvgSpecLen:29.7  StepTime:0.33  LR:0.000100\n",
      "   | > Step:13/68  GlobalStep:132770  TotalLoss:0.00138  PostnetLoss:0.00067  DecoderLoss:0.00070  StopLoss:0.04727  GradNorm:0.00360  GradNormST:0.01475  AvgTextLen:11.5  AvgSpecLen:55.2  StepTime:0.42  LR:0.000100\n",
      "   | > Step:23/68  GlobalStep:132780  TotalLoss:0.00163  PostnetLoss:0.00080  DecoderLoss:0.00083  StopLoss:0.04494  GradNorm:0.00362  GradNormST:0.01457  AvgTextLen:18.3  AvgSpecLen:72.2  StepTime:0.43  LR:0.000100\n",
      "   | > Step:33/68  GlobalStep:132790  TotalLoss:0.00179  PostnetLoss:0.00088  DecoderLoss:0.00092  StopLoss:0.04508  GradNorm:0.00316  GradNormST:0.01260  AvgTextLen:25.9  AvgSpecLen:88.9  StepTime:0.44  LR:0.000100\n",
      "   | > Step:43/68  GlobalStep:132800  TotalLoss:0.00190  PostnetLoss:0.00093  DecoderLoss:0.00097  StopLoss:0.05394  GradNorm:0.00287  GradNormST:0.01193  AvgTextLen:33.4  AvgSpecLen:115.7  StepTime:0.66  LR:0.000100\n",
      "   | > Step:53/68  GlobalStep:132810  TotalLoss:0.00203  PostnetLoss:0.00099  DecoderLoss:0.00104  StopLoss:0.03807  GradNorm:0.00290  GradNormST:0.00746  AvgTextLen:42.4  AvgSpecLen:137.6  StepTime:0.76  LR:0.000100\n",
      "   | > Step:63/68  GlobalStep:132820  TotalLoss:0.00227  PostnetLoss:0.00111  DecoderLoss:0.00117  StopLoss:0.04695  GradNorm:0.00246  GradNormST:0.02167  AvgTextLen:58.6  AvgSpecLen:185.1  StepTime:0.91  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:132825  AvgTotalLoss:0.04435  AvgPostnetLoss:0.00086  AvgDecoderLoss:0.00090  AvgStopLoss:0.04258  EpochTime:42.02  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.07096   PostnetLoss: 0.00561   DecoderLoss:0.00585  StopLoss: 0.05949  \n",
      "   | > TotalLoss: 0.07681   PostnetLoss: 0.00894   DecoderLoss:0.00926  StopLoss: 0.05860  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00086   Validation Loss: 0.00833\n",
      "\n",
      " > Epoch 925/1000\n",
      "   | > Step:4/68  GlobalStep:132830  TotalLoss:0.00128  PostnetLoss:0.00062  DecoderLoss:0.00065  StopLoss:0.06563  GradNorm:0.00423  GradNormST:0.03178  AvgTextLen:4.8  AvgSpecLen:31.1  StepTime:0.27  LR:0.000100\n",
      "   | > Step:14/68  GlobalStep:132840  TotalLoss:0.00144  PostnetLoss:0.00071  DecoderLoss:0.00074  StopLoss:0.02962  GradNorm:0.00419  GradNormST:0.01315  AvgTextLen:12.2  AvgSpecLen:52.0  StepTime:0.29  LR:0.000100\n",
      "   | > Step:24/68  GlobalStep:132850  TotalLoss:0.00170  PostnetLoss:0.00083  DecoderLoss:0.00087  StopLoss:0.03371  GradNorm:0.00342  GradNormST:0.01101  AvgTextLen:19.2  AvgSpecLen:79.0  StepTime:0.40  LR:0.000100\n",
      "   | > Step:34/68  GlobalStep:132860  TotalLoss:0.00180  PostnetLoss:0.00088  DecoderLoss:0.00092  StopLoss:0.03325  GradNorm:0.00306  GradNormST:0.01134  AvgTextLen:26.2  AvgSpecLen:95.9  StepTime:0.61  LR:0.000100\n",
      "   | > Step:44/68  GlobalStep:132870  TotalLoss:0.00195  PostnetLoss:0.00095  DecoderLoss:0.00100  StopLoss:0.02943  GradNorm:0.00294  GradNormST:0.01027  AvgTextLen:33.8  AvgSpecLen:116.3  StepTime:0.59  LR:0.000100\n",
      "   | > Step:54/68  GlobalStep:132880  TotalLoss:0.00206  PostnetLoss:0.00100  DecoderLoss:0.00105  StopLoss:0.04025  GradNorm:0.00279  GradNormST:0.00951  AvgTextLen:42.2  AvgSpecLen:144.9  StepTime:0.81  LR:0.000100\n",
      "   | > Step:64/68  GlobalStep:132890  TotalLoss:0.00216  PostnetLoss:0.00105  DecoderLoss:0.00111  StopLoss:0.03616  GradNorm:0.00263  GradNormST:0.00992  AvgTextLen:60.6  AvgSpecLen:177.6  StepTime:0.80  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:132894  AvgTotalLoss:0.04384  AvgPostnetLoss:0.00086  AvgDecoderLoss:0.00091  AvgStopLoss:0.04207  EpochTime:41.43  AvgStepTime:0.60\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.06998   PostnetLoss: 0.00544   DecoderLoss:0.00569  StopLoss: 0.05885  \n",
      "   | > TotalLoss: 0.08526   PostnetLoss: 0.00945   DecoderLoss:0.00978  StopLoss: 0.06603  \n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00086   Validation Loss: 0.00868\n",
      "\n",
      " > Epoch 926/1000\n",
      "   | > Step:5/68  GlobalStep:132900  TotalLoss:0.00129  PostnetLoss:0.00063  DecoderLoss:0.00066  StopLoss:0.04344  GradNorm:0.00456  GradNormST:0.02198  AvgTextLen:5.9  AvgSpecLen:33.2  StepTime:0.25  LR:0.000100\n",
      "   | > Step:15/68  GlobalStep:132910  TotalLoss:0.00149  PostnetLoss:0.00073  DecoderLoss:0.00076  StopLoss:0.04251  GradNorm:0.00356  GradNormST:0.01279  AvgTextLen:12.8  AvgSpecLen:59.1  StepTime:0.52  LR:0.000100\n",
      "   | > Step:25/68  GlobalStep:132920  TotalLoss:0.00163  PostnetLoss:0.00080  DecoderLoss:0.00083  StopLoss:0.03628  GradNorm:0.00373  GradNormST:0.01447  AvgTextLen:19.8  AvgSpecLen:73.3  StepTime:0.48  LR:0.000100\n",
      "   | > Step:35/68  GlobalStep:132930  TotalLoss:0.00182  PostnetLoss:0.00089  DecoderLoss:0.00093  StopLoss:0.04156  GradNorm:0.00372  GradNormST:0.01287  AvgTextLen:27.1  AvgSpecLen:104.7  StepTime:0.74  LR:0.000100\n",
      "   | > Step:45/68  GlobalStep:132940  TotalLoss:0.00189  PostnetLoss:0.00092  DecoderLoss:0.00097  StopLoss:0.05701  GradNorm:0.00350  GradNormST:0.01464  AvgTextLen:34.4  AvgSpecLen:118.6  StepTime:0.60  LR:0.000100\n",
      "   | > Step:55/68  GlobalStep:132950  TotalLoss:0.00209  PostnetLoss:0.00102  DecoderLoss:0.00107  StopLoss:0.03656  GradNorm:0.00268  GradNormST:0.01056  AvgTextLen:43.9  AvgSpecLen:145.2  StepTime:0.78  LR:0.000100\n",
      "   | > Step:65/68  GlobalStep:132960  TotalLoss:0.00233  PostnetLoss:0.00113  DecoderLoss:0.00120  StopLoss:0.05856  GradNorm:0.00262  GradNormST:0.02233  AvgTextLen:64.2  AvgSpecLen:207.4  StepTime:1.17  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:132963  AvgTotalLoss:0.04446  AvgPostnetLoss:0.00087  AvgDecoderLoss:0.00091  AvgStopLoss:0.04269  EpochTime:41.81  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.06791   PostnetLoss: 0.00565   DecoderLoss:0.00590  StopLoss: 0.05636  \n",
      "   | > TotalLoss: 0.08152   PostnetLoss: 0.00937   DecoderLoss:0.00969  StopLoss: 0.06246  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00087   Validation Loss: 0.00856\n",
      "\n",
      " > Epoch 927/1000\n",
      "   | > Step:6/68  GlobalStep:132970  TotalLoss:0.00124  PostnetLoss:0.00061  DecoderLoss:0.00063  StopLoss:0.03614  GradNorm:0.00390  GradNormST:0.01162  AvgTextLen:6.0  AvgSpecLen:34.8  StepTime:0.37  LR:0.000100\n",
      "   | > Step:16/68  GlobalStep:132980  TotalLoss:0.00152  PostnetLoss:0.00075  DecoderLoss:0.00078  StopLoss:0.04164  GradNorm:0.00377  GradNormST:0.01379  AvgTextLen:13.8  AvgSpecLen:60.6  StepTime:0.39  LR:0.000100\n",
      "   | > Step:26/68  GlobalStep:132990  TotalLoss:0.00166  PostnetLoss:0.00081  DecoderLoss:0.00085  StopLoss:0.03853  GradNorm:0.00338  GradNormST:0.01161  AvgTextLen:20.3  AvgSpecLen:77.5  StepTime:0.38  LR:0.000100\n",
      "   | > Step:36/68  GlobalStep:133000  TotalLoss:0.00179  PostnetLoss:0.00088  DecoderLoss:0.00092  StopLoss:0.04935  GradNorm:0.00319  GradNormST:0.01376  AvgTextLen:28.5  AvgSpecLen:100.2  StepTime:0.56  LR:0.000100\n",
      " | | > Checkpoint saving : /vm/TTS/models/cartman_models/mozilla-no-loc-fattn-stopnet-sigmoid-loss_masking-August-15-2019_11+26AM-de6aeb9/checkpoint_133000.pth.tar\n",
      "warning: audio amplitude out of range, auto clipped.\n",
      "   | > Step:46/68  GlobalStep:133010  TotalLoss:0.00193  PostnetLoss:0.00094  DecoderLoss:0.00099  StopLoss:0.04883  GradNorm:0.00328  GradNormST:0.01608  AvgTextLen:35.4  AvgSpecLen:122.6  StepTime:0.67  LR:0.000100\n",
      "   | > Step:56/68  GlobalStep:133020  TotalLoss:0.00206  PostnetLoss:0.00100  DecoderLoss:0.00106  StopLoss:0.06112  GradNorm:0.00268  GradNormST:0.02417  AvgTextLen:45.8  AvgSpecLen:145.4  StepTime:0.95  LR:0.000100\n",
      "   | > Step:66/68  GlobalStep:133030  TotalLoss:0.00244  PostnetLoss:0.00118  DecoderLoss:0.00126  StopLoss:0.07235  GradNorm:0.00293  GradNormST:0.02794  AvgTextLen:71.2  AvgSpecLen:220.8  StepTime:1.05  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:133032  AvgTotalLoss:0.04555  AvgPostnetLoss:0.00086  AvgDecoderLoss:0.00090  AvgStopLoss:0.04378  EpochTime:41.95  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.06940   PostnetLoss: 0.00587   DecoderLoss:0.00613  StopLoss: 0.05740  \n",
      "   | > TotalLoss: 0.07964   PostnetLoss: 0.00929   DecoderLoss:0.00962  StopLoss: 0.06074  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00086   Validation Loss: 0.00857\n",
      "\n",
      " > Epoch 928/1000\n",
      "   | > Step:7/68  GlobalStep:133040  TotalLoss:0.00136  PostnetLoss:0.00067  DecoderLoss:0.00069  StopLoss:0.02411  GradNorm:0.00440  GradNormST:0.01464  AvgTextLen:7.4  AvgSpecLen:44.6  StepTime:0.39  LR:0.000100\n",
      "   | > Step:17/68  GlobalStep:133050  TotalLoss:0.00148  PostnetLoss:0.00072  DecoderLoss:0.00075  StopLoss:0.04964  GradNorm:0.00349  GradNormST:0.01187  AvgTextLen:14.7  AvgSpecLen:66.0  StepTime:0.45  LR:0.000100\n",
      "   | > Step:27/68  GlobalStep:133060  TotalLoss:0.00172  PostnetLoss:0.00084  DecoderLoss:0.00088  StopLoss:0.07257  GradNorm:0.00347  GradNormST:0.02218  AvgTextLen:21.0  AvgSpecLen:80.2  StepTime:0.64  LR:0.000100\n",
      "   | > Step:37/68  GlobalStep:133070  TotalLoss:0.00186  PostnetLoss:0.00091  DecoderLoss:0.00095  StopLoss:0.04269  GradNorm:0.00358  GradNormST:0.01216  AvgTextLen:29.3  AvgSpecLen:102.4  StepTime:0.63  LR:0.000100\n",
      "   | > Step:47/68  GlobalStep:133080  TotalLoss:0.00198  PostnetLoss:0.00097  DecoderLoss:0.00101  StopLoss:0.03269  GradNorm:0.00331  GradNormST:0.01071  AvgTextLen:36.7  AvgSpecLen:126.9  StepTime:0.58  LR:0.000100\n",
      "   | > Step:57/68  GlobalStep:133090  TotalLoss:0.00212  PostnetLoss:0.00103  DecoderLoss:0.00109  StopLoss:0.02986  GradNorm:0.00278  GradNormST:0.00717  AvgTextLen:47.1  AvgSpecLen:161.0  StepTime:0.81  LR:0.000100\n",
      "   | > Step:67/68  GlobalStep:133100  TotalLoss:0.00247  PostnetLoss:0.00120  DecoderLoss:0.00127  StopLoss:0.04648  GradNorm:0.00255  GradNormST:0.02376  AvgTextLen:80.6  AvgSpecLen:246.5  StepTime:1.22  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:133101  AvgTotalLoss:0.04356  AvgPostnetLoss:0.00086  AvgDecoderLoss:0.00091  AvgStopLoss:0.04179  EpochTime:41.95  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.06868   PostnetLoss: 0.00563   DecoderLoss:0.00588  StopLoss: 0.05717  \n",
      "   | > TotalLoss: 0.08477   PostnetLoss: 0.00956   DecoderLoss:0.00988  StopLoss: 0.06532  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00086   Validation Loss: 0.00866\n",
      "\n",
      " > Epoch 929/1000\n",
      "   | > Step:8/68  GlobalStep:133110  TotalLoss:0.00133  PostnetLoss:0.00065  DecoderLoss:0.00068  StopLoss:0.04144  GradNorm:0.00384  GradNormST:0.01237  AvgTextLen:8.3  AvgSpecLen:44.0  StepTime:0.31  LR:0.000100\n",
      "   | > Step:18/68  GlobalStep:133120  TotalLoss:0.00150  PostnetLoss:0.00073  DecoderLoss:0.00076  StopLoss:0.03540  GradNorm:0.00398  GradNormST:0.00994  AvgTextLen:15.2  AvgSpecLen:64.5  StepTime:0.43  LR:0.000100\n",
      "   | > Step:28/68  GlobalStep:133130  TotalLoss:0.00172  PostnetLoss:0.00084  DecoderLoss:0.00088  StopLoss:0.03790  GradNorm:0.00407  GradNormST:0.01329  AvgTextLen:22.1  AvgSpecLen:85.4  StepTime:0.57  LR:0.000100\n",
      "   | > Step:38/68  GlobalStep:133140  TotalLoss:0.00181  PostnetLoss:0.00089  DecoderLoss:0.00093  StopLoss:0.02846  GradNorm:0.00401  GradNormST:0.00840  AvgTextLen:29.4  AvgSpecLen:104.1  StepTime:0.61  LR:0.000100\n",
      "   | > Step:48/68  GlobalStep:133150  TotalLoss:0.00199  PostnetLoss:0.00097  DecoderLoss:0.00102  StopLoss:0.03266  GradNorm:0.00368  GradNormST:0.00874  AvgTextLen:37.5  AvgSpecLen:123.8  StepTime:0.55  LR:0.000100\n",
      "   | > Step:58/68  GlobalStep:133160  TotalLoss:0.00215  PostnetLoss:0.00105  DecoderLoss:0.00110  StopLoss:0.06374  GradNorm:0.00303  GradNormST:0.02333  AvgTextLen:47.8  AvgSpecLen:158.0  StepTime:0.86  LR:0.000100\n",
      "   | > Step:68/68  GlobalStep:133170  TotalLoss:0.00244  PostnetLoss:0.00119  DecoderLoss:0.00126  StopLoss:0.03120  GradNorm:0.00320  GradNormST:0.01223  AvgTextLen:103.0  AvgSpecLen:308.9  StepTime:1.88  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:133170  AvgTotalLoss:0.04386  AvgPostnetLoss:0.00086  AvgDecoderLoss:0.00091  AvgStopLoss:0.04209  EpochTime:42.05  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.06785   PostnetLoss: 0.00568   DecoderLoss:0.00593  StopLoss: 0.05623  \n",
      "   | > TotalLoss: 0.08247   PostnetLoss: 0.00964   DecoderLoss:0.00996  StopLoss: 0.06287  \n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00086   Validation Loss: 0.00885\n",
      "\n",
      " > Epoch 930/1000\n",
      "   | > Step:9/68  GlobalStep:133180  TotalLoss:0.00131  PostnetLoss:0.00064  DecoderLoss:0.00067  StopLoss:0.03221  GradNorm:0.00325  GradNormST:0.01726  AvgTextLen:9.0  AvgSpecLen:42.1  StepTime:0.41  LR:0.000100\n",
      "   | > Step:19/68  GlobalStep:133190  TotalLoss:0.00154  PostnetLoss:0.00075  DecoderLoss:0.00078  StopLoss:0.06058  GradNorm:0.00354  GradNormST:0.01901  AvgTextLen:15.5  AvgSpecLen:63.2  StepTime:0.34  LR:0.000100\n",
      "   | > Step:29/68  GlobalStep:133200  TotalLoss:0.00166  PostnetLoss:0.00081  DecoderLoss:0.00085  StopLoss:0.05197  GradNorm:0.00371  GradNormST:0.01536  AvgTextLen:21.8  AvgSpecLen:83.4  StepTime:0.45  LR:0.000100\n",
      "   | > Step:39/68  GlobalStep:133210  TotalLoss:0.00187  PostnetLoss:0.00092  DecoderLoss:0.00096  StopLoss:0.02883  GradNorm:0.00361  GradNormST:0.01011  AvgTextLen:30.2  AvgSpecLen:110.4  StepTime:0.68  LR:0.000100\n",
      "   | > Step:49/68  GlobalStep:133220  TotalLoss:0.00202  PostnetLoss:0.00099  DecoderLoss:0.00103  StopLoss:0.04484  GradNorm:0.00398  GradNormST:0.01729  AvgTextLen:38.2  AvgSpecLen:127.7  StepTime:0.72  LR:0.000100\n",
      "   | > Step:59/68  GlobalStep:133230  TotalLoss:0.00215  PostnetLoss:0.00105  DecoderLoss:0.00110  StopLoss:0.04477  GradNorm:0.00290  GradNormST:0.01440  AvgTextLen:50.5  AvgSpecLen:155.9  StepTime:0.88  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:133239  AvgTotalLoss:0.04217  AvgPostnetLoss:0.00087  AvgDecoderLoss:0.00091  AvgStopLoss:0.04039  EpochTime:42.76  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.06793   PostnetLoss: 0.00572   DecoderLoss:0.00597  StopLoss: 0.05624  \n",
      "   | > TotalLoss: 0.08094   PostnetLoss: 0.00931   DecoderLoss:0.00964  StopLoss: 0.06199  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00087   Validation Loss: 0.00855\n",
      "\n",
      " > Epoch 931/1000\n",
      "   | > Step:0/68  GlobalStep:133240  TotalLoss:0.00133  PostnetLoss:0.00064  DecoderLoss:0.00069  StopLoss:0.05641  GradNorm:0.00586  GradNormST:0.01711  AvgTextLen:3.0  AvgSpecLen:25.3  StepTime:0.50  LR:0.000100\n",
      "   | > Step:10/68  GlobalStep:133250  TotalLoss:0.00139  PostnetLoss:0.00068  DecoderLoss:0.00071  StopLoss:0.03512  GradNorm:0.00342  GradNormST:0.02109  AvgTextLen:9.8  AvgSpecLen:48.6  StepTime:0.53  LR:0.000100\n",
      "   | > Step:20/68  GlobalStep:133260  TotalLoss:0.00155  PostnetLoss:0.00076  DecoderLoss:0.00079  StopLoss:0.03196  GradNorm:0.00342  GradNormST:0.01289  AvgTextLen:16.4  AvgSpecLen:64.8  StepTime:0.54  LR:0.000100\n",
      "   | > Step:30/68  GlobalStep:133270  TotalLoss:0.00170  PostnetLoss:0.00084  DecoderLoss:0.00087  StopLoss:0.03849  GradNorm:0.00350  GradNormST:0.01210  AvgTextLen:22.9  AvgSpecLen:85.8  StepTime:0.60  LR:0.000100\n",
      "   | > Step:40/68  GlobalStep:133280  TotalLoss:0.00186  PostnetLoss:0.00091  DecoderLoss:0.00095  StopLoss:0.04166  GradNorm:0.00409  GradNormST:0.01350  AvgTextLen:30.9  AvgSpecLen:105.3  StepTime:0.64  LR:0.000100\n",
      "   | > Step:50/68  GlobalStep:133290  TotalLoss:0.00203  PostnetLoss:0.00099  DecoderLoss:0.00104  StopLoss:0.03194  GradNorm:0.00424  GradNormST:0.00954  AvgTextLen:38.8  AvgSpecLen:132.0  StepTime:0.79  LR:0.000100\n",
      "   | > Step:60/68  GlobalStep:133300  TotalLoss:0.00220  PostnetLoss:0.00107  DecoderLoss:0.00113  StopLoss:0.02718  GradNorm:0.00394  GradNormST:0.00785  AvgTextLen:52.0  AvgSpecLen:172.7  StepTime:1.00  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:133308  AvgTotalLoss:0.04086  AvgPostnetLoss:0.00087  AvgDecoderLoss:0.00091  AvgStopLoss:0.03909  EpochTime:41.70  AvgStepTime:0.60\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.06898   PostnetLoss: 0.00567   DecoderLoss:0.00592  StopLoss: 0.05739  \n",
      "   | > TotalLoss: 0.08935   PostnetLoss: 0.00980   DecoderLoss:0.01013  StopLoss: 0.06941  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00087   Validation Loss: 0.00886\n",
      "\n",
      " > Epoch 932/1000\n",
      "   | > Step:1/68  GlobalStep:133310  TotalLoss:0.00129  PostnetLoss:0.00063  DecoderLoss:0.00067  StopLoss:0.03215  GradNorm:0.00579  GradNormST:0.01335  AvgTextLen:3.7  AvgSpecLen:28.6  StepTime:0.48  LR:0.000100\n",
      "   | > Step:11/68  GlobalStep:133320  TotalLoss:0.00135  PostnetLoss:0.00066  DecoderLoss:0.00069  StopLoss:0.03009  GradNorm:0.00321  GradNormST:0.01456  AvgTextLen:10.3  AvgSpecLen:49.4  StepTime:0.33  LR:0.000100\n",
      "   | > Step:21/68  GlobalStep:133330  TotalLoss:0.00154  PostnetLoss:0.00076  DecoderLoss:0.00079  StopLoss:0.04233  GradNorm:0.00302  GradNormST:0.01448  AvgTextLen:16.6  AvgSpecLen:66.3  StepTime:0.34  LR:0.000100\n",
      "   | > Step:31/68  GlobalStep:133340  TotalLoss:0.00170  PostnetLoss:0.00083  DecoderLoss:0.00087  StopLoss:0.04119  GradNorm:0.00347  GradNormST:0.01322  AvgTextLen:24.1  AvgSpecLen:82.7  StepTime:0.44  LR:0.000100\n",
      "   | > Step:41/68  GlobalStep:133350  TotalLoss:0.00189  PostnetLoss:0.00093  DecoderLoss:0.00097  StopLoss:0.02283  GradNorm:0.00344  GradNormST:0.00871  AvgTextLen:32.3  AvgSpecLen:109.4  StepTime:0.66  LR:0.000100\n",
      "   | > Step:51/68  GlobalStep:133360  TotalLoss:0.00199  PostnetLoss:0.00097  DecoderLoss:0.00102  StopLoss:0.03813  GradNorm:0.00381  GradNormST:0.01017  AvgTextLen:39.2  AvgSpecLen:126.9  StepTime:0.79  LR:0.000100\n",
      "   | > Step:61/68  GlobalStep:133370  TotalLoss:0.00219  PostnetLoss:0.00107  DecoderLoss:0.00112  StopLoss:0.04272  GradNorm:0.00392  GradNormST:0.01209  AvgTextLen:54.6  AvgSpecLen:165.5  StepTime:0.93  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:133377  AvgTotalLoss:0.04168  AvgPostnetLoss:0.00086  AvgDecoderLoss:0.00090  AvgStopLoss:0.03992  EpochTime:42.54  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.06455   PostnetLoss: 0.00567   DecoderLoss:0.00591  StopLoss: 0.05296  \n",
      "   | > TotalLoss: 0.08105   PostnetLoss: 0.00924   DecoderLoss:0.00958  StopLoss: 0.06223  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00086   Validation Loss: 0.00869\n",
      "\n",
      " > Epoch 933/1000\n",
      "   | > Step:2/68  GlobalStep:133380  TotalLoss:0.00118  PostnetLoss:0.00057  DecoderLoss:0.00060  StopLoss:0.03527  GradNorm:0.00416  GradNormST:0.02329  AvgTextLen:4.0  AvgSpecLen:31.3  StepTime:0.44  LR:0.000100\n",
      "   | > Step:12/68  GlobalStep:133390  TotalLoss:0.00135  PostnetLoss:0.00066  DecoderLoss:0.00069  StopLoss:0.03207  GradNorm:0.00320  GradNormST:0.01163  AvgTextLen:11.0  AvgSpecLen:49.3  StepTime:0.44  LR:0.000100\n",
      "   | > Step:22/68  GlobalStep:133400  TotalLoss:0.00160  PostnetLoss:0.00078  DecoderLoss:0.00082  StopLoss:0.04127  GradNorm:0.00302  GradNormST:0.01222  AvgTextLen:18.0  AvgSpecLen:72.0  StepTime:0.46  LR:0.000100\n",
      "   | > Step:32/68  GlobalStep:133410  TotalLoss:0.00172  PostnetLoss:0.00084  DecoderLoss:0.00088  StopLoss:0.04287  GradNorm:0.00302  GradNormST:0.01654  AvgTextLen:24.4  AvgSpecLen:91.5  StepTime:0.55  LR:0.000100\n",
      "   | > Step:42/68  GlobalStep:133420  TotalLoss:0.00186  PostnetLoss:0.00091  DecoderLoss:0.00096  StopLoss:0.03668  GradNorm:0.00318  GradNormST:0.01078  AvgTextLen:33.1  AvgSpecLen:109.7  StepTime:0.64  LR:0.000100\n",
      "   | > Step:52/68  GlobalStep:133430  TotalLoss:0.00206  PostnetLoss:0.00101  DecoderLoss:0.00105  StopLoss:0.05196  GradNorm:0.00359  GradNormST:0.01185  AvgTextLen:42.1  AvgSpecLen:137.5  StepTime:0.86  LR:0.000100\n",
      "   | > Step:62/68  GlobalStep:133440  TotalLoss:0.00225  PostnetLoss:0.00110  DecoderLoss:0.00116  StopLoss:0.03263  GradNorm:0.00355  GradNormST:0.00821  AvgTextLen:54.0  AvgSpecLen:174.7  StepTime:0.75  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:133446  AvgTotalLoss:0.04095  AvgPostnetLoss:0.00086  AvgDecoderLoss:0.00090  AvgStopLoss:0.03918  EpochTime:43.27  AvgStepTime:0.63\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.06740   PostnetLoss: 0.00555   DecoderLoss:0.00580  StopLoss: 0.05606  \n",
      "   | > TotalLoss: 0.09109   PostnetLoss: 0.00960   DecoderLoss:0.00993  StopLoss: 0.07156  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00086   Validation Loss: 0.00884\n",
      "\n",
      " > Epoch 934/1000\n",
      "   | > Step:3/68  GlobalStep:133450  TotalLoss:0.00119  PostnetLoss:0.00058  DecoderLoss:0.00061  StopLoss:0.07413  GradNorm:0.00363  GradNormST:0.02452  AvgTextLen:4.7  AvgSpecLen:29.7  StepTime:0.30  LR:0.000100\n",
      "   | > Step:13/68  GlobalStep:133460  TotalLoss:0.00135  PostnetLoss:0.00066  DecoderLoss:0.00069  StopLoss:0.03960  GradNorm:0.00309  GradNormST:0.01618  AvgTextLen:11.5  AvgSpecLen:55.2  StepTime:0.41  LR:0.000100\n",
      "   | > Step:23/68  GlobalStep:133470  TotalLoss:0.00161  PostnetLoss:0.00079  DecoderLoss:0.00082  StopLoss:0.03698  GradNorm:0.00305  GradNormST:0.01153  AvgTextLen:18.3  AvgSpecLen:72.2  StepTime:0.36  LR:0.000100\n",
      "   | > Step:33/68  GlobalStep:133480  TotalLoss:0.00177  PostnetLoss:0.00087  DecoderLoss:0.00090  StopLoss:0.03818  GradNorm:0.00340  GradNormST:0.01187  AvgTextLen:25.9  AvgSpecLen:88.9  StepTime:0.54  LR:0.000100\n",
      "   | > Step:43/68  GlobalStep:133490  TotalLoss:0.00191  PostnetLoss:0.00093  DecoderLoss:0.00098  StopLoss:0.04338  GradNorm:0.00351  GradNormST:0.01020  AvgTextLen:33.4  AvgSpecLen:115.7  StepTime:0.63  LR:0.000100\n",
      "   | > Step:53/68  GlobalStep:133500  TotalLoss:0.00204  PostnetLoss:0.00100  DecoderLoss:0.00104  StopLoss:0.03429  GradNorm:0.00386  GradNormST:0.00878  AvgTextLen:42.4  AvgSpecLen:137.6  StepTime:0.74  LR:0.000100\n",
      "   | > Step:63/68  GlobalStep:133510  TotalLoss:0.00228  PostnetLoss:0.00111  DecoderLoss:0.00117  StopLoss:0.03421  GradNorm:0.00282  GradNormST:0.01238  AvgTextLen:58.6  AvgSpecLen:185.1  StepTime:0.91  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:133515  AvgTotalLoss:0.03954  AvgPostnetLoss:0.00086  AvgDecoderLoss:0.00090  AvgStopLoss:0.03778  EpochTime:42.07  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.06833   PostnetLoss: 0.00591   DecoderLoss:0.00615  StopLoss: 0.05626  \n",
      "   | > TotalLoss: 0.08569   PostnetLoss: 0.00935   DecoderLoss:0.00968  StopLoss: 0.06665  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00086   Validation Loss: 0.00866\n",
      "\n",
      " > Epoch 935/1000\n",
      "   | > Step:4/68  GlobalStep:133520  TotalLoss:0.00119  PostnetLoss:0.00058  DecoderLoss:0.00061  StopLoss:0.06017  GradNorm:0.00372  GradNormST:0.03006  AvgTextLen:4.8  AvgSpecLen:31.1  StepTime:0.21  LR:0.000100\n",
      "   | > Step:14/68  GlobalStep:133530  TotalLoss:0.00144  PostnetLoss:0.00070  DecoderLoss:0.00074  StopLoss:0.03629  GradNorm:0.00328  GradNormST:0.01202  AvgTextLen:12.2  AvgSpecLen:52.0  StepTime:0.37  LR:0.000100\n",
      "   | > Step:24/68  GlobalStep:133540  TotalLoss:0.00165  PostnetLoss:0.00081  DecoderLoss:0.00084  StopLoss:0.03692  GradNorm:0.00293  GradNormST:0.01263  AvgTextLen:19.2  AvgSpecLen:79.0  StepTime:0.39  LR:0.000100\n",
      "   | > Step:34/68  GlobalStep:133550  TotalLoss:0.00179  PostnetLoss:0.00088  DecoderLoss:0.00092  StopLoss:0.02778  GradNorm:0.00316  GradNormST:0.00851  AvgTextLen:26.2  AvgSpecLen:95.9  StepTime:0.63  LR:0.000100\n",
      "   | > Step:44/68  GlobalStep:133560  TotalLoss:0.00192  PostnetLoss:0.00094  DecoderLoss:0.00098  StopLoss:0.02869  GradNorm:0.00298  GradNormST:0.01133  AvgTextLen:33.8  AvgSpecLen:116.3  StepTime:0.66  LR:0.000100\n",
      "   | > Step:54/68  GlobalStep:133570  TotalLoss:0.00210  PostnetLoss:0.00102  DecoderLoss:0.00107  StopLoss:0.03831  GradNorm:0.00464  GradNormST:0.01005  AvgTextLen:42.2  AvgSpecLen:144.9  StepTime:0.88  LR:0.000100\n",
      "   | > Step:64/68  GlobalStep:133580  TotalLoss:0.00219  PostnetLoss:0.00106  DecoderLoss:0.00112  StopLoss:0.03740  GradNorm:0.00391  GradNormST:0.01145  AvgTextLen:60.6  AvgSpecLen:177.6  StepTime:0.80  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:133584  AvgTotalLoss:0.04068  AvgPostnetLoss:0.00086  AvgDecoderLoss:0.00090  AvgStopLoss:0.03891  EpochTime:41.67  AvgStepTime:0.60\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.06732   PostnetLoss: 0.00559   DecoderLoss:0.00584  StopLoss: 0.05589  \n",
      "   | > TotalLoss: 0.07911   PostnetLoss: 0.00932   DecoderLoss:0.00966  StopLoss: 0.06013  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00086   Validation Loss: 0.00860\n",
      "\n",
      " > Epoch 936/1000\n",
      "   | > Step:5/68  GlobalStep:133590  TotalLoss:0.00127  PostnetLoss:0.00062  DecoderLoss:0.00065  StopLoss:0.03522  GradNorm:0.00397  GradNormST:0.01023  AvgTextLen:5.9  AvgSpecLen:33.2  StepTime:0.25  LR:0.000100\n",
      "   | > Step:15/68  GlobalStep:133600  TotalLoss:0.00150  PostnetLoss:0.00074  DecoderLoss:0.00077  StopLoss:0.03254  GradNorm:0.00308  GradNormST:0.00983  AvgTextLen:12.8  AvgSpecLen:59.1  StepTime:0.46  LR:0.000100\n",
      "   | > Step:25/68  GlobalStep:133610  TotalLoss:0.00160  PostnetLoss:0.00078  DecoderLoss:0.00082  StopLoss:0.04608  GradNorm:0.00305  GradNormST:0.01270  AvgTextLen:19.8  AvgSpecLen:73.3  StepTime:0.45  LR:0.000100\n",
      "   | > Step:35/68  GlobalStep:133620  TotalLoss:0.00183  PostnetLoss:0.00089  DecoderLoss:0.00093  StopLoss:0.04898  GradNorm:0.00286  GradNormST:0.01707  AvgTextLen:27.1  AvgSpecLen:104.7  StepTime:0.81  LR:0.000100\n",
      "   | > Step:45/68  GlobalStep:133630  TotalLoss:0.00189  PostnetLoss:0.00092  DecoderLoss:0.00097  StopLoss:0.04195  GradNorm:0.00303  GradNormST:0.01237  AvgTextLen:34.4  AvgSpecLen:118.6  StepTime:0.65  LR:0.000100\n",
      "   | > Step:55/68  GlobalStep:133640  TotalLoss:0.00214  PostnetLoss:0.00104  DecoderLoss:0.00109  StopLoss:0.03178  GradNorm:0.00416  GradNormST:0.01164  AvgTextLen:43.9  AvgSpecLen:145.2  StepTime:0.77  LR:0.000100\n",
      "   | > Step:65/68  GlobalStep:133650  TotalLoss:0.00232  PostnetLoss:0.00113  DecoderLoss:0.00119  StopLoss:0.06041  GradNorm:0.00298  GradNormST:0.03039  AvgTextLen:64.2  AvgSpecLen:207.4  StepTime:1.19  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:133653  AvgTotalLoss:0.04133  AvgPostnetLoss:0.00086  AvgDecoderLoss:0.00090  AvgStopLoss:0.03957  EpochTime:42.75  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.06914   PostnetLoss: 0.00596   DecoderLoss:0.00620  StopLoss: 0.05698  \n",
      "   | > TotalLoss: 0.08511   PostnetLoss: 0.00908   DecoderLoss:0.00942  StopLoss: 0.06661  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00086   Validation Loss: 0.00863\n",
      "\n",
      " > Epoch 937/1000\n",
      "   | > Step:6/68  GlobalStep:133660  TotalLoss:0.00125  PostnetLoss:0.00061  DecoderLoss:0.00064  StopLoss:0.03139  GradNorm:0.00361  GradNormST:0.01177  AvgTextLen:6.0  AvgSpecLen:34.8  StepTime:0.29  LR:0.000100\n",
      "   | > Step:16/68  GlobalStep:133670  TotalLoss:0.00151  PostnetLoss:0.00074  DecoderLoss:0.00077  StopLoss:0.04032  GradNorm:0.00319  GradNormST:0.01272  AvgTextLen:13.8  AvgSpecLen:60.6  StepTime:0.33  LR:0.000100\n",
      "   | > Step:26/68  GlobalStep:133680  TotalLoss:0.00166  PostnetLoss:0.00081  DecoderLoss:0.00085  StopLoss:0.03091  GradNorm:0.00295  GradNormST:0.01248  AvgTextLen:20.3  AvgSpecLen:77.5  StepTime:0.46  LR:0.000100\n",
      "   | > Step:36/68  GlobalStep:133690  TotalLoss:0.00179  PostnetLoss:0.00087  DecoderLoss:0.00092  StopLoss:0.04223  GradNorm:0.00274  GradNormST:0.01213  AvgTextLen:28.5  AvgSpecLen:100.2  StepTime:0.69  LR:0.000100\n",
      "   | > Step:46/68  GlobalStep:133700  TotalLoss:0.00195  PostnetLoss:0.00095  DecoderLoss:0.00100  StopLoss:0.04693  GradNorm:0.00331  GradNormST:0.01669  AvgTextLen:35.4  AvgSpecLen:122.6  StepTime:0.79  LR:0.000100\n",
      "   | > Step:56/68  GlobalStep:133710  TotalLoss:0.00210  PostnetLoss:0.00102  DecoderLoss:0.00108  StopLoss:0.03478  GradNorm:0.00388  GradNormST:0.01399  AvgTextLen:45.8  AvgSpecLen:145.4  StepTime:0.97  LR:0.000100\n",
      "   | > Step:66/68  GlobalStep:133720  TotalLoss:0.00248  PostnetLoss:0.00121  DecoderLoss:0.00128  StopLoss:0.03502  GradNorm:0.00322  GradNormST:0.01332  AvgTextLen:71.2  AvgSpecLen:220.8  StepTime:1.04  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:133722  AvgTotalLoss:0.04127  AvgPostnetLoss:0.00086  AvgDecoderLoss:0.00090  AvgStopLoss:0.03951  EpochTime:42.64  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.07083   PostnetLoss: 0.00583   DecoderLoss:0.00607  StopLoss: 0.05893  \n",
      "   | > TotalLoss: 0.08749   PostnetLoss: 0.00931   DecoderLoss:0.00965  StopLoss: 0.06854  \n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00086   Validation Loss: 0.00866\n",
      "\n",
      " > Epoch 938/1000\n",
      "   | > Step:7/68  GlobalStep:133730  TotalLoss:0.00133  PostnetLoss:0.00065  DecoderLoss:0.00068  StopLoss:0.03873  GradNorm:0.00359  GradNormST:0.01815  AvgTextLen:7.4  AvgSpecLen:44.6  StepTime:0.39  LR:0.000100\n",
      "   | > Step:17/68  GlobalStep:133740  TotalLoss:0.00147  PostnetLoss:0.00072  DecoderLoss:0.00075  StopLoss:0.04894  GradNorm:0.00331  GradNormST:0.01315  AvgTextLen:14.7  AvgSpecLen:66.0  StepTime:0.46  LR:0.000100\n",
      "   | > Step:27/68  GlobalStep:133750  TotalLoss:0.00166  PostnetLoss:0.00081  DecoderLoss:0.00085  StopLoss:0.04903  GradNorm:0.00299  GradNormST:0.01386  AvgTextLen:21.0  AvgSpecLen:80.2  StepTime:0.64  LR:0.000100\n",
      "   | > Step:37/68  GlobalStep:133760  TotalLoss:0.00182  PostnetLoss:0.00089  DecoderLoss:0.00093  StopLoss:0.03709  GradNorm:0.00281  GradNormST:0.01053  AvgTextLen:29.3  AvgSpecLen:102.4  StepTime:0.60  LR:0.000100\n",
      "   | > Step:47/68  GlobalStep:133770  TotalLoss:0.00198  PostnetLoss:0.00097  DecoderLoss:0.00101  StopLoss:0.02820  GradNorm:0.00299  GradNormST:0.00952  AvgTextLen:36.7  AvgSpecLen:126.9  StepTime:0.58  LR:0.000100\n",
      "   | > Step:57/68  GlobalStep:133780  TotalLoss:0.00217  PostnetLoss:0.00106  DecoderLoss:0.00111  StopLoss:0.03718  GradNorm:0.00382  GradNormST:0.00945  AvgTextLen:47.1  AvgSpecLen:161.0  StepTime:0.90  LR:0.000100\n",
      "   | > Step:67/68  GlobalStep:133790  TotalLoss:0.00246  PostnetLoss:0.00120  DecoderLoss:0.00127  StopLoss:0.05437  GradNorm:0.00290  GradNormST:0.02518  AvgTextLen:80.6  AvgSpecLen:246.5  StepTime:1.20  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:133791  AvgTotalLoss:0.04086  AvgPostnetLoss:0.00086  AvgDecoderLoss:0.00090  AvgStopLoss:0.03909  EpochTime:42.46  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.07016   PostnetLoss: 0.00593   DecoderLoss:0.00618  StopLoss: 0.05804  \n",
      "   | > TotalLoss: 0.08390   PostnetLoss: 0.00930   DecoderLoss:0.00964  StopLoss: 0.06495  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00086   Validation Loss: 0.00868\n",
      "\n",
      " > Epoch 939/1000\n",
      "   | > Step:8/68  GlobalStep:133800  TotalLoss:0.00134  PostnetLoss:0.00066  DecoderLoss:0.00069  StopLoss:0.04098  GradNorm:0.00382  GradNormST:0.01484  AvgTextLen:8.3  AvgSpecLen:44.0  StepTime:0.33  LR:0.000100\n",
      "   | > Step:18/68  GlobalStep:133810  TotalLoss:0.00148  PostnetLoss:0.00072  DecoderLoss:0.00075  StopLoss:0.04141  GradNorm:0.00319  GradNormST:0.01373  AvgTextLen:15.2  AvgSpecLen:64.5  StepTime:0.38  LR:0.000100\n",
      "   | > Step:28/68  GlobalStep:133820  TotalLoss:0.00170  PostnetLoss:0.00083  DecoderLoss:0.00087  StopLoss:0.03250  GradNorm:0.00301  GradNormST:0.00979  AvgTextLen:22.1  AvgSpecLen:85.4  StepTime:0.58  LR:0.000100\n",
      "   | > Step:38/68  GlobalStep:133830  TotalLoss:0.00178  PostnetLoss:0.00087  DecoderLoss:0.00091  StopLoss:0.03750  GradNorm:0.00275  GradNormST:0.01071  AvgTextLen:29.4  AvgSpecLen:104.1  StepTime:0.58  LR:0.000100\n",
      "   | > Step:48/68  GlobalStep:133840  TotalLoss:0.00197  PostnetLoss:0.00096  DecoderLoss:0.00101  StopLoss:0.03525  GradNorm:0.00290  GradNormST:0.00880  AvgTextLen:37.5  AvgSpecLen:123.8  StepTime:0.65  LR:0.000100\n",
      "   | > Step:58/68  GlobalStep:133850  TotalLoss:0.00216  PostnetLoss:0.00105  DecoderLoss:0.00110  StopLoss:0.04049  GradNorm:0.00343  GradNormST:0.00927  AvgTextLen:47.8  AvgSpecLen:158.0  StepTime:0.85  LR:0.000100\n",
      "   | > Step:68/68  GlobalStep:133860  TotalLoss:0.00243  PostnetLoss:0.00118  DecoderLoss:0.00125  StopLoss:0.02294  GradNorm:0.00283  GradNormST:0.01132  AvgTextLen:103.0  AvgSpecLen:308.9  StepTime:1.86  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:133860  AvgTotalLoss:0.04242  AvgPostnetLoss:0.00086  AvgDecoderLoss:0.00090  AvgStopLoss:0.04066  EpochTime:42.93  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.06899   PostnetLoss: 0.00600   DecoderLoss:0.00625  StopLoss: 0.05674  \n",
      "   | > TotalLoss: 0.10507   PostnetLoss: 0.00902   DecoderLoss:0.00936  StopLoss: 0.08669  \n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00086   Validation Loss: 0.00842\n",
      "\n",
      " > Epoch 940/1000\n",
      "   | > Step:9/68  GlobalStep:133870  TotalLoss:0.00135  PostnetLoss:0.00066  DecoderLoss:0.00069  StopLoss:0.03537  GradNorm:0.00354  GradNormST:0.01746  AvgTextLen:9.0  AvgSpecLen:42.1  StepTime:0.46  LR:0.000100\n",
      "   | > Step:19/68  GlobalStep:133880  TotalLoss:0.00148  PostnetLoss:0.00073  DecoderLoss:0.00076  StopLoss:0.06176  GradNorm:0.00302  GradNormST:0.01661  AvgTextLen:15.5  AvgSpecLen:63.2  StepTime:0.34  LR:0.000100\n",
      "   | > Step:29/68  GlobalStep:133890  TotalLoss:0.00166  PostnetLoss:0.00081  DecoderLoss:0.00085  StopLoss:0.06725  GradNorm:0.00307  GradNormST:0.01697  AvgTextLen:21.8  AvgSpecLen:83.4  StepTime:0.46  LR:0.000100\n",
      "   | > Step:39/68  GlobalStep:133900  TotalLoss:0.00184  PostnetLoss:0.00090  DecoderLoss:0.00094  StopLoss:0.03147  GradNorm:0.00266  GradNormST:0.01126  AvgTextLen:30.2  AvgSpecLen:110.4  StepTime:0.64  LR:0.000100\n",
      "   | > Step:49/68  GlobalStep:133910  TotalLoss:0.00248  PostnetLoss:0.00121  DecoderLoss:0.00127  StopLoss:0.03695  GradNorm:0.01626  GradNormST:0.00888  AvgTextLen:38.2  AvgSpecLen:127.7  StepTime:0.70  LR:0.000100\n",
      "   | > Step:59/68  GlobalStep:133920  TotalLoss:0.00268  PostnetLoss:0.00130  DecoderLoss:0.00138  StopLoss:0.05303  GradNorm:0.00626  GradNormST:0.01622  AvgTextLen:50.5  AvgSpecLen:155.9  StepTime:0.87  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:133929  AvgTotalLoss:0.04552  AvgPostnetLoss:0.00096  AvgDecoderLoss:0.00100  AvgStopLoss:0.04357  EpochTime:42.67  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.07210   PostnetLoss: 0.00594   DecoderLoss:0.00626  StopLoss: 0.05990  \n",
      "   | > TotalLoss: 0.09572   PostnetLoss: 0.00939   DecoderLoss:0.00983  StopLoss: 0.07650  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00096   Validation Loss: 0.00840\n",
      "\n",
      " > Epoch 941/1000\n",
      "   | > Step:0/68  GlobalStep:133930  TotalLoss:0.00161  PostnetLoss:0.00078  DecoderLoss:0.00082  StopLoss:0.08050  GradNorm:0.01097  GradNormST:0.02555  AvgTextLen:3.0  AvgSpecLen:25.3  StepTime:0.47  LR:0.000100\n",
      "   | > Step:10/68  GlobalStep:133940  TotalLoss:0.00161  PostnetLoss:0.00079  DecoderLoss:0.00083  StopLoss:0.04886  GradNorm:0.00447  GradNormST:0.02324  AvgTextLen:9.8  AvgSpecLen:48.6  StepTime:0.43  LR:0.000100\n",
      "   | > Step:20/68  GlobalStep:133950  TotalLoss:0.00177  PostnetLoss:0.00087  DecoderLoss:0.00090  StopLoss:0.04210  GradNorm:0.00384  GradNormST:0.01610  AvgTextLen:16.4  AvgSpecLen:64.8  StepTime:0.47  LR:0.000100\n",
      "   | > Step:30/68  GlobalStep:133960  TotalLoss:0.00194  PostnetLoss:0.00095  DecoderLoss:0.00099  StopLoss:0.07458  GradNorm:0.00387  GradNormST:0.01725  AvgTextLen:22.9  AvgSpecLen:85.8  StepTime:0.47  LR:0.000100\n",
      "   | > Step:40/68  GlobalStep:133970  TotalLoss:0.00213  PostnetLoss:0.00104  DecoderLoss:0.00109  StopLoss:0.05371  GradNorm:0.02015  GradNormST:0.01742  AvgTextLen:30.9  AvgSpecLen:105.3  StepTime:0.64  LR:0.000100\n",
      "   | > Step:50/68  GlobalStep:133980  TotalLoss:0.00241  PostnetLoss:0.00117  DecoderLoss:0.00124  StopLoss:0.04098  GradNorm:0.00403  GradNormST:0.01240  AvgTextLen:38.8  AvgSpecLen:132.0  StepTime:0.67  LR:0.000100\n",
      "   | > Step:60/68  GlobalStep:133990  TotalLoss:0.00246  PostnetLoss:0.00120  DecoderLoss:0.00126  StopLoss:0.07418  GradNorm:0.00405  GradNormST:0.03775  AvgTextLen:52.0  AvgSpecLen:172.7  StepTime:0.92  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:133998  AvgTotalLoss:0.05987  AvgPostnetLoss:0.00099  AvgDecoderLoss:0.00104  AvgStopLoss:0.05784  EpochTime:41.80  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.08259   PostnetLoss: 0.00587   DecoderLoss:0.00615  StopLoss: 0.07056  \n",
      "   | > TotalLoss: 0.08612   PostnetLoss: 0.00925   DecoderLoss:0.00961  StopLoss: 0.06727  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00099   Validation Loss: 0.00838\n",
      "\n",
      " > Epoch 942/1000\n",
      "   | > Step:1/68  GlobalStep:134000  TotalLoss:0.00138  PostnetLoss:0.00068  DecoderLoss:0.00071  StopLoss:0.04102  GradNorm:0.00734  GradNormST:0.01704  AvgTextLen:3.7  AvgSpecLen:28.6  StepTime:0.37  LR:0.000100\n",
      " | | > Checkpoint saving : /vm/TTS/models/cartman_models/mozilla-no-loc-fattn-stopnet-sigmoid-loss_masking-August-15-2019_11+26AM-de6aeb9/checkpoint_134000.pth.tar\n",
      "   | > Step:11/68  GlobalStep:134010  TotalLoss:0.00150  PostnetLoss:0.00074  DecoderLoss:0.00077  StopLoss:0.03401  GradNorm:0.00403  GradNormST:0.01389  AvgTextLen:10.3  AvgSpecLen:49.4  StepTime:0.30  LR:0.000100\n",
      "   | > Step:21/68  GlobalStep:134020  TotalLoss:0.00165  PostnetLoss:0.00081  DecoderLoss:0.00084  StopLoss:0.05358  GradNorm:0.00337  GradNormST:0.01705  AvgTextLen:16.6  AvgSpecLen:66.3  StepTime:0.44  LR:0.000100\n",
      "   | > Step:31/68  GlobalStep:134030  TotalLoss:0.00181  PostnetLoss:0.00088  DecoderLoss:0.00092  StopLoss:0.04248  GradNorm:0.00316  GradNormST:0.01294  AvgTextLen:24.1  AvgSpecLen:82.7  StepTime:0.53  LR:0.000100\n",
      "   | > Step:41/68  GlobalStep:134040  TotalLoss:0.00202  PostnetLoss:0.00098  DecoderLoss:0.00103  StopLoss:0.04660  GradNorm:0.00313  GradNormST:0.01947  AvgTextLen:32.3  AvgSpecLen:109.4  StepTime:0.52  LR:0.000100\n",
      "   | > Step:51/68  GlobalStep:134050  TotalLoss:0.00217  PostnetLoss:0.00106  DecoderLoss:0.00111  StopLoss:0.06441  GradNorm:0.00298  GradNormST:0.02908  AvgTextLen:39.2  AvgSpecLen:126.9  StepTime:0.65  LR:0.000100\n",
      "   | > Step:61/68  GlobalStep:134060  TotalLoss:0.00231  PostnetLoss:0.00113  DecoderLoss:0.00119  StopLoss:0.08381  GradNorm:0.00319  GradNormST:0.02509  AvgTextLen:54.6  AvgSpecLen:165.5  StepTime:0.91  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:134067  AvgTotalLoss:0.05929  AvgPostnetLoss:0.00093  AvgDecoderLoss:0.00098  AvgStopLoss:0.05738  EpochTime:41.87  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.07977   PostnetLoss: 0.00602   DecoderLoss:0.00630  StopLoss: 0.06745  \n",
      "   | > TotalLoss: 0.08296   PostnetLoss: 0.00830   DecoderLoss:0.00863  StopLoss: 0.06604  \n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00093   Validation Loss: 0.00808\n",
      "\n",
      " > Epoch 943/1000\n",
      "   | > Step:2/68  GlobalStep:134070  TotalLoss:0.00130  PostnetLoss:0.00064  DecoderLoss:0.00067  StopLoss:0.04577  GradNorm:0.00659  GradNormST:0.02036  AvgTextLen:4.0  AvgSpecLen:31.3  StepTime:0.39  LR:0.000100\n",
      "   | > Step:12/68  GlobalStep:134080  TotalLoss:0.00144  PostnetLoss:0.00070  DecoderLoss:0.00074  StopLoss:0.06820  GradNorm:0.00442  GradNormST:0.04448  AvgTextLen:11.0  AvgSpecLen:49.3  StepTime:0.44  LR:0.000100\n",
      "   | > Step:22/68  GlobalStep:134090  TotalLoss:0.00171  PostnetLoss:0.00084  DecoderLoss:0.00087  StopLoss:0.05832  GradNorm:0.00431  GradNormST:0.01612  AvgTextLen:18.0  AvgSpecLen:72.0  StepTime:0.45  LR:0.000100\n",
      "   | > Step:32/68  GlobalStep:134100  TotalLoss:0.00177  PostnetLoss:0.00087  DecoderLoss:0.00091  StopLoss:0.06101  GradNorm:0.00318  GradNormST:0.01729  AvgTextLen:24.4  AvgSpecLen:91.5  StepTime:0.47  LR:0.000100\n",
      "   | > Step:42/68  GlobalStep:134110  TotalLoss:0.00192  PostnetLoss:0.00093  DecoderLoss:0.00099  StopLoss:0.05911  GradNorm:0.00281  GradNormST:0.01978  AvgTextLen:33.1  AvgSpecLen:109.7  StepTime:0.63  LR:0.000100\n",
      "   | > Step:52/68  GlobalStep:134120  TotalLoss:0.00209  PostnetLoss:0.00102  DecoderLoss:0.00107  StopLoss:0.09335  GradNorm:0.00294  GradNormST:0.04210  AvgTextLen:42.1  AvgSpecLen:137.5  StepTime:0.72  LR:0.000100\n",
      "   | > Step:62/68  GlobalStep:134130  TotalLoss:0.00232  PostnetLoss:0.00113  DecoderLoss:0.00119  StopLoss:0.06914  GradNorm:0.00324  GradNormST:0.03150  AvgTextLen:54.0  AvgSpecLen:174.7  StepTime:0.78  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:134136  AvgTotalLoss:0.06300  AvgPostnetLoss:0.00091  AvgDecoderLoss:0.00095  AvgStopLoss:0.06114  EpochTime:41.87  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.07762   PostnetLoss: 0.00609   DecoderLoss:0.00635  StopLoss: 0.06519  \n",
      "   | > TotalLoss: 0.09084   PostnetLoss: 0.00858   DecoderLoss:0.00893  StopLoss: 0.07333  \n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00091   Validation Loss: 0.00819\n",
      "\n",
      " > Epoch 944/1000\n",
      "   | > Step:3/68  GlobalStep:134140  TotalLoss:0.00123  PostnetLoss:0.00060  DecoderLoss:0.00063  StopLoss:0.04351  GradNorm:0.00491  GradNormST:0.01500  AvgTextLen:4.7  AvgSpecLen:29.7  StepTime:0.30  LR:0.000100\n",
      "   | > Step:13/68  GlobalStep:134150  TotalLoss:0.00145  PostnetLoss:0.00071  DecoderLoss:0.00074  StopLoss:0.06254  GradNorm:0.00394  GradNormST:0.02441  AvgTextLen:11.5  AvgSpecLen:55.2  StepTime:0.42  LR:0.000100\n",
      "   | > Step:23/68  GlobalStep:134160  TotalLoss:0.00167  PostnetLoss:0.00082  DecoderLoss:0.00085  StopLoss:0.04169  GradNorm:0.00329  GradNormST:0.01522  AvgTextLen:18.3  AvgSpecLen:72.2  StepTime:0.45  LR:0.000100\n",
      "   | > Step:33/68  GlobalStep:134170  TotalLoss:0.00183  PostnetLoss:0.00089  DecoderLoss:0.00093  StopLoss:0.04617  GradNorm:0.00326  GradNormST:0.01120  AvgTextLen:25.9  AvgSpecLen:88.9  StepTime:0.54  LR:0.000100\n",
      "   | > Step:43/68  GlobalStep:134180  TotalLoss:0.00196  PostnetLoss:0.00096  DecoderLoss:0.00100  StopLoss:0.05050  GradNorm:0.00291  GradNormST:0.01491  AvgTextLen:33.4  AvgSpecLen:115.7  StepTime:0.63  LR:0.000100\n",
      "   | > Step:53/68  GlobalStep:134190  TotalLoss:0.00211  PostnetLoss:0.00103  DecoderLoss:0.00108  StopLoss:0.06319  GradNorm:0.00299  GradNormST:0.02900  AvgTextLen:42.4  AvgSpecLen:137.6  StepTime:0.66  LR:0.000100\n",
      "   | > Step:63/68  GlobalStep:134200  TotalLoss:0.00237  PostnetLoss:0.00115  DecoderLoss:0.00122  StopLoss:0.04884  GradNorm:0.00283  GradNormST:0.02453  AvgTextLen:58.6  AvgSpecLen:185.1  StepTime:0.91  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:134205  AvgTotalLoss:0.05914  AvgPostnetLoss:0.00089  AvgDecoderLoss:0.00093  AvgStopLoss:0.05731  EpochTime:42.97  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.07670   PostnetLoss: 0.00577   DecoderLoss:0.00603  StopLoss: 0.06489  \n",
      "   | > TotalLoss: 0.10906   PostnetLoss: 0.00878   DecoderLoss:0.00915  StopLoss: 0.09113  \n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00089   Validation Loss: 0.00819\n",
      "\n",
      " > Epoch 945/1000\n",
      "   | > Step:4/68  GlobalStep:134210  TotalLoss:0.00126  PostnetLoss:0.00062  DecoderLoss:0.00065  StopLoss:0.06526  GradNorm:0.00535  GradNormST:0.02699  AvgTextLen:4.8  AvgSpecLen:31.1  StepTime:0.26  LR:0.000100\n",
      "   | > Step:14/68  GlobalStep:134220  TotalLoss:0.00150  PostnetLoss:0.00073  DecoderLoss:0.00077  StopLoss:0.04461  GradNorm:0.00447  GradNormST:0.01469  AvgTextLen:12.2  AvgSpecLen:52.0  StepTime:0.35  LR:0.000100\n",
      "   | > Step:24/68  GlobalStep:134230  TotalLoss:0.00171  PostnetLoss:0.00084  DecoderLoss:0.00088  StopLoss:0.04554  GradNorm:0.00398  GradNormST:0.01323  AvgTextLen:19.2  AvgSpecLen:79.0  StepTime:0.49  LR:0.000100\n",
      "   | > Step:34/68  GlobalStep:134240  TotalLoss:0.00184  PostnetLoss:0.00090  DecoderLoss:0.00094  StopLoss:0.05910  GradNorm:0.00328  GradNormST:0.02111  AvgTextLen:26.2  AvgSpecLen:95.9  StepTime:0.58  LR:0.000100\n",
      "   | > Step:44/68  GlobalStep:134250  TotalLoss:0.00197  PostnetLoss:0.00096  DecoderLoss:0.00101  StopLoss:0.03170  GradNorm:0.00313  GradNormST:0.01188  AvgTextLen:33.8  AvgSpecLen:116.3  StepTime:0.66  LR:0.000100\n",
      "   | > Step:54/68  GlobalStep:134260  TotalLoss:0.00212  PostnetLoss:0.00103  DecoderLoss:0.00109  StopLoss:0.07374  GradNorm:0.00290  GradNormST:0.03876  AvgTextLen:42.2  AvgSpecLen:144.9  StepTime:0.71  LR:0.000100\n",
      "   | > Step:64/68  GlobalStep:134270  TotalLoss:0.00220  PostnetLoss:0.00107  DecoderLoss:0.00113  StopLoss:0.06277  GradNorm:0.00267  GradNormST:0.02811  AvgTextLen:60.6  AvgSpecLen:177.6  StepTime:0.80  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:134274  AvgTotalLoss:0.05513  AvgPostnetLoss:0.00089  AvgDecoderLoss:0.00093  AvgStopLoss:0.05331  EpochTime:42.49  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.07691   PostnetLoss: 0.00582   DecoderLoss:0.00609  StopLoss: 0.06500  \n",
      "   | > TotalLoss: 0.12047   PostnetLoss: 0.00900   DecoderLoss:0.00934  StopLoss: 0.10212  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00089   Validation Loss: 0.00824\n",
      "\n",
      " > Epoch 946/1000\n",
      "   | > Step:5/68  GlobalStep:134280  TotalLoss:0.00129  PostnetLoss:0.00063  DecoderLoss:0.00066  StopLoss:0.07102  GradNorm:0.00426  GradNormST:0.02671  AvgTextLen:5.9  AvgSpecLen:33.2  StepTime:0.36  LR:0.000100\n",
      "   | > Step:15/68  GlobalStep:134290  TotalLoss:0.00151  PostnetLoss:0.00074  DecoderLoss:0.00077  StopLoss:0.07159  GradNorm:0.00330  GradNormST:0.02929  AvgTextLen:12.8  AvgSpecLen:59.1  StepTime:0.51  LR:0.000100\n",
      "   | > Step:25/68  GlobalStep:134300  TotalLoss:0.00163  PostnetLoss:0.00080  DecoderLoss:0.00083  StopLoss:0.05713  GradNorm:0.00316  GradNormST:0.02225  AvgTextLen:19.8  AvgSpecLen:73.3  StepTime:0.48  LR:0.000100\n",
      "   | > Step:35/68  GlobalStep:134310  TotalLoss:0.00186  PostnetLoss:0.00091  DecoderLoss:0.00095  StopLoss:0.06087  GradNorm:0.00316  GradNormST:0.01760  AvgTextLen:27.1  AvgSpecLen:104.7  StepTime:0.74  LR:0.000100\n",
      "   | > Step:45/68  GlobalStep:134320  TotalLoss:0.00194  PostnetLoss:0.00094  DecoderLoss:0.00099  StopLoss:0.05103  GradNorm:0.00352  GradNormST:0.01286  AvgTextLen:34.4  AvgSpecLen:118.6  StepTime:0.66  LR:0.000100\n",
      "   | > Step:55/68  GlobalStep:134330  TotalLoss:0.00214  PostnetLoss:0.00104  DecoderLoss:0.00109  StopLoss:0.03480  GradNorm:0.00289  GradNormST:0.01225  AvgTextLen:43.9  AvgSpecLen:145.2  StepTime:0.85  LR:0.000100\n",
      "   | > Step:65/68  GlobalStep:134340  TotalLoss:0.00239  PostnetLoss:0.00116  DecoderLoss:0.00123  StopLoss:0.13261  GradNorm:0.00288  GradNormST:0.08024  AvgTextLen:64.2  AvgSpecLen:207.4  StepTime:1.19  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:134343  AvgTotalLoss:0.05915  AvgPostnetLoss:0.00088  AvgDecoderLoss:0.00092  AvgStopLoss:0.05735  EpochTime:42.23  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.07547   PostnetLoss: 0.00589   DecoderLoss:0.00614  StopLoss: 0.06344  \n",
      "   | > TotalLoss: 0.07354   PostnetLoss: 0.00865   DecoderLoss:0.00898  StopLoss: 0.05591  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00088   Validation Loss: 0.00804\n",
      "\n",
      " > Epoch 947/1000\n",
      "   | > Step:6/68  GlobalStep:134350  TotalLoss:0.00133  PostnetLoss:0.00065  DecoderLoss:0.00068  StopLoss:0.03895  GradNorm:0.00600  GradNormST:0.01556  AvgTextLen:6.0  AvgSpecLen:34.8  StepTime:0.32  LR:0.000100\n",
      "   | > Step:16/68  GlobalStep:134360  TotalLoss:0.00158  PostnetLoss:0.00077  DecoderLoss:0.00081  StopLoss:0.06450  GradNorm:0.00380  GradNormST:0.01742  AvgTextLen:13.8  AvgSpecLen:60.6  StepTime:0.29  LR:0.000100\n",
      "   | > Step:26/68  GlobalStep:134370  TotalLoss:0.00165  PostnetLoss:0.00081  DecoderLoss:0.00084  StopLoss:0.03137  GradNorm:0.00318  GradNormST:0.01429  AvgTextLen:20.3  AvgSpecLen:77.5  StepTime:0.48  LR:0.000100\n",
      "   | > Step:36/68  GlobalStep:134380  TotalLoss:0.00181  PostnetLoss:0.00089  DecoderLoss:0.00093  StopLoss:0.05005  GradNorm:0.00299  GradNormST:0.01175  AvgTextLen:28.5  AvgSpecLen:100.2  StepTime:0.67  LR:0.000100\n",
      "   | > Step:46/68  GlobalStep:134390  TotalLoss:0.00195  PostnetLoss:0.00095  DecoderLoss:0.00100  StopLoss:0.04876  GradNorm:0.00317  GradNormST:0.01601  AvgTextLen:35.4  AvgSpecLen:122.6  StepTime:0.77  LR:0.000100\n",
      "   | > Step:56/68  GlobalStep:134400  TotalLoss:0.00211  PostnetLoss:0.00103  DecoderLoss:0.00108  StopLoss:0.06818  GradNorm:0.00332  GradNormST:0.04143  AvgTextLen:45.8  AvgSpecLen:145.4  StepTime:1.01  LR:0.000100\n",
      "   | > Step:66/68  GlobalStep:134410  TotalLoss:0.00247  PostnetLoss:0.00120  DecoderLoss:0.00127  StopLoss:0.06060  GradNorm:0.00263  GradNormST:0.03477  AvgTextLen:71.2  AvgSpecLen:220.8  StepTime:1.03  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:134412  AvgTotalLoss:0.05107  AvgPostnetLoss:0.00087  AvgDecoderLoss:0.00091  AvgStopLoss:0.04928  EpochTime:42.81  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.07007   PostnetLoss: 0.00592   DecoderLoss:0.00617  StopLoss: 0.05798  \n",
      "   | > TotalLoss: 0.07323   PostnetLoss: 0.00894   DecoderLoss:0.00928  StopLoss: 0.05501  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00087   Validation Loss: 0.00822\n",
      "\n",
      " > Epoch 948/1000\n",
      "   | > Step:7/68  GlobalStep:134420  TotalLoss:0.00137  PostnetLoss:0.00067  DecoderLoss:0.00070  StopLoss:0.03863  GradNorm:0.00376  GradNormST:0.01942  AvgTextLen:7.4  AvgSpecLen:44.6  StepTime:0.30  LR:0.000100\n",
      "   | > Step:17/68  GlobalStep:134430  TotalLoss:0.00149  PostnetLoss:0.00073  DecoderLoss:0.00076  StopLoss:0.05223  GradNorm:0.00369  GradNormST:0.01444  AvgTextLen:14.7  AvgSpecLen:66.0  StepTime:0.45  LR:0.000100\n",
      "   | > Step:27/68  GlobalStep:134440  TotalLoss:0.00166  PostnetLoss:0.00081  DecoderLoss:0.00085  StopLoss:0.06903  GradNorm:0.00300  GradNormST:0.01590  AvgTextLen:21.0  AvgSpecLen:80.2  StepTime:0.68  LR:0.000100\n",
      "   | > Step:37/68  GlobalStep:134450  TotalLoss:0.00184  PostnetLoss:0.00090  DecoderLoss:0.00094  StopLoss:0.05033  GradNorm:0.00281  GradNormST:0.01439  AvgTextLen:29.3  AvgSpecLen:102.4  StepTime:0.57  LR:0.000100\n",
      "   | > Step:47/68  GlobalStep:134460  TotalLoss:0.00200  PostnetLoss:0.00097  DecoderLoss:0.00102  StopLoss:0.04275  GradNorm:0.00326  GradNormST:0.01225  AvgTextLen:36.7  AvgSpecLen:126.9  StepTime:0.75  LR:0.000100\n",
      "   | > Step:57/68  GlobalStep:134470  TotalLoss:0.00215  PostnetLoss:0.00105  DecoderLoss:0.00110  StopLoss:0.03301  GradNorm:0.00472  GradNormST:0.00824  AvgTextLen:47.1  AvgSpecLen:161.0  StepTime:0.69  LR:0.000100\n",
      "   | > Step:67/68  GlobalStep:134480  TotalLoss:0.00247  PostnetLoss:0.00120  DecoderLoss:0.00127  StopLoss:0.06949  GradNorm:0.00249  GradNormST:0.04899  AvgTextLen:80.6  AvgSpecLen:246.5  StepTime:1.21  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:134481  AvgTotalLoss:0.05000  AvgPostnetLoss:0.00087  AvgDecoderLoss:0.00091  AvgStopLoss:0.04823  EpochTime:42.78  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.06908   PostnetLoss: 0.00578   DecoderLoss:0.00604  StopLoss: 0.05726  \n",
      "   | > TotalLoss: 0.07949   PostnetLoss: 0.00898   DecoderLoss:0.00932  StopLoss: 0.06120  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00087   Validation Loss: 0.00830\n",
      "\n",
      " > Epoch 949/1000\n",
      "   | > Step:8/68  GlobalStep:134490  TotalLoss:0.00136  PostnetLoss:0.00066  DecoderLoss:0.00069  StopLoss:0.05130  GradNorm:0.00369  GradNormST:0.01543  AvgTextLen:8.3  AvgSpecLen:44.0  StepTime:0.24  LR:0.000100\n",
      "   | > Step:18/68  GlobalStep:134500  TotalLoss:0.00151  PostnetLoss:0.00074  DecoderLoss:0.00077  StopLoss:0.04786  GradNorm:0.00380  GradNormST:0.01545  AvgTextLen:15.2  AvgSpecLen:64.5  StepTime:0.45  LR:0.000100\n",
      "   | > Step:28/68  GlobalStep:134510  TotalLoss:0.00170  PostnetLoss:0.00083  DecoderLoss:0.00087  StopLoss:0.03838  GradNorm:0.00319  GradNormST:0.01190  AvgTextLen:22.1  AvgSpecLen:85.4  StepTime:0.54  LR:0.000100\n",
      "   | > Step:38/68  GlobalStep:134520  TotalLoss:0.00180  PostnetLoss:0.00088  DecoderLoss:0.00092  StopLoss:0.03411  GradNorm:0.00288  GradNormST:0.01098  AvgTextLen:29.4  AvgSpecLen:104.1  StepTime:0.64  LR:0.000100\n",
      "   | > Step:48/68  GlobalStep:134530  TotalLoss:0.00198  PostnetLoss:0.00097  DecoderLoss:0.00102  StopLoss:0.03765  GradNorm:0.00270  GradNormST:0.00949  AvgTextLen:37.5  AvgSpecLen:123.8  StepTime:0.71  LR:0.000100\n",
      "   | > Step:58/68  GlobalStep:134540  TotalLoss:0.00217  PostnetLoss:0.00106  DecoderLoss:0.00111  StopLoss:0.05421  GradNorm:0.00299  GradNormST:0.01482  AvgTextLen:47.8  AvgSpecLen:158.0  StepTime:0.70  LR:0.000100\n",
      "   | > Step:68/68  GlobalStep:134550  TotalLoss:0.00245  PostnetLoss:0.00119  DecoderLoss:0.00126  StopLoss:0.04485  GradNorm:0.00303  GradNormST:0.04030  AvgTextLen:103.0  AvgSpecLen:308.9  StepTime:1.86  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:134550  AvgTotalLoss:0.05086  AvgPostnetLoss:0.00086  AvgDecoderLoss:0.00091  AvgStopLoss:0.04909  EpochTime:43.01  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.06681   PostnetLoss: 0.00577   DecoderLoss:0.00603  StopLoss: 0.05501  \n",
      "   | > TotalLoss: 0.07786   PostnetLoss: 0.00917   DecoderLoss:0.00952  StopLoss: 0.05918  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00086   Validation Loss: 0.00837\n",
      "\n",
      " > Epoch 950/1000\n",
      "   | > Step:9/68  GlobalStep:134560  TotalLoss:0.00131  PostnetLoss:0.00064  DecoderLoss:0.00067  StopLoss:0.03130  GradNorm:0.00335  GradNormST:0.01042  AvgTextLen:9.0  AvgSpecLen:42.1  StepTime:0.42  LR:0.000100\n",
      "   | > Step:19/68  GlobalStep:134570  TotalLoss:0.00152  PostnetLoss:0.00075  DecoderLoss:0.00078  StopLoss:0.05560  GradNorm:0.00376  GradNormST:0.01397  AvgTextLen:15.5  AvgSpecLen:63.2  StepTime:0.41  LR:0.000100\n",
      "   | > Step:29/68  GlobalStep:134580  TotalLoss:0.00166  PostnetLoss:0.00081  DecoderLoss:0.00085  StopLoss:0.06557  GradNorm:0.00318  GradNormST:0.01858  AvgTextLen:21.8  AvgSpecLen:83.4  StepTime:0.56  LR:0.000100\n",
      "   | > Step:39/68  GlobalStep:134590  TotalLoss:0.00185  PostnetLoss:0.00090  DecoderLoss:0.00095  StopLoss:0.03522  GradNorm:0.00278  GradNormST:0.01032  AvgTextLen:30.2  AvgSpecLen:110.4  StepTime:0.64  LR:0.000100\n",
      "   | > Step:49/68  GlobalStep:134600  TotalLoss:0.00198  PostnetLoss:0.00097  DecoderLoss:0.00101  StopLoss:0.04018  GradNorm:0.00306  GradNormST:0.01038  AvgTextLen:38.2  AvgSpecLen:127.7  StepTime:0.72  LR:0.000100\n",
      "   | > Step:59/68  GlobalStep:134610  TotalLoss:0.00213  PostnetLoss:0.00104  DecoderLoss:0.00109  StopLoss:0.03472  GradNorm:0.00280  GradNormST:0.00935  AvgTextLen:50.5  AvgSpecLen:155.9  StepTime:0.87  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:134619  AvgTotalLoss:0.04898  AvgPostnetLoss:0.00086  AvgDecoderLoss:0.00090  AvgStopLoss:0.04721  EpochTime:42.56  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.06845   PostnetLoss: 0.00574   DecoderLoss:0.00599  StopLoss: 0.05672  \n",
      "   | > TotalLoss: 0.07615   PostnetLoss: 0.00923   DecoderLoss:0.00957  StopLoss: 0.05736  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00086   Validation Loss: 0.00841\n",
      "\n",
      " > Epoch 951/1000\n",
      "   | > Step:0/68  GlobalStep:134620  TotalLoss:0.00130  PostnetLoss:0.00062  DecoderLoss:0.00068  StopLoss:0.06814  GradNorm:0.00488  GradNormST:0.02266  AvgTextLen:3.0  AvgSpecLen:25.3  StepTime:0.52  LR:0.000100\n",
      "   | > Step:10/68  GlobalStep:134630  TotalLoss:0.00137  PostnetLoss:0.00067  DecoderLoss:0.00070  StopLoss:0.03237  GradNorm:0.00368  GradNormST:0.00903  AvgTextLen:9.8  AvgSpecLen:48.6  StepTime:0.51  LR:0.000100\n",
      "   | > Step:20/68  GlobalStep:134640  TotalLoss:0.00160  PostnetLoss:0.00078  DecoderLoss:0.00082  StopLoss:0.03562  GradNorm:0.00399  GradNormST:0.01233  AvgTextLen:16.4  AvgSpecLen:64.8  StepTime:0.47  LR:0.000100\n",
      "   | > Step:30/68  GlobalStep:134650  TotalLoss:0.00169  PostnetLoss:0.00083  DecoderLoss:0.00087  StopLoss:0.04665  GradNorm:0.00294  GradNormST:0.01355  AvgTextLen:22.9  AvgSpecLen:85.8  StepTime:0.58  LR:0.000100\n",
      "   | > Step:40/68  GlobalStep:134660  TotalLoss:0.00184  PostnetLoss:0.00090  DecoderLoss:0.00094  StopLoss:0.04867  GradNorm:0.00280  GradNormST:0.01491  AvgTextLen:30.9  AvgSpecLen:105.3  StepTime:0.62  LR:0.000100\n",
      "   | > Step:50/68  GlobalStep:134670  TotalLoss:0.00200  PostnetLoss:0.00098  DecoderLoss:0.00103  StopLoss:0.04681  GradNorm:0.00319  GradNormST:0.01379  AvgTextLen:38.8  AvgSpecLen:132.0  StepTime:0.80  LR:0.000100\n",
      "   | > Step:60/68  GlobalStep:134680  TotalLoss:0.00220  PostnetLoss:0.00107  DecoderLoss:0.00113  StopLoss:0.03704  GradNorm:0.00319  GradNormST:0.00927  AvgTextLen:52.0  AvgSpecLen:172.7  StepTime:0.94  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:134688  AvgTotalLoss:0.04933  AvgPostnetLoss:0.00086  AvgDecoderLoss:0.00090  AvgStopLoss:0.04756  EpochTime:42.67  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.06920   PostnetLoss: 0.00583   DecoderLoss:0.00608  StopLoss: 0.05730  \n",
      "   | > TotalLoss: 0.07178   PostnetLoss: 0.00895   DecoderLoss:0.00929  StopLoss: 0.05355  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00086   Validation Loss: 0.00819\n",
      "\n",
      " > Epoch 952/1000\n",
      "   | > Step:1/68  GlobalStep:134690  TotalLoss:0.00125  PostnetLoss:0.00061  DecoderLoss:0.00064  StopLoss:0.03697  GradNorm:0.00456  GradNormST:0.01497  AvgTextLen:3.7  AvgSpecLen:28.6  StepTime:0.35  LR:0.000100\n",
      "   | > Step:11/68  GlobalStep:134700  TotalLoss:0.00136  PostnetLoss:0.00066  DecoderLoss:0.00069  StopLoss:0.02855  GradNorm:0.00330  GradNormST:0.01013  AvgTextLen:10.3  AvgSpecLen:49.4  StepTime:0.40  LR:0.000100\n",
      "   | > Step:21/68  GlobalStep:134710  TotalLoss:0.00152  PostnetLoss:0.00075  DecoderLoss:0.00078  StopLoss:0.03768  GradNorm:0.00347  GradNormST:0.01366  AvgTextLen:16.6  AvgSpecLen:66.3  StepTime:0.45  LR:0.000100\n",
      "   | > Step:31/68  GlobalStep:134720  TotalLoss:0.00169  PostnetLoss:0.00083  DecoderLoss:0.00086  StopLoss:0.03126  GradNorm:0.00290  GradNormST:0.01459  AvgTextLen:24.1  AvgSpecLen:82.7  StepTime:0.47  LR:0.000100\n",
      "   | > Step:41/68  GlobalStep:134730  TotalLoss:0.00188  PostnetLoss:0.00092  DecoderLoss:0.00096  StopLoss:0.03580  GradNorm:0.00287  GradNormST:0.01107  AvgTextLen:32.3  AvgSpecLen:109.4  StepTime:0.67  LR:0.000100\n",
      "   | > Step:51/68  GlobalStep:134740  TotalLoss:0.00198  PostnetLoss:0.00096  DecoderLoss:0.00101  StopLoss:0.03477  GradNorm:0.00280  GradNormST:0.01065  AvgTextLen:39.2  AvgSpecLen:126.9  StepTime:0.74  LR:0.000100\n",
      "   | > Step:61/68  GlobalStep:134750  TotalLoss:0.00218  PostnetLoss:0.00106  DecoderLoss:0.00112  StopLoss:0.04741  GradNorm:0.00276  GradNormST:0.01357  AvgTextLen:54.6  AvgSpecLen:165.5  StepTime:0.90  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:134757  AvgTotalLoss:0.04685  AvgPostnetLoss:0.00086  AvgDecoderLoss:0.00090  AvgStopLoss:0.04509  EpochTime:43.71  AvgStepTime:0.63\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.06723   PostnetLoss: 0.00580   DecoderLoss:0.00605  StopLoss: 0.05538  \n",
      "   | > TotalLoss: 0.07416   PostnetLoss: 0.00906   DecoderLoss:0.00939  StopLoss: 0.05570  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00086   Validation Loss: 0.00837\n",
      "\n",
      " > Epoch 953/1000\n",
      "   | > Step:2/68  GlobalStep:134760  TotalLoss:0.00114  PostnetLoss:0.00056  DecoderLoss:0.00059  StopLoss:0.04384  GradNorm:0.00373  GradNormST:0.01605  AvgTextLen:4.0  AvgSpecLen:31.3  StepTime:0.42  LR:0.000100\n",
      "   | > Step:12/68  GlobalStep:134770  TotalLoss:0.00138  PostnetLoss:0.00068  DecoderLoss:0.00070  StopLoss:0.03949  GradNorm:0.00426  GradNormST:0.01318  AvgTextLen:11.0  AvgSpecLen:49.3  StepTime:0.43  LR:0.000100\n",
      "   | > Step:22/68  GlobalStep:134780  TotalLoss:0.00161  PostnetLoss:0.00079  DecoderLoss:0.00082  StopLoss:0.04676  GradNorm:0.00394  GradNormST:0.01424  AvgTextLen:18.0  AvgSpecLen:72.0  StepTime:0.47  LR:0.000100\n",
      "   | > Step:32/68  GlobalStep:134790  TotalLoss:0.00170  PostnetLoss:0.00083  DecoderLoss:0.00087  StopLoss:0.05277  GradNorm:0.00304  GradNormST:0.02689  AvgTextLen:24.4  AvgSpecLen:91.5  StepTime:0.57  LR:0.000100\n",
      "   | > Step:42/68  GlobalStep:134800  TotalLoss:0.00188  PostnetLoss:0.00091  DecoderLoss:0.00096  StopLoss:0.03934  GradNorm:0.00298  GradNormST:0.01408  AvgTextLen:33.1  AvgSpecLen:109.7  StepTime:0.48  LR:0.000100\n",
      "   | > Step:52/68  GlobalStep:134810  TotalLoss:0.00205  PostnetLoss:0.00100  DecoderLoss:0.00105  StopLoss:0.08501  GradNorm:0.00269  GradNormST:0.03106  AvgTextLen:42.1  AvgSpecLen:137.5  StepTime:0.80  LR:0.000100\n",
      "   | > Step:62/68  GlobalStep:134820  TotalLoss:0.00222  PostnetLoss:0.00108  DecoderLoss:0.00114  StopLoss:0.04428  GradNorm:0.00291  GradNormST:0.01301  AvgTextLen:54.0  AvgSpecLen:174.7  StepTime:0.76  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:134826  AvgTotalLoss:0.04839  AvgPostnetLoss:0.00086  AvgDecoderLoss:0.00090  AvgStopLoss:0.04664  EpochTime:42.43  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.06614   PostnetLoss: 0.00567   DecoderLoss:0.00592  StopLoss: 0.05455  \n",
      "   | > TotalLoss: 0.07210   PostnetLoss: 0.00914   DecoderLoss:0.00948  StopLoss: 0.05348  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00086   Validation Loss: 0.00834\n",
      "\n",
      " > Epoch 954/1000\n",
      "   | > Step:3/68  GlobalStep:134830  TotalLoss:0.00118  PostnetLoss:0.00058  DecoderLoss:0.00060  StopLoss:0.06396  GradNorm:0.00384  GradNormST:0.01938  AvgTextLen:4.7  AvgSpecLen:29.7  StepTime:0.30  LR:0.000100\n",
      "   | > Step:13/68  GlobalStep:134840  TotalLoss:0.00139  PostnetLoss:0.00068  DecoderLoss:0.00071  StopLoss:0.06014  GradNorm:0.00408  GradNormST:0.01974  AvgTextLen:11.5  AvgSpecLen:55.2  StepTime:0.41  LR:0.000100\n",
      "   | > Step:23/68  GlobalStep:134850  TotalLoss:0.00164  PostnetLoss:0.00081  DecoderLoss:0.00084  StopLoss:0.06808  GradNorm:0.00405  GradNormST:0.02027  AvgTextLen:18.3  AvgSpecLen:72.2  StepTime:0.33  LR:0.000100\n",
      "   | > Step:33/68  GlobalStep:134860  TotalLoss:0.00178  PostnetLoss:0.00087  DecoderLoss:0.00091  StopLoss:0.05115  GradNorm:0.00390  GradNormST:0.01344  AvgTextLen:25.9  AvgSpecLen:88.9  StepTime:0.55  LR:0.000100\n",
      "   | > Step:43/68  GlobalStep:134870  TotalLoss:0.00188  PostnetLoss:0.00092  DecoderLoss:0.00097  StopLoss:0.04023  GradNorm:0.00280  GradNormST:0.00924  AvgTextLen:33.4  AvgSpecLen:115.7  StepTime:0.61  LR:0.000100\n",
      "   | > Step:53/68  GlobalStep:134880  TotalLoss:0.00202  PostnetLoss:0.00098  DecoderLoss:0.00103  StopLoss:0.03288  GradNorm:0.00271  GradNormST:0.00796  AvgTextLen:42.4  AvgSpecLen:137.6  StepTime:0.81  LR:0.000100\n",
      "   | > Step:63/68  GlobalStep:134890  TotalLoss:0.00227  PostnetLoss:0.00111  DecoderLoss:0.00117  StopLoss:0.03843  GradNorm:0.00250  GradNormST:0.02084  AvgTextLen:58.6  AvgSpecLen:185.1  StepTime:0.94  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:134895  AvgTotalLoss:0.04882  AvgPostnetLoss:0.00086  AvgDecoderLoss:0.00090  AvgStopLoss:0.04706  EpochTime:42.87  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.06582   PostnetLoss: 0.00573   DecoderLoss:0.00598  StopLoss: 0.05411  \n",
      "   | > TotalLoss: 0.07581   PostnetLoss: 0.00921   DecoderLoss:0.00954  StopLoss: 0.05705  \n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00086   Validation Loss: 0.00841\n",
      "\n",
      " > Epoch 955/1000\n",
      "   | > Step:4/68  GlobalStep:134900  TotalLoss:0.00126  PostnetLoss:0.00061  DecoderLoss:0.00064  StopLoss:0.05979  GradNorm:0.00397  GradNormST:0.02809  AvgTextLen:4.8  AvgSpecLen:31.1  StepTime:0.27  LR:0.000100\n",
      "   | > Step:14/68  GlobalStep:134910  TotalLoss:0.00142  PostnetLoss:0.00069  DecoderLoss:0.00073  StopLoss:0.04958  GradNorm:0.00337  GradNormST:0.02270  AvgTextLen:12.2  AvgSpecLen:52.0  StepTime:0.34  LR:0.000100\n",
      "   | > Step:24/68  GlobalStep:134920  TotalLoss:0.00168  PostnetLoss:0.00082  DecoderLoss:0.00086  StopLoss:0.04394  GradNorm:0.00372  GradNormST:0.01136  AvgTextLen:19.2  AvgSpecLen:79.0  StepTime:0.50  LR:0.000100\n",
      "   | > Step:34/68  GlobalStep:134930  TotalLoss:0.00180  PostnetLoss:0.00088  DecoderLoss:0.00092  StopLoss:0.02874  GradNorm:0.00314  GradNormST:0.01102  AvgTextLen:26.2  AvgSpecLen:95.9  StepTime:0.62  LR:0.000100\n",
      "   | > Step:44/68  GlobalStep:134940  TotalLoss:0.00194  PostnetLoss:0.00095  DecoderLoss:0.00099  StopLoss:0.02825  GradNorm:0.00304  GradNormST:0.01117  AvgTextLen:33.8  AvgSpecLen:116.3  StepTime:0.66  LR:0.000100\n",
      "   | > Step:54/68  GlobalStep:134950  TotalLoss:0.00204  PostnetLoss:0.00100  DecoderLoss:0.00105  StopLoss:0.03620  GradNorm:0.00264  GradNormST:0.00992  AvgTextLen:42.2  AvgSpecLen:144.9  StepTime:0.71  LR:0.000100\n",
      "   | > Step:64/68  GlobalStep:134960  TotalLoss:0.00212  PostnetLoss:0.00103  DecoderLoss:0.00109  StopLoss:0.06467  GradNorm:0.00241  GradNormST:0.03306  AvgTextLen:60.6  AvgSpecLen:177.6  StepTime:0.81  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:134964  AvgTotalLoss:0.04824  AvgPostnetLoss:0.00085  AvgDecoderLoss:0.00090  AvgStopLoss:0.04649  EpochTime:42.31  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.06736   PostnetLoss: 0.00577   DecoderLoss:0.00602  StopLoss: 0.05558  \n",
      "   | > TotalLoss: 0.07705   PostnetLoss: 0.00921   DecoderLoss:0.00954  StopLoss: 0.05830  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00085   Validation Loss: 0.00840\n",
      "\n",
      " > Epoch 956/1000\n",
      "   | > Step:5/68  GlobalStep:134970  TotalLoss:0.00125  PostnetLoss:0.00061  DecoderLoss:0.00064  StopLoss:0.04343  GradNorm:0.00314  GradNormST:0.01709  AvgTextLen:5.9  AvgSpecLen:33.2  StepTime:0.32  LR:0.000100\n",
      "   | > Step:15/68  GlobalStep:134980  TotalLoss:0.00149  PostnetLoss:0.00073  DecoderLoss:0.00076  StopLoss:0.05491  GradNorm:0.00327  GradNormST:0.01981  AvgTextLen:12.8  AvgSpecLen:59.1  StepTime:0.55  LR:0.000100\n",
      "   | > Step:25/68  GlobalStep:134990  TotalLoss:0.00159  PostnetLoss:0.00078  DecoderLoss:0.00081  StopLoss:0.03711  GradNorm:0.00358  GradNormST:0.01254  AvgTextLen:19.8  AvgSpecLen:73.3  StepTime:0.40  LR:0.000100\n",
      "   | > Step:35/68  GlobalStep:135000  TotalLoss:0.00179  PostnetLoss:0.00088  DecoderLoss:0.00092  StopLoss:0.04546  GradNorm:0.00283  GradNormST:0.01434  AvgTextLen:27.1  AvgSpecLen:104.7  StepTime:0.79  LR:0.000100\n",
      " | | > Checkpoint saving : /vm/TTS/models/cartman_models/mozilla-no-loc-fattn-stopnet-sigmoid-loss_masking-August-15-2019_11+26AM-de6aeb9/checkpoint_135000.pth.tar\n",
      "   | > Step:45/68  GlobalStep:135010  TotalLoss:0.00190  PostnetLoss:0.00093  DecoderLoss:0.00097  StopLoss:0.04344  GradNorm:0.00282  GradNormST:0.01272  AvgTextLen:34.4  AvgSpecLen:118.6  StepTime:0.70  LR:0.000100\n",
      "   | > Step:55/68  GlobalStep:135020  TotalLoss:0.00208  PostnetLoss:0.00102  DecoderLoss:0.00107  StopLoss:0.03519  GradNorm:0.00255  GradNormST:0.01164  AvgTextLen:43.9  AvgSpecLen:145.2  StepTime:0.80  LR:0.000100\n",
      "   | > Step:65/68  GlobalStep:135030  TotalLoss:0.00232  PostnetLoss:0.00113  DecoderLoss:0.00119  StopLoss:0.09082  GradNorm:0.00239  GradNormST:0.05414  AvgTextLen:64.2  AvgSpecLen:207.4  StepTime:1.22  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:135033  AvgTotalLoss:0.04569  AvgPostnetLoss:0.00086  AvgDecoderLoss:0.00090  AvgStopLoss:0.04393  EpochTime:42.35  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.06589   PostnetLoss: 0.00566   DecoderLoss:0.00591  StopLoss: 0.05432  \n",
      "   | > TotalLoss: 0.07653   PostnetLoss: 0.00937   DecoderLoss:0.00972  StopLoss: 0.05745  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00086   Validation Loss: 0.00843\n",
      "\n",
      " > Epoch 957/1000\n",
      "   | > Step:6/68  GlobalStep:135040  TotalLoss:0.00123  PostnetLoss:0.00060  DecoderLoss:0.00063  StopLoss:0.03704  GradNorm:0.00340  GradNormST:0.01114  AvgTextLen:6.0  AvgSpecLen:34.8  StepTime:0.32  LR:0.000100\n",
      "   | > Step:16/68  GlobalStep:135050  TotalLoss:0.00150  PostnetLoss:0.00073  DecoderLoss:0.00076  StopLoss:0.03827  GradNorm:0.00335  GradNormST:0.01587  AvgTextLen:13.8  AvgSpecLen:60.6  StepTime:0.39  LR:0.000100\n",
      "   | > Step:26/68  GlobalStep:135060  TotalLoss:0.00166  PostnetLoss:0.00081  DecoderLoss:0.00085  StopLoss:0.03741  GradNorm:0.00366  GradNormST:0.01380  AvgTextLen:20.3  AvgSpecLen:77.5  StepTime:0.51  LR:0.000100\n",
      "   | > Step:36/68  GlobalStep:135070  TotalLoss:0.00178  PostnetLoss:0.00087  DecoderLoss:0.00091  StopLoss:0.04684  GradNorm:0.00314  GradNormST:0.01474  AvgTextLen:28.5  AvgSpecLen:100.2  StepTime:0.67  LR:0.000100\n",
      "   | > Step:46/68  GlobalStep:135080  TotalLoss:0.00192  PostnetLoss:0.00094  DecoderLoss:0.00098  StopLoss:0.05963  GradNorm:0.00289  GradNormST:0.02498  AvgTextLen:35.4  AvgSpecLen:122.6  StepTime:0.77  LR:0.000100\n",
      "   | > Step:56/68  GlobalStep:135090  TotalLoss:0.00207  PostnetLoss:0.00101  DecoderLoss:0.00106  StopLoss:0.07912  GradNorm:0.00251  GradNormST:0.04725  AvgTextLen:45.8  AvgSpecLen:145.4  StepTime:0.83  LR:0.000100\n",
      "   | > Step:66/68  GlobalStep:135100  TotalLoss:0.00240  PostnetLoss:0.00117  DecoderLoss:0.00123  StopLoss:0.06188  GradNorm:0.00246  GradNormST:0.03507  AvgTextLen:71.2  AvgSpecLen:220.8  StepTime:1.04  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:135102  AvgTotalLoss:0.04811  AvgPostnetLoss:0.00085  AvgDecoderLoss:0.00090  AvgStopLoss:0.04636  EpochTime:41.50  AvgStepTime:0.60\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.06702   PostnetLoss: 0.00571   DecoderLoss:0.00596  StopLoss: 0.05535  \n",
      "   | > TotalLoss: 0.07898   PostnetLoss: 0.00938   DecoderLoss:0.00971  StopLoss: 0.05989  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00085   Validation Loss: 0.00855\n",
      "\n",
      " > Epoch 958/1000\n",
      "   | > Step:7/68  GlobalStep:135110  TotalLoss:0.00131  PostnetLoss:0.00064  DecoderLoss:0.00067  StopLoss:0.03264  GradNorm:0.00350  GradNormST:0.01542  AvgTextLen:7.4  AvgSpecLen:44.6  StepTime:0.39  LR:0.000100\n",
      "   | > Step:17/68  GlobalStep:135120  TotalLoss:0.00146  PostnetLoss:0.00071  DecoderLoss:0.00075  StopLoss:0.06187  GradNorm:0.00318  GradNormST:0.01653  AvgTextLen:14.7  AvgSpecLen:66.0  StepTime:0.45  LR:0.000100\n",
      "   | > Step:27/68  GlobalStep:135130  TotalLoss:0.00167  PostnetLoss:0.00082  DecoderLoss:0.00085  StopLoss:0.04995  GradNorm:0.00343  GradNormST:0.01156  AvgTextLen:21.0  AvgSpecLen:80.2  StepTime:0.69  LR:0.000100\n",
      "   | > Step:37/68  GlobalStep:135140  TotalLoss:0.00180  PostnetLoss:0.00088  DecoderLoss:0.00092  StopLoss:0.04561  GradNorm:0.00303  GradNormST:0.01553  AvgTextLen:29.3  AvgSpecLen:102.4  StepTime:0.63  LR:0.000100\n",
      "   | > Step:47/68  GlobalStep:135150  TotalLoss:0.00197  PostnetLoss:0.00096  DecoderLoss:0.00101  StopLoss:0.03216  GradNorm:0.00315  GradNormST:0.00971  AvgTextLen:36.7  AvgSpecLen:126.9  StepTime:0.62  LR:0.000100\n",
      "   | > Step:57/68  GlobalStep:135160  TotalLoss:0.00212  PostnetLoss:0.00103  DecoderLoss:0.00109  StopLoss:0.03169  GradNorm:0.00246  GradNormST:0.00977  AvgTextLen:47.1  AvgSpecLen:161.0  StepTime:0.73  LR:0.000100\n",
      "   | > Step:67/68  GlobalStep:135170  TotalLoss:0.00242  PostnetLoss:0.00118  DecoderLoss:0.00125  StopLoss:0.07016  GradNorm:0.00238  GradNormST:0.04541  AvgTextLen:80.6  AvgSpecLen:246.5  StepTime:1.21  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:135171  AvgTotalLoss:0.05052  AvgPostnetLoss:0.00085  AvgDecoderLoss:0.00089  AvgStopLoss:0.04877  EpochTime:42.99  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.06855   PostnetLoss: 0.00573   DecoderLoss:0.00599  StopLoss: 0.05683  \n",
      "   | > TotalLoss: 0.07633   PostnetLoss: 0.00916   DecoderLoss:0.00949  StopLoss: 0.05768  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00085   Validation Loss: 0.00850\n",
      "\n",
      " > Epoch 959/1000\n",
      "   | > Step:8/68  GlobalStep:135180  TotalLoss:0.00129  PostnetLoss:0.00063  DecoderLoss:0.00066  StopLoss:0.03767  GradNorm:0.00335  GradNormST:0.00983  AvgTextLen:8.3  AvgSpecLen:44.0  StepTime:0.24  LR:0.000100\n",
      "   | > Step:18/68  GlobalStep:135190  TotalLoss:0.00147  PostnetLoss:0.00072  DecoderLoss:0.00075  StopLoss:0.05309  GradNorm:0.00332  GradNormST:0.01893  AvgTextLen:15.2  AvgSpecLen:64.5  StepTime:0.43  LR:0.000100\n",
      "   | > Step:28/68  GlobalStep:135200  TotalLoss:0.00167  PostnetLoss:0.00082  DecoderLoss:0.00085  StopLoss:0.04563  GradNorm:0.00343  GradNormST:0.01471  AvgTextLen:22.1  AvgSpecLen:85.4  StepTime:0.46  LR:0.000100\n",
      "   | > Step:38/68  GlobalStep:135210  TotalLoss:0.00180  PostnetLoss:0.00088  DecoderLoss:0.00092  StopLoss:0.03224  GradNorm:0.00326  GradNormST:0.01094  AvgTextLen:29.4  AvgSpecLen:104.1  StepTime:0.50  LR:0.000100\n",
      "   | > Step:48/68  GlobalStep:135220  TotalLoss:0.00197  PostnetLoss:0.00096  DecoderLoss:0.00101  StopLoss:0.02905  GradNorm:0.00333  GradNormST:0.00773  AvgTextLen:37.5  AvgSpecLen:123.8  StepTime:0.71  LR:0.000100\n",
      "   | > Step:58/68  GlobalStep:135230  TotalLoss:0.00213  PostnetLoss:0.00104  DecoderLoss:0.00109  StopLoss:0.04308  GradNorm:0.00262  GradNormST:0.01044  AvgTextLen:47.8  AvgSpecLen:158.0  StepTime:0.75  LR:0.000100\n",
      "   | > Step:68/68  GlobalStep:135240  TotalLoss:0.00239  PostnetLoss:0.00116  DecoderLoss:0.00123  StopLoss:0.10789  GradNorm:0.00245  GradNormST:0.08750  AvgTextLen:103.0  AvgSpecLen:308.9  StepTime:1.87  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:135240  AvgTotalLoss:0.04954  AvgPostnetLoss:0.00085  AvgDecoderLoss:0.00089  AvgStopLoss:0.04780  EpochTime:42.77  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.06817   PostnetLoss: 0.00579   DecoderLoss:0.00605  StopLoss: 0.05633  \n",
      "   | > TotalLoss: 0.07718   PostnetLoss: 0.00961   DecoderLoss:0.00995  StopLoss: 0.05763  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00085   Validation Loss: 0.00860\n",
      "\n",
      " > Epoch 960/1000\n",
      "   | > Step:9/68  GlobalStep:135250  TotalLoss:0.00130  PostnetLoss:0.00064  DecoderLoss:0.00066  StopLoss:0.04083  GradNorm:0.00351  GradNormST:0.01768  AvgTextLen:9.0  AvgSpecLen:42.1  StepTime:0.45  LR:0.000100\n",
      "   | > Step:19/68  GlobalStep:135260  TotalLoss:0.00149  PostnetLoss:0.00073  DecoderLoss:0.00076  StopLoss:0.06439  GradNorm:0.00325  GradNormST:0.01625  AvgTextLen:15.5  AvgSpecLen:63.2  StepTime:0.32  LR:0.000100\n",
      "   | > Step:29/68  GlobalStep:135270  TotalLoss:0.00166  PostnetLoss:0.00081  DecoderLoss:0.00085  StopLoss:0.04885  GradNorm:0.00326  GradNormST:0.01327  AvgTextLen:21.8  AvgSpecLen:83.4  StepTime:0.56  LR:0.000100\n",
      "   | > Step:39/68  GlobalStep:135280  TotalLoss:0.00185  PostnetLoss:0.00090  DecoderLoss:0.00094  StopLoss:0.02535  GradNorm:0.00335  GradNormST:0.01113  AvgTextLen:30.2  AvgSpecLen:110.4  StepTime:0.53  LR:0.000100\n",
      "   | > Step:49/68  GlobalStep:135290  TotalLoss:0.00195  PostnetLoss:0.00095  DecoderLoss:0.00100  StopLoss:0.04201  GradNorm:0.00320  GradNormST:0.01203  AvgTextLen:38.2  AvgSpecLen:127.7  StepTime:0.71  LR:0.000100\n",
      "   | > Step:59/68  GlobalStep:135300  TotalLoss:0.00211  PostnetLoss:0.00103  DecoderLoss:0.00108  StopLoss:0.03447  GradNorm:0.00295  GradNormST:0.01127  AvgTextLen:50.5  AvgSpecLen:155.9  StepTime:0.90  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:135309  AvgTotalLoss:0.04729  AvgPostnetLoss:0.00085  AvgDecoderLoss:0.00089  AvgStopLoss:0.04554  EpochTime:42.00  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.06848   PostnetLoss: 0.00586   DecoderLoss:0.00611  StopLoss: 0.05651  \n",
      "   | > TotalLoss: 0.07538   PostnetLoss: 0.00951   DecoderLoss:0.00984  StopLoss: 0.05604  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00085   Validation Loss: 0.00865\n",
      "\n",
      " > Epoch 961/1000\n",
      "   | > Step:0/68  GlobalStep:135310  TotalLoss:0.00124  PostnetLoss:0.00060  DecoderLoss:0.00065  StopLoss:0.05369  GradNorm:0.00502  GradNormST:0.01858  AvgTextLen:3.0  AvgSpecLen:25.3  StepTime:0.41  LR:0.000100\n",
      "   | > Step:10/68  GlobalStep:135320  TotalLoss:0.00135  PostnetLoss:0.00066  DecoderLoss:0.00069  StopLoss:0.03552  GradNorm:0.00315  GradNormST:0.01433  AvgTextLen:9.8  AvgSpecLen:48.6  StepTime:0.43  LR:0.000100\n",
      "   | > Step:20/68  GlobalStep:135330  TotalLoss:0.00154  PostnetLoss:0.00076  DecoderLoss:0.00079  StopLoss:0.03121  GradNorm:0.00324  GradNormST:0.01398  AvgTextLen:16.4  AvgSpecLen:64.8  StepTime:0.49  LR:0.000100\n",
      "   | > Step:30/68  GlobalStep:135340  TotalLoss:0.00168  PostnetLoss:0.00082  DecoderLoss:0.00085  StopLoss:0.03768  GradNorm:0.00339  GradNormST:0.00940  AvgTextLen:22.9  AvgSpecLen:85.8  StepTime:0.59  LR:0.000100\n",
      "   | > Step:40/68  GlobalStep:135350  TotalLoss:0.00185  PostnetLoss:0.00090  DecoderLoss:0.00094  StopLoss:0.06159  GradNorm:0.00379  GradNormST:0.01492  AvgTextLen:30.9  AvgSpecLen:105.3  StepTime:0.61  LR:0.000100\n",
      "   | > Step:50/68  GlobalStep:135360  TotalLoss:0.00199  PostnetLoss:0.00097  DecoderLoss:0.00102  StopLoss:0.03236  GradNorm:0.00370  GradNormST:0.00791  AvgTextLen:38.8  AvgSpecLen:132.0  StepTime:0.70  LR:0.000100\n",
      "   | > Step:60/68  GlobalStep:135370  TotalLoss:0.00219  PostnetLoss:0.00107  DecoderLoss:0.00112  StopLoss:0.03620  GradNorm:0.00385  GradNormST:0.00919  AvgTextLen:52.0  AvgSpecLen:172.7  StepTime:1.00  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:135378  AvgTotalLoss:0.04702  AvgPostnetLoss:0.00085  AvgDecoderLoss:0.00089  AvgStopLoss:0.04528  EpochTime:42.50  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.07049   PostnetLoss: 0.00584   DecoderLoss:0.00610  StopLoss: 0.05856  \n",
      "   | > TotalLoss: 0.07272   PostnetLoss: 0.00939   DecoderLoss:0.00973  StopLoss: 0.05360  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00085   Validation Loss: 0.00855\n",
      "\n",
      " > Epoch 962/1000\n",
      "   | > Step:1/68  GlobalStep:135380  TotalLoss:0.00120  PostnetLoss:0.00058  DecoderLoss:0.00062  StopLoss:0.03649  GradNorm:0.00435  GradNormST:0.01508  AvgTextLen:3.7  AvgSpecLen:28.6  StepTime:0.25  LR:0.000100\n",
      "   | > Step:11/68  GlobalStep:135390  TotalLoss:0.00134  PostnetLoss:0.00066  DecoderLoss:0.00069  StopLoss:0.02938  GradNorm:0.00312  GradNormST:0.01634  AvgTextLen:10.3  AvgSpecLen:49.4  StepTime:0.39  LR:0.000100\n",
      "   | > Step:21/68  GlobalStep:135400  TotalLoss:0.00150  PostnetLoss:0.00074  DecoderLoss:0.00077  StopLoss:0.04186  GradNorm:0.00301  GradNormST:0.01962  AvgTextLen:16.6  AvgSpecLen:66.3  StepTime:0.43  LR:0.000100\n",
      "   | > Step:31/68  GlobalStep:135410  TotalLoss:0.00169  PostnetLoss:0.00083  DecoderLoss:0.00087  StopLoss:0.04063  GradNorm:0.00323  GradNormST:0.01288  AvgTextLen:24.1  AvgSpecLen:82.7  StepTime:0.53  LR:0.000100\n",
      "   | > Step:41/68  GlobalStep:135420  TotalLoss:0.00188  PostnetLoss:0.00092  DecoderLoss:0.00096  StopLoss:0.02917  GradNorm:0.00359  GradNormST:0.00898  AvgTextLen:32.3  AvgSpecLen:109.4  StepTime:0.63  LR:0.000100\n",
      "   | > Step:51/68  GlobalStep:135430  TotalLoss:0.00199  PostnetLoss:0.00097  DecoderLoss:0.00102  StopLoss:0.03790  GradNorm:0.00370  GradNormST:0.00819  AvgTextLen:39.2  AvgSpecLen:126.9  StepTime:0.72  LR:0.000100\n",
      "   | > Step:61/68  GlobalStep:135440  TotalLoss:0.00216  PostnetLoss:0.00105  DecoderLoss:0.00111  StopLoss:0.04192  GradNorm:0.00305  GradNormST:0.01129  AvgTextLen:54.6  AvgSpecLen:165.5  StepTime:0.90  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:135447  AvgTotalLoss:0.04520  AvgPostnetLoss:0.00085  AvgDecoderLoss:0.00089  AvgStopLoss:0.04346  EpochTime:41.64  AvgStepTime:0.60\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.06858   PostnetLoss: 0.00562   DecoderLoss:0.00587  StopLoss: 0.05709  \n",
      "   | > TotalLoss: 0.08214   PostnetLoss: 0.00955   DecoderLoss:0.00989  StopLoss: 0.06270  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00085   Validation Loss: 0.00868\n",
      "\n",
      " > Epoch 963/1000\n",
      "   | > Step:2/68  GlobalStep:135450  TotalLoss:0.00115  PostnetLoss:0.00056  DecoderLoss:0.00059  StopLoss:0.03816  GradNorm:0.00352  GradNormST:0.01631  AvgTextLen:4.0  AvgSpecLen:31.3  StepTime:0.50  LR:0.000100\n",
      "   | > Step:12/68  GlobalStep:135460  TotalLoss:0.00135  PostnetLoss:0.00066  DecoderLoss:0.00069  StopLoss:0.04184  GradNorm:0.00322  GradNormST:0.01282  AvgTextLen:11.0  AvgSpecLen:49.3  StepTime:0.39  LR:0.000100\n",
      "   | > Step:22/68  GlobalStep:135470  TotalLoss:0.00155  PostnetLoss:0.00076  DecoderLoss:0.00079  StopLoss:0.04483  GradNorm:0.00295  GradNormST:0.01144  AvgTextLen:18.0  AvgSpecLen:72.0  StepTime:0.48  LR:0.000100\n",
      "   | > Step:32/68  GlobalStep:135480  TotalLoss:0.00168  PostnetLoss:0.00082  DecoderLoss:0.00086  StopLoss:0.03756  GradNorm:0.00306  GradNormST:0.01721  AvgTextLen:24.4  AvgSpecLen:91.5  StepTime:0.57  LR:0.000100\n",
      "   | > Step:42/68  GlobalStep:135490  TotalLoss:0.00185  PostnetLoss:0.00090  DecoderLoss:0.00095  StopLoss:0.04181  GradNorm:0.00311  GradNormST:0.01931  AvgTextLen:33.1  AvgSpecLen:109.7  StepTime:0.64  LR:0.000100\n",
      "   | > Step:52/68  GlobalStep:135500  TotalLoss:0.00202  PostnetLoss:0.00098  DecoderLoss:0.00103  StopLoss:0.05771  GradNorm:0.00312  GradNormST:0.01218  AvgTextLen:42.1  AvgSpecLen:137.5  StepTime:0.76  LR:0.000100\n",
      "   | > Step:62/68  GlobalStep:135510  TotalLoss:0.00220  PostnetLoss:0.00107  DecoderLoss:0.00113  StopLoss:0.03031  GradNorm:0.00302  GradNormST:0.00790  AvgTextLen:54.0  AvgSpecLen:174.7  StepTime:0.76  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:135516  AvgTotalLoss:0.04436  AvgPostnetLoss:0.00085  AvgDecoderLoss:0.00089  AvgStopLoss:0.04263  EpochTime:42.06  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.06931   PostnetLoss: 0.00583   DecoderLoss:0.00608  StopLoss: 0.05740  \n",
      "   | > TotalLoss: 0.08331   PostnetLoss: 0.00956   DecoderLoss:0.00989  StopLoss: 0.06386  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00085   Validation Loss: 0.00869\n",
      "\n",
      " > Epoch 964/1000\n",
      "   | > Step:3/68  GlobalStep:135520  TotalLoss:0.00119  PostnetLoss:0.00058  DecoderLoss:0.00061  StopLoss:0.05752  GradNorm:0.00401  GradNormST:0.01643  AvgTextLen:4.7  AvgSpecLen:29.7  StepTime:0.23  LR:0.000100\n",
      "   | > Step:13/68  GlobalStep:135530  TotalLoss:0.00135  PostnetLoss:0.00066  DecoderLoss:0.00069  StopLoss:0.04682  GradNorm:0.00311  GradNormST:0.01932  AvgTextLen:11.5  AvgSpecLen:55.2  StepTime:0.33  LR:0.000100\n",
      "   | > Step:23/68  GlobalStep:135540  TotalLoss:0.00158  PostnetLoss:0.00077  DecoderLoss:0.00081  StopLoss:0.04909  GradNorm:0.00298  GradNormST:0.01534  AvgTextLen:18.3  AvgSpecLen:72.2  StepTime:0.44  LR:0.000100\n",
      "   | > Step:33/68  GlobalStep:135550  TotalLoss:0.00173  PostnetLoss:0.00085  DecoderLoss:0.00088  StopLoss:0.05201  GradNorm:0.00329  GradNormST:0.01163  AvgTextLen:25.9  AvgSpecLen:88.9  StepTime:0.54  LR:0.000100\n",
      "   | > Step:43/68  GlobalStep:135560  TotalLoss:0.00186  PostnetLoss:0.00091  DecoderLoss:0.00095  StopLoss:0.04430  GradNorm:0.00315  GradNormST:0.01398  AvgTextLen:33.4  AvgSpecLen:115.7  StepTime:0.51  LR:0.000100\n",
      "   | > Step:53/68  GlobalStep:135570  TotalLoss:0.00199  PostnetLoss:0.00097  DecoderLoss:0.00102  StopLoss:0.02872  GradNorm:0.00345  GradNormST:0.00792  AvgTextLen:42.4  AvgSpecLen:137.6  StepTime:0.64  LR:0.000100\n",
      "   | > Step:63/68  GlobalStep:135580  TotalLoss:0.00226  PostnetLoss:0.00110  DecoderLoss:0.00116  StopLoss:0.02463  GradNorm:0.00304  GradNormST:0.00765  AvgTextLen:58.6  AvgSpecLen:185.1  StepTime:0.91  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:135585  AvgTotalLoss:0.04396  AvgPostnetLoss:0.00085  AvgDecoderLoss:0.00089  AvgStopLoss:0.04221  EpochTime:41.81  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.06913   PostnetLoss: 0.00574   DecoderLoss:0.00600  StopLoss: 0.05740  \n",
      "   | > TotalLoss: 0.07947   PostnetLoss: 0.00958   DecoderLoss:0.00992  StopLoss: 0.05997  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00085   Validation Loss: 0.00873\n",
      "\n",
      " > Epoch 965/1000\n",
      "   | > Step:4/68  GlobalStep:135590  TotalLoss:0.00122  PostnetLoss:0.00059  DecoderLoss:0.00063  StopLoss:0.06161  GradNorm:0.00353  GradNormST:0.01660  AvgTextLen:4.8  AvgSpecLen:31.1  StepTime:0.25  LR:0.000100\n",
      "   | > Step:14/68  GlobalStep:135600  TotalLoss:0.00139  PostnetLoss:0.00068  DecoderLoss:0.00071  StopLoss:0.03510  GradNorm:0.00324  GradNormST:0.01382  AvgTextLen:12.2  AvgSpecLen:52.0  StepTime:0.33  LR:0.000100\n",
      "   | > Step:24/68  GlobalStep:135610  TotalLoss:0.00164  PostnetLoss:0.00080  DecoderLoss:0.00084  StopLoss:0.04070  GradNorm:0.00291  GradNormST:0.01134  AvgTextLen:19.2  AvgSpecLen:79.0  StepTime:0.49  LR:0.000100\n",
      "   | > Step:34/68  GlobalStep:135620  TotalLoss:0.00177  PostnetLoss:0.00087  DecoderLoss:0.00091  StopLoss:0.03795  GradNorm:0.00302  GradNormST:0.01240  AvgTextLen:26.2  AvgSpecLen:95.9  StepTime:0.63  LR:0.000100\n",
      "   | > Step:44/68  GlobalStep:135630  TotalLoss:0.00191  PostnetLoss:0.00093  DecoderLoss:0.00098  StopLoss:0.03394  GradNorm:0.00308  GradNormST:0.01336  AvgTextLen:33.8  AvgSpecLen:116.3  StepTime:0.53  LR:0.000100\n",
      "   | > Step:54/68  GlobalStep:135640  TotalLoss:0.00205  PostnetLoss:0.00100  DecoderLoss:0.00105  StopLoss:0.04123  GradNorm:0.00339  GradNormST:0.00992  AvgTextLen:42.2  AvgSpecLen:144.9  StepTime:0.82  LR:0.000100\n",
      "   | > Step:64/68  GlobalStep:135650  TotalLoss:0.00215  PostnetLoss:0.00104  DecoderLoss:0.00110  StopLoss:0.06772  GradNorm:0.00257  GradNormST:0.03034  AvgTextLen:60.6  AvgSpecLen:177.6  StepTime:0.80  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:135654  AvgTotalLoss:0.04342  AvgPostnetLoss:0.00085  AvgDecoderLoss:0.00089  AvgStopLoss:0.04168  EpochTime:41.72  AvgStepTime:0.60\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.06871   PostnetLoss: 0.00576   DecoderLoss:0.00600  StopLoss: 0.05695  \n",
      "   | > TotalLoss: 0.08385   PostnetLoss: 0.00960   DecoderLoss:0.00993  StopLoss: 0.06432  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00085   Validation Loss: 0.00879\n",
      "\n",
      " > Epoch 966/1000\n",
      "   | > Step:5/68  GlobalStep:135660  TotalLoss:0.00126  PostnetLoss:0.00061  DecoderLoss:0.00065  StopLoss:0.03190  GradNorm:0.00339  GradNormST:0.01113  AvgTextLen:5.9  AvgSpecLen:33.2  StepTime:0.36  LR:0.000100\n",
      "   | > Step:15/68  GlobalStep:135670  TotalLoss:0.00146  PostnetLoss:0.00072  DecoderLoss:0.00075  StopLoss:0.04475  GradNorm:0.00303  GradNormST:0.01370  AvgTextLen:12.8  AvgSpecLen:59.1  StepTime:0.60  LR:0.000100\n",
      "   | > Step:25/68  GlobalStep:135680  TotalLoss:0.00160  PostnetLoss:0.00078  DecoderLoss:0.00082  StopLoss:0.03928  GradNorm:0.00302  GradNormST:0.01212  AvgTextLen:19.8  AvgSpecLen:73.3  StepTime:0.48  LR:0.000100\n",
      "   | > Step:35/68  GlobalStep:135690  TotalLoss:0.00181  PostnetLoss:0.00088  DecoderLoss:0.00093  StopLoss:0.04144  GradNorm:0.00288  GradNormST:0.01720  AvgTextLen:27.1  AvgSpecLen:104.7  StepTime:0.74  LR:0.000100\n",
      "   | > Step:45/68  GlobalStep:135700  TotalLoss:0.00187  PostnetLoss:0.00091  DecoderLoss:0.00096  StopLoss:0.04560  GradNorm:0.00282  GradNormST:0.01374  AvgTextLen:34.4  AvgSpecLen:118.6  StepTime:0.65  LR:0.000100\n",
      "   | > Step:55/68  GlobalStep:135710  TotalLoss:0.00211  PostnetLoss:0.00103  DecoderLoss:0.00108  StopLoss:0.03604  GradNorm:0.00333  GradNormST:0.00935  AvgTextLen:43.9  AvgSpecLen:145.2  StepTime:0.77  LR:0.000100\n",
      "   | > Step:65/68  GlobalStep:135720  TotalLoss:0.00229  PostnetLoss:0.00111  DecoderLoss:0.00117  StopLoss:0.09897  GradNorm:0.00266  GradNormST:0.06307  AvgTextLen:64.2  AvgSpecLen:207.4  StepTime:1.19  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:135723  AvgTotalLoss:0.04487  AvgPostnetLoss:0.00085  AvgDecoderLoss:0.00089  AvgStopLoss:0.04312  EpochTime:41.67  AvgStepTime:0.60\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.06789   PostnetLoss: 0.00572   DecoderLoss:0.00597  StopLoss: 0.05620  \n",
      "   | > TotalLoss: 0.08858   PostnetLoss: 0.00995   DecoderLoss:0.01030  StopLoss: 0.06833  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00085   Validation Loss: 0.00894\n",
      "\n",
      " > Epoch 967/1000\n",
      "   | > Step:6/68  GlobalStep:135730  TotalLoss:0.00122  PostnetLoss:0.00060  DecoderLoss:0.00062  StopLoss:0.02477  GradNorm:0.00372  GradNormST:0.01040  AvgTextLen:6.0  AvgSpecLen:34.8  StepTime:0.38  LR:0.000100\n",
      "   | > Step:16/68  GlobalStep:135740  TotalLoss:0.00146  PostnetLoss:0.00072  DecoderLoss:0.00075  StopLoss:0.03568  GradNorm:0.00341  GradNormST:0.01693  AvgTextLen:13.8  AvgSpecLen:60.6  StepTime:0.36  LR:0.000100\n",
      "   | > Step:26/68  GlobalStep:135750  TotalLoss:0.00162  PostnetLoss:0.00079  DecoderLoss:0.00083  StopLoss:0.03437  GradNorm:0.00297  GradNormST:0.01394  AvgTextLen:20.3  AvgSpecLen:77.5  StepTime:0.50  LR:0.000100\n",
      "   | > Step:36/68  GlobalStep:135760  TotalLoss:0.00176  PostnetLoss:0.00086  DecoderLoss:0.00090  StopLoss:0.05405  GradNorm:0.00283  GradNormST:0.01330  AvgTextLen:28.5  AvgSpecLen:100.2  StepTime:0.47  LR:0.000100\n",
      "   | > Step:46/68  GlobalStep:135770  TotalLoss:0.00192  PostnetLoss:0.00093  DecoderLoss:0.00098  StopLoss:0.04731  GradNorm:0.00312  GradNormST:0.01511  AvgTextLen:35.4  AvgSpecLen:122.6  StepTime:0.66  LR:0.000100\n",
      "   | > Step:56/68  GlobalStep:135780  TotalLoss:0.00207  PostnetLoss:0.00101  DecoderLoss:0.00106  StopLoss:0.05322  GradNorm:0.00345  GradNormST:0.02671  AvgTextLen:45.8  AvgSpecLen:145.4  StepTime:0.94  LR:0.000100\n",
      "   | > Step:66/68  GlobalStep:135790  TotalLoss:0.00239  PostnetLoss:0.00116  DecoderLoss:0.00123  StopLoss:0.06129  GradNorm:0.00260  GradNormST:0.03275  AvgTextLen:71.2  AvgSpecLen:220.8  StepTime:1.03  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:135792  AvgTotalLoss:0.04514  AvgPostnetLoss:0.00085  AvgDecoderLoss:0.00089  AvgStopLoss:0.04340  EpochTime:42.30  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.07223   PostnetLoss: 0.00591   DecoderLoss:0.00617  StopLoss: 0.06015  \n",
      "   | > TotalLoss: 0.08749   PostnetLoss: 0.00985   DecoderLoss:0.01017  StopLoss: 0.06747  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00085   Validation Loss: 0.00896\n",
      "\n",
      " > Epoch 968/1000\n",
      "   | > Step:7/68  GlobalStep:135800  TotalLoss:0.00131  PostnetLoss:0.00064  DecoderLoss:0.00067  StopLoss:0.02734  GradNorm:0.00360  GradNormST:0.01224  AvgTextLen:7.4  AvgSpecLen:44.6  StepTime:0.38  LR:0.000100\n",
      "   | > Step:17/68  GlobalStep:135810  TotalLoss:0.00146  PostnetLoss:0.00072  DecoderLoss:0.00075  StopLoss:0.05270  GradNorm:0.00350  GradNormST:0.01948  AvgTextLen:14.7  AvgSpecLen:66.0  StepTime:0.45  LR:0.000100\n",
      "   | > Step:27/68  GlobalStep:135820  TotalLoss:0.00168  PostnetLoss:0.00082  DecoderLoss:0.00085  StopLoss:0.07204  GradNorm:0.00345  GradNormST:0.02723  AvgTextLen:21.0  AvgSpecLen:80.2  StepTime:0.66  LR:0.000100\n",
      "   | > Step:37/68  GlobalStep:135830  TotalLoss:0.00180  PostnetLoss:0.00088  DecoderLoss:0.00092  StopLoss:0.03469  GradNorm:0.00281  GradNormST:0.01285  AvgTextLen:29.3  AvgSpecLen:102.4  StepTime:0.48  LR:0.000100\n",
      "   | > Step:47/68  GlobalStep:135840  TotalLoss:0.00194  PostnetLoss:0.00095  DecoderLoss:0.00099  StopLoss:0.03452  GradNorm:0.00277  GradNormST:0.01395  AvgTextLen:36.7  AvgSpecLen:126.9  StepTime:0.75  LR:0.000100\n",
      "   | > Step:57/68  GlobalStep:135850  TotalLoss:0.00211  PostnetLoss:0.00103  DecoderLoss:0.00108  StopLoss:0.03092  GradNorm:0.00329  GradNormST:0.00808  AvgTextLen:47.1  AvgSpecLen:161.0  StepTime:0.74  LR:0.000100\n",
      "   | > Step:67/68  GlobalStep:135860  TotalLoss:0.00244  PostnetLoss:0.00119  DecoderLoss:0.00125  StopLoss:0.06766  GradNorm:0.00244  GradNormST:0.04776  AvgTextLen:80.6  AvgSpecLen:246.5  StepTime:1.22  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:135861  AvgTotalLoss:0.04573  AvgPostnetLoss:0.00085  AvgDecoderLoss:0.00089  AvgStopLoss:0.04400  EpochTime:42.33  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.06890   PostnetLoss: 0.00585   DecoderLoss:0.00611  StopLoss: 0.05694  \n",
      "   | > TotalLoss: 0.09066   PostnetLoss: 0.01024   DecoderLoss:0.01059  StopLoss: 0.06982  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00085   Validation Loss: 0.00908\n",
      "\n",
      " > Epoch 969/1000\n",
      "   | > Step:8/68  GlobalStep:135870  TotalLoss:0.00133  PostnetLoss:0.00065  DecoderLoss:0.00068  StopLoss:0.03617  GradNorm:0.00362  GradNormST:0.01033  AvgTextLen:8.3  AvgSpecLen:44.0  StepTime:0.32  LR:0.000100\n",
      "   | > Step:18/68  GlobalStep:135880  TotalLoss:0.00144  PostnetLoss:0.00071  DecoderLoss:0.00074  StopLoss:0.04129  GradNorm:0.00317  GradNormST:0.01560  AvgTextLen:15.2  AvgSpecLen:64.5  StepTime:0.41  LR:0.000100\n",
      "   | > Step:28/68  GlobalStep:135890  TotalLoss:0.00165  PostnetLoss:0.00081  DecoderLoss:0.00085  StopLoss:0.03699  GradNorm:0.00307  GradNormST:0.01416  AvgTextLen:22.1  AvgSpecLen:85.4  StepTime:0.58  LR:0.000100\n",
      "   | > Step:38/68  GlobalStep:135900  TotalLoss:0.00179  PostnetLoss:0.00088  DecoderLoss:0.00092  StopLoss:0.03448  GradNorm:0.00272  GradNormST:0.00942  AvgTextLen:29.4  AvgSpecLen:104.1  StepTime:0.59  LR:0.000100\n",
      "   | > Step:48/68  GlobalStep:135910  TotalLoss:0.00192  PostnetLoss:0.00094  DecoderLoss:0.00098  StopLoss:0.03531  GradNorm:0.00258  GradNormST:0.01267  AvgTextLen:37.5  AvgSpecLen:123.8  StepTime:0.59  LR:0.000100\n",
      "   | > Step:58/68  GlobalStep:135920  TotalLoss:0.00210  PostnetLoss:0.00103  DecoderLoss:0.00108  StopLoss:0.03928  GradNorm:0.00267  GradNormST:0.00989  AvgTextLen:47.8  AvgSpecLen:158.0  StepTime:0.84  LR:0.000100\n",
      "   | > Step:68/68  GlobalStep:135930  TotalLoss:0.00242  PostnetLoss:0.00118  DecoderLoss:0.00124  StopLoss:0.05747  GradNorm:0.00259  GradNormST:0.04526  AvgTextLen:103.0  AvgSpecLen:308.9  StepTime:1.89  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:135930  AvgTotalLoss:0.04259  AvgPostnetLoss:0.00085  AvgDecoderLoss:0.00089  AvgStopLoss:0.04085  EpochTime:42.33  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.06767   PostnetLoss: 0.00587   DecoderLoss:0.00613  StopLoss: 0.05567  \n",
      "   | > TotalLoss: 0.08515   PostnetLoss: 0.00981   DecoderLoss:0.01014  StopLoss: 0.06519  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00085   Validation Loss: 0.00897\n",
      "\n",
      " > Epoch 970/1000\n",
      "   | > Step:9/68  GlobalStep:135940  TotalLoss:0.00129  PostnetLoss:0.00063  DecoderLoss:0.00066  StopLoss:0.02921  GradNorm:0.00327  GradNormST:0.01226  AvgTextLen:9.0  AvgSpecLen:42.1  StepTime:0.47  LR:0.000100\n",
      "   | > Step:19/68  GlobalStep:135950  TotalLoss:0.00149  PostnetLoss:0.00073  DecoderLoss:0.00076  StopLoss:0.05370  GradNorm:0.00312  GradNormST:0.01765  AvgTextLen:15.5  AvgSpecLen:63.2  StepTime:0.42  LR:0.000100\n",
      "   | > Step:29/68  GlobalStep:135960  TotalLoss:0.00164  PostnetLoss:0.00080  DecoderLoss:0.00084  StopLoss:0.04514  GradNorm:0.00330  GradNormST:0.00986  AvgTextLen:21.8  AvgSpecLen:83.4  StepTime:0.59  LR:0.000100\n",
      "   | > Step:39/68  GlobalStep:135970  TotalLoss:0.00182  PostnetLoss:0.00089  DecoderLoss:0.00093  StopLoss:0.03441  GradNorm:0.00281  GradNormST:0.01050  AvgTextLen:30.2  AvgSpecLen:110.4  StepTime:0.67  LR:0.000100\n",
      "   | > Step:49/68  GlobalStep:135980  TotalLoss:0.00195  PostnetLoss:0.00095  DecoderLoss:0.00100  StopLoss:0.03423  GradNorm:0.00262  GradNormST:0.01186  AvgTextLen:38.2  AvgSpecLen:127.7  StepTime:0.72  LR:0.000100\n",
      "   | > Step:59/68  GlobalStep:135990  TotalLoss:0.00209  PostnetLoss:0.00102  DecoderLoss:0.00107  StopLoss:0.03011  GradNorm:0.00259  GradNormST:0.00756  AvgTextLen:50.5  AvgSpecLen:155.9  StepTime:0.84  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:135999  AvgTotalLoss:0.04479  AvgPostnetLoss:0.00085  AvgDecoderLoss:0.00089  AvgStopLoss:0.04306  EpochTime:42.19  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.07169   PostnetLoss: 0.00580   DecoderLoss:0.00605  StopLoss: 0.05984  \n",
      "   | > TotalLoss: 0.08479   PostnetLoss: 0.00984   DecoderLoss:0.01016  StopLoss: 0.06479  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00085   Validation Loss: 0.00891\n",
      "\n",
      " > Epoch 971/1000\n",
      "   | > Step:0/68  GlobalStep:136000  TotalLoss:0.00128  PostnetLoss:0.00062  DecoderLoss:0.00066  StopLoss:0.04718  GradNorm:0.00560  GradNormST:0.02612  AvgTextLen:3.0  AvgSpecLen:25.3  StepTime:0.50  LR:0.000100\n",
      " | | > Checkpoint saving : /vm/TTS/models/cartman_models/mozilla-no-loc-fattn-stopnet-sigmoid-loss_masking-August-15-2019_11+26AM-de6aeb9/checkpoint_136000.pth.tar\n",
      "   | > Step:10/68  GlobalStep:136010  TotalLoss:0.00135  PostnetLoss:0.00066  DecoderLoss:0.00069  StopLoss:0.03073  GradNorm:0.00350  GradNormST:0.01334  AvgTextLen:9.8  AvgSpecLen:48.6  StepTime:0.51  LR:0.000100\n",
      "   | > Step:20/68  GlobalStep:136020  TotalLoss:0.00156  PostnetLoss:0.00076  DecoderLoss:0.00080  StopLoss:0.03122  GradNorm:0.00358  GradNormST:0.01115  AvgTextLen:16.4  AvgSpecLen:64.8  StepTime:0.48  LR:0.000100\n",
      "   | > Step:30/68  GlobalStep:136030  TotalLoss:0.00168  PostnetLoss:0.00082  DecoderLoss:0.00086  StopLoss:0.03833  GradNorm:0.00351  GradNormST:0.00991  AvgTextLen:22.9  AvgSpecLen:85.8  StepTime:0.57  LR:0.000100\n",
      "   | > Step:40/68  GlobalStep:136040  TotalLoss:0.00182  PostnetLoss:0.00089  DecoderLoss:0.00093  StopLoss:0.04915  GradNorm:0.00313  GradNormST:0.01372  AvgTextLen:30.9  AvgSpecLen:105.3  StepTime:0.60  LR:0.000100\n",
      "   | > Step:50/68  GlobalStep:136050  TotalLoss:0.00199  PostnetLoss:0.00097  DecoderLoss:0.00102  StopLoss:0.03267  GradNorm:0.00267  GradNormST:0.00836  AvgTextLen:38.8  AvgSpecLen:132.0  StepTime:0.85  LR:0.000100\n",
      "   | > Step:60/68  GlobalStep:136060  TotalLoss:0.00215  PostnetLoss:0.00105  DecoderLoss:0.00110  StopLoss:0.04785  GradNorm:0.00272  GradNormST:0.01513  AvgTextLen:52.0  AvgSpecLen:172.7  StepTime:1.00  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:136068  AvgTotalLoss:0.04483  AvgPostnetLoss:0.00085  AvgDecoderLoss:0.00089  AvgStopLoss:0.04309  EpochTime:42.50  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.06754   PostnetLoss: 0.00566   DecoderLoss:0.00591  StopLoss: 0.05597  \n",
      "   | > TotalLoss: 0.09234   PostnetLoss: 0.01031   DecoderLoss:0.01065  StopLoss: 0.07138  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00085   Validation Loss: 0.00917\n",
      "\n",
      " > Epoch 972/1000\n",
      "   | > Step:1/68  GlobalStep:136070  TotalLoss:0.00125  PostnetLoss:0.00061  DecoderLoss:0.00064  StopLoss:0.03019  GradNorm:0.00450  GradNormST:0.01297  AvgTextLen:3.7  AvgSpecLen:28.6  StepTime:0.36  LR:0.000100\n",
      "   | > Step:11/68  GlobalStep:136080  TotalLoss:0.00134  PostnetLoss:0.00066  DecoderLoss:0.00069  StopLoss:0.03140  GradNorm:0.00350  GradNormST:0.01530  AvgTextLen:10.3  AvgSpecLen:49.4  StepTime:0.39  LR:0.000100\n",
      "   | > Step:21/68  GlobalStep:136090  TotalLoss:0.00153  PostnetLoss:0.00075  DecoderLoss:0.00078  StopLoss:0.03955  GradNorm:0.00324  GradNormST:0.01590  AvgTextLen:16.6  AvgSpecLen:66.3  StepTime:0.37  LR:0.000100\n",
      "   | > Step:31/68  GlobalStep:136100  TotalLoss:0.00167  PostnetLoss:0.00082  DecoderLoss:0.00085  StopLoss:0.03909  GradNorm:0.00303  GradNormST:0.01107  AvgTextLen:24.1  AvgSpecLen:82.7  StepTime:0.43  LR:0.000100\n",
      "   | > Step:41/68  GlobalStep:136110  TotalLoss:0.00184  PostnetLoss:0.00090  DecoderLoss:0.00094  StopLoss:0.02749  GradNorm:0.00320  GradNormST:0.00916  AvgTextLen:32.3  AvgSpecLen:109.4  StepTime:0.64  LR:0.000100\n",
      "   | > Step:51/68  GlobalStep:136120  TotalLoss:0.00193  PostnetLoss:0.00094  DecoderLoss:0.00099  StopLoss:0.03536  GradNorm:0.00259  GradNormST:0.00876  AvgTextLen:39.2  AvgSpecLen:126.9  StepTime:0.79  LR:0.000100\n",
      "   | > Step:61/68  GlobalStep:136130  TotalLoss:0.00212  PostnetLoss:0.00103  DecoderLoss:0.00109  StopLoss:0.04207  GradNorm:0.00252  GradNormST:0.01199  AvgTextLen:54.6  AvgSpecLen:165.5  StepTime:0.89  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:136137  AvgTotalLoss:0.04244  AvgPostnetLoss:0.00084  AvgDecoderLoss:0.00089  AvgStopLoss:0.04071  EpochTime:42.61  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.07187   PostnetLoss: 0.00587   DecoderLoss:0.00613  StopLoss: 0.05987  \n",
      "   | > TotalLoss: 0.08600   PostnetLoss: 0.00984   DecoderLoss:0.01017  StopLoss: 0.06599  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00084   Validation Loss: 0.00878\n",
      "\n",
      " > Epoch 973/1000\n",
      "   | > Step:2/68  GlobalStep:136140  TotalLoss:0.00115  PostnetLoss:0.00056  DecoderLoss:0.00059  StopLoss:0.03776  GradNorm:0.00420  GradNormST:0.01264  AvgTextLen:4.0  AvgSpecLen:31.3  StepTime:0.38  LR:0.000100\n",
      "   | > Step:12/68  GlobalStep:136150  TotalLoss:0.00135  PostnetLoss:0.00066  DecoderLoss:0.00069  StopLoss:0.02970  GradNorm:0.00325  GradNormST:0.01035  AvgTextLen:11.0  AvgSpecLen:49.3  StepTime:0.44  LR:0.000100\n",
      "   | > Step:22/68  GlobalStep:136160  TotalLoss:0.00158  PostnetLoss:0.00077  DecoderLoss:0.00081  StopLoss:0.05064  GradNorm:0.00324  GradNormST:0.01575  AvgTextLen:18.0  AvgSpecLen:72.0  StepTime:0.46  LR:0.000100\n",
      "   | > Step:32/68  GlobalStep:136170  TotalLoss:0.00169  PostnetLoss:0.00083  DecoderLoss:0.00086  StopLoss:0.04747  GradNorm:0.00336  GradNormST:0.01415  AvgTextLen:24.4  AvgSpecLen:91.5  StepTime:0.54  LR:0.000100\n",
      "   | > Step:42/68  GlobalStep:136180  TotalLoss:0.00185  PostnetLoss:0.00090  DecoderLoss:0.00095  StopLoss:0.04326  GradNorm:0.00317  GradNormST:0.02142  AvgTextLen:33.1  AvgSpecLen:109.7  StepTime:0.65  LR:0.000100\n",
      "   | > Step:52/68  GlobalStep:136190  TotalLoss:0.00200  PostnetLoss:0.00098  DecoderLoss:0.00102  StopLoss:0.04855  GradNorm:0.00284  GradNormST:0.01011  AvgTextLen:42.1  AvgSpecLen:137.5  StepTime:0.85  LR:0.000100\n",
      "   | > Step:62/68  GlobalStep:136200  TotalLoss:0.00222  PostnetLoss:0.00108  DecoderLoss:0.00114  StopLoss:0.03328  GradNorm:0.00255  GradNormST:0.00852  AvgTextLen:54.0  AvgSpecLen:174.7  StepTime:0.75  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:136206  AvgTotalLoss:0.05046  AvgPostnetLoss:0.00085  AvgDecoderLoss:0.00089  AvgStopLoss:0.04872  EpochTime:43.13  AvgStepTime:0.63\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.06389   PostnetLoss: 0.00570   DecoderLoss:0.00595  StopLoss: 0.05223  \n",
      "   | > TotalLoss: 0.07556   PostnetLoss: 0.01032   DecoderLoss:0.01063  StopLoss: 0.05461  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00085   Validation Loss: 0.00916\n",
      "\n",
      " > Epoch 974/1000\n",
      "   | > Step:3/68  GlobalStep:136210  TotalLoss:0.00122  PostnetLoss:0.00059  DecoderLoss:0.00062  StopLoss:0.06825  GradNorm:0.00350  GradNormST:0.02074  AvgTextLen:4.7  AvgSpecLen:29.7  StepTime:0.32  LR:0.000100\n",
      "   | > Step:13/68  GlobalStep:136220  TotalLoss:0.00141  PostnetLoss:0.00069  DecoderLoss:0.00072  StopLoss:0.08672  GradNorm:0.00425  GradNormST:0.03289  AvgTextLen:11.5  AvgSpecLen:55.2  StepTime:0.43  LR:0.000100\n",
      "   | > Step:23/68  GlobalStep:136230  TotalLoss:0.00162  PostnetLoss:0.00079  DecoderLoss:0.00083  StopLoss:0.04750  GradNorm:0.00313  GradNormST:0.01802  AvgTextLen:18.3  AvgSpecLen:72.2  StepTime:0.40  LR:0.000100\n",
      "   | > Step:33/68  GlobalStep:136240  TotalLoss:0.00179  PostnetLoss:0.00087  DecoderLoss:0.00091  StopLoss:0.05398  GradNorm:0.00303  GradNormST:0.01534  AvgTextLen:25.9  AvgSpecLen:88.9  StepTime:0.53  LR:0.000100\n",
      "   | > Step:43/68  GlobalStep:136250  TotalLoss:0.00194  PostnetLoss:0.00095  DecoderLoss:0.00099  StopLoss:0.04084  GradNorm:0.00313  GradNormST:0.01286  AvgTextLen:33.4  AvgSpecLen:115.7  StepTime:0.63  LR:0.000100\n",
      "   | > Step:53/68  GlobalStep:136260  TotalLoss:0.00201  PostnetLoss:0.00098  DecoderLoss:0.00103  StopLoss:0.04872  GradNorm:0.00258  GradNormST:0.01908  AvgTextLen:42.4  AvgSpecLen:137.6  StepTime:0.63  LR:0.000100\n",
      "   | > Step:63/68  GlobalStep:136270  TotalLoss:0.00228  PostnetLoss:0.00111  DecoderLoss:0.00117  StopLoss:0.03416  GradNorm:0.00268  GradNormST:0.01728  AvgTextLen:58.6  AvgSpecLen:185.1  StepTime:0.91  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:136275  AvgTotalLoss:0.05163  AvgPostnetLoss:0.00087  AvgDecoderLoss:0.00091  AvgStopLoss:0.04985  EpochTime:42.11  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.07023   PostnetLoss: 0.00590   DecoderLoss:0.00616  StopLoss: 0.05817  \n",
      "   | > TotalLoss: 0.07406   PostnetLoss: 0.00961   DecoderLoss:0.00995  StopLoss: 0.05449  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00087   Validation Loss: 0.00881\n",
      "\n",
      " > Epoch 975/1000\n",
      "   | > Step:4/68  GlobalStep:136280  TotalLoss:0.00121  PostnetLoss:0.00059  DecoderLoss:0.00062  StopLoss:0.04534  GradNorm:0.00345  GradNormST:0.02129  AvgTextLen:4.8  AvgSpecLen:31.1  StepTime:0.25  LR:0.000100\n",
      "   | > Step:14/68  GlobalStep:136290  TotalLoss:0.00142  PostnetLoss:0.00070  DecoderLoss:0.00072  StopLoss:0.04121  GradNorm:0.00323  GradNormST:0.01655  AvgTextLen:12.2  AvgSpecLen:52.0  StepTime:0.33  LR:0.000100\n",
      "   | > Step:24/68  GlobalStep:136300  TotalLoss:0.00166  PostnetLoss:0.00081  DecoderLoss:0.00085  StopLoss:0.04727  GradNorm:0.00326  GradNormST:0.01425  AvgTextLen:19.2  AvgSpecLen:79.0  StepTime:0.49  LR:0.000100\n",
      "   | > Step:34/68  GlobalStep:136310  TotalLoss:0.00178  PostnetLoss:0.00087  DecoderLoss:0.00091  StopLoss:0.02746  GradNorm:0.00297  GradNormST:0.00814  AvgTextLen:26.2  AvgSpecLen:95.9  StepTime:0.62  LR:0.000100\n",
      "   | > Step:44/68  GlobalStep:136320  TotalLoss:0.00193  PostnetLoss:0.00094  DecoderLoss:0.00099  StopLoss:0.03002  GradNorm:0.00297  GradNormST:0.01100  AvgTextLen:33.8  AvgSpecLen:116.3  StepTime:0.69  LR:0.000100\n",
      "   | > Step:54/68  GlobalStep:136330  TotalLoss:0.00205  PostnetLoss:0.00100  DecoderLoss:0.00105  StopLoss:0.03785  GradNorm:0.00266  GradNormST:0.01229  AvgTextLen:42.2  AvgSpecLen:144.9  StepTime:0.79  LR:0.000100\n",
      "   | > Step:64/68  GlobalStep:136340  TotalLoss:0.00214  PostnetLoss:0.00104  DecoderLoss:0.00110  StopLoss:0.04975  GradNorm:0.00246  GradNormST:0.02726  AvgTextLen:60.6  AvgSpecLen:177.6  StepTime:0.84  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:136344  AvgTotalLoss:0.04768  AvgPostnetLoss:0.00085  AvgDecoderLoss:0.00090  AvgStopLoss:0.04594  EpochTime:43.04  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.06934   PostnetLoss: 0.00600   DecoderLoss:0.00627  StopLoss: 0.05707  \n",
      "   | > TotalLoss: 0.08207   PostnetLoss: 0.00970   DecoderLoss:0.01002  StopLoss: 0.06236  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00085   Validation Loss: 0.00880\n",
      "\n",
      " > Epoch 976/1000\n",
      "   | > Step:5/68  GlobalStep:136350  TotalLoss:0.00122  PostnetLoss:0.00060  DecoderLoss:0.00063  StopLoss:0.03284  GradNorm:0.00345  GradNormST:0.01201  AvgTextLen:5.9  AvgSpecLen:33.2  StepTime:0.35  LR:0.000100\n",
      "   | > Step:15/68  GlobalStep:136360  TotalLoss:0.00149  PostnetLoss:0.00073  DecoderLoss:0.00076  StopLoss:0.03775  GradNorm:0.00334  GradNormST:0.01763  AvgTextLen:12.8  AvgSpecLen:59.1  StepTime:0.51  LR:0.000100\n",
      "   | > Step:25/68  GlobalStep:136370  TotalLoss:0.00164  PostnetLoss:0.00080  DecoderLoss:0.00084  StopLoss:0.04723  GradNorm:0.00327  GradNormST:0.01335  AvgTextLen:19.8  AvgSpecLen:73.3  StepTime:0.45  LR:0.000100\n",
      "   | > Step:35/68  GlobalStep:136380  TotalLoss:0.00181  PostnetLoss:0.00088  DecoderLoss:0.00093  StopLoss:0.03562  GradNorm:0.00297  GradNormST:0.01085  AvgTextLen:27.1  AvgSpecLen:104.7  StepTime:0.69  LR:0.000100\n",
      "   | > Step:45/68  GlobalStep:136390  TotalLoss:0.00191  PostnetLoss:0.00093  DecoderLoss:0.00098  StopLoss:0.04089  GradNorm:0.00308  GradNormST:0.01762  AvgTextLen:34.4  AvgSpecLen:118.6  StepTime:0.65  LR:0.000100\n",
      "   | > Step:55/68  GlobalStep:136400  TotalLoss:0.00208  PostnetLoss:0.00102  DecoderLoss:0.00106  StopLoss:0.02886  GradNorm:0.00266  GradNormST:0.01434  AvgTextLen:43.9  AvgSpecLen:145.2  StepTime:0.80  LR:0.000100\n",
      "   | > Step:65/68  GlobalStep:136410  TotalLoss:0.00234  PostnetLoss:0.00114  DecoderLoss:0.00120  StopLoss:0.02444  GradNorm:0.00270  GradNormST:0.01217  AvgTextLen:64.2  AvgSpecLen:207.4  StepTime:1.17  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:136413  AvgTotalLoss:0.03886  AvgPostnetLoss:0.00086  AvgDecoderLoss:0.00090  AvgStopLoss:0.03709  EpochTime:42.83  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.06730   PostnetLoss: 0.00565   DecoderLoss:0.00590  StopLoss: 0.05575  \n",
      "   | > TotalLoss: 0.07702   PostnetLoss: 0.00930   DecoderLoss:0.00962  StopLoss: 0.05810  \n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00086   Validation Loss: 0.00866\n",
      "\n",
      " > Epoch 977/1000\n",
      "   | > Step:6/68  GlobalStep:136420  TotalLoss:0.00124  PostnetLoss:0.00061  DecoderLoss:0.00063  StopLoss:0.03145  GradNorm:0.00355  GradNormST:0.01006  AvgTextLen:6.0  AvgSpecLen:34.8  StepTime:0.34  LR:0.000100\n",
      "   | > Step:16/68  GlobalStep:136430  TotalLoss:0.00148  PostnetLoss:0.00073  DecoderLoss:0.00076  StopLoss:0.03244  GradNorm:0.00326  GradNormST:0.00960  AvgTextLen:13.8  AvgSpecLen:60.6  StepTime:0.39  LR:0.000100\n",
      "   | > Step:26/68  GlobalStep:136440  TotalLoss:0.00166  PostnetLoss:0.00081  DecoderLoss:0.00085  StopLoss:0.04271  GradNorm:0.00305  GradNormST:0.01274  AvgTextLen:20.3  AvgSpecLen:77.5  StepTime:0.48  LR:0.000100\n",
      "   | > Step:36/68  GlobalStep:136450  TotalLoss:0.00176  PostnetLoss:0.00086  DecoderLoss:0.00090  StopLoss:0.04357  GradNorm:0.00290  GradNormST:0.01176  AvgTextLen:28.5  AvgSpecLen:100.2  StepTime:0.58  LR:0.000100\n",
      "   | > Step:46/68  GlobalStep:136460  TotalLoss:0.00191  PostnetLoss:0.00093  DecoderLoss:0.00098  StopLoss:0.02927  GradNorm:0.00298  GradNormST:0.01026  AvgTextLen:35.4  AvgSpecLen:122.6  StepTime:0.77  LR:0.000100\n",
      "   | > Step:56/68  GlobalStep:136470  TotalLoss:0.00208  PostnetLoss:0.00102  DecoderLoss:0.00107  StopLoss:0.02352  GradNorm:0.00319  GradNormST:0.01050  AvgTextLen:45.8  AvgSpecLen:145.4  StepTime:0.82  LR:0.000100\n",
      "   | > Step:66/68  GlobalStep:136480  TotalLoss:0.00242  PostnetLoss:0.00118  DecoderLoss:0.00124  StopLoss:0.02373  GradNorm:0.00253  GradNormST:0.01454  AvgTextLen:71.2  AvgSpecLen:220.8  StepTime:1.05  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:136482  AvgTotalLoss:0.03670  AvgPostnetLoss:0.00085  AvgDecoderLoss:0.00089  AvgStopLoss:0.03495  EpochTime:42.81  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.06662   PostnetLoss: 0.00572   DecoderLoss:0.00596  StopLoss: 0.05494  \n",
      "   | > TotalLoss: 0.07994   PostnetLoss: 0.00960   DecoderLoss:0.00991  StopLoss: 0.06043  \n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00085   Validation Loss: 0.00880\n",
      "\n",
      " > Epoch 978/1000\n",
      "   | > Step:7/68  GlobalStep:136490  TotalLoss:0.00129  PostnetLoss:0.00063  DecoderLoss:0.00066  StopLoss:0.03786  GradNorm:0.00364  GradNormST:0.02689  AvgTextLen:7.4  AvgSpecLen:44.6  StepTime:0.33  LR:0.000100\n",
      "   | > Step:17/68  GlobalStep:136500  TotalLoss:0.00148  PostnetLoss:0.00072  DecoderLoss:0.00075  StopLoss:0.04863  GradNorm:0.00337  GradNormST:0.01551  AvgTextLen:14.7  AvgSpecLen:66.0  StepTime:0.42  LR:0.000100\n",
      "   | > Step:27/68  GlobalStep:136510  TotalLoss:0.00165  PostnetLoss:0.00081  DecoderLoss:0.00084  StopLoss:0.06708  GradNorm:0.00280  GradNormST:0.02357  AvgTextLen:21.0  AvgSpecLen:80.2  StepTime:0.62  LR:0.000100\n",
      "   | > Step:37/68  GlobalStep:136520  TotalLoss:0.00178  PostnetLoss:0.00087  DecoderLoss:0.00091  StopLoss:0.03451  GradNorm:0.00279  GradNormST:0.01306  AvgTextLen:29.3  AvgSpecLen:102.4  StepTime:0.59  LR:0.000100\n",
      "   | > Step:47/68  GlobalStep:136530  TotalLoss:0.00196  PostnetLoss:0.00096  DecoderLoss:0.00100  StopLoss:0.02735  GradNorm:0.00315  GradNormST:0.01137  AvgTextLen:36.7  AvgSpecLen:126.9  StepTime:0.73  LR:0.000100\n",
      "   | > Step:57/68  GlobalStep:136540  TotalLoss:0.00213  PostnetLoss:0.00104  DecoderLoss:0.00109  StopLoss:0.02658  GradNorm:0.00309  GradNormST:0.00716  AvgTextLen:47.1  AvgSpecLen:161.0  StepTime:0.83  LR:0.000100\n",
      "   | > Step:67/68  GlobalStep:136550  TotalLoss:0.00244  PostnetLoss:0.00119  DecoderLoss:0.00126  StopLoss:0.01845  GradNorm:0.00257  GradNormST:0.01161  AvgTextLen:80.6  AvgSpecLen:246.5  StepTime:1.21  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:136551  AvgTotalLoss:0.03762  AvgPostnetLoss:0.00085  AvgDecoderLoss:0.00089  AvgStopLoss:0.03588  EpochTime:42.75  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.06796   PostnetLoss: 0.00579   DecoderLoss:0.00604  StopLoss: 0.05612  \n",
      "   | > TotalLoss: 0.07754   PostnetLoss: 0.00960   DecoderLoss:0.00990  StopLoss: 0.05804  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00085   Validation Loss: 0.00880\n",
      "\n",
      " > Epoch 979/1000\n",
      "   | > Step:8/68  GlobalStep:136560  TotalLoss:0.00133  PostnetLoss:0.00065  DecoderLoss:0.00068  StopLoss:0.03078  GradNorm:0.00367  GradNormST:0.00930  AvgTextLen:8.3  AvgSpecLen:44.0  StepTime:0.30  LR:0.000100\n",
      "   | > Step:18/68  GlobalStep:136570  TotalLoss:0.00147  PostnetLoss:0.00072  DecoderLoss:0.00075  StopLoss:0.03617  GradNorm:0.00340  GradNormST:0.01229  AvgTextLen:15.2  AvgSpecLen:64.5  StepTime:0.41  LR:0.000100\n",
      "   | > Step:28/68  GlobalStep:136580  TotalLoss:0.00165  PostnetLoss:0.00081  DecoderLoss:0.00084  StopLoss:0.03524  GradNorm:0.00301  GradNormST:0.01300  AvgTextLen:22.1  AvgSpecLen:85.4  StepTime:0.53  LR:0.000100\n",
      "   | > Step:38/68  GlobalStep:136590  TotalLoss:0.00178  PostnetLoss:0.00087  DecoderLoss:0.00091  StopLoss:0.02609  GradNorm:0.00282  GradNormST:0.00882  AvgTextLen:29.4  AvgSpecLen:104.1  StepTime:0.64  LR:0.000100\n",
      "   | > Step:48/68  GlobalStep:136600  TotalLoss:0.00194  PostnetLoss:0.00095  DecoderLoss:0.00099  StopLoss:0.02569  GradNorm:0.00292  GradNormST:0.00926  AvgTextLen:37.5  AvgSpecLen:123.8  StepTime:0.66  LR:0.000100\n",
      "   | > Step:58/68  GlobalStep:136610  TotalLoss:0.00213  PostnetLoss:0.00104  DecoderLoss:0.00109  StopLoss:0.03319  GradNorm:0.00296  GradNormST:0.01479  AvgTextLen:47.8  AvgSpecLen:158.0  StepTime:0.70  LR:0.000100\n",
      "   | > Step:68/68  GlobalStep:136620  TotalLoss:0.00240  PostnetLoss:0.00117  DecoderLoss:0.00123  StopLoss:0.01524  GradNorm:0.00263  GradNormST:0.00815  AvgTextLen:103.0  AvgSpecLen:308.9  StepTime:1.86  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:136620  AvgTotalLoss:0.03688  AvgPostnetLoss:0.00085  AvgDecoderLoss:0.00089  AvgStopLoss:0.03515  EpochTime:42.16  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.06631   PostnetLoss: 0.00588   DecoderLoss:0.00613  StopLoss: 0.05430  \n",
      "   | > TotalLoss: 0.07697   PostnetLoss: 0.00952   DecoderLoss:0.00984  StopLoss: 0.05761  \n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00085   Validation Loss: 0.00881\n",
      "\n",
      " > Epoch 980/1000\n",
      "   | > Step:9/68  GlobalStep:136630  TotalLoss:0.00132  PostnetLoss:0.00064  DecoderLoss:0.00067  StopLoss:0.04343  GradNorm:0.00349  GradNormST:0.02604  AvgTextLen:9.0  AvgSpecLen:42.1  StepTime:0.42  LR:0.000100\n",
      "   | > Step:19/68  GlobalStep:136640  TotalLoss:0.00148  PostnetLoss:0.00072  DecoderLoss:0.00076  StopLoss:0.05461  GradNorm:0.00350  GradNormST:0.01831  AvgTextLen:15.5  AvgSpecLen:63.2  StepTime:0.32  LR:0.000100\n",
      "   | > Step:29/68  GlobalStep:136650  TotalLoss:0.00164  PostnetLoss:0.00080  DecoderLoss:0.00084  StopLoss:0.05744  GradNorm:0.00316  GradNormST:0.01370  AvgTextLen:21.8  AvgSpecLen:83.4  StepTime:0.59  LR:0.000100\n",
      "   | > Step:39/68  GlobalStep:136660  TotalLoss:0.00181  PostnetLoss:0.00088  DecoderLoss:0.00093  StopLoss:0.02745  GradNorm:0.00273  GradNormST:0.00969  AvgTextLen:30.2  AvgSpecLen:110.4  StepTime:0.62  LR:0.000100\n",
      "   | > Step:49/68  GlobalStep:136670  TotalLoss:0.00194  PostnetLoss:0.00095  DecoderLoss:0.00099  StopLoss:0.02787  GradNorm:0.00298  GradNormST:0.01065  AvgTextLen:38.2  AvgSpecLen:127.7  StepTime:0.59  LR:0.000100\n",
      "   | > Step:59/68  GlobalStep:136680  TotalLoss:0.00208  PostnetLoss:0.00101  DecoderLoss:0.00106  StopLoss:0.02637  GradNorm:0.00270  GradNormST:0.00971  AvgTextLen:50.5  AvgSpecLen:155.9  StepTime:0.86  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:136689  AvgTotalLoss:0.03771  AvgPostnetLoss:0.00085  AvgDecoderLoss:0.00089  AvgStopLoss:0.03598  EpochTime:41.95  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.06861   PostnetLoss: 0.00590   DecoderLoss:0.00615  StopLoss: 0.05656  \n",
      "   | > TotalLoss: 0.07743   PostnetLoss: 0.00942   DecoderLoss:0.00973  StopLoss: 0.05827  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00085   Validation Loss: 0.00877\n",
      "\n",
      " > Epoch 981/1000\n",
      "   | > Step:0/68  GlobalStep:136690  TotalLoss:0.00129  PostnetLoss:0.00062  DecoderLoss:0.00067  StopLoss:0.05579  GradNorm:0.00493  GradNormST:0.02572  AvgTextLen:3.0  AvgSpecLen:25.3  StepTime:0.53  LR:0.000100\n",
      "   | > Step:10/68  GlobalStep:136700  TotalLoss:0.00135  PostnetLoss:0.00066  DecoderLoss:0.00069  StopLoss:0.06595  GradNorm:0.00365  GradNormST:0.05197  AvgTextLen:9.8  AvgSpecLen:48.6  StepTime:0.49  LR:0.000100\n",
      "   | > Step:20/68  GlobalStep:136710  TotalLoss:0.00155  PostnetLoss:0.00076  DecoderLoss:0.00079  StopLoss:0.02367  GradNorm:0.00362  GradNormST:0.01065  AvgTextLen:16.4  AvgSpecLen:64.8  StepTime:0.40  LR:0.000100\n",
      "   | > Step:30/68  GlobalStep:136720  TotalLoss:0.00167  PostnetLoss:0.00082  DecoderLoss:0.00086  StopLoss:0.03272  GradNorm:0.00302  GradNormST:0.00970  AvgTextLen:22.9  AvgSpecLen:85.8  StepTime:0.60  LR:0.000100\n",
      "   | > Step:40/68  GlobalStep:136730  TotalLoss:0.00182  PostnetLoss:0.00089  DecoderLoss:0.00093  StopLoss:0.03565  GradNorm:0.00289  GradNormST:0.01262  AvgTextLen:30.9  AvgSpecLen:105.3  StepTime:0.57  LR:0.000100\n",
      "   | > Step:50/68  GlobalStep:136740  TotalLoss:0.00196  PostnetLoss:0.00096  DecoderLoss:0.00100  StopLoss:0.02873  GradNorm:0.00308  GradNormST:0.01003  AvgTextLen:38.8  AvgSpecLen:132.0  StepTime:0.67  LR:0.000100\n",
      "   | > Step:60/68  GlobalStep:136750  TotalLoss:0.00217  PostnetLoss:0.00106  DecoderLoss:0.00111  StopLoss:0.03055  GradNorm:0.00311  GradNormST:0.00861  AvgTextLen:52.0  AvgSpecLen:172.7  StepTime:0.90  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:136758  AvgTotalLoss:0.03653  AvgPostnetLoss:0.00085  AvgDecoderLoss:0.00089  AvgStopLoss:0.03479  EpochTime:42.36  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.06983   PostnetLoss: 0.00590   DecoderLoss:0.00614  StopLoss: 0.05780  \n",
      "   | > TotalLoss: 0.07825   PostnetLoss: 0.00958   DecoderLoss:0.00989  StopLoss: 0.05878  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00085   Validation Loss: 0.00893\n",
      "\n",
      " > Epoch 982/1000\n",
      "   | > Step:1/68  GlobalStep:136760  TotalLoss:0.00123  PostnetLoss:0.00060  DecoderLoss:0.00063  StopLoss:0.03334  GradNorm:0.00453  GradNormST:0.01509  AvgTextLen:3.7  AvgSpecLen:28.6  StepTime:0.31  LR:0.000100\n",
      "   | > Step:11/68  GlobalStep:136770  TotalLoss:0.00133  PostnetLoss:0.00065  DecoderLoss:0.00068  StopLoss:0.02841  GradNorm:0.00328  GradNormST:0.01326  AvgTextLen:10.3  AvgSpecLen:49.4  StepTime:0.31  LR:0.000100\n",
      "   | > Step:21/68  GlobalStep:136780  TotalLoss:0.00151  PostnetLoss:0.00074  DecoderLoss:0.00077  StopLoss:0.04009  GradNorm:0.00325  GradNormST:0.01405  AvgTextLen:16.6  AvgSpecLen:66.3  StepTime:0.34  LR:0.000100\n",
      "   | > Step:31/68  GlobalStep:136790  TotalLoss:0.00168  PostnetLoss:0.00082  DecoderLoss:0.00086  StopLoss:0.02661  GradNorm:0.00310  GradNormST:0.01244  AvgTextLen:24.1  AvgSpecLen:82.7  StepTime:0.54  LR:0.000100\n",
      "   | > Step:41/68  GlobalStep:136800  TotalLoss:0.00185  PostnetLoss:0.00091  DecoderLoss:0.00095  StopLoss:0.02480  GradNorm:0.00293  GradNormST:0.01020  AvgTextLen:32.3  AvgSpecLen:109.4  StepTime:0.61  LR:0.000100\n",
      "   | > Step:51/68  GlobalStep:136810  TotalLoss:0.00193  PostnetLoss:0.00094  DecoderLoss:0.00099  StopLoss:0.03327  GradNorm:0.00267  GradNormST:0.01021  AvgTextLen:39.2  AvgSpecLen:126.9  StepTime:0.80  LR:0.000100\n",
      "   | > Step:61/68  GlobalStep:136820  TotalLoss:0.00213  PostnetLoss:0.00104  DecoderLoss:0.00109  StopLoss:0.02668  GradNorm:0.00287  GradNormST:0.01031  AvgTextLen:54.6  AvgSpecLen:165.5  StepTime:0.99  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:136827  AvgTotalLoss:0.03572  AvgPostnetLoss:0.00085  AvgDecoderLoss:0.00089  AvgStopLoss:0.03398  EpochTime:42.68  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.06616   PostnetLoss: 0.00584   DecoderLoss:0.00609  StopLoss: 0.05424  \n",
      "   | > TotalLoss: 0.07920   PostnetLoss: 0.00957   DecoderLoss:0.00988  StopLoss: 0.05975  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00085   Validation Loss: 0.00893\n",
      "\n",
      " > Epoch 983/1000\n",
      "   | > Step:2/68  GlobalStep:136830  TotalLoss:0.00115  PostnetLoss:0.00056  DecoderLoss:0.00059  StopLoss:0.02829  GradNorm:0.00382  GradNormST:0.00995  AvgTextLen:4.0  AvgSpecLen:31.3  StepTime:0.51  LR:0.000100\n",
      "   | > Step:12/68  GlobalStep:136840  TotalLoss:0.00134  PostnetLoss:0.00065  DecoderLoss:0.00068  StopLoss:0.03190  GradNorm:0.00311  GradNormST:0.01150  AvgTextLen:11.0  AvgSpecLen:49.3  StepTime:0.45  LR:0.000100\n",
      "   | > Step:22/68  GlobalStep:136850  TotalLoss:0.00158  PostnetLoss:0.00077  DecoderLoss:0.00080  StopLoss:0.04183  GradNorm:0.00364  GradNormST:0.01132  AvgTextLen:18.0  AvgSpecLen:72.0  StepTime:0.37  LR:0.000100\n",
      "   | > Step:32/68  GlobalStep:136860  TotalLoss:0.00171  PostnetLoss:0.00084  DecoderLoss:0.00087  StopLoss:0.04241  GradNorm:0.00330  GradNormST:0.01372  AvgTextLen:24.4  AvgSpecLen:91.5  StepTime:0.56  LR:0.000100\n",
      "   | > Step:42/68  GlobalStep:136870  TotalLoss:0.00185  PostnetLoss:0.00090  DecoderLoss:0.00095  StopLoss:0.03844  GradNorm:0.00299  GradNormST:0.02312  AvgTextLen:33.1  AvgSpecLen:109.7  StepTime:0.64  LR:0.000100\n",
      "   | > Step:52/68  GlobalStep:136880  TotalLoss:0.00201  PostnetLoss:0.00098  DecoderLoss:0.00103  StopLoss:0.04337  GradNorm:0.00279  GradNormST:0.01226  AvgTextLen:42.1  AvgSpecLen:137.5  StepTime:0.68  LR:0.000100\n",
      "   | > Step:62/68  GlobalStep:136890  TotalLoss:0.00220  PostnetLoss:0.00107  DecoderLoss:0.00113  StopLoss:0.02083  GradNorm:0.00283  GradNormST:0.00856  AvgTextLen:54.0  AvgSpecLen:174.7  StepTime:0.75  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:136896  AvgTotalLoss:0.03649  AvgPostnetLoss:0.00084  AvgDecoderLoss:0.00088  AvgStopLoss:0.03476  EpochTime:42.63  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.06896   PostnetLoss: 0.00589   DecoderLoss:0.00614  StopLoss: 0.05692  \n",
      "   | > TotalLoss: 0.08086   PostnetLoss: 0.00959   DecoderLoss:0.00990  StopLoss: 0.06137  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00084   Validation Loss: 0.00882\n",
      "\n",
      " > Epoch 984/1000\n",
      "   | > Step:3/68  GlobalStep:136900  TotalLoss:0.00118  PostnetLoss:0.00058  DecoderLoss:0.00060  StopLoss:0.05107  GradNorm:0.00369  GradNormST:0.02105  AvgTextLen:4.7  AvgSpecLen:29.7  StepTime:0.27  LR:0.000100\n",
      "   | > Step:13/68  GlobalStep:136910  TotalLoss:0.00134  PostnetLoss:0.00066  DecoderLoss:0.00068  StopLoss:0.03720  GradNorm:0.00345  GradNormST:0.01299  AvgTextLen:11.5  AvgSpecLen:55.2  StepTime:0.42  LR:0.000100\n",
      "   | > Step:23/68  GlobalStep:136920  TotalLoss:0.00159  PostnetLoss:0.00078  DecoderLoss:0.00081  StopLoss:0.04829  GradNorm:0.00352  GradNormST:0.01481  AvgTextLen:18.3  AvgSpecLen:72.2  StepTime:0.34  LR:0.000100\n",
      "   | > Step:33/68  GlobalStep:136930  TotalLoss:0.00173  PostnetLoss:0.00085  DecoderLoss:0.00088  StopLoss:0.03870  GradNorm:0.00326  GradNormST:0.01540  AvgTextLen:25.9  AvgSpecLen:88.9  StepTime:0.43  LR:0.000100\n",
      "   | > Step:43/68  GlobalStep:136940  TotalLoss:0.00187  PostnetLoss:0.00091  DecoderLoss:0.00096  StopLoss:0.04624  GradNorm:0.00302  GradNormST:0.01375  AvgTextLen:33.4  AvgSpecLen:115.7  StepTime:0.62  LR:0.000100\n",
      "   | > Step:53/68  GlobalStep:136950  TotalLoss:0.00196  PostnetLoss:0.00096  DecoderLoss:0.00100  StopLoss:0.02562  GradNorm:0.00264  GradNormST:0.00703  AvgTextLen:42.4  AvgSpecLen:137.6  StepTime:0.65  LR:0.000100\n",
      "   | > Step:63/68  GlobalStep:136960  TotalLoss:0.00224  PostnetLoss:0.00109  DecoderLoss:0.00115  StopLoss:0.02069  GradNorm:0.00245  GradNormST:0.00663  AvgTextLen:58.6  AvgSpecLen:185.1  StepTime:0.90  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:136965  AvgTotalLoss:0.03657  AvgPostnetLoss:0.00085  AvgDecoderLoss:0.00089  AvgStopLoss:0.03484  EpochTime:42.54  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.06741   PostnetLoss: 0.00587   DecoderLoss:0.00611  StopLoss: 0.05543  \n",
      "   | > TotalLoss: 0.07845   PostnetLoss: 0.00952   DecoderLoss:0.00985  StopLoss: 0.05908  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00085   Validation Loss: 0.00875\n",
      "\n",
      " > Epoch 985/1000\n",
      "   | > Step:4/68  GlobalStep:136970  TotalLoss:0.00122  PostnetLoss:0.00059  DecoderLoss:0.00063  StopLoss:0.04623  GradNorm:0.00357  GradNormST:0.01582  AvgTextLen:4.8  AvgSpecLen:31.1  StepTime:0.27  LR:0.000100\n",
      "   | > Step:14/68  GlobalStep:136980  TotalLoss:0.00142  PostnetLoss:0.00069  DecoderLoss:0.00073  StopLoss:0.04374  GradNorm:0.00331  GradNormST:0.01456  AvgTextLen:12.2  AvgSpecLen:52.0  StepTime:0.31  LR:0.000100\n",
      "   | > Step:24/68  GlobalStep:136990  TotalLoss:0.00165  PostnetLoss:0.00081  DecoderLoss:0.00084  StopLoss:0.03680  GradNorm:0.00365  GradNormST:0.01434  AvgTextLen:19.2  AvgSpecLen:79.0  StepTime:0.40  LR:0.000100\n",
      "   | > Step:34/68  GlobalStep:137000  TotalLoss:0.00177  PostnetLoss:0.00086  DecoderLoss:0.00091  StopLoss:0.02523  GradNorm:0.00318  GradNormST:0.00678  AvgTextLen:26.2  AvgSpecLen:95.9  StepTime:0.61  LR:0.000100\n",
      " | | > Checkpoint saving : /vm/TTS/models/cartman_models/mozilla-no-loc-fattn-stopnet-sigmoid-loss_masking-August-15-2019_11+26AM-de6aeb9/checkpoint_137000.pth.tar\n",
      "   | > Step:44/68  GlobalStep:137010  TotalLoss:0.00191  PostnetLoss:0.00093  DecoderLoss:0.00098  StopLoss:0.02150  GradNorm:0.00312  GradNormST:0.00839  AvgTextLen:33.8  AvgSpecLen:116.3  StepTime:0.54  LR:0.000100\n",
      "   | > Step:54/68  GlobalStep:137020  TotalLoss:0.00203  PostnetLoss:0.00099  DecoderLoss:0.00104  StopLoss:0.02699  GradNorm:0.00279  GradNormST:0.00725  AvgTextLen:42.2  AvgSpecLen:144.9  StepTime:0.81  LR:0.000100\n",
      "   | > Step:64/68  GlobalStep:137030  TotalLoss:0.00210  PostnetLoss:0.00102  DecoderLoss:0.00108  StopLoss:0.02597  GradNorm:0.00245  GradNormST:0.00723  AvgTextLen:60.6  AvgSpecLen:177.6  StepTime:0.80  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:137034  AvgTotalLoss:0.03592  AvgPostnetLoss:0.00085  AvgDecoderLoss:0.00089  AvgStopLoss:0.03419  EpochTime:42.02  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.06598   PostnetLoss: 0.00586   DecoderLoss:0.00611  StopLoss: 0.05400  \n",
      "   | > TotalLoss: 0.07971   PostnetLoss: 0.00945   DecoderLoss:0.00978  StopLoss: 0.06047  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00085   Validation Loss: 0.00871\n",
      "\n",
      " > Epoch 986/1000\n",
      "   | > Step:5/68  GlobalStep:137040  TotalLoss:0.00124  PostnetLoss:0.00060  DecoderLoss:0.00063  StopLoss:0.04592  GradNorm:0.00326  GradNormST:0.02176  AvgTextLen:5.9  AvgSpecLen:33.2  StepTime:0.27  LR:0.000100\n",
      "   | > Step:15/68  GlobalStep:137050  TotalLoss:0.00146  PostnetLoss:0.00072  DecoderLoss:0.00075  StopLoss:0.05376  GradNorm:0.00321  GradNormST:0.02480  AvgTextLen:12.8  AvgSpecLen:59.1  StepTime:0.49  LR:0.000100\n",
      "   | > Step:25/68  GlobalStep:137060  TotalLoss:0.00160  PostnetLoss:0.00078  DecoderLoss:0.00082  StopLoss:0.03542  GradNorm:0.00377  GradNormST:0.01057  AvgTextLen:19.8  AvgSpecLen:73.3  StepTime:0.46  LR:0.000100\n",
      "   | > Step:35/68  GlobalStep:137070  TotalLoss:0.00176  PostnetLoss:0.00086  DecoderLoss:0.00090  StopLoss:0.03616  GradNorm:0.00346  GradNormST:0.01563  AvgTextLen:27.1  AvgSpecLen:104.7  StepTime:0.74  LR:0.000100\n",
      "   | > Step:45/68  GlobalStep:137080  TotalLoss:0.00186  PostnetLoss:0.00091  DecoderLoss:0.00095  StopLoss:0.03181  GradNorm:0.00329  GradNormST:0.01655  AvgTextLen:34.4  AvgSpecLen:118.6  StepTime:0.63  LR:0.000100\n",
      "   | > Step:55/68  GlobalStep:137090  TotalLoss:0.00205  PostnetLoss:0.00100  DecoderLoss:0.00105  StopLoss:0.02487  GradNorm:0.00276  GradNormST:0.01025  AvgTextLen:43.9  AvgSpecLen:145.2  StepTime:0.77  LR:0.000100\n",
      "   | > Step:65/68  GlobalStep:137100  TotalLoss:0.00229  PostnetLoss:0.00111  DecoderLoss:0.00117  StopLoss:0.02568  GradNorm:0.00247  GradNormST:0.01229  AvgTextLen:64.2  AvgSpecLen:207.4  StepTime:1.18  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:137103  AvgTotalLoss:0.03459  AvgPostnetLoss:0.00085  AvgDecoderLoss:0.00089  AvgStopLoss:0.03285  EpochTime:42.69  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.06787   PostnetLoss: 0.00581   DecoderLoss:0.00606  StopLoss: 0.05599  \n",
      "   | > TotalLoss: 0.08157   PostnetLoss: 0.00989   DecoderLoss:0.01021  StopLoss: 0.06148  \n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00085   Validation Loss: 0.00901\n",
      "\n",
      " > Epoch 987/1000\n",
      "   | > Step:6/68  GlobalStep:137110  TotalLoss:0.00125  PostnetLoss:0.00061  DecoderLoss:0.00064  StopLoss:0.02950  GradNorm:0.00386  GradNormST:0.01328  AvgTextLen:6.0  AvgSpecLen:34.8  StepTime:0.38  LR:0.000100\n",
      "   | > Step:16/68  GlobalStep:137120  TotalLoss:0.00146  PostnetLoss:0.00071  DecoderLoss:0.00075  StopLoss:0.03207  GradNorm:0.00346  GradNormST:0.01454  AvgTextLen:13.8  AvgSpecLen:60.6  StepTime:0.29  LR:0.000100\n",
      "   | > Step:26/68  GlobalStep:137130  TotalLoss:0.00164  PostnetLoss:0.00080  DecoderLoss:0.00084  StopLoss:0.04169  GradNorm:0.00363  GradNormST:0.01384  AvgTextLen:20.3  AvgSpecLen:77.5  StepTime:0.48  LR:0.000100\n",
      "   | > Step:36/68  GlobalStep:137140  TotalLoss:0.00174  PostnetLoss:0.00085  DecoderLoss:0.00089  StopLoss:0.03332  GradNorm:0.00333  GradNormST:0.01060  AvgTextLen:28.5  AvgSpecLen:100.2  StepTime:0.68  LR:0.000100\n",
      "   | > Step:46/68  GlobalStep:137150  TotalLoss:0.00191  PostnetLoss:0.00094  DecoderLoss:0.00098  StopLoss:0.03092  GradNorm:0.00358  GradNormST:0.00922  AvgTextLen:35.4  AvgSpecLen:122.6  StepTime:0.78  LR:0.000100\n",
      "   | > Step:56/68  GlobalStep:137160  TotalLoss:0.00205  PostnetLoss:0.00100  DecoderLoss:0.00105  StopLoss:0.01705  GradNorm:0.00305  GradNormST:0.00793  AvgTextLen:45.8  AvgSpecLen:145.4  StepTime:0.94  LR:0.000100\n",
      "   | > Step:66/68  GlobalStep:137170  TotalLoss:0.00241  PostnetLoss:0.00117  DecoderLoss:0.00124  StopLoss:0.02964  GradNorm:0.00266  GradNormST:0.01479  AvgTextLen:71.2  AvgSpecLen:220.8  StepTime:1.04  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:137172  AvgTotalLoss:0.03650  AvgPostnetLoss:0.00085  AvgDecoderLoss:0.00089  AvgStopLoss:0.03477  EpochTime:42.51  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.06648   PostnetLoss: 0.00595   DecoderLoss:0.00619  StopLoss: 0.05434  \n",
      "   | > TotalLoss: 0.07812   PostnetLoss: 0.00961   DecoderLoss:0.00994  StopLoss: 0.05857  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00085   Validation Loss: 0.00881\n",
      "\n",
      " > Epoch 988/1000\n",
      "   | > Step:7/68  GlobalStep:137180  TotalLoss:0.00131  PostnetLoss:0.00064  DecoderLoss:0.00067  StopLoss:0.05352  GradNorm:0.00337  GradNormST:0.04522  AvgTextLen:7.4  AvgSpecLen:44.6  StepTime:0.37  LR:0.000100\n",
      "   | > Step:17/68  GlobalStep:137190  TotalLoss:0.00148  PostnetLoss:0.00072  DecoderLoss:0.00075  StopLoss:0.05830  GradNorm:0.00313  GradNormST:0.01538  AvgTextLen:14.7  AvgSpecLen:66.0  StepTime:0.42  LR:0.000100\n",
      "   | > Step:27/68  GlobalStep:137200  TotalLoss:0.00167  PostnetLoss:0.00082  DecoderLoss:0.00085  StopLoss:0.05909  GradNorm:0.00327  GradNormST:0.03129  AvgTextLen:21.0  AvgSpecLen:80.2  StepTime:0.68  LR:0.000100\n",
      "   | > Step:37/68  GlobalStep:137210  TotalLoss:0.00182  PostnetLoss:0.00089  DecoderLoss:0.00093  StopLoss:0.03678  GradNorm:0.00345  GradNormST:0.01025  AvgTextLen:29.3  AvgSpecLen:102.4  StepTime:0.60  LR:0.000100\n",
      "   | > Step:47/68  GlobalStep:137220  TotalLoss:0.00196  PostnetLoss:0.00096  DecoderLoss:0.00100  StopLoss:0.02347  GradNorm:0.00369  GradNormST:0.00983  AvgTextLen:36.7  AvgSpecLen:126.9  StepTime:0.70  LR:0.000100\n",
      "   | > Step:57/68  GlobalStep:137230  TotalLoss:0.00212  PostnetLoss:0.00104  DecoderLoss:0.00109  StopLoss:0.02682  GradNorm:0.00336  GradNormST:0.00652  AvgTextLen:47.1  AvgSpecLen:161.0  StepTime:0.83  LR:0.000100\n",
      "   | > Step:67/68  GlobalStep:137240  TotalLoss:0.00242  PostnetLoss:0.00118  DecoderLoss:0.00125  StopLoss:0.01953  GradNorm:0.00287  GradNormST:0.01074  AvgTextLen:80.6  AvgSpecLen:246.5  StepTime:1.22  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:137241  AvgTotalLoss:0.03625  AvgPostnetLoss:0.00085  AvgDecoderLoss:0.00089  AvgStopLoss:0.03452  EpochTime:41.97  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.06560   PostnetLoss: 0.00595   DecoderLoss:0.00619  StopLoss: 0.05346  \n",
      "   | > TotalLoss: 0.07851   PostnetLoss: 0.00942   DecoderLoss:0.00974  StopLoss: 0.05935  \n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00085   Validation Loss: 0.00868\n",
      "\n",
      " > Epoch 989/1000\n",
      "   | > Step:8/68  GlobalStep:137250  TotalLoss:0.00131  PostnetLoss:0.00064  DecoderLoss:0.00067  StopLoss:0.03654  GradNorm:0.00366  GradNormST:0.01102  AvgTextLen:8.3  AvgSpecLen:44.0  StepTime:0.33  LR:0.000100\n",
      "   | > Step:18/68  GlobalStep:137260  TotalLoss:0.00145  PostnetLoss:0.00071  DecoderLoss:0.00074  StopLoss:0.03545  GradNorm:0.00315  GradNormST:0.01380  AvgTextLen:15.2  AvgSpecLen:64.5  StepTime:0.33  LR:0.000100\n",
      "   | > Step:28/68  GlobalStep:137270  TotalLoss:0.00165  PostnetLoss:0.00081  DecoderLoss:0.00084  StopLoss:0.04127  GradNorm:0.00338  GradNormST:0.01458  AvgTextLen:22.1  AvgSpecLen:85.4  StepTime:0.54  LR:0.000100\n",
      "   | > Step:38/68  GlobalStep:137280  TotalLoss:0.00179  PostnetLoss:0.00088  DecoderLoss:0.00091  StopLoss:0.02483  GradNorm:0.00351  GradNormST:0.00704  AvgTextLen:29.4  AvgSpecLen:104.1  StepTime:0.60  LR:0.000100\n",
      "   | > Step:48/68  GlobalStep:137290  TotalLoss:0.00196  PostnetLoss:0.00096  DecoderLoss:0.00100  StopLoss:0.03227  GradNorm:0.00381  GradNormST:0.00816  AvgTextLen:37.5  AvgSpecLen:123.8  StepTime:0.60  LR:0.000100\n",
      "   | > Step:58/68  GlobalStep:137300  TotalLoss:0.00212  PostnetLoss:0.00104  DecoderLoss:0.00109  StopLoss:0.03158  GradNorm:0.00383  GradNormST:0.01135  AvgTextLen:47.8  AvgSpecLen:158.0  StepTime:0.91  LR:0.000100\n",
      "   | > Step:68/68  GlobalStep:137310  TotalLoss:0.00239  PostnetLoss:0.00116  DecoderLoss:0.00123  StopLoss:0.01326  GradNorm:0.00273  GradNormST:0.00724  AvgTextLen:103.0  AvgSpecLen:308.9  StepTime:1.86  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:137310  AvgTotalLoss:0.03564  AvgPostnetLoss:0.00085  AvgDecoderLoss:0.00089  AvgStopLoss:0.03391  EpochTime:42.87  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.06441   PostnetLoss: 0.00582   DecoderLoss:0.00606  StopLoss: 0.05253  \n",
      "   | > TotalLoss: 0.08389   PostnetLoss: 0.00985   DecoderLoss:0.01019  StopLoss: 0.06386  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00085   Validation Loss: 0.00892\n",
      "\n",
      " > Epoch 990/1000\n",
      "   | > Step:9/68  GlobalStep:137320  TotalLoss:0.00128  PostnetLoss:0.00063  DecoderLoss:0.00065  StopLoss:0.03768  GradNorm:0.00348  GradNormST:0.01664  AvgTextLen:9.0  AvgSpecLen:42.1  StepTime:0.32  LR:0.000100\n",
      "   | > Step:19/68  GlobalStep:137330  TotalLoss:0.00148  PostnetLoss:0.00072  DecoderLoss:0.00075  StopLoss:0.06201  GradNorm:0.00336  GradNormST:0.02073  AvgTextLen:15.5  AvgSpecLen:63.2  StepTime:0.44  LR:0.000100\n",
      "   | > Step:29/68  GlobalStep:137340  TotalLoss:0.00163  PostnetLoss:0.00080  DecoderLoss:0.00083  StopLoss:0.04674  GradNorm:0.00328  GradNormST:0.01393  AvgTextLen:21.8  AvgSpecLen:83.4  StepTime:0.56  LR:0.000100\n",
      "   | > Step:39/68  GlobalStep:137350  TotalLoss:0.00181  PostnetLoss:0.00088  DecoderLoss:0.00092  StopLoss:0.02431  GradNorm:0.00311  GradNormST:0.00763  AvgTextLen:30.2  AvgSpecLen:110.4  StepTime:0.68  LR:0.000100\n",
      "   | > Step:49/68  GlobalStep:137360  TotalLoss:0.00196  PostnetLoss:0.00096  DecoderLoss:0.00100  StopLoss:0.02983  GradNorm:0.00384  GradNormST:0.00960  AvgTextLen:38.2  AvgSpecLen:127.7  StepTime:0.72  LR:0.000100\n",
      "   | > Step:59/68  GlobalStep:137370  TotalLoss:0.00209  PostnetLoss:0.00102  DecoderLoss:0.00107  StopLoss:0.02258  GradNorm:0.00353  GradNormST:0.00748  AvgTextLen:50.5  AvgSpecLen:155.9  StepTime:0.86  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:137379  AvgTotalLoss:0.03719  AvgPostnetLoss:0.00085  AvgDecoderLoss:0.00089  AvgStopLoss:0.03545  EpochTime:42.94  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.06704   PostnetLoss: 0.00587   DecoderLoss:0.00611  StopLoss: 0.05506  \n",
      "   | > TotalLoss: 0.08194   PostnetLoss: 0.00980   DecoderLoss:0.01010  StopLoss: 0.06204  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00085   Validation Loss: 0.00885\n",
      "\n",
      " > Epoch 991/1000\n",
      "   | > Step:0/68  GlobalStep:137380  TotalLoss:0.00132  PostnetLoss:0.00064  DecoderLoss:0.00069  StopLoss:0.06096  GradNorm:0.00567  GradNormST:0.02044  AvgTextLen:3.0  AvgSpecLen:25.3  StepTime:0.38  LR:0.000100\n",
      "   | > Step:10/68  GlobalStep:137390  TotalLoss:0.00134  PostnetLoss:0.00065  DecoderLoss:0.00069  StopLoss:0.06065  GradNorm:0.00386  GradNormST:0.04352  AvgTextLen:9.8  AvgSpecLen:48.6  StepTime:0.61  LR:0.000100\n",
      "   | > Step:20/68  GlobalStep:137400  TotalLoss:0.00154  PostnetLoss:0.00076  DecoderLoss:0.00079  StopLoss:0.02510  GradNorm:0.00322  GradNormST:0.00860  AvgTextLen:16.4  AvgSpecLen:64.8  StepTime:0.51  LR:0.000100\n",
      "   | > Step:30/68  GlobalStep:137410  TotalLoss:0.00167  PostnetLoss:0.00082  DecoderLoss:0.00086  StopLoss:0.03878  GradNorm:0.00314  GradNormST:0.00957  AvgTextLen:22.9  AvgSpecLen:85.8  StepTime:0.60  LR:0.000100\n",
      "   | > Step:40/68  GlobalStep:137420  TotalLoss:0.00183  PostnetLoss:0.00090  DecoderLoss:0.00094  StopLoss:0.03661  GradNorm:0.00378  GradNormST:0.01168  AvgTextLen:30.9  AvgSpecLen:105.3  StepTime:0.64  LR:0.000100\n",
      "   | > Step:50/68  GlobalStep:137430  TotalLoss:0.00198  PostnetLoss:0.00097  DecoderLoss:0.00101  StopLoss:0.02805  GradNorm:0.00360  GradNormST:0.00832  AvgTextLen:38.8  AvgSpecLen:132.0  StepTime:0.79  LR:0.000100\n",
      "   | > Step:60/68  GlobalStep:137440  TotalLoss:0.00216  PostnetLoss:0.00106  DecoderLoss:0.00111  StopLoss:0.02025  GradNorm:0.00324  GradNormST:0.00483  AvgTextLen:52.0  AvgSpecLen:172.7  StepTime:0.92  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:137448  AvgTotalLoss:0.03624  AvgPostnetLoss:0.00085  AvgDecoderLoss:0.00089  AvgStopLoss:0.03451  EpochTime:42.70  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.06886   PostnetLoss: 0.00618   DecoderLoss:0.00643  StopLoss: 0.05624  \n",
      "   | > TotalLoss: 0.07843   PostnetLoss: 0.00928   DecoderLoss:0.00960  StopLoss: 0.05956  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00085   Validation Loss: 0.00863\n",
      "\n",
      " > Epoch 992/1000\n",
      "   | > Step:1/68  GlobalStep:137450  TotalLoss:0.00126  PostnetLoss:0.00061  DecoderLoss:0.00064  StopLoss:0.03753  GradNorm:0.00609  GradNormST:0.01856  AvgTextLen:3.7  AvgSpecLen:28.6  StepTime:0.44  LR:0.000100\n",
      "   | > Step:11/68  GlobalStep:137460  TotalLoss:0.00136  PostnetLoss:0.00067  DecoderLoss:0.00069  StopLoss:0.04533  GradNorm:0.00468  GradNormST:0.03261  AvgTextLen:10.3  AvgSpecLen:49.4  StepTime:0.38  LR:0.000100\n",
      "   | > Step:21/68  GlobalStep:137470  TotalLoss:0.00153  PostnetLoss:0.00075  DecoderLoss:0.00078  StopLoss:0.03330  GradNorm:0.00316  GradNormST:0.00917  AvgTextLen:16.6  AvgSpecLen:66.3  StepTime:0.43  LR:0.000100\n",
      "   | > Step:31/68  GlobalStep:137480  TotalLoss:0.00166  PostnetLoss:0.00081  DecoderLoss:0.00085  StopLoss:0.02626  GradNorm:0.00281  GradNormST:0.00886  AvgTextLen:24.1  AvgSpecLen:82.7  StepTime:0.43  LR:0.000100\n",
      "   | > Step:41/68  GlobalStep:137490  TotalLoss:0.00187  PostnetLoss:0.00091  DecoderLoss:0.00096  StopLoss:0.02520  GradNorm:0.00367  GradNormST:0.00804  AvgTextLen:32.3  AvgSpecLen:109.4  StepTime:0.56  LR:0.000100\n",
      "   | > Step:51/68  GlobalStep:137500  TotalLoss:0.00197  PostnetLoss:0.00096  DecoderLoss:0.00101  StopLoss:0.03143  GradNorm:0.00416  GradNormST:0.01008  AvgTextLen:39.2  AvgSpecLen:126.9  StepTime:0.72  LR:0.000100\n",
      "   | > Step:61/68  GlobalStep:137510  TotalLoss:0.00215  PostnetLoss:0.00105  DecoderLoss:0.00110  StopLoss:0.02589  GradNorm:0.00414  GradNormST:0.00789  AvgTextLen:54.6  AvgSpecLen:165.5  StepTime:0.89  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:137517  AvgTotalLoss:0.03534  AvgPostnetLoss:0.00085  AvgDecoderLoss:0.00089  AvgStopLoss:0.03360  EpochTime:42.33  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.07032   PostnetLoss: 0.00605   DecoderLoss:0.00630  StopLoss: 0.05797  \n",
      "   | > TotalLoss: 0.09118   PostnetLoss: 0.00960   DecoderLoss:0.00992  StopLoss: 0.07166  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00085   Validation Loss: 0.00894\n",
      "\n",
      " > Epoch 993/1000\n",
      "   | > Step:2/68  GlobalStep:137520  TotalLoss:0.00116  PostnetLoss:0.00057  DecoderLoss:0.00059  StopLoss:0.02124  GradNorm:0.00451  GradNormST:0.00887  AvgTextLen:4.0  AvgSpecLen:31.3  StepTime:0.50  LR:0.000100\n",
      "   | > Step:12/68  GlobalStep:137530  TotalLoss:0.00136  PostnetLoss:0.00066  DecoderLoss:0.00069  StopLoss:0.04846  GradNorm:0.00316  GradNormST:0.02444  AvgTextLen:11.0  AvgSpecLen:49.3  StepTime:0.44  LR:0.000100\n",
      "   | > Step:22/68  GlobalStep:137540  TotalLoss:0.00153  PostnetLoss:0.00075  DecoderLoss:0.00078  StopLoss:0.03401  GradNorm:0.00304  GradNormST:0.00973  AvgTextLen:18.0  AvgSpecLen:72.0  StepTime:0.48  LR:0.000100\n",
      "   | > Step:32/68  GlobalStep:137550  TotalLoss:0.00170  PostnetLoss:0.00083  DecoderLoss:0.00087  StopLoss:0.04145  GradNorm:0.00283  GradNormST:0.01652  AvgTextLen:24.4  AvgSpecLen:91.5  StepTime:0.55  LR:0.000100\n",
      "   | > Step:42/68  GlobalStep:137560  TotalLoss:0.00183  PostnetLoss:0.00089  DecoderLoss:0.00094  StopLoss:0.02888  GradNorm:0.00288  GradNormST:0.01195  AvgTextLen:33.1  AvgSpecLen:109.7  StepTime:0.60  LR:0.000100\n",
      "   | > Step:52/68  GlobalStep:137570  TotalLoss:0.00202  PostnetLoss:0.00099  DecoderLoss:0.00103  StopLoss:0.03901  GradNorm:0.00346  GradNormST:0.01096  AvgTextLen:42.1  AvgSpecLen:137.5  StepTime:0.79  LR:0.000100\n",
      "   | > Step:62/68  GlobalStep:137580  TotalLoss:0.00219  PostnetLoss:0.00107  DecoderLoss:0.00112  StopLoss:0.02222  GradNorm:0.00308  GradNormST:0.00795  AvgTextLen:54.0  AvgSpecLen:174.7  StepTime:0.75  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:137586  AvgTotalLoss:0.03476  AvgPostnetLoss:0.00085  AvgDecoderLoss:0.00089  AvgStopLoss:0.03302  EpochTime:42.19  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.06372   PostnetLoss: 0.00612   DecoderLoss:0.00638  StopLoss: 0.05122  \n",
      "   | > TotalLoss: 0.07757   PostnetLoss: 0.00915   DecoderLoss:0.00948  StopLoss: 0.05895  \n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00085   Validation Loss: 0.00867\n",
      "\n",
      " > Epoch 994/1000\n",
      "   | > Step:3/68  GlobalStep:137590  TotalLoss:0.00119  PostnetLoss:0.00058  DecoderLoss:0.00061  StopLoss:0.05406  GradNorm:0.00395  GradNormST:0.01636  AvgTextLen:4.7  AvgSpecLen:29.7  StepTime:0.32  LR:0.000100\n",
      "   | > Step:13/68  GlobalStep:137600  TotalLoss:0.00136  PostnetLoss:0.00066  DecoderLoss:0.00069  StopLoss:0.06735  GradNorm:0.00334  GradNormST:0.02978  AvgTextLen:11.5  AvgSpecLen:55.2  StepTime:0.41  LR:0.000100\n",
      "   | > Step:23/68  GlobalStep:137610  TotalLoss:0.00160  PostnetLoss:0.00078  DecoderLoss:0.00081  StopLoss:0.03765  GradNorm:0.00320  GradNormST:0.01246  AvgTextLen:18.3  AvgSpecLen:72.2  StepTime:0.45  LR:0.000100\n",
      "   | > Step:33/68  GlobalStep:137620  TotalLoss:0.00173  PostnetLoss:0.00085  DecoderLoss:0.00088  StopLoss:0.03719  GradNorm:0.00305  GradNormST:0.00984  AvgTextLen:25.9  AvgSpecLen:88.9  StepTime:0.44  LR:0.000100\n",
      "   | > Step:43/68  GlobalStep:137630  TotalLoss:0.00184  PostnetLoss:0.00090  DecoderLoss:0.00094  StopLoss:0.03234  GradNorm:0.00287  GradNormST:0.00870  AvgTextLen:33.4  AvgSpecLen:115.7  StepTime:0.65  LR:0.000100\n",
      "   | > Step:53/68  GlobalStep:137640  TotalLoss:0.00198  PostnetLoss:0.00097  DecoderLoss:0.00101  StopLoss:0.02387  GradNorm:0.00337  GradNormST:0.00786  AvgTextLen:42.4  AvgSpecLen:137.6  StepTime:0.74  LR:0.000100\n",
      "   | > Step:63/68  GlobalStep:137650  TotalLoss:0.00224  PostnetLoss:0.00109  DecoderLoss:0.00115  StopLoss:0.01845  GradNorm:0.00316  GradNormST:0.00543  AvgTextLen:58.6  AvgSpecLen:185.1  StepTime:0.91  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:137655  AvgTotalLoss:0.03558  AvgPostnetLoss:0.00085  AvgDecoderLoss:0.00089  AvgStopLoss:0.03384  EpochTime:41.86  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.06460   PostnetLoss: 0.00608   DecoderLoss:0.00632  StopLoss: 0.05220  \n",
      "   | > TotalLoss: 0.07510   PostnetLoss: 0.00912   DecoderLoss:0.00944  StopLoss: 0.05655  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00085   Validation Loss: 0.00856\n",
      "\n",
      " > Epoch 995/1000\n",
      "   | > Step:4/68  GlobalStep:137660  TotalLoss:0.00121  PostnetLoss:0.00059  DecoderLoss:0.00062  StopLoss:0.05366  GradNorm:0.00339  GradNormST:0.02220  AvgTextLen:4.8  AvgSpecLen:31.1  StepTime:0.27  LR:0.000100\n",
      "   | > Step:14/68  GlobalStep:137670  TotalLoss:0.00141  PostnetLoss:0.00069  DecoderLoss:0.00072  StopLoss:0.03064  GradNorm:0.00340  GradNormST:0.01192  AvgTextLen:12.2  AvgSpecLen:52.0  StepTime:0.35  LR:0.000100\n",
      "   | > Step:24/68  GlobalStep:137680  TotalLoss:0.00164  PostnetLoss:0.00080  DecoderLoss:0.00084  StopLoss:0.03538  GradNorm:0.00333  GradNormST:0.01162  AvgTextLen:19.2  AvgSpecLen:79.0  StepTime:0.49  LR:0.000100\n",
      "   | > Step:34/68  GlobalStep:137690  TotalLoss:0.00174  PostnetLoss:0.00085  DecoderLoss:0.00089  StopLoss:0.02017  GradNorm:0.00270  GradNormST:0.00780  AvgTextLen:26.2  AvgSpecLen:95.9  StepTime:0.57  LR:0.000100\n",
      "   | > Step:44/68  GlobalStep:137700  TotalLoss:0.00191  PostnetLoss:0.00093  DecoderLoss:0.00098  StopLoss:0.01970  GradNorm:0.00288  GradNormST:0.00940  AvgTextLen:33.8  AvgSpecLen:116.3  StepTime:0.69  LR:0.000100\n",
      "   | > Step:54/68  GlobalStep:137710  TotalLoss:0.00203  PostnetLoss:0.00099  DecoderLoss:0.00104  StopLoss:0.03187  GradNorm:0.00393  GradNormST:0.00872  AvgTextLen:42.2  AvgSpecLen:144.9  StepTime:0.87  LR:0.000100\n",
      "   | > Step:64/68  GlobalStep:137720  TotalLoss:0.00211  PostnetLoss:0.00103  DecoderLoss:0.00108  StopLoss:0.02252  GradNorm:0.00268  GradNormST:0.00550  AvgTextLen:60.6  AvgSpecLen:177.6  StepTime:0.80  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:137724  AvgTotalLoss:0.03592  AvgPostnetLoss:0.00085  AvgDecoderLoss:0.00089  AvgStopLoss:0.03419  EpochTime:42.48  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.06382   PostnetLoss: 0.00598   DecoderLoss:0.00623  StopLoss: 0.05161  \n",
      "   | > TotalLoss: 0.07334   PostnetLoss: 0.00910   DecoderLoss:0.00943  StopLoss: 0.05481  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00085   Validation Loss: 0.00855\n",
      "\n",
      " > Epoch 996/1000\n",
      "   | > Step:5/68  GlobalStep:137730  TotalLoss:0.00126  PostnetLoss:0.00061  DecoderLoss:0.00065  StopLoss:0.03832  GradNorm:0.00418  GradNormST:0.01502  AvgTextLen:5.9  AvgSpecLen:33.2  StepTime:0.34  LR:0.000100\n",
      "   | > Step:15/68  GlobalStep:137740  TotalLoss:0.00149  PostnetLoss:0.00073  DecoderLoss:0.00076  StopLoss:0.04773  GradNorm:0.00370  GradNormST:0.03179  AvgTextLen:12.8  AvgSpecLen:59.1  StepTime:0.52  LR:0.000100\n",
      "   | > Step:25/68  GlobalStep:137750  TotalLoss:0.00159  PostnetLoss:0.00078  DecoderLoss:0.00081  StopLoss:0.03377  GradNorm:0.00325  GradNormST:0.01504  AvgTextLen:19.8  AvgSpecLen:73.3  StepTime:0.45  LR:0.000100\n",
      "   | > Step:35/68  GlobalStep:137760  TotalLoss:0.00178  PostnetLoss:0.00087  DecoderLoss:0.00091  StopLoss:0.04563  GradNorm:0.00290  GradNormST:0.02346  AvgTextLen:27.1  AvgSpecLen:104.7  StepTime:0.74  LR:0.000100\n",
      "   | > Step:45/68  GlobalStep:137770  TotalLoss:0.00183  PostnetLoss:0.00089  DecoderLoss:0.00094  StopLoss:0.03687  GradNorm:0.00264  GradNormST:0.01266  AvgTextLen:34.4  AvgSpecLen:118.6  StepTime:0.62  LR:0.000100\n",
      "   | > Step:55/68  GlobalStep:137780  TotalLoss:0.00208  PostnetLoss:0.00102  DecoderLoss:0.00106  StopLoss:0.02575  GradNorm:0.00340  GradNormST:0.00858  AvgTextLen:43.9  AvgSpecLen:145.2  StepTime:0.78  LR:0.000100\n",
      "   | > Step:65/68  GlobalStep:137790  TotalLoss:0.00229  PostnetLoss:0.00112  DecoderLoss:0.00117  StopLoss:0.02342  GradNorm:0.00335  GradNormST:0.00827  AvgTextLen:64.2  AvgSpecLen:207.4  StepTime:1.18  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:137793  AvgTotalLoss:0.03607  AvgPostnetLoss:0.00084  AvgDecoderLoss:0.00088  AvgStopLoss:0.03434  EpochTime:42.51  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.06595   PostnetLoss: 0.00606   DecoderLoss:0.00630  StopLoss: 0.05359  \n",
      "   | > TotalLoss: 0.07728   PostnetLoss: 0.00922   DecoderLoss:0.00954  StopLoss: 0.05852  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00084   Validation Loss: 0.00861\n",
      "\n",
      " > Epoch 997/1000\n",
      "   | > Step:6/68  GlobalStep:137800  TotalLoss:0.00124  PostnetLoss:0.00061  DecoderLoss:0.00063  StopLoss:0.02549  GradNorm:0.00382  GradNormST:0.00959  AvgTextLen:6.0  AvgSpecLen:34.8  StepTime:0.33  LR:0.000100\n",
      "   | > Step:16/68  GlobalStep:137810  TotalLoss:0.00149  PostnetLoss:0.00073  DecoderLoss:0.00076  StopLoss:0.03461  GradNorm:0.00317  GradNormST:0.01305  AvgTextLen:13.8  AvgSpecLen:60.6  StepTime:0.29  LR:0.000100\n",
      "   | > Step:26/68  GlobalStep:137820  TotalLoss:0.00163  PostnetLoss:0.00080  DecoderLoss:0.00083  StopLoss:0.03468  GradNorm:0.00309  GradNormST:0.01081  AvgTextLen:20.3  AvgSpecLen:77.5  StepTime:0.47  LR:0.000100\n",
      "   | > Step:36/68  GlobalStep:137830  TotalLoss:0.00173  PostnetLoss:0.00085  DecoderLoss:0.00089  StopLoss:0.03520  GradNorm:0.00266  GradNormST:0.00882  AvgTextLen:28.5  AvgSpecLen:100.2  StepTime:0.68  LR:0.000100\n",
      "   | > Step:46/68  GlobalStep:137840  TotalLoss:0.00186  PostnetLoss:0.00091  DecoderLoss:0.00095  StopLoss:0.02811  GradNorm:0.00270  GradNormST:0.00771  AvgTextLen:35.4  AvgSpecLen:122.6  StepTime:0.79  LR:0.000100\n",
      "   | > Step:56/68  GlobalStep:137850  TotalLoss:0.00205  PostnetLoss:0.00100  DecoderLoss:0.00105  StopLoss:0.01829  GradNorm:0.00287  GradNormST:0.00720  AvgTextLen:45.8  AvgSpecLen:145.4  StepTime:0.96  LR:0.000100\n",
      "   | > Step:66/68  GlobalStep:137860  TotalLoss:0.00240  PostnetLoss:0.00117  DecoderLoss:0.00124  StopLoss:0.02765  GradNorm:0.00266  GradNormST:0.01331  AvgTextLen:71.2  AvgSpecLen:220.8  StepTime:1.03  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:137862  AvgTotalLoss:0.03499  AvgPostnetLoss:0.00085  AvgDecoderLoss:0.00089  AvgStopLoss:0.03326  EpochTime:42.19  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.06508   PostnetLoss: 0.00602   DecoderLoss:0.00626  StopLoss: 0.05281  \n",
      "   | > TotalLoss: 0.07389   PostnetLoss: 0.00921   DecoderLoss:0.00953  StopLoss: 0.05515  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00085   Validation Loss: 0.00864\n",
      "\n",
      " > Epoch 998/1000\n",
      "   | > Step:7/68  GlobalStep:137870  TotalLoss:0.00129  PostnetLoss:0.00063  DecoderLoss:0.00066  StopLoss:0.03615  GradNorm:0.00341  GradNormST:0.02168  AvgTextLen:7.4  AvgSpecLen:44.6  StepTime:0.38  LR:0.000100\n",
      "   | > Step:17/68  GlobalStep:137880  TotalLoss:0.00145  PostnetLoss:0.00071  DecoderLoss:0.00074  StopLoss:0.06229  GradNorm:0.00308  GradNormST:0.02188  AvgTextLen:14.7  AvgSpecLen:66.0  StepTime:0.41  LR:0.000100\n",
      "   | > Step:27/68  GlobalStep:137890  TotalLoss:0.00165  PostnetLoss:0.00081  DecoderLoss:0.00084  StopLoss:0.04962  GradNorm:0.00303  GradNormST:0.01809  AvgTextLen:21.0  AvgSpecLen:80.2  StepTime:0.63  LR:0.000100\n",
      "   | > Step:37/68  GlobalStep:137900  TotalLoss:0.00180  PostnetLoss:0.00088  DecoderLoss:0.00092  StopLoss:0.03479  GradNorm:0.00272  GradNormST:0.00940  AvgTextLen:29.3  AvgSpecLen:102.4  StepTime:0.59  LR:0.000100\n",
      "   | > Step:47/68  GlobalStep:137910  TotalLoss:0.00193  PostnetLoss:0.00094  DecoderLoss:0.00099  StopLoss:0.02769  GradNorm:0.00270  GradNormST:0.00981  AvgTextLen:36.7  AvgSpecLen:126.9  StepTime:0.58  LR:0.000100\n",
      "   | > Step:57/68  GlobalStep:137920  TotalLoss:0.00213  PostnetLoss:0.00104  DecoderLoss:0.00109  StopLoss:0.02944  GradNorm:0.00341  GradNormST:0.00897  AvgTextLen:47.1  AvgSpecLen:161.0  StepTime:0.84  LR:0.000100\n",
      "   | > Step:67/68  GlobalStep:137930  TotalLoss:0.00241  PostnetLoss:0.00117  DecoderLoss:0.00124  StopLoss:0.01940  GradNorm:0.00253  GradNormST:0.00956  AvgTextLen:80.6  AvgSpecLen:246.5  StepTime:1.22  LR:0.000100\n",
      "   | > EPOCH END -- GlobalStep:137931  AvgTotalLoss:0.03606  AvgPostnetLoss:0.00084  AvgDecoderLoss:0.00088  AvgStopLoss:0.03433  EpochTime:42.41  AvgStepTime:0.61\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.06192   PostnetLoss: 0.00596   DecoderLoss:0.00620  StopLoss: 0.04977  \n",
      "   | > TotalLoss: 0.07565   PostnetLoss: 0.00908   DecoderLoss:0.00941  StopLoss: 0.05717  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00084   Validation Loss: 0.00857\n",
      "\n",
      " > Epoch 999/1000\n",
      "   | > Step:8/68  GlobalStep:137940  TotalLoss:0.00132  PostnetLoss:0.00065  DecoderLoss:0.00067  StopLoss:0.04375  GradNorm:0.00362  GradNormST:0.01444  AvgTextLen:8.3  AvgSpecLen:44.0  StepTime:0.29  LR:0.000100\n",
      "   | > Step:18/68  GlobalStep:137950  TotalLoss:0.00146  PostnetLoss:0.00071  DecoderLoss:0.00075  StopLoss:0.04133  GradNorm:0.00322  GradNormST:0.01504  AvgTextLen:15.2  AvgSpecLen:64.5  StepTime:0.32  LR:0.000100\n",
      "   | > Step:28/68  GlobalStep:137960  TotalLoss:0.00168  PostnetLoss:0.00082  DecoderLoss:0.00086  StopLoss:0.02832  GradNorm:0.00296  GradNormST:0.00923  AvgTextLen:22.1  AvgSpecLen:85.4  StepTime:0.54  LR:0.000100\n",
      "   | > Step:38/68  GlobalStep:137970  TotalLoss:0.00174  PostnetLoss:0.00085  DecoderLoss:0.00089  StopLoss:0.02683  GradNorm:0.00289  GradNormST:0.00770  AvgTextLen:29.4  AvgSpecLen:104.1  StepTime:0.60  LR:0.000100\n",
      "   | > Step:48/68  GlobalStep:137980  TotalLoss:0.00192  PostnetLoss:0.00094  DecoderLoss:0.00098  StopLoss:0.03294  GradNorm:0.00278  GradNormST:0.00851  AvgTextLen:37.5  AvgSpecLen:123.8  StepTime:0.55  LR:0.000100\n",
      "   | > Step:58/68  GlobalStep:137990  TotalLoss:0.00210  PostnetLoss:0.00102  DecoderLoss:0.00107  StopLoss:0.02907  GradNorm:0.00274  GradNormST:0.00951  AvgTextLen:47.8  AvgSpecLen:158.0  StepTime:0.91  LR:0.000100\n",
      "   | > Step:68/68  GlobalStep:138000  TotalLoss:0.00239  PostnetLoss:0.00116  DecoderLoss:0.00123  StopLoss:0.01335  GradNorm:0.00299  GradNormST:0.00642  AvgTextLen:103.0  AvgSpecLen:308.9  StepTime:1.86  LR:0.000100\n",
      " | | > Checkpoint saving : /vm/TTS/models/cartman_models/mozilla-no-loc-fattn-stopnet-sigmoid-loss_masking-August-15-2019_11+26AM-de6aeb9/checkpoint_138000.pth.tar\n",
      "   | > EPOCH END -- GlobalStep:138000  AvgTotalLoss:0.03585  AvgPostnetLoss:0.00084  AvgDecoderLoss:0.00088  AvgStopLoss:0.03412  EpochTime:42.89  AvgStepTime:0.62\n",
      "\n",
      " > Validation\n",
      "   | > TotalLoss: 0.06561   PostnetLoss: 0.00600   DecoderLoss:0.00624  StopLoss: 0.05338  \n",
      "   | > TotalLoss: 0.07653   PostnetLoss: 0.00915   DecoderLoss:0.00947  StopLoss: 0.05792  \n",
      "warning: audio amplitude out of range, auto clipped.\n",
      " | > Synthesizing test sentences\n",
      " | > Training Loss: 0.00084   Validation Loss: 0.00857\n"
     ]
    }
   ],
   "source": [
    "#/path/to/your/model.pth.tar\n",
    "cd /vm/TTS\n",
    "#remove phonemes (useful if we have changed the dataset)\n",
    "rm -rf cartman_phonemes\n",
    "python train.py --config_path config.json --restore_path /vm/TTS/models/cartman_models/mozilla-no-loc-fattn-stopnet-sigmoid-loss_masking-August-14-2019_09+40AM-de6aeb9/checkpoint_69000.pth.tar 2> error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SoS",
   "language": "sos",
   "name": "sos"
  },
  "language_info": {
   "codemirror_mode": "sos",
   "file_extension": ".sos",
   "mimetype": "text/x-sos",
   "name": "sos",
   "nbconvert_exporter": "sos_notebook.converter.SoS_Exporter",
   "pygments_lexer": "sos"
  },
  "sos": {
   "kernels": [
    [
     "Bash",
     "bash",
     "Bash",
     "#E6EEFF"
    ],
    [
     "SoS",
     "sos",
     "",
     ""
    ]
   ],
   "version": "0.9.15.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

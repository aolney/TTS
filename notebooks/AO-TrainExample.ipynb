{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "# Mozilla TTS Training Example\n",
    "\n",
    "Adapted [this gist](https://gist.github.com/erogol/97516ad65b44dbddb8cd694953187c5b) for [SoS notebook](https://vatlab.github.io/sos-docs/).\n",
    "\n",
    "Assumes you have already [cloned TTS](https://github.com/mozilla/TTS) and done `apt install espeak`\n",
    "\n",
    "## Prepare Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "Bash"
   },
   "outputs": [],
   "source": [
    "exit #comment to allow execution\n",
    "wget http://data.keithito.com/data/speech/LJSpeech-1.1.tar.bz2\n",
    "tar -xjf LJSpeech-1.1.tar.bz2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "Bash"
   },
   "outputs": [],
   "source": [
    "exit #comment to allow execution\n",
    "shuf LJSpeech-1.1/metadata.csv > LJSpeech-1.1/metadata_shuf.csv\n",
    "head -n 12000 LJSpeech-1.1/metadata_shuf.csv > LJSpeech-1.1/metadata_train.csv\n",
    "tail -n 1100 LJSpeech-1.1/metadata_shuf.csv > LJSpeech-1.1/metadata_val.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "Bash"
   },
   "source": [
    "## Create config file for this training data\n",
    "\n",
    "Some of my local paths are hardcoded below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "Bash"
   },
   "outputs": [],
   "source": [
    "#exit #comment to allow execution\n",
    "cd /vm/TTS\n",
    "cp config.json \"config.json.$(date +%Y%m%d)\"\n",
    "cat > config.json << EOM\n",
    "{\n",
    "    \"model_name\": \"queue\",\n",
    "    \"model_description\": \"Queue memory and change lower r incrementatlly\",\n",
    "\n",
    "    \"audio\":{\n",
    "        // Audio processing parameters\n",
    "        \"num_mels\": 80,         // size of the mel spec frame. \n",
    "        \"num_freq\": 1025,       // number of stft frequency levels. Size of the linear spectogram frame.\n",
    "        \"sample_rate\": 22050,   // wav sample-rate. If different than the original data, it is resampled.\n",
    "        \"frame_length_ms\": 50,  // stft window length in ms.\n",
    "        \"frame_shift_ms\": 12.5, // stft window hop-lengh in ms.\n",
    "        \"preemphasis\": 0.98,    // pre-emphasis to reduce spec noise and make it more structured. If 0.0, no -pre-emphasis.\n",
    "        \"min_level_db\": -100,   // normalization range\n",
    "        \"ref_level_db\": 20,     // reference level db, theoretically 20db is the sound of air.\n",
    "        \"power\": 1.5,           // value to sharpen wav signals after GL algorithm.\n",
    "        \"griffin_lim_iters\": 60,// #griffin-lim iterations. 30-60 is a good range. Larger the value, slower the generation.\n",
    "        // Normalization parameters\n",
    "        \"signal_norm\": true,    // normalize the spec values in range [0, 1]\n",
    "        \"symmetric_norm\": false, // move normalization to range [-1, 1]\n",
    "        \"max_norm\": 1,          // scale normalization to range [-max_norm, max_norm] or [0, max_norm]\n",
    "        \"clip_norm\": true,      // clip normalized values into the range.\n",
    "        \"mel_fmin\": null,         // minimum freq level for mel-spec. ~50 for male and ~95 for female voices. Tune for dataset!!\n",
    "        \"mel_fmax\": null,        // maximum freq level for mel-spec. Tune for dataset!!\n",
    "        \"do_trim_silence\": true  // enable trimming of slience of audio as you load it. LJspeech (false), TWEB (false), Nancy (true)\n",
    "    },\n",
    "\n",
    "    \"distributed\":{\n",
    "        \"backend\": \"nccl\",\n",
    "        \"url\": \"tcp:\\/\\/localhost:54321\"\n",
    "    },\n",
    "\n",
    "    \"embedding_size\": 256,  // Character embedding vector length. You don't need to change it in general.\n",
    "    \"text_cleaner\": \"phoneme_cleaners\",\n",
    "    \"epochs\": 1000,         // total number of epochs to train (originally 1000)\n",
    "    \"lr\": 0.0001,            // Initial learning rate. If Noam decay is active, maximum learning rate.\n",
    "    \"lr_decay\": false,      // if true, Noam learning rate decaying is applied through training.\n",
    "    \"loss_weight\": 0.0,     // loss weight to emphasize lower frequencies. Lower frequencies are in general more important for speech signals.\n",
    "    \"warmup_steps\": 4000,   // Noam decay steps to increase the learning rate from 0 to \"lr\"\n",
    "    \"windowing\": false,      // Enables attention windowing. Used only in eval mode.\n",
    "    \"memory_size\": 5,       // memory queue size used to queue network predictions to feed autoregressive connection. Useful if r < 5. \n",
    "\n",
    "    \"batch_size\": 32,       // Batch size for training. Lower values than 32 might cause hard to learn attention.\n",
    "    \"eval_batch_size\":32,   \n",
    "    \"r\": 5,                 // Number of frames to predict for step.\n",
    "    \"wd\": 0.00001,          // Weight decay weight.\n",
    "    \"checkpoint\": true,     // If true, it saves checkpoints per \"save_step\"\n",
    "    \"save_step\": 5000,      // Number of training steps expected to save traning stats and checkpoints.\n",
    "    \"print_step\": 50,       // Number of steps to log traning on console.\n",
    "    \"tb_model_param_stats\": false,     // true, plots param stats per layer on tensorboard. Might be memory consuming, but good for debugging. \n",
    "    \"batch_group_size\": 8,  //Number of batches to shuffle after bucketing.\n",
    "\n",
    "    \"test_delay_epochs\":1,\n",
    "    \"run_eval\": true,\n",
    "    \"data_path\": \"/vm/LJSpeech-1.1/\",  // can overwritten from command argument\n",
    "    \"meta_file_train\": \"metadata_train.csv\",      // metafile for training dataloader\n",
    "    \"meta_file_val\": \"metadata_val.csv\",    // metafile for validation dataloader\n",
    "    \"data_loader\": \"TTSDataset\",      // dataloader, [\"TTSDataset\", \"TTSDatasetCached\", \"TTSDatasetMemory\"]\n",
    "    \"dataset\": \"ljspeech\",     // one of TTS.dataset.preprocessors, only valid id dataloader == \"TTSDataset\", rest uses \"tts_cache\" by default.\n",
    "    \"min_seq_len\": 0,       // DATASET-RELATED: minimum text length to use in training\n",
    "    \"max_seq_len\": 300,     // DATASET-RELATED: maximum text length\n",
    "    \"output_path\": \"/vm/TTS/models/ljspeech_models/\",\n",
    "    \"num_loader_workers\": 2,\n",
    "    \"num_val_loader_workers\": 2,\n",
    "     \"phoneme_cache_path\": \"ljspeech_phonemes\",  // phoneme computation is slow, therefore, it caches results in the given folder.\n",
    "    \"use_phonemes\": true,           // use phonemes instead of raw characters. It is suggested for better pronounciation.\n",
    "    \"phoneme_language\": \"en-us\",     // depending on your target language, pick one from  https://github.com/bootphon/phonemizer#languages\n",
    "    \"text_cleaner\": \"phoneme_cleaners\"\n",
    "}\n",
    "EOM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "Bash"
   },
   "source": [
    "## Start Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "Bash"
   },
   "outputs": [],
   "source": [
    "cd /vm/TTS\n",
    "python train.py --config_path config.json\n",
    "#start tensorboard in another terminal\n",
    "#tensorboard --logdir=my_run:<output_path in config.json>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "Bash"
   },
   "source": [
    "## Continue Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "Bash"
   },
   "outputs": [],
   "source": [
    "python train.py --config_path config.json --restore_path /vm/TTS/models/ljspeech_models/queue-April-26-2019_03+46PM-59bbdb3\n",
    "#/path/to/your/model.pth.tar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "hello = tf.constant(\"hello TensorFlow!\")\n",
    "sess=tf.Session() \n",
    "print(sess.run(hello))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SoS",
   "language": "sos",
   "name": "sos"
  },
  "language_info": {
   "codemirror_mode": "sos",
   "file_extension": ".sos",
   "mimetype": "text/x-sos",
   "name": "sos",
   "nbconvert_exporter": "sos_notebook.converter.SoS_Exporter",
   "pygments_lexer": "sos"
  },
  "sos": {
   "kernels": [
    [
     "Bash",
     "bash",
     "Bash",
     "#E6EEFF"
    ],
    [
     "Python3",
     "python3",
     "Python3",
     "#FFD91A"
    ]
   ],
   "version": "0.9.15.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
